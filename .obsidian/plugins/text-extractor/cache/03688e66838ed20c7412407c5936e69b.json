{"path":"_assets/shapley-value-knn.pdf","text":"Efﬁcient Task-Speciﬁc Data Valuation for Nearest Neighbor Algorithms RUOXI JIA, DAVID DAO, BOXIN WANG, FRANCES ANN HUBIS, NEZIHE MERVE GUREL, BO LI, CE ZHANG, COSTAS J. SPANOS, and DAWN SONG Abstract Given a data set D containing millions of data points and a data consumer who is willing to pay for $X to train a machine learning (ML) model over D, how should we distribute this $X to each data point to reﬂect its “value”? In this paper, we deﬁne the “relative value of data” via the Shapley value, as it uniquely possesses properties with appealing real-world interpretations, such as fairness, rationality and decentralizability. For general, bounded utility functions, the Shapley value is known to be challenging to compute: to get Shapley values for all N data points, it requires O(2N) model evaluations for exact computation and O(N log N) for (ϵ, δ)-approximation. In this paper, we focus on one popular family of ML models relying on K-nearest neighbors (KNN). The most surprising result is that for unweighted KNN classiﬁers and regressors, the Shapley value of all N data points can be computed, exactly, in O(N log N) time – an exponential improvement on computational complexity! Moreover, for (ϵ, δ)-approximation, we are able to develop an algorithm based on Locality Sensitive Hashing (LSH) with only sublinear complexity O(Nh(ϵ,K) log N) when ϵ is not too small and K is not too large. We empirically evaluate our algorithms on up to 10 million data points and even our exact algorithm is up to three orders of magnitude faster than the baseline approximation algorithm. The LSH-based approximation algorithm can accelerate the value calculation process even further. We then extend our algorithms to other scenarios such as (1) weighed KNN classiﬁers, (2) different data points are clustered by different data curators, and (3) there are data analysts providing compu- tation who also requires proper valuation. Some of these extensions, although also being improved exponentially, are less practical for exact computation (e.g., O(NK) complexity for weighted KNN). We thus propose a Monte Carlo approximation algorithm, which is O(N(log N)2/(log K)2) times more efﬁcient than the baseline approximation algorithm. 1arXiv:1908.08619v4 [cs.LG] 29 Mar 2020 Contents 1. Introduction 3 2. Preliminaries 7 2.1. Data Valuation based on the SV 7 2.2. A Baseline Algorithm 8 3. Valuing Data for KNN Classiﬁers 9 3.1. Exact SV Calculation 9 3.2. LSH-based Approximation 12 4. Extensions 13 5. Improved MC Approximation 16 6. Experiments 17 6.1. Experimental Setup 17 6.2. Experimental Results 18 7. Discussion 26 8. Related Work 29 9. Conclusion 30 Acknowledgement 31 References 31 Appendix A. Additional Experiments 34 A.1. Runtime Comparision for Computing the Unweighted KNN SV 34 Appendix B. Proof of Lemma 1 34 Appendix C. Proof of Theorem 2 35 Appendix D. Proof of Theorem 3 36 Appendix E. Detailed Algorithms and Proofs for the Extensions 37 E.1. Unweighted KNN Regression 37 E.2. Weighted KNN 41 E.3. Multiple Data Per Contributor 43 E.4. Valuing Computation 43 Appendix F. Generalization to Piecewise Utility Difference 45 Appendix G. Proof of Theorem 5 46 Appendix H. Derivation of the Approximate Lower Bound on Sample Complexity for the Improved MC Approximation 49 2 1. Introduction “Data is the new oil” — large-scale, high-quality datasets are an enabler for business and scientiﬁc discovery and recent years have witnessed the commoditization of data. In fact, there are not only marketplaces providing access to data, e.g., IOTA [IOT], DAWEX [DAW], Xignite [xig], but also marketplaces charging for running (relational) queries over the data, e.g., Google BigQuery [BIG]. Many researchers start to envision marketplaces for ML models [CKK18]. Data commoditization is highly likely to continue and not surprisingly, it starts to attract interests from the database community. One series of seminal work is conducted by Koutris et al. [KUB+15, KUB+13] who systematically studied the theory and practice of “query pricing,” the problem of attaching value to running relational queries over data. Recently, Chen et al. [CKK18, CKK17] discussed “model pricing”, the problem of valuing ML models. This paper is inspired by the prior work on query and model pricing, but focuses on a different scenario. In many real-world applications, the datasets that support queries and ML are often contributed by multiple individuals. One example is that complex ML tasks such as chatbot training often relies on massive crowdsourcing efforts. A critical challenge for building a data marketplace is thus to allocate the revenue generated from queries and ML models fairly between different data contributors. In this paper, we ask: How can we attach value to every single data point in relative terms, with respect to a speciﬁc ML model trained over the whole dataset? Apart from being inspired by recent research, this paper is also motivated by our current effort in building a data market based on privacy-preserving machine learning [HDY+18, DAMZ18] and an ongoing clinical trial at the Stanford Hospital, as illustrated in Figure 1. In this clinical trial, each patient uploads their encrypted medical record (one “data point”) onto a blockchain-backed data store. A “data consumer”, or “buyer”, chooses a subset of patients (selected according to some non-sensitive information that is not encrypted) and trains a ML model. The buyer pays a certain amount of money that will be distributed back to each patient. In this paper, we focus on the data valuation problem that is abstracted from this real use case and propose novel, practical algorithms for this problem. Figure 1. Motivating Example of Data Valuation. 3 Figure 2. Time complexity for computing the SV for KNN models. N is the total number of training data points. M is the number of data contributors. h(ϵ, K) < 1 if K∗ = max{1/ϵ, K} < C for some dataset-dependent constant C. Exact Approximate Baseline 2NN log N N2 ϵ2 log N log N δ Unweighted KNN classiﬁer N log N Nh(ϵ,K) log N log K∗ δ Unweighted KNN regression N log N — Weighted KNN NK N ϵ2 log K log K δ Multiple-data-per-curator KNN MK N ϵ2 log K log K δ Speciﬁcally, we focus on the Shapley value (SV), arguably one of the most popular way of revenue sharing. It has been applied to various applications, such as power grids [BS13], supply chains [BKZ05], cloud computing [UBS12], among others. The reason for its wide adoption is that the SV deﬁnes a unique proﬁt allocation scheme that satisﬁes a set of appealing properties, such as fairness, rationality, and decentralizability. Speciﬁcally, let D = {z1, ..., zN} be N data points and ν(·) be the “utility” of the ML model trained over a subset of the data points; the SV of a given data point zi is (1) si = 1 N ∑ S⊆D\\zi 1 ( N−1 |S| ) [ν(S ∪ {zi}) − ν(S)] Intuitively, the SV measures the marginal improvement of utility attributed to the data point zi, averaged over all possible subsets of data points. Calculating exact SVs requires exponentially many utility evaluations. This poses a radical challenge to using the SV for data valuation–how can we compute the SV efﬁciently and scale to millions or even billions of data points? This scale is rare to the previous applications of the SV but is not uncommon for real-world data valuation tasks. To tackle this challenge, we focus on a speciﬁc family of ML models which restrict the class of utility functions ν(·) that we consider. Speciﬁcally, we study K-nearest neighbors (KNN) classi- ﬁers [Dud76], a simple yet popular supervised learning method used in image recognition [HE15], recommendation systems [AWY16], healthcare [LZZ+12], etc. Given a test set, we focus on a natural utility function, called the KNN utility, which, intuitively, measures the boost of the likelihood that KNN assigns the correct label to each test data point. When K = 1, this utility is the same as the test accuracy. Although some of our techniques also apply to a broader class of utility functions (See Section 4), the KNN utility is our main focus. The contribution of this work is a collection of novel algorithms for efﬁcient data valuation within the above scope. Figure 2 summarizes our technical results. Speciﬁcally, we made four technical contributions: Contribution 1: Data Valuation for KNN Classiﬁers. The main challenge of adopting the SV for data valuation is its computational complexity — for general, bounded utility functions, calculating the SV requires O(2N) utility evaluations for N data points. Even getting an (ϵ, δ)-approximation (error bounded by ϵ with probability at least 1 − δ) for all data points requires O(N log N) utility evaluations using state-of-the-art methods (See Section 2.2). For the KNN utility, each utility evaluation requires to sort the training data, which has asymptotic complexity O(N log N). 4 C1.1 Exact Computation We ﬁrst propose a novel algorithm speciﬁcally designed for KNN classiﬁers. We observe that the KNN utility satisﬁes what we call the piecewise utility difference property: the difference in the marginal contribution of two data points zi and zj over has a “piecewise form” (See Section 3.1): U(S ∪ {zi}) − U(S ∪ {zj}) = T∑ t=1 C(t) i,j 1 [S ∈ St], ∀S ∈ D\\{zi, zj} where St ⊆ 2D\\{zi,zj} and C(t) i,j ∈ R. This combinatorial structure allows us to design a very efﬁcient algorithm that only has O(N log N) complexity for exact computation of SVs on all N data points. This is an exponential improvement over the O(2NN log N) baseline! C1.2 Sublinear Approximation The exact computation requires to sort the entire training set for each test point, thus becoming time-consuming for large and high-dimensional datasets. Moreover, in some applications such as document retrieval, test points could arrive sequentially and the values of each training point needs to get updated and accumulated on the ﬂy, which makes it impossible to complete sorting ofﬂine. Thus, we investigate whether higher efﬁciency can be achieved by ﬁnding approximate SVs instead. We study the problem of getting (ϵ, δ)-approximation of the SVs for the KNN utility. This happens to be reducible to the problem of answering approximate max{K, 1/ϵ}- nearest neighbor queries with probability 1 − δ. We designed a novel algorithm by taking advantage of LSH, which only requires O(Nh(ϵ,K) log N) computation where h(ϵ, K) is dataset-dependent and typically less than 1 when ϵ is not too small and K is not too large. Limitation of LSH The h(ϵ, K) term monotonically increases with max{ 1 ϵ , K}. In experiments, we found that the LSH can handle mild error requirements (e.g., ϵ = 0.1) but appears to be less efﬁcient than the exact calculation algorithm for stringent error requirements. Moreover, we can extend the exact algorithm to cope with KNN regressors and other scenarios detailed in Contribution 2; however, the application of the LSH-based approximation is still conﬁned to the classiﬁcation case. To our best knowledge, the above results are one of the very ﬁrst studies of efﬁcient SV evaluation designed speciﬁcally for utilities arising from ML applications. Contribution 2: Extensions. Our second contribution is to extend our results to different settings beyond a standard KNN classiﬁer and the KNN utility (Section 4). Speciﬁcally, we studied: C2.1 Unweighted KNN regressors. C2.2 Weighted KNN classiﬁers and regressors. C2.3 One “data curator” contributes multiple data points and has the freedom to delete all data points at the same time. C2.4 One “data analyst” provides ML analytics and the system attaches value to both the analyst and data curators. The connection between different settings are illustrated in Figure 3, where each vertical layer represents a different slicing to the data valuation problem. In some of these scenarios, we successfully designed algorithms that are as efﬁcient as the one for KNN classiﬁers. In some other cases, including weigthed KNN and the multiple-data-per-curator setup, the exact computation algorithm is less practical although being improved exponentially. Contribution 3: Improved Monte Carlo Approximation for KNN. To further improve the efﬁ- ciency in the less efﬁcient cases, we strengthen the sample complexity bound of the state-of-the-art 5 Figure 3. Classiﬁcation of data valuation problems. approximation algorithm, achieving an O(N log 2 N/ log2 K) complexity improvement over the state- of-the-art. Our algorithm requires in total O(N/ϵ2 log2 K) computation and is often practical for reasonable ϵ. Contribution 4: Implementation and Evaluation. We implement our algorithms and evaluate them on datasets up to ten million data points. We observe that our exact SV calculation algorithm can provide up to three orders of magnitude speed-up over the state-of-the-art Monte Carlo approxi- mation approach. With the LSH-based approximation method, we can accelerate the SV calculation even further by allowing approximation errors. The actual performance improvement of the LSH- based method over the exact algorithm depends the dataset as well as the error requirements. For instance, on a 10M subset of the Yahoo Flickr Creative Commons 100M dataset, we observe that the LSH-based method can bring another 4.6× speed-up. Moreover, to our best knowledge, this work is also one of the ﬁrst papers to evaluate data valua- tion at scale. We make our datasets publicly available and document our evaluation methodology in details, with the hope to facilitate future research on data valuation. Relationship with Our Previous Work. Unlike this work which focuses on KNN, our previous work [JDW+19] considered some generic properties of ML models, such as boundedness of the utility functions, stability of the learning algorithms, etc, and studied their implications for computing the SV. Also, the algorithms presented in our previous work only produce approximation to the SV. When the desired approximation error is small, these algorithms may still incur considerable computational costs, thus not able to scale up to large datasets. In contrast, this paper presents a scalable algorithm that can calculate the exact SV for KNN. The rest of this paper is organized as follows. We provide background information in Section 2, and present our efﬁcient algorithms for KNN classiﬁers in Section 3. We discuss the extensions in Section 4 and propose a Monte Carlo approximation algorithm in Section 5, which signiﬁcantly boosts the efﬁciency for the extensions that have less practical exact algorithms. We evaluate our approach in Section 6. We discuss the integration with real-world applications in Section 7 and present a survey of related work in Section 8. 6 2. Preliminaries We present the setup of the data marketplace and introduce the framework for data valuation based on the SV. We then discuss a baseline algorithm to compute the SV. 2.1. Data Valuation based on the SV. We consider two types of agents that interact in a data marketplace: the sellers (or data curators) and the buyer. Sellers provide training data instances, each of which is a pair of a feature vector and the corresponding label. The buyer is interested in analyzing the training dataset aggregated from various sellers and producing an ML model, which can predict the labels for unseen features. The buyer pays a certain amount of money which depends on the utility of the ML model. Our goal is to distribute the payment fairly between the sellers. A natural way to tackle the question of revenue allocation is to view ML as a cooperative game and model each seller as a player. This game-theoretic viewpoint allows us to formally characterize the “power” of each seller and in turn determine their deserved share of the revenue. For ease of exposition, we assume that each seller contributes one data instance in the training set; later in Section 4, we will discuss the extension to the case where a seller contributes multiple data instances. Cooperative game theory studies the behaviors of coalitions formed by game players. Formally, a cooperative game is deﬁned by a pair (I, ν), where I = {1, . . . , N} denotes the set of all players and ν : 2N → R is the utility function, which maps each possible coalition to a real number that describes the utility of a coalition, i.e., how much collective payoff a set of players can gain by forming the coalition. One of the fundamental questions in cooperative game theory is to characterize how important each player is to the overall cooperation. The SV [Sha53] is a classic method to distribute the total gains generated by the coalition of all players. The SV of player i with respect to the utility function ν is deﬁned as the average marginal contribution of i to coalition S over all S ⊆ I \\ {i}: s(ν, i) = 1 N ∑ S⊆I\\{i} 1 ( N−1 |S| ) [ν(S ∪ {i}) − ν(S)] (2) We suppress the dependency on ν when the utility is self-evident and use si to represent the value allocated to player i. The formula in (2) can also be stated in the equivalent form: si = 1 N! ∑ π∈Π(I) [ν(Pπ i ∪ {i}) − ν(Pπ i )] (3) where π ∈ Π(I) is a permutation of players and Pπ i is the set of players which precede player i in π. Intuitively, imagine all players join a coalition in a random order, and that every player i who has joined receives the marginal contribution that his participation would bring to those already in the coalition. To calculate si, we average these contributions over all the possible orders. Transforming these game theory concepts to data valuation, one can think of the players as training data instances and the utility function ν(S) as a performance measure of the model trained on the set of training data S. The SV of each training point thus measures its importance to learning a performant ML model. The following desirable properties that the SV uniquely possesses motivate us to adopt it for data valuation. i Group Rationality: The value of the entire training dataset is completely distributed among all sellers, i.e., ν(I) = ∑i∈I si. 7 ii Fairness: (1) Two sellers who are identical with respect to what they contribute to a dataset’s utility should have the same value. That is, if seller i and j are equivalent in the sense that ν(S ∪ {i}) = ν(S ∪ {j}), ∀S ⊆ I \\ {i, j}, then si = sj. (2) Sellers with zero marginal contributions to all subsets of the dataset receive zero payoff, i.e., si = 0 if ν(S ∪ {i}) = 0 for all S ⊆ I \\ {i}. iii Additivity: The values under multiple utilities sum up to the value under a utility that is the sum of all these utilities: s(ν1, i) + s(ν2, i) = s(ν1 + ν2, i) for i ∈ I. The group rationality property states that any rational group of sellers would expect to distribute the full yield of their coalition. The fairness property requires that the names of the sellers play no role in determining the value, which should be sensitive only to how the utility function responds to the presence of a seller. The additivity property facilitates efﬁcient value calculation when the ML model is used for multiple applications, each of which is associated with a speciﬁc utility function. With additivity, one can decompose a given utility function into an arbitrary sum of utility functions and compute value shares separately, resulting in transparency and decentralizability. The fact that the SV is the only value division scheme that meets these desirable criteria, combined with its ﬂexibility to support different utility functions, leads us to employ the SV to attribute the total gains generated from a dataset to each seller. In addition to its theoretical soundness, our previous work [JDW+19] empirically demonstrated that the SV also coincides with people’s intuition of data value. For instance, noisy images tend to have lower SVs than the high-ﬁdelity ones; the training data whose distribution is closer to the test data distribution tends to have higher SVs. These empirical results further back up the use of the SV for data valuation. For more details, we refer the readers to [JDW+19]. 2.2. A Baseline Algorithm. One challenge of applying SV is its computational complexity. Eval- uating the exact SV using Eq. (2) involves computing the marginal utility of every user to every coalition, which is O(2N). Such exponential computation is clearly impractical for valuating a large number of training points. Even worse, in many ML tasks, evaluating the utility function per se (e.g., testing accuracy) is computationally expensive as it requires training a ML model. For large datasets, the only feasible approach currently in the literature is Monte Carlo (MC) sampling [Mal15]. In this paper, we will use it as a baseline for evaluation. The central idea behind the baseline algorithm is to regard the SV deﬁnition in (3) as the expectation of a training instance’s marginal contribution over a random permutation and then use the sample mean to approximate it. More speciﬁcally, let π be a random permutation of I and each permutation has a probability of 1/N!. Consider the random variable ϕi = ν(Pπ i ∪ {i}) − ν(Pπ i ). By (3), the SV si is equal to E[ϕi]. Thus, ˆsi = 1 T T∑ t=1 ν(Pπt i ∪ {i}) − ν(Pπt i )(4) is a consistent estimator of si, where πt be tth sample permutation uniformly drawn from all possible permutations Π(I). We say that ˆs ∈ RN is an (ϵ, δ)-approximation to the true SV s = [s1, · · · , sN]T ∈ RN if P[maxi |ˆsi − si| ⩽ ϵ] ⩾ 1 − δ. Let r be the range of utility differences ϕi. By applying the Hoeffding’s inequality, [MTTH+13] shows that for general, bounded utility functions, the number of permuta- tions T needed to achieve an (ϵ, δ)-approximation is r2 2ϵ2 log 2N δ . For each permutation, the baseline algorithm evaluates the utility function for N times in order to compute the SV for N training 8 instances; therefore, the total utility evaluations involved in the baseline approach is O(N log N). In general, evaluating ν(S) in the ML context requires to re-train the model on the subset S of the training data. Therefore, despite its improvements over the exact SV calculation, the baseline algorithm is not efﬁcient for large datasets. Take the KNN classiﬁer as an example and assume that ν(·) represents the testing accuracy of the classiﬁer. Then, evaluating ν(S) needs to sort the training data in S according to their distances to the test point, which has O(|S| log |S|) complexity. Since on average |S| = N/2, the asymptotic complexity of calculating the SV for a KNN classiﬁer via the baseline algorithm is O(N2 log2 N), which is prohibitive for large-scale datasets. In the sequel, we will show that it is indeed possible to develop much more efﬁcient algorithms to compute the SV by leveraging the locality of KNN models. 3. Valuing Data for KNN Classiﬁers In this section, we present an algorithm that can calculate the exact SV for KNN classiﬁers in quasi-linear time. Further, we exhibit an approximate algorithm based on LSH that could achieve sublinear complexity. 3.1. Exact SV Calculation. KNN algorithms are popular supervised learning methods, widely adopted in a multitude of applications such as computer vision, information retrieval, etc. Suppose the dataset D consisting of pairs (x1, y1), (x2, y2), . . ., (xN, yN) taking values in X × Y, where X is the feature space and Y is the label space. Depending on whether the nearest neighbor algorithm is used for classiﬁcation or regression, Y is either discrete or continuous. The training phase of KNN consists only of storing the features and labels in D. The testing phase is aimed at ﬁnding the label for a given query (or test) feature. This is done by searching for the K training features most similar to the query feature and assigning a label to the query according to the labels of its K nearest neighbors. Given a single testing point xtest with the label ytest, the simplest, unweighted version of a KNN classiﬁer ﬁrst ﬁnds the top-K training points (xα1, · · · , xαK) that are most similar to xtest and outputs the probability of xtest taking the label ytest as P[xtest → ytest] = 1 K ∑K k=1 1 [yαk = ytest], where αk is the index of the kth nearest neighbor. One natural way to deﬁne the utility of a KNN classiﬁer is by the likelihood of the right label: ν(S) = 1 K min{K,|S|}∑ k=1 1 [yαk(S) = ytest](5) where αk(S) represents the index of the training feature that is kth closest to xtest among the training examples in S. Speciﬁcally, αk(I) is abbreviated to αk. Using this utility function, we can derive an efﬁcient, but exact way of computing the SV. THEOREM 1. Consider the utility function in (5). Then, the SV of each training point can be calculated recursively as follows: sαN = 1 [yαN = ytest] N (6) sαi = sαi+1+ 1 [yαi = ytest] − 1 [yαi+1 = ytest] K min{K, i} i (7) 9 Note that the above result for a single test point can be readily extended to the multiple-test- point case, in which the utility function is deﬁned by ν(S) = 1 Ntest Ntest∑ j=1 1 K min{K,|S|}∑ k=1 1 [yα (j) k (S) = ytest,j](8) where α (j) k (S) is the index of the kth nearest neighbor in S to xtest,j. By the additivity property, the SV for multiple test points is the average of the SV for every single test point. The pseudo-code for calculating the SV for an unweighted KNN classiﬁer is presented in Algorithm 1. The computational complexity is only O(N log NNtest) for N training data points and Ntest test data points—this is simply to sort Ntest arrays of N numbers! Algorithm 1: Exact algorithm for calculating the SV for an unweighted KNN classiﬁer. input : Training data D = {(xi, yi)}N i=1, test data Dtest = {(xtest,i, ytest,i)} Ntest i=1 output : The SV {si}N i=1 1 for j ← 1 to Ntest do 2 (α1, ..., αN) ← Indices of training data in an ascending order using d(·, xtest); 3 sj,αN ← 1 [yαN =ytest] N ; 4 for i ← N − 1 to 1 do 5 sj,αi ← sj,αi+1 + 1 [yαi =ytest,j]−1 [yαi+1 =ytest,j] K min{K,i} i ; 6 end 7 end 8 for i ← 1 to N do 9 si ← 1 Ntest ∑Ntest j=1 sj,i; 10 end The proof of Theorem 1 relies on the following lemma, which states that the difference in the utility gain induced by either point i or point j translates linearly to the difference in the respective SVs. LEMMA 1. For any i, j ∈ I, the difference in SVs between i and j is si − sj = 1 N − 1 ∑ S⊆I\\{i,j} ν(S ∪ {i}) − ν(S ∪ {j}) ( N−2 |S| )(9) Proof of Theorem 1. W.l.o.g., we assume that x1, . . . , xn are sorted according to their similarity to xtest, that is, xi = xαi. For any given subset S ⊆ I \\ {i, i + 1} of size k, we split the subset into two disjoint sets S1 and S2 such that S = S1 ∪ S2 and |S1| + |S2| = |S| = k. Given two neighboring points with indices i, i + 1 ∈ I, we constrain S1 and S2 to S1 ⊆ {1, ..., i − 1} and S2 ⊆ {i + 2, ..., N}. Let si be the SV of data point xi. By Lemma 1, we can draw conclusions about the SV difference si − si+1 by inspecting the utility difference ν(S ∪ {i}) − ν(S ∪ {i + 1}) for any S ⊆ I \\ {i, i + 1}. We analyze ν(S ∪ {i}) − ν(S ∪ {i + 1}) by considering the following cases. (1) |S1| ⩾ K. In this case, we know that i, i + 1 > K and therefore ν(S ∪ {i}) = ν(S ∪ {i + 1}) = ν(S), hence ν(S ∪ {i}) − ν(S ∪ {i + 1}) = 0. 10 (2) |S1| < K. In this case, we know that i ⩽ K and therefore ν(S ∪ {i}) − ν(S) might be nonzero. Note that including a point i into S can only expel the Kth nearest neighbor from the original set of K nearest neighbors. Thus, ν(S ∪ {i}) − ν(S) = 1 K (1 [yi = ytest] − 1 [yK = ytest]). The same hold for the inclusion of point i + 1: ν(S ∪ {i + 1}) − ν(S) = 1 K (1 [yi+1 = ytest] − 1 [yK = ytest]). Combining the two equations, we have ν(S ∪ {i}) − ν(S ∪ {i + 1}) = 1 [yi = ytest] − 1 [yi+1 = ytest] K Combining the two cases discussed above and applying Lemma 1, we have si − si+1 = 1 N − 1 N−2∑ k=0 1 ( N−2 k ) ∑ S1⊆{1,...,i−1}, S2⊆{i+2,...,N}: |S1|+|S2|=k,|S1|<K 1 [yi = ytest] − 1 [yi+1 = ytest] K = 1 [yi = ytest] − 1 [yi+1 = ytest] K × 1 N − 1 N−2∑ k=0 1 ( N−2 k ) min(K−1,k)∑ m=0 ( i − 1 m )( N − i − 1 k − m ) (10) The sum of binomial coefﬁcients in (10) can be simpliﬁed as follows: N−2∑ k=0 1 ( N−2 k ) min{K−1,k}∑ m=0 ( i − 1 m )( N − i − 1 k − m ) (11) = min{K−1,i−1}∑ m=0 N−i−1∑ k ′=0 ( i−1 m )( N−i−1 k ′ ) ( N−2 m+k ′)(12) = min{K, i}(N − 1) i (13) where the ﬁrst equality is due to the exchange of the inner and outer summation and the second one is by taking v = N − i − 1 and u = i − 1 in the binomial identity ∑v j=0 (u i)(v j) (u+v i+j ) = u+v+1 u+1 . Therefore, we have the following recursion si − si+1 = 1 [yi = ytest] − 1 [yi+1 = ytest] K min{K, i} i (14) Now, we analyze the formula for sN, the starting point of the recursion. Since xN is farthest to xtest among all training points, xN results in non-zero marginal utility only when it is added to the subsets of size smaller than K. Hence, sN can be written as sN = 1 N K−1∑ k=0 1 ( N−1 k ) ∑ |S|=k,S⊆I\\{N} ν(S ∪ N) − ν(S)(15) = 1 N K−1∑ k=0 1 ( N−1 k ) ∑ |S|=k,S⊆I\\{N} 1 [yN = ytest] K (16) = 1 [yN = ytest] N (17) □ 11 3.2. LSH-based Approximation. The exact calculation of the KNN SV for a query instance requires to sort the entire training dataset, and has computation complexity O(Ntest(Nd + N log(N))), where d is the feature dimension. Thus, the exact method becomes expensive for large and high- dimensional datasets. We now present a sublinear algorithm to approximate the KNN SV for classiﬁcation tasks. The key to boosting efﬁciency is to realize that only O(1/ϵ) nearest neighbors are needed to estimate the KNN SV with up to ϵ error. Therefore, we can avert the need of sorting the entire database for every new query point. THEOREM 2. Consider the utility function deﬁned in (5). Consider {ˆsi}N i=1 deﬁned recursively by ˆsαi = 0 if i ⩾ K∗(18) ˆsαi = ˆsαi+1 + 1 [yαi = ytest] − 1 [yαi+1 = ytest] K min{K, i} i if i ⩽ K∗ − 1(19) where K∗ = max{K, ⌈1/ϵ⌉} for some ϵ > 0. Then, [ˆsα1,. . ., ˆsαN] is an (ϵ, 0)-approximation to the true SV [sα1,. . ., sαN] and ˆsi − ˆsi+1 = si − si+1 for i ⩽ K∗ − 1. Theorem 2 indicates that we only need to ﬁnd max{K, ⌈1/ϵ⌉}(≜ K∗) nearest neighbors to obtain an (ϵ, 0)-approximation. Moreover, since ˆsi − ˆsi+1 = si − si+1 for i ⩽ K∗ − 1, the approximation retains the original value rank for K∗ nearest neighbors. The question on how to efﬁciently retrieve nearest neighbors to a query in large-scale databases has been studied extensively in the past decade. Various techniques, such as the kd-tree [MA98], LSH [DIIM04], have been proposed to ﬁnd approximate nearest neighbors. Although all of these techniques can potentially help improve the efﬁciency of the data valuation algorithms for KNN, we focus on LSH in this paper, as it was experimentally shown to achieve large speedup over several tree-based data structures [GIM+99,HPIM12,DIIM04]. In LSH, every training instance x is converted into codes in each hash table by using a series of hash functions hj(x), j = 1, . . . , m. Each hash function is designed to preserve the relative distance between different training instances; similar instances have the same hashed value with high probability. Various hash functions have been proposed to approximate KNN under different distance metrics [Cha02, DIIM04]. We will focus on the distance measured in l2 norm; in that case, a commonly used hash function is h(x) = ⌊ wT x+b r ⌋, where w is a vector with entries sampled from a p-stable distribution, and b is uniformly chosen from the range [0, r]. It is shown in [DIIM04]: P[h(xi) = h(xtest)] = fh(∥xi − xtest∥2)(20) where the function fh(c) = ∫r 0 1 c f2( z c )(1 − z r )dz is a monotonically decreasing with c. Here, f2 is the probability density function of the absolute value of a 2-stable random variable. We now present a theorem which relates the success rate of ﬁnding approximate nearest neighbors to the intrinsic property of the dataset and the parameters of LSH. THEOREM 3. LSH with O(d log(N)Ng(CK) log K δ ) time complexity, O(Nd + Ng(CK)+1 log K δ ) space complexity, and O(Ng(CK) log K δ ) hash tables can ﬁnd the exact K nearest neighbors with probability 1 − δ, where g(CK) = log fh(1/CK)/ log fh(1) is a monotonically decreasing function. CK = Dmean/DK, where Dmean is the expected distance of a random training instance to a query xtest and DK is the expected distance between xtest to its Kth nearest neighbor denoted by xαi(xtest), i.e., Dmean = Ex,xtest[D(x, xtest)](21) 12 DK = Extest[D(xαi(xtest), xtest](22) The above theorem essentially extends the 1NN hardness analysis in Theorem 3.1 of [HKC12] to KNN. CK measures the ratio between the distance from a query instance to a random training instance and that to its Kth nearest neighbor. We will hereinafter refer to CK as Kth relative contrast. Intuitively, CK signiﬁes the difﬁculty of ﬁnding the Kth nearest neighbor. A smaller CK implies that some random training instances are likely to have the same hashed value as the Kth nearest neighbor, thus entailing a high computational cost to differentiate the true nearest neighbors from the false positives. Theorem 3 shows that among the datasets of the same size, the one with higher relative contrast will need lower time and space complexity and fewer hash tables to approximate the K nearest neighbors. Combining Theorem 2 and Theorem 3, we obtain the following theorem that explicates the tradeoff between KNN SV approximation errors and computational complexity. THEOREM 4. Consider the utility function deﬁned in (8). Let ˆx α (j) k denote the kth closest train- ing point to xtest,j output by LSH with O(Ntestd log(N)Ng(CK∗)log NtestK∗ δ ) time complexity, O(Nd + Ng(CK∗)+1log NtestK∗ δ ) space complexity, and O(Ng(CK∗)log NtestK∗ δ ) hash tables, where K∗ = max(K, ⌈1/ϵ⌉). Suppose that {ˆsi}N i=1 is computed via ˆsi = 1 Ntest ∑Ntest j=1 ˆsi,j and ˆsi,j (j = 1, . . . , Ntest) are deﬁned recursively by ˆsα(j) i ,j = 0 if i ⩾ K∗(23) ˆsα (j) i ,j = ˆsα (j) i+1,j + 1 [ˆyα (j) i = ytest,j] − 1 [ˆyα (j) i+1 = ytest,j] K min{K, i} i if i ⩽ K∗ − 1(24) where ˆyα (j) i and ytest,j are the labels associated with ˆx α (j) i and xtest,j, respectively. Let the true SV of ˆxαk be denoted by sαi. Then, [ˆsα1, . . . , ˆsαN] is an (ϵ, δ)-approximation to the true SV [sα1, . . . , sαN]. The gist of the LSH-based approximation is to focus only on the SV of the retrieved nearest neighbors and neglect the values of the rest of the training points since their values are small enough. For a error requirement ϵ not too small such that CK∗ > 1, the LSH-based approximation has sublinear time complexity, thus enjoying higher efﬁciency than the exact algorithm. 4. Extensions We extend the exact algorithm for unweighted KNN to other settings. Speciﬁcally, as illustrated by Figure 3, we categorize a data valuation problem according to whether data contributors are valued in tandem with a data analyst; whether each data contributor provides a single data instance or multiple ones; whether the underlying ML model is a weighted KNN or unweighted; and whether the model solves a regression or a classiﬁcation task. We will discuss the valuation algorithm for each of the above settings. Unweighted KNN Regression. For regression tasks, we deﬁne the utility function by the negative mean square error of an unweighted KNN regressor: U(S) = − ( 1 K min{K,|S|}∑ k=1 yαk(S) − ytest )2 (25) Using similar proof techniques to Theorem 1, we provide a simple iterative procedure to compute the SV for unweighted KNN regression in Appendix E.1. 13 Figure 4. Illustration of the idea to compute the SV for weighted KNN. Weighted KNN. A weighted KNN estimate produced by a training set S can be expressed as ˆy(S) = ∑min{K,|S|} k=1 wαk(S)yαk, where wαk(S) is the weight associated with the kth nearest neighbor in S. The weight assigned to a neighbor in the weighted KNN estimate often varies with the neighbor-to-test distance so that the evidence from more nearby neighbors is weighted more heavily [Dud76]. Correspondingly, we deﬁne the utility function associated with weighted KNN classiﬁcation and regression tasks as U(S) = min{K,|S|}∑ k=1 wαk(S)1 [yαk(S) = ytest](26) and U(S) = − ( min{K,|S|}∑ k=1 wαk(S)yαk(S) − ytest )2.(27) For weighted KNN classiﬁcation and regression, the SV can no longer be computed exactly in O(N log N) time. In Appendix E.2, we present a theorem showing that it is however possible to compute the exact SV for weighted KNN in O(NK) time. Figure 4 illustrates the origin of the polynomial complexity result. When applying (2) to KNN, we only need to focus on the subsets whose utility might be affected by the addition of ith training instance. Since there are only NK possible distinctive combinations for K nearest neighbors, the number of distinct utility values for all S ⊆ I is upper bounded by NK. Multiple Data Per Contributor. We now study the case where each seller provides more than one data instance. The goal is to fairly value individual sellers in lieu of individual training points. In Appendix E.3, we show that for both unweighted/weighted classiﬁers/regressors, the complexity for computing the SV of each seller is O(MK), where M is the number of sellers. Particularly, when K = 1, even though each seller can provision multiple instances, the utility function only depends on the training point that is nearest to the query point. Thus, for 1NN, the problem of computing the multi-data-per-seller KNN SV reduces to the single-data-per-seller case; thus, the corresponding computational complexity is O(M log M). 14 Valuing Computation. Oftentimes, the buyer may outsource data analytics to a third party, which we call the analyst throughout the rest of the paper. The analyst analyzes the training dataset aggregated from different sellers and returns an ML model to the buyer. In this process, the analyst contributes various computation efforts, which may include intellectual property pertaining to data anlytics, usage of computing infrastructure, among others. Here, we want to address the problem of appraising both sellers (data contributors) and analysts (computation contributors) within a uniﬁed game-theoretic framework. Firstly, we extend the game-theoretic framework for data valuation to model the interplay between data and computation. The resultant game is termed a composite game. By contrast, the game discussed previously which involves only the sellers is termed a data-only game. In the composite game, there are M + 1 players, consisting of M sellers denoted by Is and one analyst denoted by C. We can express the utility function νc associated with the game in terms of the utility function ν in the data-only game as follows. Since in the case of outsourced analytics, both contributions from data sellers and data analysts are necessary for building models, the value of a set S ⊆ Is ∪ {C} in the composite game is zero if S only contains the sellers or the analyst; otherwise, it is equal to ν evaluated on all the sellers in S. Formally, we deﬁne the utility function νc by νc(S) = { 0, if S = {C} or S ⊆ Is ν(S \\ {C}), otherwise (28) The goal in the composite game is to allocate νc({Is, C}) to the individual sellers and the analyst. s(νc, i) and s(νc, C) represent the value received by seller i and the analyst, respectively. We suppress the dependency of s on the utility function whenever it is self-evident, denoting the value allocated to seller i and the analyst by si and sc, respectively. In Appendix E.4, we show that one can compute the SV for both the sellers and the analyst with the same computational complexity as the one needed for the data-only game. Comments on the Proof Techniques. We have shown that we can circumvent the exponential complexity for computing the SV for a standard unweighted KNN classiﬁer and its extensions. A natural question is whether it is possible to abstract the commonality of these cases and provide a general property of the utility function that one can exploit to derive efﬁcient algorithms. Suppose that some group of S’s induce the same ν(S ∪ {i}) − ν(S ∪ {j}) and there only exists T number of such groups. More formally, consider that ν(S ∪ {i}) − ν(S ∪ {j}) can be represented by a “piecewise” form: ν(S ∪ {i}) − ν(S ∪ {j}) = T∑ t=1 C(t) ij 1 [S ∈ St](29) where St ⊆ 2I\\{i,j} and C(t) i,j ∈ R is a constant associated with tth “group.” An application of Lemma 1 to the utility functions with the piecewise utility difference form indicates that the SV difference between i and j is si − sj = 1 N − 1 ∑ S⊆I\\{i,j} T∑ t=1 C(t) ij ( N−2 |S| ) 1 [S ∈ St](30) = 1 N − 1 T∑ t=1 C(t) ij [ N−2∑ k=0 |{S : S ∈ St, |S| = k}| ( N−2 k ) ] (31) 15 With the piecewise property (29), the SV calculation is reduced to a counting problem. As long as the quantity in the bracket of (31) can be efﬁciently evaluated, the SV difference between any pair of training points can be computed in O(T N). Indeed, one can verify that the utility function for unweighted KNN classiﬁcation, regression and weighted KNN have the aforementioned “piecewise” utility difference property with T = 1, N − 1, ∑K k=0 ( N−2 k ) , respectively. More details can be found in Appendix F. 5. Improved MC Approximation As discussed previously, the SV for unweighted KNN classiﬁcation and regression can be computed exactly with O(N log N) complexity. However, for the variants including the weighted KNN and multiple-data-per-seller KNN, the complexity to compute the exact SV is O(NK) and O(MK), respectively, which are clearly not scalable. We propose a more efﬁcient way to evaluate the SV up to provable approximation errors, which modiﬁes the existing MC algorithm presented in Section 2.2. By exploiting the locality property of the KNN-type algorithms, we propose a tighter upper bound on the number of permutations for a given approximation error and exhibit a novel implementation of the algorithm using efﬁcient data structures. The existing sample complexity bound is based on Hoeffding’s inequality, which bounds the number of permutations needed in terms of the range of utility difference ϕi. This bound is not always optimal as it depends on the extremal values that a random variable can take and thus accounts for the worst case. For KNN, the utility does not change after adding training instance i for many subsets; therefore, the variance of ϕi is much smaller than its range. This inspires us to use Bennett’s inequality, which bounds the sample complexity in terms of the variance of a random variable and often results in a much tighter bound than Hoeffding’s inequality. THEOREM 5. Given the range [−r, r] of the utility difference ϕi, an error bound ϵ, and a conﬁdence 1 − δ, the sample size required such that P[∥ˆs − s∥∞ ⩾ ϵ] ⩽ δ is T ⩾ T ∗. T ∗ is the solution of N∑ i=1 exp(−T ∗(1 − q2 i )h( ϵ (1 − q2 i )r )) = δ/2.(32) where h(u) = (1 + u) log(1 + u) − u and qi = { 0, i = 1, . . . , K i−K i , i = K + 1, . . . , N (33) Given ϵ, δ, and r, the required permutation size T ∗ derived from Bennett’s bound can be computed numerically. For general utility functions the range r of the utility difference is twice the range of the utility function, while for the special case of the unweighted KNN classiﬁer, r = 1 K . Although determining exact T ∗ requires numerical calculation, we can nevertheless gain insights into the relationship between N, ϵ, δ and T ∗ through some approximation. We leave the detailed derivation to Appendix H, but it is often reasonable to use the following ˜T as an approximation of T ∗: ˜T ⩾ r2 ϵ2 log 2K δ (34) 16 Algorithm 2: Improved MC Approach input : Training set - D = {(xi, yi)}N i=1, utility function ν(·), the number of measurements - M, the number of permutations - T output : The SV of each training point - ˆs ∈ RN 11 for t ← 1 to T do 12 πt ← GenerateUniformRandomPermutation(D); 13 Initialize a length-K max-heap H to maintain the KNN; 14 for i ← 1 to N do 15 Insert πt,i to H; 16 if H changes then 17 ϕt πt,i ← ν(πt,1:i) − ν(πt,1:i−1); 18 else 19 ϕt πt,i ← ϕt πt,i−1; 20 end 21 end 22 end 23 ˆsi = 1 T ∑T t=1 ϕt i for i = 1, . . . , N; The sample complexity bound derived above does not change with N. On the one hand, a larger training data size implies more unknown SVs to be estimated, thus requiring more random permuta- tions. On the other hand, the variance of the SV across all training data decreases with the training data size, because an increasing proportion of training points makes insigniﬁcant contributions to the query result and results in small SVs. These two opposite driving forces make the required permutation size about the same across all training data sizes. The algorithm for the improved MC approximation is provided in Algorithm 2. We use a max-heap to organize the KNN. Since inserting any training data to the heap costs O(log K), incre- mentally updating the KNN in a permutation costs O(N log K). Using the bound on the number of permutations in (34), we can show that the total time complexity for our improved MC algorithm is O( N ϵ2 log K log K δ ). 6. Experiments We evaluate the proposed approaches to computing the SV of training data for various nearest neighbor algorithms. 6.1. Experimental Setup. Datasets. We used the following popular benchmark datasets of different sizes: (1) dog-fish [KL17] contains the features of dog and cat images extracted from ImageNet, with 900 training examples and 300 test examples for each class. The features have 2048 dimensions, generated by the state-of-the-art Inception v3 network [SVI+16] with all but the top layer. (2) MNIST [LC10] is a handwritten digit dataset with 60000 training images and 10000 test images. We extracted 1024-dimensional features via a convolutional network. (3) The CIFAR-10 dataset consists of 60000 32 × 32 color images in 10 classes, with 6000 images per class. The deep features have 2048 dimensions and were extracted via the ResNet-50 [HZRS16]. (4) ImageNet [DDS+09] is an image dataset with more than 1 million 17 Figure 5. The SV produced by the exact algorithm and the baseline MC approxima- tion algorithm. images organized according to the WordNet hierarchy. We chose 1000 classes which have in total around 1 million images and extracted 2048-dimensional deep features by the ResNet-50 network. (5) Yahoo Flickr Creative Commons 100M that consists of 99.2 million photos. We randomly chose a 10-million subset (referred to as Yahoo10m hereinafter) for our experiment, and used the deep features extracted by [AFGR16]. Parameter selection for LSH. The three main parameters that affect the performance of the LSH are the number of projections per hash value (m), the number of hash tables (h), and the width of the project (r). Decreasing r decreases the probability of collision for any two points, which is equivalent to increasing m. Since a smaller m will lead to better efﬁciency, we would like to set r as small as possible. However, decreasing r below a certain threshold increases the quantity g(CK), thereby requiring us to increase h. Following [DIIM04], we performed grid search to ﬁnd the optimal value of r which we used in our experiments. Following [GIM+99], we set m = α log N/ log(fh(Dmean)−1). For a given value of m, it is easy to ﬁnd the optimal value of h which will guarantee that the SV approximation error is no more than a user-speciﬁed threshold. We tried a few values for α and reported the m that leads to lowest runtime. For all experiments pertaining to the LSH, we divided the dataset into two disjoint parts: one for selecting the parameters, and another for testing the performance of LSH for computing the SV. 6.2. Experimental Results. 6.2.1. Unweighted KNN Classiﬁer. Correctness. We ﬁrst empirically validate our theortical result. We randomly selected 1000 training points and 100 test points from MNIST. We computed the SV of each training point with respect to the KNN utility using the exact algorithm and the baseline MC method. Figure 5 shows that the MC estimate of the SV for each training point converges to the result of the exact algorithm. Performance. We validated the hypothesis that our exact algorithm and the LSH-based method outperform the baseline MC method. We take the approximation error ϵ = 0.1 and δ = 0.1 for 18 Figure 6. Performance of unweighted KNN classiﬁcation in the single-data-per-seller case. both MC and LSH-based approximations. We bootstrapped the MNIST dataset to synthesize training datasets of various sizes. The three SV calculation methods were implemented on a machine with 2.6 GHz Intel Core i7 CPU. The runtime of the three methods for different datasets is illustrated in Figure 6 (a). The proposed exact algorithm is faster than the baseline approximation by several orders magnitude and it produces the exact SV. By circumventing the computational complexity of sorting a large array, the LSH-based approximation can signiﬁcantly outperform the exact algorithm, especially when the training size is large. Figure 6 (b) sheds light on the increasing performance gap between the LSH-based approximation and the exact method with respect to the training size. The relative contrast of these bootstrapped datasets grows with the number of training points, thus requiring fewer hash tables and less time to search for approximate nearest neighbors. We also tested the approximation approach proposed in our prior work [JDW+19], which achieves the-start-of-the-art performance for ML models that cannot be incrementally maintained. However, for models that have efﬁcient incremental training algorithms, like KNN, it is less efﬁcient than the baseline approximation, and the experiment for 1000 training points did not ﬁnish in 4 hours. Using a machine with the Intel Xeon E5-2690 CPU and 256 GB RAM, we benchmarked the runtime of the exact and the LSH-based approximation algorithm on three popular datasets, including CIFAR-10, ImageNet, and Yahoo10m. For each dataset, we randomly selected 100 test points, computed the SV of all training points with respect to each test point, and reported the average runtime across all test points. The results for K = 1 are reported in Figure 7. We can see that the LSH- based method can bring a 3×-5× speed-up compared with the exact algorithm. The performance of LSH depends heavily on the dataset, especially in terms of its relative contrast. This effect will be thoroughly studied in the sequel. We compare the prediction accuracy of KNN (K = 1, 2, 5) with the commonly used logistic regression and the result is illustrated in Figure 8. We can see that KNN achieves comparable prediction power to logistic regression when using features extracted via deep neural networks. The runtime of the exact and the LSH-based approximation for K = 2, 5 is similar to the K = 1 case in Figure 7, so we will leave their corresponding results to Appendix A.1. 19 Figure 7. Average runtime of the exact and the LSH-based approximation algorithm for computing the unweighted KNN SV for a single test point. We take ϵ, δ = 0.1 and K = 1. Dataset Size Estimated Contrast Runtime (Exact) Runtime (LSH) CIFAR-10 6E+4 1.2802 0.78s 0.23s ImageNet 1E+6 1.2163 11.34s 2.74s Yahoo10m 1E+7 1.3456 203.43s 44.13s Figure 8. Comparison of prediction accuracy of KNN vs. logistic regression on deep features. Dataset 1NN 2NN 5NN Logistic Regression CIFAR-10 81% 83% 80% 87% ImageNet 77% 73% 84% 82% Yahoo10m 90% 96% 98% 96% Effect of relative contrast on the LSH-based method. Our theoretical result suggests that the K∗th relative contrast (K∗ = max{K, ⌈1/ϵ⌉}) determines the complexity of the LSH-based approximation. We veriﬁed the effect of relative contrast by experimenting on three datasets, namely, dog-fish, deep and gist. deep and gist were constructed by extracting the deep features and gist features [SI07] from MNIST, respectively. All of these datasets were normalized such that Dmean = 1. Figure 9 (a) shows that the relative contrast of each dataset decreases as K∗ increases. In this experiment, we take ϵ = 0.01 and K = 2, so the corresponding K∗ = 1/ϵ = 100. At this value of K∗, the relative contrast is in the following order: deep (1.57) > gist (1.48) > dog-fish (1.17). From Figure 9 (b) and (c), we see that the number of hash tables and the number of returned points required to meet the ϵ error tolerance for the three datasets follow the reversed order of their relative contrast, as predicted by Theorem 4. Therefore, the LSH-based approximation will be less efﬁcient if the K in the nearest neighbor algorithm is very large or the desired error ϵ is small. Figure 9 (d) shows that the LSH-based method can better approximate the true SV as the recall of the underlying nearest neighbor retrieval gets higher. For the datasets with high relative contrast, e.g., deep and gist, a moderate value of recall (∼ 0.7) can already lead to an approximation error below the desired threshold. On the other hand, dog-fish, which has low relative contrast, will need fairly accurate nearest neighbor retrieval (recall ∼ 1) to obtain a tolerable approximation error. The reason for the different retrieval accuracy requirements is that for the dataset with higher relative contrast, even if the retrieval of the nearest neighbors is inaccurate, the rank of the erroneous elements in the retrieved set may still be close to that of the missed true nearest neighbors. Thus, these erroneous elements will have only little impacts on SV approximation errors. Simulation of the theoretical bound of LSH. According to Theorem 4, the complexity of the LSH-based approximation is dominated by the exponent g(CK∗), where K∗ = min{K, 1/ϵ} and g(·) depends on the width r of the p-stable distribution used for LSH. We computed CK∗ and g(CK∗) for ϵ ∈ {0.001, 0.01, 0.1, 1} and let K = 1 in this simulation. The orange line in Figure 10 (a) shows that a larger ϵ induces a larger value of relative contrast CK∗, rendering the underlying nearest neighbor 20 Figure 9. Performance of LSH on three datasets: deep, gist, dog-fish. (a) Relative contrast CK∗ vs. K∗. (b), (c) and (d) illustrate the trend of the SV approximation error for different number of hash tables, returned points and recalls. retrieval problem of the LSH-based approximation method easier. In particular, CK∗ is greater than 1 for all epsilons considered except for ϵ = 0.001. Recall that g(CK) = log fh(1/CK)/ log fh(1); thus, g(CK∗) will exhibit different trends for the epsilons with CK∗ > 1 and the ones with CK∗ < 1, as shown in Figure 10 (b). Moreover, Figure 10 (b) shows that the value of g(CK∗) is more or less insensitive to r after a certain point. For ϵ that is not too small, we can choose r to be the value at which g(CK∗) is minimized. It does not make sense to use the LSH-based approximation if the desired error ϵ is too small to have the corresponding g(CK∗) less than one, since its complexity is 21 Figure 10. (a) The exponent g(CK∗) in the complexity bound of the LSH-based method and the relative contrast CK∗ computed for different ϵ. K is ﬁxed to 1. (b) g(CK∗) vs. the projection width r of the LSH. theoretically higher than the exact algorithm. The blue line in Figure 10 (a) illustrates the exponent g(CK∗) as a function of ϵ when r is chosen to minimize g(CK∗). We observe that g(CK∗) is always below 1 except when ϵ = 0.001. 6.2.2. Evaluation of Other Extensions. We introduced the extensions of the exact SV calculation algorithm to the settings beyond unweighted KNN classiﬁcation. Some of these settings require polynomial time to compute the exact SV, which is impractical for large-scale datasets. For those settings, we need to resort to the MC approximation method. We ﬁrst compare the sample complexity of different MC methods, including the baseline and our improved MC method (Section 5). Then, we demonstrate data values computed in various settings. Sample complexity for MC methods. The time complexity of the MC-based SV approximation algorithms is largely dependent on the number of permutations. Figure 11 compares the permutation sizes used in the following three methods against the actual permutation size needed to achieve a given approximation error (marked as “ground truth” in the ﬁgure): (1) “Hoeffding”, which is the baseline approach and uses the Hoeffding’s inequality to decide the number of permutations; (2) “Bennett”, which is our proposed approach and exploits Bennett’s inequality to derive the permutation size; (3) ”Heuristic”, which terminates MC simulations when the change of the SV estimates in the two consecutive iterations is below a certain value, which we set to ϵ/50 in this experiment. We notice that the ground truth requirement for the permutation size decreases at ﬁrst and remains constant when the training data size is large enough. From Figure 11, the bound based on the Hoeffding’s inequality is too loose to correctly predict the correct trend of the required permutation size. By contrast, our bound based on Bennett’s inequality exhibits the correct trend of permutation size with respect to training data size. In terms of runtime, our improved MC method based on Bennett’s inequality is more than 2× faster than the baseline method when the training size is above 1 million. Moreover, using the aforementioned heuristic, we were able to terminate the 22 Figure 11. Comparison of the required permutation sizes for different number of training points derived from the Hoeffding’s inequality (baseline), Bennett’s inequality and the heuristic method against the ground truth. MC approximation algorithm even earlier while satisfying the requirement of the approximation error. Performance. We conducted experiments on the dog-fish dataset to compare the runtime of the exact algorithm and our improved MC method. We took ϵ = 0.01 and δ = 0.01 in the approximation algorithm and used the heuristic to decide the stopping iteration. Figure 12 compares the runtime of the exact algorithm and our improved MC approximation for weighted KNN classiﬁcation. In the ﬁrst plot, we ﬁxed K = 3 and varied the number of training points. In the second plot, we set the training size to be 100 and changed K. We can see that the runtime of the exact algorithm exhibits polynomial and exponential growth with respect to the training size and K, respectively. By contrast, the runtime of the approximation algorithm increases slightly with the number of training points and remains unchanged for different values of K. Figure 13 compares the runtime of the exact algorithm and the MC approximation for the unweighted KNN classiﬁcation when each seller can own multiple data instances. To generate Figure 13 (a), we set K = 2 and varied the number of sellers. We kept the total number of training instances of all sellers constant and randomly assigned the same number of training instances to each seller. We can see that the exact calculation of the SV in the multi-data-per-seller case has polynomial time complexity, while the runtime of the approximation algorithm barely changes with the number of sellers. Since the training data in our approximation algorithm were sequentially inserted into a heap, the complexity of the approximation algorithm is mainly determined by the total number of training data held by all sellers. Moreover, as we kept the total number of training points constant, the approximation algorithm appears invariant over the number of sellers. Figure 13 (b) shows that the runtime of exact algorithm increases with K, while the approximation algorithm’s 23 Figure 12. Performance of the weighted KNN classiﬁcation. Figure 13. Performance of the KNN classiﬁcation in the multi-data-per-seller case. runtime is not sensitive to K. To summarize, the approximation algorithm is preferable to the exact algorithm when the number of sellers and K are large. Unweighted vs. weighted KNN SV. We constructed an unweighted KNN classiﬁer using the dog-fish. Figure 14 (a) illustrates the training points with top KNN SVs with respect to a speciﬁc test image. We see that the returned images are semantically correlated with the test one. We further trained a weighted KNN on the same training set using the weight function that weighs each nearest neighbor inversely proportional to the distance to a given test point; and compared the SV with the ones obtained from the unweighted KNN classiﬁer. We computed the average SV across 24 Figure 14. Data valuation on DOG-FISH dataset (K = 3). (a) top valued data points; (b) unweighted vs. weighted KNN SV on the whole test set; (c) Per-class top-K neighbors labeled inconsistently with the misclassiﬁed test example. all test images for each training point and demonstrated the result in Figure 14 (b). Every point in the ﬁgure represents the SVs of a training point under the two classiﬁers. We can see that the unweighted KNN SV is close to the weighted one. This is because in the high-dimensional feature space, the distances from the retrieved nearest neighbors to the query point are large, in which case the weights tend to be small and uniform. Another observation from Figure 14 (b) is that the KNN SV assigns more values to dog images than ﬁsh images. Figure 14 (c) plots the distribution of the number test examples with regard to the number of their top-K neighbors in the training set are with a label inconsistent with the true label of the test example. We see that most of the nearest neighbors with inconsistent labels belong to the ﬁsh class. In other words, the ﬁsh training images are more close to the dog images in the test set than the dog training images to the test ﬁsh. Thus, the ﬁsh training images are more susceptible to mislead the predictions and should have lower values. This intuitively explains why the KNN SV places a higher importance on the dog images. Data-only vs. composite game. We introduced two game-theoretic models for distributing the gains from an ML model and would like to understand how the shares of the analyst and the data contributors differ in the two models. We constructed an unweighted KNN classiﬁer with K = 10 on 25 the dog-fish dataset and compute the SV of each player in the data-only and the composite game. Recall that the total utility of both games is deﬁned as the average test accuracy trained on the full set of training data. Figure 15 (a) shows that the SV for the analyst increases with the total utility. Therefore, under the composite game formulation, the analyst has huge incentive to train a good ML model as the values assigned to the analyst gets larger with a better ML model. In addition, in the composite game formulation, the analyst has exclusive control over the computational resources and the data only creates value when it is analyzed with computational modules, the analyst should take the greatest share of the utility extracted from the ML model. This intuition is reﬂected in Figure 15 (a). Figure 15 (b) demonstrates that the SV of the data contributors in the composite game is correlated with that in the data-only game, although the actual value is much smaller. Figure 15 (c) exhibits the trend of the SV of the analyst and data contributors as more data contributors participate in a data transaction. The SV of the analyst gets larger with more data contributors, while the average value obtained by each data contributor decreases in both composite and data-only games. Figure 15 (d) zooms into the change of the maximum and minimum value among all data contributors in the data-only game setting (the result in the composite game setting is similar). We can see that both the maximum and minimum value decreases at the beginning; as more data contributors are involved in a data transaction, the minimum value demonstrates a small increment. The points with lowest values tend to hurt the ML model performance when they are added into the training set. With more data contributors and more training points, the negative impacts of these “outliers” can get mitigated. Remarks. We summarize several takeaways from our experimental evaluation. (1) For unweighted KNN classiﬁers, the LSH-based approximation is more preferable than the exact algorithm when a moderate amount of approximation error can be tolerated and K is relatively small. Otherwise, it is recommended to use the exact algorithm as a default approach for data valuation. (2) For weighted KNN regressors or classiﬁers, computing the exact SV has O(NK) compleixty, thus not scalable for large datasets and large K. Hence, it is recommended to adopt the Monte Carlo method in Algorithm 2. Moreover, using the heuristic based on the change of SV estimates in two consecutive iterations to decide the termination point of the algorithm is much more efﬁcient than using the theoretical bounds, such as Hoeffding or Bennett. 7. Discussion From the KNN SV to Monetary Reward. Thus far, we have focused on the problem of attributing the KNN utility and its extensions to each data and computation contributor. In practice, the buyer pays a certain amount of money depending on the model utility and it is required to determine the share of each contributor in terms of monetary rewards. Thus, a remaining question is how to map the KNN SV, a share of the total model utility, to a share of the total revenue acquired from the buyer. A simple method for such mapping is to assume that the revenue is an afﬁne function of the model utility, i.e., R(S) = aν(S) + b where a and b are some constants which can be determined via market research. Due to the additivity property, we have s(R, i) = as(ν, i) + b. Thus, we can apply the same afﬁne function to the KNN SV to obtain the the monetary reward for each contributor. Computing the SV for Models Beyond KNN. The efﬁcient algorithms presented in this paper are possible only because of the “locality” property of KNN. However, given many previous empirical results showing that a KNN classiﬁer can often achieve a classiﬁcation accuracy that is comparable with classiﬁers such as SVMs and logistic regression given sufﬁcient memory, we could use the KNN 26 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 Total UtilityAnalystShapley Value -1.2E-3 -7.0E-4 -2.0E-4 3.0E-4 8.0E-4 -5.0E-03 -1.0E-03 3.0E-03 Data contributor Shapley value ( data - only)Data contributor Shapley value (composite) (b) (a) 1.E-5 1.E-4 1.E-3 1.E-2 1.E-1 1.E+0 0 600 1200 1800 # data contributorsShapley Value analyst d ata contributor ( data - only) d ata contributor ( composite) (c) -0.02 0 0.02 0.04 0.06 0 600 1200 1800Shapley Value # data contributors mean min max (d) Figure 15. (a) The SV of the analyst in the composite game vs. total utility obtained from the ML model; (b) the correlation between the data contributors’ SV in the composite game with that in the data-only game; (c) The SV of all players in the two games for different number of data contributors; (d) The mean, maximum, minimum of the data contributors’ SVs in the data-only game. SV as a proxy for other classiﬁers. We compute the SV for a logistic regression classiﬁer and a KNN classiﬁer trained on the same dataset namely Iris, and the result shows that the SVs under these two classiﬁers are indeed correlated (see Figure 16). The only caveat is that KNN SV does not distinguish between neighboring data points that have the same label. If this caveat is acceptable, we believe that the KNN SV provides an efﬁcient way to approximately assess the relative contribution of different data points for other classiﬁers as well. Moreover, for calculating the SV for general deep neural networks, we can take the deep features (i.e., the input to the last softmax layer) and corresponding labels, and train a KNN classiﬁer on the deep features. We calibrate K such that the 27 Figure 16. Comparison of the SV for a logistic regression and a KNN trained on the Iris dataset. resulting KNN mimics the performance of the original deep net and then employ the techniques presented in this paper to calculate a surrogate for the SV under the deep net. Implications of Task-Speciﬁc Data Valuation. Since the SV depends on the utility function asso- ciated with the game, data dividends based on the SV are contingent on the deﬁnition of model usefulness in speciﬁc ML tasks. The task-speciﬁc nature of our data valuation framework offers clear advantages—it allows to accommodate the variability of a data point’s utility from one application to another and assess its worth accordingly. Moreover, it enables the data buyer to defend against data poisoning attacks, wherein the attacker intentionally contributes adversarial training data points crafted speciﬁcally to degrade the performance of the ML model. In our framework, the “bad” training points will naturally have low SVs because they contribute little to boosting the performance of the model. Having the data values dependent on the ML task, on the other hand, may raise some concerns about whether the data values may inherit the ﬂaws of the ML models as to which the values are computed: if the ML model is biased towards a subpopulation with speciﬁc sensitive attributes (e.g., gender, race), will the data values reﬂect the same bias? Indeed, these concerns can be addressed by designing proper utility functions that devalue the unwanted properties of ML models. For instance, even if the ML model may be biased towards speciﬁc subpopulation, the buyer and data contributors can agree on a utility function that gives lower score to unfair models and compute the data values with respect to the concordant utility function. In this case, the training points will be appraised partially according to how much they contribute to improving the model fairness and the resulting data values would not be affected by the bias of the underlying model. Moreover, there is a venerable line of works studying algorithms to help improve fairness [ZWS+13, WGOS17, HPS+16]. These algorithms can also be applied to resolve the potential bias in value assignments. For instance, before providing the data to the data buyer, data contributors can preprocess the training data so that the “sanitized” data removes the information correlated with sensitive attributes [ZWS+13]. However, to ensure that the data values are accurately computed according to an appropriate utility function that the buyer and the data contributors agree on or that the models are trained with 28 proper fairness criteria, it is necessary to develop systems that can support transparent machine learning processes. Recent work has been studying training machine learning models on blockchains for removing the middleman to audit the model performance and enhancing transparency [blo]. We are currently implementing the data valuation framework on a blockchain-based data market, which can naturally resolve the problems of transparency and trust. Since the focus of this work is the algorithmic foundation of data valuation, we will leave the discussion of the combination of blockchains and data valuation for future work. 8. Related Work The problem of data pricing has received a lot of attention recently. The pricing schemes deployed in the existing data marketplaces are simplistic, typically setting a ﬁxed price for the whole or parts of the dataset. Before withdrawn by Microsoft, the Azure Data Marketplace adopted a subscription model that gave users access to a certain number of result pages per month [KUB+15]. Xignite [xig] sells ﬁnancial datasets and prices data based on the data type, size, query frequency, etc. There is rich literature on query-based pricing [KUB+15, KUB+13, KUB+12, DKB17, LK14, LM12, UBS16], aimed at design pricing schemes for ﬁne-grained queries over a dataset. In query-based pricing, a seller can assign prices to a few views and the price for any queries purchased by a buyer is automatically derived from the explicit prices over the views. Koutris et al. [KUB+15] identiﬁed two important properties that the pricing function must satisfy, namely, arbitrage-freeness and discount-freeness. The arbitrage-freeness indicates that whenever query Q1 discloses more information than query Q2, we want to ensure that the price of Q1 is higher than Q2; otherwise, the data buyer has an arbitrage opportunity to purchase the desired information at a lower price. The discount-freeness requires that the prices offer no additional discounts than the ones speciﬁed by the data seller. The authors further proved the uniqueness of the pricing function with the two properties, and established a dichotomy on the complexity of the query pricing problem when all views are selection queries. Li et al. [LM12] proposed additional criteria for data pricing, including non-disclosiveness (preventing the buyers from inferring unpaid query answers by analyzing the publicly available prices of queries) and regret-freeness (ensuring that the price of asking a sequence of queries in multiple interactions is not higher than asking them all-at-once), and investigated the class of pricing functions that meet these criteria. Zheng et al. [ZPW+19] studied how data uncertainty should affect the price of data, and proposed a data pricing framework for mobile crowd- sensed data. Recent work on query-based pricing focuses on enabling efﬁcient pricing over a wider range of queries, overcoming the issues such as double-charging arising from building practical data marketplaces [KUB+13, DKB17, UBS16], and compensating data owners for their privacy loss [LLMS17]. Due to the increasing pervasiveness of ML-based analytics, there is an emerging interest in studying the cost of acquiring data for ML. Chen et al. [CKK18, CKK17] proposed a formal framework to price ML model instances, wherein an optimization problem was formulated to ﬁnd the arbitrage-free price that maximizes the revenue of a seller. The model price can be also used for pricing its training dataset. This paper is complementary to these works in that we consider the scenario where the training set is contributed by multiple sellers and focus on the revenue sharing problem thereof. While the interaction between data analytics and economics has been extensively studied in the context of both relational database queries and ML, few works have dived into the vital problem of 29 allocating revenues among data owners. [KUB+12] presented a technique for fair revenue sharing when multiple sellers are involved in a relational query. By contrast, our paper focuses on the revenue allocation for nearest neighbor algorithms, which are widely adopted in the ML community. Moreover, our approach establishes a formal notion of fairness based on the SV. The use of the SV for pricing personal data can be traced back to [KPR01], which studied the SV in the context of marketing survey, collaborative ﬁltering, and recommendation systems. [CL17] also applied the SV to quantify the value of personal information when the population of data contributors can be modeled as a network. [MAS+13] showed that for speciﬁc network games, the exact SV can be computed efﬁciently. There exist various methods to rank the importance of training data, which can also potentially be used for data valuation. For instance, inﬂuence functions [KL17] approximate the change of the model performance after removing a training point for smooth parametric ML models. Ogawa et al. [OST13] proposed rules to identify and remove the least inﬂuential data when training support vector machines (SVM) to reduce the computation cost. However, unlike the SV, these approaches do not satisfy the group rationality, fairness, and additivity properties simultaneously. Despite the desirable properties of the SV, computing the SV is known to be expensive. In its most general form, the SV can be #P-complete to compute [DP94]. For bounded utility functions, Maleki et al. [MTTH+13] described a sampling-based approach that requires O(N log N) samples to achieve a desired approximation error. By taking into account special properties of the utility function, one can derive more efﬁcient approximation algorithms. For instance, Fatima et al. [FWJ08] proposed a probabilistic approximation algorithm with O(N) complexity for weighted voting games. Ghorbani et al. [GZ19] developed two heuristics to accelerate the estimation of the SV for complex learning algorithms, such as neural networks. One is to truncate the calculation of the marginal contributions as the change in performance by adding only one more training point becomes smaller and smaller. Another is to use one-step gradient to approximate the marginal contribution. The authors also demonstrate the use of the approixmate SV for outlier identiﬁcation and informed acquisition of new training data. However, their algorithms do not provide any guarantees on the approximation error, thus limiting its viability for practical data valuation. Raskar et al [RVSS19] presented a taxonomy of data valuation problems for data markets and discussed challenges associated with data sharing. 9. Conclusion The SV has been long advocated as a useful economic concept to measure data value but has not found its way into practice due to the issue of exponential computational complexity. This paper presents a step towards practical algorithms for data valuation based on the SV. We focus on the case where data are used for training a KNN classiﬁer and develop algorithms that can calculate data values exactly in quasi-linear time and approximate them in sublinear time. We extend the algorithms to the case of KNN regression, the situations where a contributor can own multiple data points, and the task of valuing data contributions and analytics simultaneously. For future work, we will integrate our proposed data valuation algorithms into the clinical data market that we are currently building. We will also explore efﬁcient algorithms to compute the data values for other popular ML algorithms such as gradient boosting, logistic regression, and deep neural networks. 30 Acknowledgement This work is supported in part by the Republic of Singapores National Research Foundation through a grant to the Berkeley Education Alliance for Research in Singapore (BEARS) for the Singapore-Berkeley Building Efﬁciency and Sustainability in the Tropics (SinBerBEST) Program. This work is also supported in part by the CLTC (Center for Long-Term Cybersecurity); FORCES (Foundations Of Resilient CybEr-Physical Systems), which receives support from the National Science Foundation (NSF award numbers CNS-1238959, CNS-1238962, CNS-1239054, CNS1239166); the National Science Foundation under Grant No. TWC-1518899; and DARPA FA8650-18-2-7882. CZ and the DS3Lab gratefully acknowledge the support from the Swiss National Science Foundation (Project Number 200021 184628) and a Google Focused Research Award. References [blo] A Decentralized Kaggle: Inside Algorithmias Approach to Blockchain-Based AI Competi- tions, https://towardsdatascience.com/a-decentralized-kaggle-inside-algorithmias-approach- to-blockchain-based-ai-competitions-8c6aec99e89b. [DAW] Dawex, https://www.dawex.com/en/. [BIG] Google bigquery, https://cloud.google.com/bigquery/. [IOT] Iota, https://data.iota.org/#/. [xig] Xignite, https://apollomapping.com/. [AWY16] D. ADENIYI, Z. WEI, and Y. YONGQUAN, Automated web usage data mining and recommendation system using k-nearest neighbor (knn) classiﬁcation method, Applied Computing and Informatics 12 (2016), 90–108. [AFGR16] G. AMATO, F. FALCHI, C. GENNARO, and F. RABITTI, Yfcc100m-hnfc6: a large-scale deep features benchmark for similarity search, in International Conference on Similarity Search and Applications, Springer, 2016, pp. 196–209. [BKZ05] J. J. BARTHOLDI and E. KEMAHLIO˘GLU-ZIYA, Using shapley value to allocate savings in a supply chain, in Supply chain optimization, Springer, 2005, pp. 169–208. [BS13] J. BREMER and M. SONNENSCHEIN, Estimating shapley values for fair proﬁt distribution in power planning smart grid coalitions, in German Conference on Multiagent System Technologies, Springer, 2013, pp. 208–221. [Cha02] M. S. CHARIKAR, Similarity estimation techniques from rounding algorithms, in Proceedings of the thiry-fourth annual ACM symposium on Theory of computing, ACM, 2002, pp. 380–388. [CKK17] L. CHEN, P. KOUTRIS, and A. KUMAR, Model-based pricing: Do not pay for more than what you learn!, in Proceedings of the 1st Workshop on Data Management for End-to-End Machine Learning, ACM, 2017, p. 1. [CKK18] L. CHEN, P. KOUTRIS, and A. KUMAR, Model-based pricing for machine learning in a data marketplace, arXiv preprint arXiv:1805.11450 (2018). [CL17] M. CHESSA and P. LOISEAU, A cooperative game-theoretic approach to quantify the value of personal data in networks, in Proceedings of the 12th workshop on the Economics of Networks, Systems and Computation, ACM, 2017, p. 9. [DAMZ18] D. DAO, D. ALISTARH, C. MUSAT, and C. ZHANG, Databright: Towards a global exchange for decentralized data ownership and trusted computation, arXiv preprint arXiv:1802.04780 (2018). [DIIM04] M. DATAR, N. IMMORLICA, P. INDYK, and V. S. MIRROKNI, Locality-sensitive hashing scheme based on p-stable distributions, in Proceedings of the twentieth annual symposium on Computa- tional geometry, ACM, 2004, pp. 253–262. 31 [DKB17] S. DEEP, P. KOUTRIS, and Y. BIDASARIA, Qirana demonstration: real time scalable query pricing, PVLDB 10 (2017), 1949–1952. [DDS+09] J. DENG, W. DONG, R. SOCHER, L.-J. LI, K. LI, and L. FEI-FEI, ImageNet: A Large-Scale Hierarchical Image Database, in CVPR09, 2009. [DP94] X. DENG and C. H. PAPADIMITRIOU, On the complexity of cooperative solution concepts, Mathe- matics of Operations Research 19 (1994), 257–266. [Dud76] S. A. DUDANI, The distance-weighted k-nearest-neighbor rule, IEEE Transactions on Systems, Man, and Cybernetics (1976), 325–327. [FWJ08] S. S. FATIMA, M. WOOLDRIDGE, and N. R. JENNINGS, A linear approximation method for the shapley value, Artiﬁcial Intelligence 172 (2008), 1673–1699. [GZ19] A. GHORBANI and J. ZOU, Data shapley: Equitable valuation of data for machine learning, arXiv preprint arXiv:1904.02868 (2019). [GIM+99] A. GIONIS, P. INDYK, R. MOTWANI, and OTHERS, Similarity search in high dimensions via hashing, in Vldb, 99, 1999, pp. 518–529. [HPIM12] S. HAR-PELED, P. INDYK, and R. MOTWANI, Approximate nearest neighbor: Towards removing the curse of dimensionality, Theory of computing 8 (2012), 321–350. [HPS+16] M. HARDT, E. PRICE, N. SREBRO, and OTHERS, Equality of opportunity in supervised learning, in Advances in neural information processing systems, 2016, pp. 3315–3323. [HE15] J. HAYS and A. A. EFROS, Large-scale image geolocalization, in Multimodal Location Estimation of Videos and Images, Springer, 2015, pp. 41–62. [HKC12] J. HE, S. KUMAR, and S.-F. CHANG, On the difﬁculty of nearest neighbor search, in Proceedings of the 29th International Coference on International Conference on Machine Learning, Omnipress, 2012, pp. 41–48. [HZRS16] K. HE, X. ZHANG, S. REN, and J. SUN, Deep residual learning for image recognition, in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 770–778. [HDY+18] N. HYNES, D. DAO, D. YAN, R. CHENG, and D. SONG, A demonstration of sterling: a privacy- preserving data marketplace, PVLDB 11 (2018), 2086–2089. [JDW +19] R. JIA, D. DAO, B. WANG, F. A. HUBIS, N. HYNES, B. LI, C. ZHANG, D. SONG, and C. SPANOS, Towards efﬁcient data valuation based on the shapley value, AISTATS (2019). [KPR01] J. KLEINBERG, C. H. PAPADIMITRIOU, and P. RAGHAVAN, On the value of private information, in Proceedings of the 8th conference on Theoretical aspects of rationality and knowledge, Morgan Kaufmann Publishers Inc., 2001, pp. 249–257. [KL17] P. W. KOH and P. LIANG, Understanding black-box predictions via inﬂuence functions, in International Conference on Machine Learning, 2017, pp. 1885–1894. [KUB +12] P. KOUTRIS, P. UPADHYAYA, M. BALAZINSKA, B. HOWE, and D. SUCIU, Querymarket demonstra- tion: Pricing for online data markets, PVLDB 5 (2012), 1962–1965. [KUB +13] P. KOUTRIS, P. UPADHYAYA, M. BALAZINSKA, B. HOWE, and D. SUCIU, Toward practical query pricing with querymarket, in proceedings of the 2013 ACM SIGMOD international conference on management of data, ACM, 2013, pp. 613–624. [KUB +15] P. KOUTRIS, P. UPADHYAYA, M. BALAZINSKA, B. HOWE, and D. SUCIU, Query-based data pricing, Journal of the ACM (JACM) 62 (2015), 43. [LC10] Y. LECUN and C. CORTES, MNIST handwritten digit database, (2010). Available at http:// yann.lecun.com/exdb/mnist/. [LLMS17] C. LI, D. Y. LI, G. MIKLAU, and D. SUCIU, A theory of pricing private data, Communications of the ACM 60 (2017), 79–86. [LM12] C. LI and G. MIKLAU, Pricing aggregate queries in a data marketplace., in WebDB, 2012, pp. 19–24. 32 [LZZ+12] C. LI, S. ZHANG, H. ZHANG, L. PANG, K. LAM, C. HUI, and S. ZHANG, Using the k-nearest neigh- bor algorithm for the classiﬁcation of lymph node metastasis in gastric cancer, Computational and mathematical methods in medicine 2012 (2012). [LK14] B.-R. LIN and D. KIFER, On arbitrage-free pricing for general data queries, PVLDB 7 (2014), 757–768. [Mal15] S. MALEKI, Addressing the computational issues of the Shapley value with applications in the smart grid, Ph.D. thesis, University of Southampton, 2015. [MTTH+13] S. MALEKI, L. TRAN-THANH, G. HINES, T. RAHWAN, and A. ROGERS, Bounding the estimation error of sampling-based shapley value approximation, arXiv preprint arXiv:1306.4265 (2013). [MAS +13] T. P. MICHALAK, K. V. AADITHYA, P. L. SZCZEPANSKI, B. RAVINDRAN, and N. R. JENNINGS, Efﬁcient computation of the shapley value for game-theoretic network centrality, Journal of Artiﬁcial Intelligence Research 46 (2013), 607–650. [MA98] D. M. MOUNT and S. ARYA, Ann: library for approximate nearest neighbour searching, (1998). [OST13] K. OGAWA, Y. SUZUKI, and I. TAKEUCHI, Safe screening of non-support vectors in pathwise svm computation, in International Conference on Machine Learning, 2013, pp. 1382–1390. [RVSS19] R. RASKAR, P. VEPAKOMMA, T. SWEDISH, and A. SHARAN, Data markets to support ai for all: Pricing, valuation and governance, arXiv preprint arXiv:1905.06462 (2019). [Sha53] L. S. SHAPLEY, A value for n-person games, Contributions to the Theory of Games 2 (1953), 307–317. [SI07] C. SIAGIAN and L. ITTI, Rapid biologically-inspired scene classiﬁcation using features shared with visual attention, IEEE transactions on pattern analysis and machine intelligence 29 (2007), 300–312. [SVI +16] C. SZEGEDY, V. VANHOUCKE, S. IOFFE, J. SHLENS, and Z. WOJNA, Rethinking the inception architecture for computer vision, in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 2818–2826. [Top06] F. TOPSOK, Some bounds for the logarithmic function, Inequality theory and applications 4 (2006), 137. [UBS12] P. UPADHYAYA, M. BALAZINSKA, and D. SUCIU, How to price shared optimizations in the cloud, PVLDB 5 (2012), 562–573. [UBS16] P. UPADHYAYA, M. BALAZINSKA, and D. SUCIU, Price-optimal querying with data apis, PVLDB 9 (2016), 1695–1706. [WGOS17] B. WOODWORTH, S. GUNASEKAR, M. I. OHANNESSIAN, and N. SREBRO, Learning non- discriminatory predictors, arXiv preprint arXiv:1702.06081 (2017). [ZWS+13] R. ZEMEL, Y. WU, K. SWERSKY, T. PITASSI, and C. DWORK, Learning fair representations, in International Conference on Machine Learning, 2013, pp. 325–333. [ZPW+19] Z. ZHENG, Y. PENG, F. WU, S. TANG, and G. CHEN, Arete: On designing joint online pricing and reward sharing mechanisms for mobile data markets, IEEE Transactions on Mobile Computing (2019). 33 Appendix A. Additional Experiments A.1. Runtime Comparision for Computing the Unweighted KNN SV. For each dataset, we ran- domly selected 100 test points, computed the SV of all training points with respect to each test point, and reported the average runtime across all test points. The results for K = 2, 5 are presented in Figure 17. We can see that the LSH-based method can bring a 3×-5× speed-up compared with the exact algorithm. Figure 17. Average runtime of the exact and the LSH-based approximation algorithm for computing the unweighted KNN SV for a single test point. We take ϵ, δ = 0.1 and K = 2, 5. Dataset Size Estimated Contrast K=2 K=5 Runtime (Exact) Runtime (LSH) Runtime (Exact) Runtime (LSH) CIFAR-10 6E+4 1.2802 0.83 0.25 0.82 0.26 ImageNet 1E+6 1.2163 12.71 3.29 12.57 3.25 Yahoo10m 1E+7 1.3456 198.73 41.83 200.06 39.20 Appendix B. Proof of Lemma 1 Proof. si − sj(35) = ∑ S⊆I\\{i} |S|!(N − |S| − 1)! N! [ν(S ∪ {i}) − ν(S)] − ∑ S⊆I\\{j} |S|!(N − |S| − 1)! N! [ν(S ∪ {j}) − ν(S)] (36) = ∑ S⊆I\\{i,j} |S|!(N − |S| − 1)! N! [ν(S ∪ {i}) − ν(S ∪ {j})] + ∑ S∈{T |T ⊆I,i/∈T,j∈T } |S|!(N − |S| − 1)! N! [ν(S ∪ {i}) − ν(S)] − ∑ S∈{T |T ⊆I,i∈T,j/∈T } |S|!(N − |S| − 1)! N! [ν(S ∪ {j}) − ν(S)] (37) = ∑ S⊆I\\{i,j} |S|!(N − |S| − 1)! N! [ν(S ∪ {i}) − ν(S ∪ {j})] + ∑ S ′⊆I\\{i,j} (|S ′| + 1)!(N − |S ′| − 2)! N! [ν(S ′ ∪ {i}) − ν(S ′ ∪ {j})] (38) = ∑ S⊆I\\{i,j} ( |S|!(N − |S| − 1)! N! + (|S| + 1)!(N − |S| − 2)! N! )[ ν(S ∪ {i}) − ν(S ∪ {j})] (39) = 1 N − 1 ∑ S⊆I\\{i,j} 1 C|S| N−2 [ν(S ∪ {i}) − ν(S ∪ {j})] .(40) □ 34 Appendix C. Proof of Theorem 2 Proof. We ﬁrst observe that if the true Shapley value |sαi| ⩽ min( 1 i , 1 K ), then |si| ⩽ ϵ for i ⩾ i∗ = max(K, ⌈1/ϵ⌉). Hence, when i ⩾ i∗, the approximation error is given by |ˆsαi − sαi| = |sαi| ⩽ ϵ.(41) When i ⩽ i∗ − 1, ˆsαi and sαi follow the same recursion, i.e., ˆsαi − ˆsαi+1 = sαi − sαi+1 = 1 [yαi = ytest] − 1 [yαi+1 = ytest] K min(K − 1, i − 1) + 1 i .(42) As a result, we have |ˆsαi − sαi| = |ˆsαi+1 − sαi+1| = · · · = |ˆsαi∗ − sαi∗ | ⩽ ϵ(43) To sum up, |ˆsαi − sαi| ⩽ ϵ for all i = 1, . . . , N, provided that |sαi| ⩽ min( 1 i , 1 K ). In the following, we will prove that the aforementioned condition is satisﬁed. We can convert the recursive expression of the KNN Shapley value in Theorem 1 to a non- recursive one: sαN = 1 [yαN = ytest] N (44) sαi = 1 [yαi = ytest] i − N∑ j=i+1 1 [yαj = ytest] j(j − 1) for i ⩾ K(45) sαi = 1 [yαi = ytest] K − N∑ j=K+1 1 [yαj = ytest] j(j − 1) for i ⩽ K − 1(46) We examine the bound on the absolute value of the Shapley value in three cases: (1) i = N, (2) i ⩾ K, and (3) i ⩽ K − 1. Case (1). It is easy to verify that |sαN| ⩽ 1 N . Case (2). We can bound the second term in (45) by 0 ⩽ N∑ j=i+1 1 [yαj = ytest] j(j − 1) ⩽ N∑ j=i+1 1 j(j − 1) = N∑ j=i+1( 1 j − 1 − 1 j ) = 1 i − 1 N (47) Thus, sαi can be bounded by −( 1 i − 1 N ) ⩽ sαi ⩽ 1 i ,(48) which yields the bound on the absolute value of sαi: |sαi| ⩽ 1 i .(49) Case (3). The absolute value of sαi for i ⩽ K − 1 can be bounded using a similar technique as in Case (2). By (46), we have −( 1 K − 1 N ) ⩽ sαi ⩽ 1 K (50) Therefore, |sαi| ⩽ 1/K. Summarizing the results in Case (1), (2), and (3), we obtain |sαi| ⩽ min(1/i, 1/K) for i = 1, . . . , N. □ 35 Appendix D. Proof of Theorem 3 Proof. For the hashing function h(x) = ⌊ wT x+b t ⌋, [DIIM04] have shown that P(h(xi) = h(xtest)) = fh(∥xi − xtest∥p)(51) where the function fh(a) = ∫t 0 1 a fp( z a (1 − z t )dz is monotonically decreasing with a. fp is the probability density function of the absolute value of a p-stable random variable. Suppose the data are normalized by a factor such that Dmean = 1. Since such a normalization does not change the nearest neighbor search results, Dk = 1/Ck for k = 1, . . . , K. Denote the probability for one random test point xtest and a random training point to have the same code with one hash function by prand and the probability for xtest and its k-nearest neighbor to have the same code by pk nn. According to (51), prand = fh(1)(52) and pnn,k = fh(1/Ck)(53) because the expected distance between xtest and a random training point is Dmean = 1, and the expected distance between xtest and its k-nearest neighbor is 1/Ck. Let Ek denote the event that the k-nearest neighbor of xtest is included by one of the hash tables. Then, the probability of the inclusion of all K nearest neighbors is P(E1, . . . , EK) = 1 − P(∪K k=1¯Ek)(54) ⩾ 1 − K∑ k=1 P(¯Ek).(55) We want to make sure that P(E1, . . . , EK) ⩾ 1 − δ, so it sufﬁces to let P(¯Ek) ⩽ δ/K for all k = 1, . . . , K. Suppose there are m hash bits in one table and l hash tables in LSH. The probability that the true k-nearest neighbor has the same code as the query in one hash table is pm nn,k. Hence, the probability that the true k-nearest neighbor is missed by l hash tables is P(¯Ek) = (1 − pm nn,k)l. In order to ensure P(¯Ek) ⩽ δ/K, we need l ⩾ log δ K log(1 − pm nn,k) (56) The RHS is upper bounded by − log δ K pm nn,k = p−m nn,k log K δ . Therefore, it sufﬁces to ensure l ⩾ p−m nn,k log K δ (57) Note that pnn,k = p log pnn,k log prand rand and we can choose Npm rand = O(1), i.e., m = O( log N log p −1 rand ), as discussed in [GIM+99]. Hence, pm nn,k = pm log pnn,k log prand rand = O(( 1 N ) log pnn,k log prand ) = O(N−g(Ck))(58) where g(Ck) = log pnn,k log prand = log fh(1/Ck) log fh(1) . Plugging (58) into (56), we obtain l ⩾ O(Ng(Ck) log K δ )(59) 36 In order to guarantee P(¯Ek) ⩽ δ/K for all k = 1, · · · , K, the number of hash tables needed is O(Ng(CK) log K δ )(60) □ Appendix E. Detailed Algorithms and Proofs for the Extensions We ﬁrst extend the algorithms to calculate the SV for unweighted KNN regression and weighted KNN. Further, we address the data valuation problem wherein a data curator contributes multiple data points. We then discuss how to valuate the parties offering computation in the data market. E.1. Unweighted KNN Regression. For regression tasks, we deﬁne the utility function by the negative mean square error of an unweighted KNN regressor: ν(S) = − ( 1 K min{K,|S|}∑ k=1 yαk(S) − ytest )2 (61) The following theorem provides a simple iterative procedure to compute the SV for unweighted KNN regression. The derivation of the theorem requires to analyze the utility difference between two adjacent training points, similar to KNN classiﬁcation. THEOREM 6. Consider the KNN regression utility function in (61). Then, the SV of each training point can be calculated recursively as follows: sαN = − K − 1 NK yαN [ 1 K yαN − 2ytest + 1 N − 1 ∑ l∈I\\{N} yαl ] − 1 N [ 1 K yαN − ytest ]2 (62) sαi = sαi+1 + 1 K (yαi+1 − yαi) min{K, i} i ( 1 K N∑ l=1 A(l) i yαl − 2ytest)(63) where A(l) i =  | | min{K−1,i−1} i−1 if 1 ⩽ l ⩽ i − 1 1 if l ∈ {i, i + 1} min{K,l−1} min{K−1,l−2}i (l−1)(l−2) min{K,i} if i + 2 ⩽ l ⩽ N (64) According to (63), two adjacent training points will have the same SV if they have the same label. Otherwise, their SV difference will depend on three terms: (1) their difference in the labels yαi+1 − yαi, (2) the rank of their distances to the test point min(K,i) i , and (3) the goodness of ﬁt term 1 K ∑N l=1 A(l) i yαl − 2ytest of a “weighted” KNN regression model in which A(l) i stands for the weight. By simple algebraic operations, it can be obtained that yαi and yαi+1 are weighted highest among all training points; therefore, the third term can be roughly thought of as how much error yαi and yαi+1 induce for predicting ytest. If the goodness of ﬁt term represents a positive error and yαi > yαi+1, then adding (xαi, yαi) into the training dataset will even enlarge the positive prediction error. Thus, (xαi, yαi) is less valuable than (xαi+1, yαi+1) in terms of the SV. Similar intuition about the interaction between the ﬁrst and third term can be established when yαi < yαi+1. Moreover, the training points closer to the test point are more inﬂuential to the prediction result; this phenomenon is captured by the second term. In summary, the SV difference between two adjacent training points 37 is large when their labels differ largely, their distances to the test point are small, and their presence in the training set leads to large prediction errors. Proof of Theorem 6. W.l.o.g., we assume that x1, . . . , xn are sorted according to their similarity to xtest, that is, xi = xαi. We split a subset S ⊆ I \\ {i, i + 1} into two disjoint sets S1 and S2 such that S = S1 ∪ S2 and S1 ∩ S2 = ∅. Given two neighboring points with indices i, i + 1 ∈ I, we constrain S1 and S2 to S1 ⊆ {1, . . . , i − 1} and S2 ⊆ {i + 2, . . . , N}. We analyze the difference between si and si+1 by considering the following cases: Case 1. Consider the case |S1| ⩾ K. We know that i > K and therefore ν(S∪{i}) = ν(S∪{i+1}) = ν(S). From Lemma 1, it follows that si − si+1 = 1 N − 1 N−2∑ k=0 1 ( N−2 k ) ∑ S1⊆{1,...,i−1}, S2⊆{i+2,...,N}: |S1|+|S2|=k,|S1|⩾K [ν(S ∪ {i}) − ν(S ∪ {i + 1})] = 0. Case 2. Consider the case |S1| < K. The difference between ν(S ∪ {i}) and ν(S ∪ {i + 1}) can be expressed as ν(S ∪ {i}) − ν(S ∪ {i + 1}) =( 1 K K∑ j=1 yαj(S∪{i+1}) − ytest)2 − ( 1 K K∑ j=1 yαj(S∪{i}) − ytest)2 = 1 K (yi+1 − yi) · ( 1 K (yi+1 + yi) − 2ytest + 2 K ∑ j=1,...,K−1 yαj(S) ) By Lemma 1, the Shapley difference between i and i + 1 is si − si+1 = 1 K (yi+1 − yi) · ( 1 N − 1 N−2∑ k=0 1 ( N−2 k ) ∑ S1⊆{1,...,i−1}, S2⊆{i+2,...,N}: |S1|+|S2|=k,|S1|⩽K−1 ( 1 K (yi+1 + yi) − 2ytest) | {z } U1 + 2 K 1 N − 1 N−2∑ k=0 1 ( N−2 k ) ∑ S1⊆{1,...,i−1}, S2⊆{i+2,...,N}: |S1|+|S2|=k,|S1|⩽K−1 ∑ j=1,...,K−1 yαj(S) | {z } U2 ) We ﬁrstly simplify U1. Note that 1 K (yi+1 + yi) − 2ytest does not depend on the summation; as a result, we have U1 = ( 1 K (yi+1 + yi) − 2ytest) 1 N − 1 N−2∑ k=0 1 ( N−2 k ) ( ∑ S1⊆{1,...,i−1}, S2⊆{i+2,...,N}: |S1|+|S2|=k,|S1|⩽K−1 1) 38 = ( 1 K (yi+1 + yi) − 2ytest) 1 N − 1 N−2∑ k=0 1 ( N−2 k ) min(K−1,k)∑ m=0 ( i − 1 m )( N − i − 1 k − m ) (65) The sum of binomial coefﬁcients in (65) can be further simpliﬁed as follows: N−2∑ k=0 1 ( N−2 k ) min(K−1,k)∑ m=0 ( i − 1 m )( N − i − 1 k − m ) = min(K−1,i−1)∑ m=0 N−i−1∑ k=0 ( i−1 m )( N−i−1 k ) (N−2 m+k ) = min(K−1,i−1)∑ m=0 N − 1 i = min(K, i) N − 1 i where the second equality follows from the binomial coefﬁcient identity ∑M j=0 (N i )(M j ) (N+M i+j ) = M+N+1 N+1 . Hence, U1 = ( 1 K (yi+1 + yi) − 2ytest) min(K, i) i Then, we analyze U2. We let ∑ S1⊆{1,...,i−1}, S2⊆{i+2,...,N}: |S1|+|S2|=k,|S1|⩽K−1 ∑ j=1,...,K−1 yαj(S) = ∑ l∈I\\{i,i+1} clyl(66) where cl counts the number of occurrences of yl in the left-hand side expression and cl = { ∑min(K−2,k−1) m=0 ( i−2 m )( N−i−1 k−m−1) if l ∈ {1, . . . , i − 1} ∑min(K−2,k−1) m=0 ( l−3 m )( N−l k−m−1) if l ∈ {i + 2, . . . , N} (67) Plugging in (66) and (67) into U2 yields U2 = 2 K(N − 1) N−2∑ k=0 1 ( N−2 k ) [ ∑ l∈{1,...,i−1} min(K−2,k−1)∑ m=0 ( i − 2 m )( N − i − 1 k − m − 1 ) yl + ∑ l∈{i+2,...,N} min(K−2,k−1)∑ m=0 ( l − 3 m )( N − l k − m − 1 ) yl ] = 2 K(N − 1) [ ∑ l∈{1,...,i−1} yl ] · [ N−2∑ k=0 1 ( N−2 k ) min(K−2,k−1)∑ m=0 ( i − 2 m )( N − i − 1 k − m − 1 ) | {z } U21 ] + 2 K(N − 1) [ ∑ l∈{i+2,...,N} yl · N−2∑ k=0 1 ( N−2 k ) min(K−2,k−1)∑ m=0 ( l − 3 m )( N − l k − m − 1 ) | {z } U22 ] (68) 39 Using the binomial coefﬁcient identity ∑M j=0 (N i )(M j ) (N+M+1 i+j+1 ) = (i+1)(M+N+2) (N+2)(N+1) , we obtain U21 = min(K−2,i−2)∑ m=0 N−i−1∑ k=0 ( i−2 m )( N−i−1 k ) ( N−2 k+m+1) = min(K−2,i−2)∑ m=0 N − 1 (i − 1)i (m + 1) = N − 1 (i − 1)i min(K, i) min(K − 1, i − 1) 2 (69) and U22 = min(K−2,l−3)∑ m=0 N−l∑ k=0 ( l−3 m )( N−l k ) ( N−2 k+m+1) = min(K−2,l−3)∑ m=0 N − 1 (l − 1)(l − 2) (m + 1) = N − 1 (l − 1)(l − 2) min(K, l − 1) min(K − 1, l − 2) 2 (70) Now, we plug (69) and (70) into the expression of U2 in (68). Rearranging (68) gives us U2 = 1 K ∑ l∈{1,...,i−1} yl min(K, i) min(K − 1, i − 1) (i − 1)i + 1 K ∑ l∈{i+2,...,N} yl min(K, l − 1) min(K − 1, l − 2) (l − 1)(l − 2) Therefore, we have si − si+1 = 1 K (yi+1 − yi)(U1 + U2) = 1 K (yi+1 − yi) · [( 1 K (yi+1 + yi) − 2ytest) min(K − 1, i − 1) + 1 i + 1 K ∑ l∈{1,...,i−1} yl min(K, i) min(K − 1, i − 1) (i − 1)i + 1 K ∑ l∈{i+2,...,N} yl min(K, l − 1) min(K − 1, l − 2) (l − 1)(l − 2) ] Now, we analyze the formula for sN, the starting point of the recursion. Since xN is farthest to xtest among all training points, xN results in non-zero marginal utility only when it is added to a set of size smaller than K. Hence, sN can be written as sN = 1 N K−1∑ k=0 1 ( N−1 k ) ∑ |S|=k,S⊆I\\{N}, ν(S ∪ {N}) − ν(S) = 1 N K−1∑ k=1 1 ( N−1 k ) ∑ |S|=k,S⊆I\\{N} [( 1 K ∑ i∈S yi − ytest)2 − ( 1 K ∑ i∈S∪{N} yi − ytest)2] + ν({N}) N = 1 N K−1∑ k=1 1 ( N−1 k ) ∑ |S|=k,S⊆I\\{N} [(− 1 K yN) · ( 2 K ∑ i∈S yi + 1 K yN − 2ytest)] + ν({N}) N 40 = − K − 1 NK yN( 1 K yN − 2ytest) − 2 NK2 yN K−1∑ k=1 ( N−2 k−1 ) ( N−1 k ) ∑ l∈I\\{N} yl + 1 N ν({N}) = − 1 N yN [ K − 1 K ( 1 K yN − 2ytest) + 2 K2 ( ∑ l∈I\\{N} yl) K−1∑ k=1 k N − 1 ] + ν({N}) N = − K − 1 NK yN [ 1 K yN − 2ytest + 1 N − 1 ∑ l∈I\\{N} yl ] + ν({N}) N □ E.2. Weighted KNN. A weighted KNN estimate produced by a training set S can be expressed as ˆy(S) = min{K,|S|}∑ k=1 wαk(S)yαk(71) where wαk(S) is the weight associated with the kth nearest neighbor of the test point in S. The weight assigned to a neighbor in the weighted KNN estimate often varies with the neighbor-to-test distance so that the evidence from more nearby neighbors are weighted more heavily [Dud76]. Correspondingly, we deﬁne the utility function associated with weighted KNN classiﬁcation and regression tasks as ν(S) = min{K,|S|}∑ k=1 wαk(S)1 [yαk(S) = ytest](72) and ν(S) = − ( min{K,|S|}∑ k=1 wαk(S)yαk(S) − ytest )2.(73) For weighted KNN classiﬁcation and regression, the SV can no longer be computed exactly in O(N log(N)) time. The next theorem shows that it is however possible to compute the exact SV for weighted KNN in O(NK) time. The theorem applies the deﬁnition (2) to calculating the SV and relies on the following idea to circumvent the exponential complexity: when applying (2) to KNN, we only need to focus on the sets S whose utility might be affected by the addition of ith training instance. Moreover, since there are only NK possible distinctive combinations for K nearest neighbors, the number of distinct utility values for all S ⊆ I is upper bounded by NK, in contrast to 2N for general utility functions. THEOREM 7. Consider the utility function in (72) or (73) with some weights wαk(S). Let Bk(i) = {S : |S| = k, i /∈ S, S ⊆ I}, for i = 1, . . . , N and k = 0, . . . , K. Let r(·) be a function that maps the set of training data to their ranks of similarity to xtest. Then, the SV of each training point can be calculated recursively as follows: sαN = 1 N K−1∑ k=0 1 ( N−1 k ) ∑ S∈Bk(αN) [ν(S ∪ {αN}) − ν(S)] (74) sαi+1 = sαi + 1 N − 1 N−2∑ k=0 1 ( N−2 k ) ∑ S∈Di,k Ai,k(75) 41 where Di,k = { Bk(αi) ∩ Bk(αi+1), 0 ⩽ k ⩽ K − 2 BK−1(αi) ∩ BK−1(αi+1, K − 1 ⩽ k ⩽ N − 2 (76) and Ai,k = { 1, 0 ⩽ k ⩽ K − 2 (N−max r(S∪{αi,αi+1}) k−K+1 ) , K − 1 ⩽ k ⩽ N − 2 (77) Note that |Bk(i)| ⩽ ( N−1 k ) . Thus, the complexity for computing the weighted KNN SV is at most N(N − 1) × ( N − 1 K − 1 ) ⩽ ( e K − 1 )K−1NK+1(78) Proof of Theorem 7. Without loss of generality, we assume that the training points are sorted according to their distance to xtest, such that d(x1, xtest) ⩽ . . . ⩽ d(xN, xtest). We start by analyzing the SV for xN. Since the farthest training point does not affect the utility of S unless |S| ⩽ K − 1, we have sN = 1 N K−1∑ k=0 1 ( N−1 k ) ∑ |S|=k,S⊆I\\{N} [ν(S ∪ {N}) − ν(S)] For i ⩽ N − 1, the application of Lemma 1 yields si − si+1 = 1 N − 1 N−2∑ k=0 ∑ |S|=k,S⊆I\\{i,i+1} 1 ( N−2 k ) · [ν(S ∪ {i}) − ν(S ∪ {i + 1})] (79) Recall that for KNN utility functions, ν(S) only depends on the K training points closest to xtest. Therefore, we can also write si − si+1 as follows: si − si+1 = 1 N − 1 K−1∑ k ′=0 ∑ S ′∈Bk ′ (i)∩Bk ′ (i+1) M k ′ i,i+1[ν(S ′ ∪ {i}) − ν(S ′ ∪ {i + 1})] (80) which can be computed in at most ∑K−1 k ′=0 ( N−2 k ′ ) ∼ O(NK), in contrast to O(2N−2) with (79). Our goal is thus to ﬁnd Mk ′ i,i+1 such that the right-hand sides of (80) and (79) are equal. More speciﬁcally, for each S ′ ∈ Bk ′(i) ∩ Bk ′(i + 1), we want to count the number of S ⊆ I \\ {i, i + 1} such that |S| = k, and ν(S ∪ {i}) = ν(S ′ ∪ {i}) and ν(S ∪ {i + 1}) = ν(S ′ ∪ {i + 1}); denoting the count by Ck,k ′ i,i+1, we have M k ′ i,i+1 = N−2∑ k=0 Ck,k ′ i,i+1/( N − 2 k ) .(81) When k ′ ⩽ K − 2, only S = S ′ satisﬁes ν(S ∪ {i}) = ν(S ′ ∪ {i}) and ν(S ∪ {i + 1}) = ν(S ′ ∪ {i + 1}). Therefore, Ck,k ′ i,i+1 = { 1 if k ′ ⩽ K − 2 and k = k ′ 0 otherwise (82) When k ′ = K − 1, there will be multiple subsets S of I \\ {i, i + 1} that obey ν(S ∪ {i}) = ν(S ′ ∪ {i}) and ν(S ∪ {i + 1}) = ν(S ′ ∪ {i + 1}). Let r denote the index of the training point that is farthest to xtest 42 among S ∪ {i, i + 1}, i.e., r = max S ∪ {i, i + 1}. Note that adding any training points with indices larger than r into S ′ ∪ {i} or S ′ ∪ {i + 1} would not affect their utility. Hence, Ck,k ′ i,i+1 = { ( N−r k−K+1) if k ′ = K − 1, k ⩾ k ′ 0 otherwise (83) Combining (80), (81), (82), and (83) yields the recursion in (74) and (75). □ E.3. Multiple Data Per Contributor. Now, we investigate the method to compute the SV when each seller provides more than one data instance. The goal is to fairly value individual sellers in lieu of individual training points. Following the previous notations, we still use I = {1, . . . , N} to denote the set of all training instances and use Is to denote the set of all sellers, i.e., Is = {1, . . . , M}. The number of training instances owned by jth seller is Nj. We denote the ith training point contributed by jth seller as x (i) j . Without loss of generality, we assume that every seller’s data is sorted such that d(x (1) j , xtest) ⩽ . . . ⩽ d(x (Nj) j , xtest). Let h(i) denote the owner of ith training instance. With slight abuse of notations, we denote the owners of a set S of training instance as h(S), where S ⊆ I, and denote the training instances from the set of sellers ˜S ⊆ Is by h−1(˜S). Let N(S) = {α1(S), . . . , αmin{K,|S|}(S)} be a function that maps a set of training instances to its K-nearest neighbors. Let A = {S : ˜S ⊆ Is, |˜S| ⩽ K, S = N(h−1(˜S))} be the collection of all possible K-nearest neighbors formed by sellers; |˜S| ⩽ K because the top K instances cannot belong to more than K sellers. The next theorem shows that we can compute the SV of each seller with O(MK). THEOREM 8. Consider the utility functions (5), (25), (26) or (27). Let A\\j = {S : S ∈ A, j /∈ h(S)} be the set of top-K elements that do not contain sell j’s data, D(˜S) = {S : S ∈ A, h(S) = ˜S} be the set of top-K elements of the data from the set ˜S of sellers, and G(S, j) = {j ′ : d(x (1) j ′ , xtest) ⩾ maxx∈S d(x, xtest), S ∈ A\\j, j ′ ∈ Is \\ {h(S), j}} be the set of sellers that do not affect the K-nearest neighbors when added into the sellers h(S) and S does not include seller j’s data. Then, the SV of seller j can be represented as sj = 1 M ∑ S∈A\\j |G(S,j)|∑ k=0 (|G(S,j)| k ) ( M−1 |h(S)|+k ) [ν(D(h(S) ∪ {j}))−ν(S)] (84) E.4. Valuing Computation. We show that one can compute the SV for both the sellers and the analyst with the same computational complexity as the one needed for the data-only game. The procedures to compute the SV for unweighted/weighted KNN classiﬁcation/regression in the composite game setup are exhibited in the theorems below. E.4.1. Unweighted KNN classiﬁcation. THEOREM 9. Consider the utility function νc in (28), where ν(·) is the KNN classiﬁcation perfor- mance measure in (5). Then, the SV of each training point and the computation contributor can be calculated recursively as follows: sαN = K + 1 2(N + 1)N 1 [yαN = ytest](85) sαi = sαi+1 + 1 [yαi = ytest] − 1 [yαi+1 = ytest] K · min{i, K}(min{i, K} + 1) 2i(i + 1) (86) 43 sC = ν(I) − N∑ i=1 si(87) Comparing s(ν, i) in Theorem 1 and s(νc, i) in the above theorem, we have s(νc, αN) s(ν, αN) = min{N, K} + 1 2(N + 1) (88) s(νc, αi) − s(νc, αi+1) s(ν, αi) − s(ν, αi+1) = min{i, K} + 1 2(i + 1) (89) Note that the right-hand side of (88) and (89) are at most 1/2 for all i = 1, . . . , N − 1; thus, each seller will receive a much smaller share of the total revenue in the composite game than that in the data-only game. Moreover, the analyst obtains a least one half of the total revenue in the composite game setup. E.4.2. Unweighted KNN Regression. THEOREM 10. Consider the utility function in (28), where ν(·) is the KNN regression performance measure in (25). Then, the SV of each training point and the computation contributor can be calculated recursively as follows: sαN = − 1 K(N + 1) yαN [ (K + 2)(K − 1) 2N ( 1 K yαN − 2ytest) + 2(K − 1)(K + 1) 3N(N − 1) ∑ l∈I\\{αN} yl ] − 1 N(N + 1) [ 1 K yαk(N) − ytest ]2 (90) sαi = sαi+1 + 1 K (yαi+1 − yαi) · [( 1 K (yαi+1 + yαi) − 2ytest) · min{K + 1, i + 1} · min{K, i} 2i(i + 1) + 1 K ∑ l∈{1,...,i−1} yαl · 2 min(K + 1, i + 1) min(K, i) min(K − 1, i − 1) 3(i − 1)i(i + 1) + 1 K ∑ l∈{i+2,...,N} yαl · 2 min(K + 1, l) min(K, l − 1) min(K − 1, l − 2) 3l(l − 1)(l − 2) ] (91) sC = ν(I) − N∑ i=1 si(92) E.4.3. Weighted KNN. THEOREM 11. Consider the utility function in (28), where ν(·) is the weighted KNN performance measure in (26) or (27) with some weights wαk(S). Let Bk(i) = {S : |S| = k, i /∈ S, S ⊆ I}, for i = 1, . . . , N and k = 0, . . . , K. Let r(·) be a function that maps the set of training data to their ranks in terms of similarity to xtest. Then, the SV of each training point and the computation contributor can be calculated recursively as follows: sαN = 1 N + 1 K−1∑ k=0 1 ( N k+1) ∑ S∈Bk(αN) ν(S ∪ {αN}) − ν(S) (93) 44 sαi+1 = sαi + 1 N K−2∑ k=0 1 ( N−1 k+1 ) ∑ S∈Bk(αi)∩Bk(αi+1) ν(S ∪ {αi}) − ν(S ∪ {αi+1}) + 1 N N−2∑ k=K−1 1 ( N−1 k+1 ) ∑ S∈BK−1(αi)∩BK−1(αi+1) ( N − max r(S ∪ {αi, αi+1}) k − K + 1 ) ν(S ∪ {αi}) − ν(S ∪ {αi+1}) (94) sC = ν(I) − N∑ i=1 si (95) E.4.4. Multi-data-per-seller KNN. THEOREM 12. Consider the utility functions (5), (25), (26) or (27). Let A\\j = {S : S ∈ A, j /∈ h(S)} be the set of top-K elements that do not contain sell j’s data, D(˜S) = {S : S ∈ A, h(S) = ˜S} be the set of top-K elements of the data from the set ˜S of sellers, and G(S, j) = {j ′ : d(x (1) j ′ , xtest) ⩾ maxx∈S d(x, xtest), S ∈ A\\j, j ′ ∈ Is \\ {h(S), j}} be the set of sellers that do not affect the K-nearest neighbors when added into the sellers h(S) and S does not include seller j’s data. Then, the SV of seller j can be represented as sj = 1 M + 1 ∑ S∈A\\j |G(S,j)|∑ k=0 (|G(S,j)| k ) ( M |h(S)|+k+1) [ν(D(h(S) ∪ {j})) − ν(S)] (96) and the SV of the computation contributor is sC = ν(I) − M∑ i=1 si(97) Appendix F. Generalization to Piecewise Utility Difference A commonality of the utility functions between the unweighted KNN classiﬁer and its extensions is that the difference in the marginal contribution of i and j to a set S ⊆ I \\ {i, j} has a “piecewise” form: ν(S ∪ i) − ν(S ∪ j) = T∑ t=1 C(t) ij 1 [S ∈ St](98) where St ⊆ 2I\\{i,j}. For instance, the utility difference for unweighted KNN classiﬁcation obeys ν(S ∪ {i}) − ν(S ∪ {i + 1}) = 1 [yi = ytest] − 1 [yi+1 = ytest] K 1 [S ∈ S1](99) where we assume the training data is sorted according to their similarity to the test point and S1 = {S : ∑ l∈S 1 [d(xl, xtest) − d(xi, xtest) < 0] < K}(100) Hence, we have T = 1 and C1 ij = (1 [yi = ytest] − 1 [yi+1 = ytest])/K for unweighted KNN classiﬁcation utility function. The utility difference for unweighted KNN regression can be expressed as ν(S ∪ {i}) − ν(S ∪ {i + 1}) 45 = 1 K (yi+1 − yi)( 1 K (yi+1 + yi) − 2ytest)1 [S ∈ S1] + 2 K2 (yi+1 − yi) ∑ l∈I\\{i,i+1} yl1 [S ∈ S1, S ∋ l](101) where S1 is deﬁned in (100). Therefore, we can obtain the piecewise form of the utility difference in (98) by letting T = N − 1, C(1) ij = 1 K (yi+1 − yi)( 1 K (yi+1 + yi) − 2ytest), {St} N−1 t=2 = {Sl}l∈I\\{i,i+1} where Sl = S1 ∩ {S : l ∈ S, S ⊆ I \\ {i, i + 1}}, and the corresponding {C(t) ij } N−1 t=2 = { 2 K2 (yi+1 − yi)yl}l∈I\\{i,i+1}. For weighted KNN utility functions, we can instantiate the utility difference (98) with T = ∑K k=0 ( N−2 k ) adn St ⊆ 2I\\{i,j} is a collection of sets that have the same top K elements. An application of Lemma 1 to the utility functions with the piecewise utility difference form indicates that the difference in the SV between i and j can be represented as si − sj = 1 N − 1 ∑ S⊆I\\{i,j} T∑ t=1 C(t) ij ( N−2 |S| ) 1 [S ∈ St](102) = 1 N − 1 T∑ t=1 C(t) ij [ N−2∑ k=0 |{S : S ⊆ I \\ {i, j}, S ∈ St, |S| = k}| ( N−2 k ) ] (103) With the piecewise property (98), the SV calculation is reduced to a counting problem. As long as the quantity in the bracket of (103) can be efﬁciently evaluated, the SV can be obtained in O(NT ). Appendix G. Proof of Theorem 5 Proof. We will use Bennett’s inequality to derive the approximation error associated with the estimator in (4). Bennett’s inequality provides an upper bound on the deviation of the empirical mean from the true mean in terms of the variance of the underlying random variable. Thus, we ﬁrst provide an upper bound on the variance of ϕi for i = 1, . . . , N. Let the range of ϕi for i = 1, . . . , N be denoted by [−r, r]. Further, let qi = P[ϕi = 0]. Let Wi be an indicator of whether or not ϕi = 0, i.e., Wi = 1 [ϕi ̸= 0]; thus P[Wi = 0] = qi and P[Wi = 1] = 1 − qi. We analyze the variance of ϕi. By the law of total variance, Var[ϕi] = E[Var[ϕi|Wi]] + Var[E[ϕi|Wi]](104) Recall ϕi ∈ [−r, r]. Then, the ﬁrst term can be bounded by E[Var[ϕi|Wi]] = P[Wi = 0]Var[ϕi|Wi = 0] + P[Wi = 1]Var[ϕi|Wi = 1](105) = qiVar[ϕi|ϕi = 0] + (1 − qi)Var[ϕi|ϕi ̸= 0](106) = (1 − qi)Var[ϕi|ϕi ̸= 0](107) ⩽ (1 − qi)r 2(108) where the last inequality follows from the fact that if a random variable is in the range [m, M], then its variance is bounded by (M−m)2 4 . The second term can be expressed as Var[E[ϕi|Wi]] = EWi[(E[ϕi|Wi] − E[ϕi])2](109) = P[Wi = 0](E[ϕi|Wi = 0] − E[ϕi])2 + P[Wi = 1](E[ϕi|Wi = 1] − E[ϕi])2(110) 46 = qi(E[ϕi|ϕi = 0] − E[ϕi])2 + (1 − qi)(E[ϕi|ϕi ̸= 0] − E[ϕi])2(111) = qi(E[ϕi])2 + (1 − qi)(E[ϕi|ϕi ̸= 0] − E[ϕi])2(112) Note that E[ϕi] = P[Wi = 0]E[ϕi|ϕi = 0] + P[Wi = 1]E[ϕi|ϕi ̸= 0](113) = (1 − qi)E[ϕi|ϕi ̸= 0](114) Plugging (114) into (109), we obtain Var[E[ϕi|W]] = (qi(1 − qi)2 + q2 i (1 − qi))(E[ϕi|ϕi ̸= 0])2(115) Since |ϕi| ⩽ r, (E[ϕi|ϕi ̸= 0])2 ⩽ r2. Therefore, Var[E[ϕi|W]] ⩽ qi(1 − qi)r 2(116) It follows that Var[ϕi] ⩽ (1 − q2 i )r 2(117) Therefore, we can upper bound the variance of ϕi in terms of the probability that ϕ=0. Now, let us compuate P[ϕi = 0] for i = 1, . . . , N. Without loss of generality, we assume that xi are sorted according to their distance to the test point xtest in an ascending order. When i ⩽ K, then whatever place xi appears in the permutation π, adding xi to the set of points preceding i in the permutation will always potentially lead to a non-zero utility change. Therefore, we know that qi ⩾ 0 and Var[ϕi] ⩽ r 2 ≡ σ 2 i for i = 1, . . . , K(118) When i ⩾ K + 1, adding xi to Pϕ i may lead to zero utility change. More speciﬁcally, if there are no less than K elements in {x1, . . . , xi−1} appearing in Pϕ i , then adding i would not change the K nearest neighbors of Pϕ i and thus ϕi. Let the position of xi in the permutation pi be denoted by k. Note that if there are at least K elements in {x1, . . . , xi−1} appearing before xi in the permutation, then xi must at least locate in order K + 1 in the permutation, i.e., k ⩾ K + 1. The number of permutations such that xi is in the kth slot and there are at least K elements appearing before xi is min{i−1,k−1}∑ m=K ( k − 1 m )( N − k i − 1 − m ) (i − 1)!(N − i)!(119) Thus, the probability that ϕi is zero is lower bounded by q∗ i = ∑N k=K+1 ∑min{i−1,k−1} m=K ( k−1 m )( N−k i−1−m) (i − 1)!(N − i)! N! (120) = ∑N k=K+1 ∑min{i−1,k−1} m=K ( k−1 m )( N−k i−1−m) ( N−1 i−1 ) N (121) = i − K i (122) By (117), we have Var[ϕi] ⩽ (1 − q∗2 i )r 2 for i = K + 1, . . . , N(123) 47 By Bennett’s inequality, we can bound the approximation error associated with ˆsi by P[|ˆsi − si| > ϵ] ⩽ 2 exp(− T σ2 i r2 h( rϵ σ2 i ))(124) By the union bound, if P[|ˆsi − si| > ϵ] ⩽ δi for all i = 1, . . . , N and ∑N i=1 δi = δ, then we have P[max i |ˆsi − si| > ϵ] = P[∪i=1,...,N]{|ˆsi−si| > ϵ}] ⩽ N∑ i=1 P[|ˆsi − si| > ϵ] ⩽ N∑ i=1 δi = δ(125) Thus, to ensure that P[maxi |ˆsi − si| > ϵ] ⩽ δ, we only need to choose T such that 2 exp(− T σ2 i r2 h( rϵ σ2 i )) ⩽ δi(126) which yields T ⩾ r2 σ2 i h( rϵ σ2 i ) log 2 δi (127) Since r2 σ2 i h( rϵ σ2 i ) ⩽ 1 (1 − q2 i )h( ϵ (1−q2 i )r ) (128) it sufﬁces to let T ⩾ log 2 δi (1 − q2 i )h( ϵ (1−q2 i )r ) (129) for all i = 1, . . . , N. Therefore, we would like to choose {δi}N i=1 such that maxi=1,...,N T ∗ i is minimized. We can do this by letting log 2 δi (1 − q2 i )h( ϵ (1−q2 i )r ) = T ∗(130) which gives us δi = 2 exp(−T ∗(1 − q2 i )h( ϵ (1 − q2 i )r ))(131) Since ∑N i=1 δi = δ, we get N∑ i=1 exp(−T ∗(1 − q2 i )h( ϵ (1 − q2 i )r )) = δ/2(132) and the value of T ∗ can be solved numerically. □ 48 Appendix H. Derivation of the Approximate Lower Bound on Sample Complexity for the Improved MC Approximation Because log(1+u) > 2x 2+x [Top06], we have h(u) > x2 2+x . Thus, (1−q2 i )h( ϵ (1−q2 i )r )) > ϵ2 2(1−q2 i )r+ϵr . Furthermore, by the deﬁnition of qi, (1 − qi)2 = 1 for i = 1, . . . , K and decreases approximately with the speed 2K/i otherwise. Thus, the lower bound of (1 − q2 i )h( ϵ (1−q2 i )r )) increases linearly with i when i ⩾ K + 1. Letting x = exp(−T ∗), we can rewrite (32) as ∑N i=1 x (1−q2 i )h( ϵ (1−q2 i )r )) = δ/2. In light of the above analysis, x (1−q2 i )h( ϵ (1−q2 i )r )) will have signiﬁcant values when i ⩾ K and is comparatively negligible otherwise. Therefore, we can derive an approximate solution ˜T to T ∗ by solving the following equation K exp(−˜T h( ϵ r )) = δ/2.(133) which gives us ˜T = 1 h(ϵ/r) log 2K δ (134) Due to the inequality h(u) ⩽ u2, we can obtain the following lower bound on ˜T : ˜T ⩾ r2 ϵ2 log 2K δ (135) UNIVERSITY OF CALIFORNIA, BERKELEY E-mail : ruoxijia@berkeley.edu ETH ZURICH E-mail : dwddao@gmail.com ZHEJIANG UNIVERSITY E-mail : boxin.wang@outlook.com ETH ZURICH E-mail : hubisf@student.ethz.ch ETH ZURICH E-mail : nezihe.guerel@inf.ethz.ch UNIVERSITY OF ILLINOIS AT URBANACHAMPAIGN E-mail : lxbosky@gmail.com ETH ZURICH E-mail : ce.zhang@inf.ethz.ch UNIVERSITY OF CALIFORNIA, BERKELEY E-mail : spanos@berkeley.edu UNIVERSITY OF CALIFORNIA, BERKELEY E-mail : dawnsong@gmail.com 49","libVersion":"0.2.3","langs":""}