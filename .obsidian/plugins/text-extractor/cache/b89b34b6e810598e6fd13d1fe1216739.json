{"path":"1-项目/1-科研/_assets/ai-mdp.pdf","text":"Finite State Markovian Decision Processes This is Volume 67 in MATHEMATICS IN SCIENCE AND ENGINEERING A series of monographs and textbooks Edited by RICHARD BELLMAN, University of Southern California A complete list of the books in this series appears at the end of this volume. FINITE STATE MARKOVIAN DECISION PROCESSES CYRUS DERMAN -DIVISION OF MATHEMATICAL METHODS OF ENGINEERING AND OPERATIONS RESEARCH SCHOOL OF ENGINEERING AND APP LIED SCIENCE COL UMBIA U NIVERSITY NEW YORK, NEW YORK 1970 ACADEMIC PRESS New York and London CoPYRIGHT © 1970, BY AcADEMic PRESS, INc. ALL RIGHTS RES ERVED NO PART OF THIS BOOK MAY BE REPRODUCED IN ANY FORM, BY PHOTOSTAT, MICROFILM, RETRIEVAL SYSTE M, OR ANY OTHER MEANS, WITHOUT WRITTEN PERMISSION FROM THE PUBLISHERS. ACADEMIC PRESS, INC. 111 Fifth Avenue, New York. New York 10003 United Kingdom Edition published by ACADEMIC PRESS, INC. (LONDON) LTD. Berkeley Square H ouse, London W1X 6BA LmRARY o F CoN GREss CATALOG CARD NuMB ER: 70-117083 PRINT ED IN T HE UNITED STATES OF AM ERICA TO MY PARENTS Samuel and Bessie (S egal) Derman Contents Preface A cknowledgments 1. Introduction Preliminary Remarks The Markovian Decision Model Problems to Be Treated Hierarchical Classification of Policies Bibliographical Remarks Problems 2. Finite Horizon Expected Cost Minimization Dynamic Programming Computational Example Pathologies Bibliographical R emarks Problem vii xi xiii 2 5 6 7 8 11 15 16 17 17 viii 3. Some Existence Theorems Summary Expected Discounted Cost Problem Expected Average Cost Problem First-Passage Problem First Passage as a Discounted Cost Problem Bibliographical Remarks Problems 4. ·Computational Methods for the Discounted Cost Problem Introduction and Summary Method of Successive Approximations Policy Improvement Procedure Linear Programming Computational Examples Bibliographical Remarks Problems 5. Computational Procedures for the Optimal First-Passage Problem Introduction Method of Successive Approximations Policy Improvement Procedure Linear Programming Formulations Computational Examples The Finite Horizon Problem as a First-Passage Problem Bibliographical Remarks Problems 6. Expected Average Cost Criterion Computational Procedures Summary Policy Improvement Procedure Linear Programming Formulations Policy Improvement, Linear Programming under Irreducibility Assumption Computational Example Bibliographical Remarks Problems Contents 19 20 25 28 31 32 33 35 36 39 41 46 49 50 53 54 56 57 59 61 62 63 65 66 73 78 81 83 84 Contents 7. State-Action Frequencies and Problems with Constraints Introduction and Summary Expected State-Action Frequencies Some Examples The Main Theorems and Applications State-Action Frequencies Bibliographical Remarks 8. Optimal Stopping of a Markov Chain Statement of the Problem Stopping Problem as an Expected Average Gain Problem A Different Approach Computational Example The Dual Linear Programming Problem Some Other Forms of the Stopping Problem Bibliographical Remarks Problems 9. Some Applications A Replacement Model A Surveillance-Maintenance-Replacement Model AOQL of Continuous Sampling Plans A Sequential Search Problem A Stochastic Traveling Salesman Problem Bibliographical Remarks Appendix A. Markov Chains Bibliographical Notes Appendix B. Some Theorems from Analysis and Probability Theory Bibliographical Notes ix 87 88 89 91 98 102 103 104 105 112 113 114 116 117 121 125 130 132 135 137 139 142 143 147 X Appendix C. Convex Sets and Linear Programming Bibliographical Notes References Index Contents 149 152 153 157 P reface We are concerned with the optimal sequential control of certain types of dynamic systems. We assume such a system is observed periodically. After each observation the system is classified into one of a possible number of states; after each classification one of a possible number of decisions is implemented. The sequence of implemented decisions interacts with the chance environment to effect the evolution of the system. We call the mathematical abstraction of this process a Markovian decision process; however, some authors use the term discrete dynamic programming. Just as linear programming provides a general framework for formulating and solving certain optimization problems, so does the Markovian decision process provide a structure within which optimal control of dynamic systems can be formulated and solved. xi xii Pre fate Recognizing its potential usefulness we presented a co urse on Markovian decision processes in 1966 and 1968 for operations research students of the Columbia University School of Engineering and Applied Science. The lectures for the course have served as a basis for the preparation of this monograph. This book is intended for operations researchers, statisticians, mathematicians, and engineers interested in mathematical methods for the control of dynamic systems. It can serve as a text for a course on dynamic programming which is intended to provide students with the basic computational algorithms as well as to prepare theip. for research in subject. Prerequisites include a reasonable grounding in real analysis or advanced calculus, a knowledge of the elementary theory of Markov chains, and an acquaintance with the rudiments of linear programming. An appendix on these prerequisites is provided ; however, its primary purpose is to collect those facts which are explicitly used in the text and is not intended to serve as a source for obtaining the necessary background for reading this book. Acknowledgments Acknowledgments are due to Richard Bellman for his initial and recurrent encouragement to write this book; to Arthur Veinott for a reading of the manuscript and helpful comments; to, at least, Edward Ignall, Morton Klein, Peter K olesar, and Howard Taylor for enlighten- ing conversations ; and to Regina Tetens and Stennett Parris for preparation of the manuscript. T his work was supported, in part, by the Army, Navy, Air F orce, and NASA under Contract NOOO 14- 67-A-0108-0008 with the Office of Naval Research. xili Introduction Preliminary Remarks Markovian decision processes are stochastic processes that describe the evolution of dynamic systems controlled by sequences of decisions or actions. Thus, in this monograph we shall be concerned with certain types of dynamic systems which are observed periodically, and in- fluenced at the time of observation by the taking of one of a possible number of different actions. The evolution of the system will be the result of the interaction between the \"laws of motion\" of the system 1 2 1 Introduction and the sequence of actions taken over time. The different paths of the system will have associated economic consequences; the ultimate aim is to take those actions that control the system in an optimal manner. Optimality will be defined relative to a stipulated criterion. A typical system is an inv.entory system for a given product where the inventory level is under periodic review. After each review, the action taken is that of adding a certain amount of the product to the inventory level. The laws of motion of the system are determined by the pattern of demand for the product between times of review. Various costs associated with ordering new product, holding inventory in storage, shortages, etc. contribute to the economic consequences of the actions taken at the various times. A criterion for optimality would ordinarily be a function of long term costs. Another typical system might consist of a: component or group of components that is under periodic surveillance and subjected to periodic maintenance or replacement of one or more components. At each inspection the system is classified in some appropriate way and a decision is made as to what degree of maintenance to employ. The properties of the system together with the demands upon it determine the laws of motion. Economic aspects involve the various costs associa- ted with maintenance and also the attributed costs due to failure of the system. Occasionally, failure costs may be difficult or impossible to ascertain, in which case\" system reliability\" may be a more appropriate yardstick with which to measure the effectiveness of a surveillance and maintenance procedure. The Markovian Decision Model To introduce the general model, let us assume that at points of time t = 0, I, .. . the system is observed and classified into one of a possible number of states. We let { Y,, t = 0, I , ... } denote the sequence of observed states. The letter I will denote the space of possible states. Throughout this volume, I will be a finite space. After each observation of the system, one of a possible number of The Markovian Decision Model 3 actions is taken. We let {A,, t = 0, 1, ... } denote the sequence of actions. K 1 denotes the number of actions possible when the system is in state i. More frequently, we shall also use K, to denote the set of possible actions when the system is in state i. No confusion should result from the double use of the notation. Throughout, K 1 will be finite. A rule or policy, to be denoted by R , is a prescription for taking . actions at each point in time. We shall permit a policy for taking an action at time t to be a function of the entire \" history\" of the system up to time t. We will allow actions to be taken which are determined by a random mechanism, the random mechanism will be a function of the \"history.\" For example, when in state i, a coin may be tossed to determine which of two possible actions to take. However, the kind of coin used may depend on the previous sequence of states and actions taken. In this volume the use of policies employing randomization enables the use of linear prograrruning formulations of the problems of interest and allows one to obtain optimal policies in the face of certain constraints. Thus, a policy R is a set of functions {Da(Hr- 1• Y,), a E Kr , ' t = 0, 1, .. . } satisfying and .a where H, denotes the history of the system up to time t; that is, H , = { Y 0 , A 0 , •• . , Y, , A,}. One interprets D 0 (H, _ 1 , Y,) as follows: if H ,_ 1 denotes the history of the system up to time t - 1 and Y, the state of the system at time t, then a random mechanism is to be used which assigns the probability Du(H,_ 1 , Y,) of taking action a at time t. We assume throughout that the laws of motio n of the system can be characterized by a time invariant set of transition probabilities. N amely, whenever the system is in state i and action a is taken, then, regardless of its history, %(a) denotes the probability of the system being in state j at the next instant the system is observed. An alternative way of stating this assumption is that no matter what policy R is employed, the con- ditional probability that Y,+ 1 = j, given that H,_1 , Y, = i, and A, = a 4 1 Introduction is equal to qij(a). In this volume it will be assumed that the set of numbers { q ii(a), a E K ;, i E J, j E I} are known and, of course, satisfy 0 q,j 1 and For example, consider the laws of motion of an inventory system under periodic review. Let Y, denote the level of inventory at timet and let A, denote the amount ordered after observing Y, ., Assume delivery of the A, units is instantaneous so that at the moment of ordering, the inventory level is Y, +A;. Suppose the sequence of demands {D, } for the product during each of the periods is a sequence of independent and identically distributed random variables. If, for simplicity, we allow negative inventory (that is, backlogging of demand) a nd a denumerable state space, then q,j(a)=P{Y,+ 1 = } 1H, _1 , Y ,=i,' A, = a} = P{Demand = i + a- j}. Given a distribution P{Y0 = i} over the initial states of the system and a policy R, then the sequence { Y,, A, , t = 0, 1, ... } is a stochastic process. We call this process a Markovian decision process. The term Markovian is employed because of the special assumptjons regarding the laws of motion. However, we point out that the process { Y., A,, t = 0, 1, ... } is not necessarily a Markov process. Since a policy R may be such that the prescription for taking actions is dependent upon the entire history of the process, the Markov property may not be satisfied by{Y,,A,}. In order to indicate the dependence of the probabilities on the policy R, the notation PR{E} will denote the probability of an event E occurring when policy R is used. The probability of the event E given the initial state Y0 = i and the use of the policy R will be deno ted by P R { E I Y0 = i}. We assume a certain cost structure superimposed on the Markovian decision process. W henever the system is in state i and action a is taken, we assume that a known cost w,a is incurred. For most of that considered in this volume, w,a may also denote an expected cost rather tha n an actual cost. H owever, the important aspect of the assumption is that this cost is a function only of the state and action taken. For example, in Problems to Be Treated 5 our inventory problem the cost incurred in a period is a function of the ordering costs and the inventory level at the end of that period. The expected value of this function taken with respect to the distribution of demand will yield our assumed cost W ;a, the expected cost associated with inventory level i, and the action of ordering a units. We define the random variables { W,, t = 0, 1, ... } : W,= W;0 if Y, = i, A , = a, a E K,, i E J. We can then speak of expected costs; that is, ER W, =II PR{Y; = i, A, = a }w;a · i a Since I and K , will be finite, no question of existence of ER W, will arise. Problems to Be Treated In terms of our model, we are now in the position to state some of the pro blems of interest in this volume. I n each of the problems, it will be ass umed that the initial state Y0 = i is given; that is, P{ Y0 = i} = l. Let T T S R, r( i) = E R I Yfc = I I I P R { Y; = j, A, = a } W ja · t= O r= O J- a In words, SR ,r(i) is the expected total cost of operating the system up to and including the time \" horizon \" t = T, given that the initial state is i and the policy used is R . The problem of interest is that of obtaining that policy R which minimizes SR ,r(i). This is the type of problem which is commonly dealt with by the straightforward method of dynamic programming. We discuss its solution in the next chapter. Another problem treated is that of finding R to minimize O\"R(i) = SR,.(i), where ' denotes the smallest positive value of t such that Y, = j, j being a\" target\" state at which the process is stopped. We refer to this as an optimal first-passage problem. It should be noted that ' is a random variable so that O\"R(i) is the expected value of a rando m sum 6 1 Introduction of random variables. For an obvious generalization, j need not refer to a single state, but may be a class of states. A third problem to be dealt with is that involving the discounted cost criterion 00 I!'R(i, a)= ER L a'w;, t=O where 0 I (the discount factor) is a given number. We shall be interested in finding R to minimize 'I' R(i, a). A fourth problem arises from the expected average cost per unit time criterion: A.(')-! ' SR,T(i) 'I'R L -1m . T-+ oo T + l For some policies R, the limit of the above expression may not exist. In those cases we shall deal with the upper or lower limit, whichever seems appropriate. We shall be interested in finding a policy R to minimize <P R(i). Several other problems will be treated as well. We may have other criteria to minimize or we might be interested in minimizing </J R(i) subject to certain side constraints. In Chapter 7 we hope,to develop the theory so that these problems can be dealt with; in Chapter 8 a stopping problem is considered. ,. Hierarchical Classification of Policies In each case we shall be concerned with three questions: existence, structure, and computational procedures. With regard to each of these it is convenient to introduce a hierarchical classification of the totality of possible policies. We Jet e denote the class of all policies under con- sideration; that is, those with possible dependency on the complete history of the system. We Jet eM denote the class of all memory less or Markovian type policies. T hat is, eM consists of all policies R such that D.(H ,_ 1 , Y,) is a function only of Y,, t, and a. When R E eM, then { Y,, t = 0, 1, ... } is a M arkov chain, not necessarily stationary. We let Bibliographical Remarks 7 e5 denote the class of all Markovian policies which are time invariant. That is, e5 consists of all policies R such that D.(H,_ 1 , Y,) are functions only of Y, and a. Let D;. = D.{H,_ 1, Y, = i} when R E es. Then {Y,, t = 0, l, ... } is a Markov chain with stationary transition prob- abilities i, j E J. Finally, we let ev denote the subclass of e5 consisting of the determinis- tic policies. That is, R E ev whenever D;a is 0 or 1 for every i E J. In this case we can think of R as defining a single-valued transformation from the states to the actions; that is, when R E ev, to each state i there corresponds an action a;, among the possible actions K;, such that R prescribes action a; when the system is in state i. Accordingly, when convenient and R E ev, we shall employ the notation W;R and qiJ(R) to denote W;01 and q;j(a;). Since the class e of all policies is infinite, the question of existence of an optimal policy will be important for each problem considered. In all problems dealt with here, we shall want to assert that not only does an optimal policy exist but that one is also a member of eM, e5 , or ev. In other words, we shall want to say something about the structure of at least one of the optimal policies. In certain special cases, perhaps, more can be said about structure. When the structure is such that an optimal policy is a member of eM, es, or ev, then frequently, a computational procedure can be obtained for its determination. Bibliographical Remarks The title of this book might well have been called \" Dynamic Pro- gramming,\" or better, \"Discrete Dynamic Programming\" as used by Blackwell [6]. The descriptive phrase \"Markovian Decision Process\" is due to Bellman [2], and because of the connections of the material treated herein with Markov chains, we prefer the latter description. The Markovian decision model in recent years has been the subject of an increased amount of research activity. Early papers, in a special 8 1 Introduction context by Bellman and LaSalle [4], Bellman and Blackwell [5], and later, more generally by Shapley [48] were among the first formulations of the model in the context of two-person dynamic games. Its first explicit formulation outside the game context is given by Bellman [2]. The model has a large number of parameters and is readily adaptable to many dynamic systems as a descriptive model. Although some of the methods of dynamic programming such as backward induction and method of successive approximations predate the formal conception of dynamic programming and, in particular, the Markovian decision model, it was not until computational breakthroughs by Howard [34], Manne [46], and D'Epenoux [14], some seven years or so after Shapley's [48] treatment, that interest in the model increased and an awareness in its potential usefulness developed. In the Shapley [48] two-person sto,chastic games model, as in the earlier papers by Bellman and LaSalle [4], and Bellman and Black- well [5], the process { Y,, t = 0, 1, ... } is controlled by two sets of simul- taneous action. The \"laws of motion\" are in the form of numbers {q 1j(a, b), a E K/, bE K!\\ i E I,j E I}, where K/ andK/1 , i E I, are sets of possible actions for a\" player I\" and a\" player II\" at state i. That is, if the process is in state i and player I takes action a and player II takes action b then the probability is q1j(a, b) that the ne.xt period will find the process in state j .. }'he costs in this model are of the form w iab, to be interpreted as the cost to player I and the gain to player II when the process is in state i and player I takes action a and player II takes action b. Thus, the process under consideration in this volume concerns the special case where player II has only one available action at each state. Problems (!) Derive the form of the laws of motion, the qij(a)'s, for the example of the inventory system under periodic review when backlogging is not permitted. Problems (2) For an inventory system under periodic review for the cases of backlogging and no backlogging, construct the forms of the costs (3) For the inventory system under periodic review construct a policy R that belongs to Cv; to C5 - Cv; to C- C5 . 9 2 Finite Horizon Expeeted Cost Minimization Dynamic Programming This chapter is concerned with the determination of that policy T R E C which minimizes SR,T(i) = ER L W1 , where the horizon T and i = O initial state i are given. We will show that a backward inducti'On method, which is the essence of dynamic programming, provides a computational procedure for obtaining the optimal policy. Although I is assumed to be J1 12 2 Finite Horizon Expected Cost Minimization finite, the method of this chapter holds for countable I as long as the costs {wia} are such that ER W, is well defined for all R and t. Let us denote by Vn(R,j, h\"_ 1), 0 n T, the conditional expected total cost of a process from time t = n to time t = T given the history H\"_ 1 = h\"_ 1 , Y\" = j and policy R; that is, V,(R, j, hn- J = ER{.tn w; I Yn = j, Hn-1 = 1 }• 0 n T. When n = 0, we have, since there is no history, V0 (R, i) = SR,r(i). Set V/ (i, hn-1) = inf V,(R, i, hn_J, 0 n T. ReC Then providing V0 *(i) =min V0 (R, i), the of S R,r(i) is V0 * (i)when R e C an optimal policy is used. It will be seen that the minimum over all R E C is obtained. The method of backward induction employs a recursion formula by which v:-1 can be expressed in terms of vn * fo r n = T, T- I, ... , 1, thereby achieving the value V0 *(i). At the same time the optimal policy is perceived. We first prove: LEMMA I. For every Hn-1 = hn - 1• Vn*(i, hn- 1) = Vn*(i), n = I' ... ' T, i E I ; that is to say, vn *(i, hn -1) is independent of hn- 1· Proof: Fix i. Let v*(i) = inf Vn(R, i, h\"_ 1). Let 13 >0 be given R, hn - 1 arbitrarily. Let R 0 , be such that Vn(R 0 , i, < v*(i) +e. D efine R 1 as follows: D:'(H, _ 1 , Y;) = D: 0(H,_ 1 , Y;), t = 0, ... , n- 1, = Y\", ... , Y;), -( = !!, . . . , T; . . 4n •· I H' l ( I that 1s, R 1 1s the same as R 0 for t = 0, . .. , n - , but, for t = n, ... , T, Dynamic P rogramming '13 R 1 prescribes actions as if policy R 0 were in effect and the history Hn-1 = had been observed up to time n- 1. Then, for every Hn-1 = hn- 1• V,(R1, i, hn_ 1) = V,(R 0 , i, v*(i) + e . Therefore, since 13 > 0 is arbitrary, Vn *(i, h\"_1) v*(i). On the o ther hand, by definition of v*(i), we have that V,(R, i, hn-1) v*(i) for every Rand hn - t· Hence, V/(i, h\"_ 1) = v*(i) independent of h\"_ 1 and the lemma is proved. For the following theorem, we set = 0, i E I . We prove: THEOREM I . If R * is defined as a policy which a t time n takes action ai* (a function of n) satisfying ) : wia•, + 2:: %(ai*)Vn\\ 1(j) = min{wia +I qij(a) Vn\\ 1(j)} J a J for i E I and n = 0, ... , T, then V\"(R* , i, H\"_ 1) = V, *(i), n = 0, I, . . . , T, i E I. In particular, R* is optimal for minimizing SR,r(i). Proof: We will use backward induction on n. Suppose n = T. Then for any Rand hr- 1 = m}n{wia + qij(a)Vi +1(j)} = Vr(R*, i, hr_ 1). Since, in fact, Vr(R*, i, hr _ 1) is independent of hy_ 1, we have that it is equal to Vr*(i) . Now assume that V,(R* , i, h,_ 1) = V,*(i) for t = n + I , .. . , T. We shall show that the same holds for t = n. For any R a nd hn-1> 14 2 Finite H orizon Expected Cost Minimization v.(R , i, h.-1) = ERtt. wt I Y. = i, H n-1 = hn-1} = 'f. D/(h._ 1, i)wia + 'f.'f.D/(hn-1• i)q ij(a) a j a X ER{ I W.l Yn+1 =j, Y. = i, _:1. =a, h.-1} r=n+ 1 .; ....... _l.'--'- c,-,_.o.!.\\..'- -(j-(; .... ...-1' ) L\\ min{wia + q;j(a)v:+ 1(j)} -.. a J ><(: =V.(R*,i,hn-1). ell- 0 '\\\\(_ The first inequality follows from Lemma 1 and the last equation follows from the induction assumption. The right-hand side is again independent of h. -1· Hence, v.(R*' i, h. -1) = v. *(i), i E I. This proves the theorem. COROLLARY 1. V_*(i) = i E I, n = 0, 1, ... , T. j- Proof: The equations follow from the fact that v. *(i) = VnCR *, i, h._ 1 ), for i E I and n = 0, l, . . . , T and the last equality of the proof of the theorem. COROLLARY 2. R* is a member of CM . Proof : This is apparent from the definition of R *. The defining equations of R* of Corollary 1 are known as the func- tional equations of dynamic programming. T hey provide a simple but extremely useful recursive scheme for obtaining the optimal policy as long as the state space I is not too large. They also express what is ComputatiooaJ Example 15 commonly referred to as the \"princip le of optimality\" which asserts that an optimal policy for minimizing SR,T(i) must also minimize V.(R, i, h. _ 1) for every n = 0, I, . .. , T. Computational Example Suppose I= {0, l } ; K; = 2, i = 0, I with {wo1 Wo2} = {I o}, w11 w12 2 2 and We want to find R to minimize SR ,T(i ), (i = 0, I) for T = 2. F irst, we calculate V2 *(i), (i = 0, !) : V2*(0) = min{w0 1 , w0 2 } = 0, V2*(1) = min{w1 1 , w12 } = 2, keeping in mind that a0 *(2) (the optimal action taken at t = 2 when in state 0) is 2 and that a1 *(2) =I or 2. Now v1 *(O) = min(l + t v2 *(O) + t v2 *(!), 3 + t V2 *(O) + i V2 *(! )) = min(l + 1, 3 + -!) = 2; V1*(1) = min(2 + fV2 *(0) + tV2 *(1), 1 + tV2 *(0) + f V2 *(1)) = min(2 + t, I + 4) - 1.. - 3\" 16 2 Finite Horizon Expected Cost Minimization Here a 0 *(1) = 1, a 1*(1) = 2. Then V0 *(0) = min(1 + tV1*(0) + !V2 *(0), 3 + }V1*(0) + i-V1*(1)) = min(1 + r__;, 3 + r D -ll· - 6, =u 9 , with a0 *(0) = 1, a1 *(0) = 2. ·, Therefore, the optimal policy with respect to mm1m1zing SR, 2(i) is to take actions 1, 1, and 2 when in state 0 at times t = 0, 1, 2, respec- tively, and to take actions 2, 2, and 1 or 2 when in state 1 at times t = 0, 1, 2, respectively. Pathologies When the state and action spaces are noncountably infinite, one may encounter difficulties which restrict the universal validity of the backward induction method. These difficulties arise in patho- logical cases where not all policies possess an associated ER W, defined for all t; only those policies which satisfy certain measurability con- ditions can be so evaluated. Thus, the use of the criterion of mathematical expectation or, indeed, the assumption that { Y,} be a stochastic process has the effect of imposing subtle constraints on the space of possible policies, which in turn raises the possibility that the functional equation approach has some flaws. In order to appreciate how constraints on acceptable policies can invalidate the dynamic programming procedure, even in the finite state and action case, it should be noted that the imposition of the gross restriction that R be a member of CD nullifies the fact that the optimal policy will satisfy the functional equations. Problem 17 Bibliographical Rem.arks For earlier and fuller expositions of the material of this chapter one should refer to Bellman (3]. So intuitive is the dynamic programming or backward induction approach, that one rarely encounters a formal proof that the method yields an optimal policy; hence, the p roof is given here, despite the fact that for discrete state spaces and finite actions at each state the procedure is clearly correct. T he pathology alluded to when state and action spaces are noncountable was revealed by Blackwell [7]. Problem (1) Suppose I= {0, 1, 2}; K; = 2, i = 0, 1, 2, where and ( (qooCl), qoo(2)) (qto(l), qto(2)) (qzo(1), qzo(2)) Find R* for T = 3. = g lwzl Wzz l1 2 .•! I 3 Some Existenee Theorems Summary In this chapter we shall prove the existence of optimal policies in the class CD for the expected discounted cost, expected average cost, and first-passage problems. The criterion in the expected average cost problem is at first ¢ R(i) = lim sup SR,r(i)/T + 1. We then obtain the same result for the problem based on the lower limit definition of cPR(i). Our method here involves discussion of the discounted cost 19 20 3 Some Existence Theorems problem first and then, using the results obtained together with elemen- tary Abelian theorems, proving existence theorems for the other two problems. Expected DiscollDted Cost Problem Our basic approach to existence in the case is to first establish that 'P R(i, a) for fixed i and a, 0 a < 1, is fl. continuous function of R and that Cis a compact space. Thus an R E C minimizing 'P R(i, a) must exist. From there, we establish that there exists an R E Cv mini- mizing 'P R(i, a). We say a sequence {R\", n = 1, 2, ... } of policies converges to a policy R if for every a, Yr, h,, t = 0, 1, '. .. ,lim n:\"(h,, y,) = D. R(h,, y,). -... n-+ ro We say a class of policies is compact if for every sequence of policies {Rn, n = 1, 2, ... } there exists a subsequence {Rnk, k = 1, 2, ... } that converges to a policy in the class. We first prove: LEMMA 1. The class C is compact. Proof: For ·· every H, = h,, Y, = y , the space D(h, , y ,) = {D1(h1 , y ,), ... , D K (h,, y 1)} is compact since K , is finite for every Y r i E J. By Tychonov's theorem (T heorem 3 of Appendix B), the product space IT D(h,, y,) is also compact. However, every point ht,Yt ,f in this product space is by definition a policy and every policy corres- ponds to a point in the product space. Hence, the product space is the space C. Thus C is compact. The temptation is to assert that Lemma 1 holds for any state space I and compact action spaces. However, because of the measurability constraints alluded to in Chapter 2, C, being the class of all policies. for which stochastic processes { Y,, A,, t = 0, I, ... } and expectations Expected Discounted Cost Problem 21 E W,, t = 0, 1, . . . are well defined, may be a proper subclass of the product space IT D(h ,, y,) and therefore, may not be compact. ht,Yt,t LEMMA 2. Let t be arbitrary and H , = h, = { Y0 = i, A 0 = a0 , .• • , Y, = i,, A,= a,} be given, then PR{H, = h, I Y0 = i} is a continuous function of R. Proof: For t = 0, PR{H 0 = h0 I Y0 = i} = = i), and hence, the assertion is true for t = 0. Assume it true for t = 0, ... , T. Then since PR{HT+l = hr +1 IY0 = i} = PR{Hr = hr iY0 = i}qir, iT+,(ar)D:r . ,(h r , YT+ l = ir+ 1) . we have by induction that the assertion is true for t = T + 1 and the lemma is proven. LEMMA 3. Let t be arbitrary and Y, = j, A , = a, be given, then P R { Y, = j, A, = a, I Y0 = i} is a continuous function of R. Proof: We can write PR{Y; = j, A,=tiY0 = i} = L PR{Y; =j, A, = a, I Y0 = i, H, _1 = h,_ 1 } hr- 1 X PR{H,_ 1 = h,_ 1 1 Y 0 = i} = L Y, = j) · PR{Hr- 1 = hr- t l Yo= i}. ht- 1 Since there is only a fini te number of h,_ 1's, Lemma 3 follows from Lemma 2. We remark that if I is countable so that a countable number of histories h, exist for each t, Lemma 3 can still be established. 22 3 Some Existence Theorems LEMMA 4. ER W,, t = 0, 1, ... and \\}' R(i , a) for a given 0 1 are co ntinuous functions of R. Proof: For a given t, ER W, is a finite linear combination of terms PR{Y, =},A,= a I Y0 = i}. Thus from Lemma 3, ER W, is continuous. T Similarly \\}' R(i, T) = La' ER W, is continuous for every T = 0, 1, . . . . r = O Since 'I'R(i, a)= lim \\fR{i, a, T) uniformly in 'R,,'I'R(i, a) is also con- T-+ oo tinuous. We are now in a position to prove: LEMMA 5. Let Y0 = i be given. T.here exists an R* E C such that \\f R'(i, a)= inf \\}' R(i, a), i E I. REC Proof: Let \\f R(o:) = L /31 'I' R(i, a) , i where f3,, i E I are given positive numbers. Since by Lemma 1, C is compact and by Lemma 4, \\fR(i, a) is a continuous function of R, then \\}' R(a) is also a continuous function of R and hence, from the well- known fact (Theorem 2 of Appendix B) that continuous functions over compact spaces achieve their extremes, \\f R(a) is minimized by a policy R* E C. H owever, R* must also minimize \\}' R(i, a) for each i E I , otherwise a different policy could easily be constructed which would yield a smaller value for 'I' R(a). Define a 1 , i E I , as those actions which satisfy w,a, +a L qu(a 1)\\fR.(j, a) j i E J. (1) Expected Discounted Cost Problem 23 If a, is not uniquely defined, let it be any one of the several satisfying (1 ). Let Ro be defined as that policy which takes actiOn a; when the system is in state i, i E I . Here, R 0 is a member of Cn ·We now prove · THEOREM 1. T he policy R o minimizes \\}' R(i, a) for all i E I , and \\}' Ro(i, a) uniquely satisfies 'PR 0( i , a)= min{ w;a +a L % (a )'PR/ }, a)}, i E J. a Proof· For each n = I, 2, . .. let R\" be a policy defined as follows: and where for every H,_ 1 = h,_ t and t = 0, 1, ... , n - 1 for t n, n = Y, , n = A, ' H (n) _ { y ( n) A ( n) y(n) A ( n) } . e-n- 0 ' 0 ' · · · ' r -n ' r-n (2) I n words for t = O, . .. , n - 1, R\" prescribes action a, whenever the system is ' in state i. Thereafter it prescribes according to R * as if the process started anew at time t = n. Now, from Lemma 1 of Chapter 2, ER{ I a' W'; I Y0 = i, A 0 =a, Y1 = 1} I a'W'; I Yt = }} r =l R • C r = l for all R E C ; therefore, for each i E I we have 'PR.( i, a) = L n:'( Yo = i)wia + a 4: I D:'( Yo = i)qij(a) a J a x ER·{Ia'- 1 W, I Y0 = i, A 0 = a, t = l = L = i){w;a +a L q;j(a) a 1 (equation continued) 24 3 Some Existence Theorems = i){wia + qij(a) inf ER( I a t-l W,l Y1 = j)} a J ReC t=l = n:·c Yo = i){ w;a + qiJ(a)'P R.(j, a)} m:n{wta + a qiJ(a)'PR.(j, a)} = 'PR,(i, a). .. I ' Consequently, since 'PR.(i, a) is minimal we have that the equality holds. By repeated iteration it follows that 'P R •. (i, a) = 'P R\"(i, a) for n = 1, 2, .. . and all i E J. Since 'P R(i, a )Is a continuous function of R, and {R\",n=l,2, ... } converges to R 0 ,' it follows that 'PR.(i,a)= 'f'Ro(i, a), i E I . This establishes the optimality of R 0 and also that 'P Ro(i, a) satisfies (2). Uniqueness will be shown in Corollary 1 of Theorem I of Chapter 4. CoROLLARY 1. There exists an R E CD such ,that for each i E I, 'P .R(i, a) = inf 'P R(i, a) for all a near enough to 1. ReC Proof: F rom Theorem 1 we need o nly show that inf R e Cv for all a near enough to 1. H owever, for any R E CD, it is easily seen that · 'PR( i, a) = W;R + a I % (R)'PR(j, a), i E I, j from which it follows (see Theorem 3 of Appendix A) that 'P R(i, a) is a rational function of a for 0 a < 1. Let {ex\", n = 1, 2, ... } be a sequence such that lim a\" = 1 and R a, = R a 2 = · · · = R (say) where Ra\" E CD minimizes 'P R(i, an). Such a sequence can be chosen since CD Expected Average Cost Problem 25 is a finite set. Since the difference 'P R(i, a) - R(i, a) is also ratismal it is either identically zero or has, at most, a finite number of zeros. Thus, there is an interval (a(R, i), 1) for which 'P R(i, a) - 'P Jl:(i, a) 0 for all a E (cx(R, i), 1). Let a= max a(R , i). Then for a> a, R E CD, we have R, i 'P R(i, a) Jl:(i, a) and the corollary is proved. Expected Average Cost Problem We now turn to the problem of finding R E Cto mi nimize </>R(i) =lim sup SR,r( i) j(T + I) the expected average cost per unit time over an infin ite time horizon, given the initial state Y0 = i. Since lim SR,r (i)/T + 1 does not exist in general, we define ¢R(i) by the upper limit. However, Corollary 1 to Theorem 2 below will treat the case where 1> R(i) is defined by the lower limit. We prove: THEOREM 2. There exists a policy R* E CD such that P roof: Let R* E CD be such that 'PR.(i, a) = inf 'PR(i, a), i E I , R e C for every a near enough to 1. Corollary 1 to Theorem 1 guarantees the existence of such a policy R*. We shall now show that R* is an optimal policy for the criterion ¢R(i) . From Theorem J(b) of Appendix B, since ¢ R(i) = lim SR,r(i)JT + 1 when R E CD (a consequence of T heorem I of Appendix A), we have ¢ R.(i) = lim(l - a)'PR.(i, a); 26 3 Some Existence Thwtems from Theorem l (c) of Appendix B we have for all R E C that lim sup(1- a)'PR(i, lim sup SR, rO), i E I. T + 1 Using the fact that 'PR.(i, 'PR(i, a) for all a near enough to 1 combined with the two above inequalities yields ,and the theorem is proved. We remark that more than one optimal policy may exist. In our construction of the proofwe showed that the policy R* , which is optimcil with respect to 'I' R(i, a) for every a near enough to l , is also optimal with respect to </> R(i). However, not every policy that is optimal with respect to 4>R(i) will be optimal with respect to 'PR(i, a) for every a near enough to 1. T he following example demonstrates this. Let I consist of the states 0 and l. Let K0 = 2, K1 = l, where qoo(l) = f3 > 0, qo 1(2) = l, q11(l) = 1, w01 = 1, w02 = 0, w11 = 0. Here, CD contains two policies. Let R1 denote the policy in CD which takes action l in state 0 and R 2 , which takes action 2 in state 0. Clearly, 00 </> R,(O) = </> R 2(0). H;owever, 'I' R,(O, a) = I (a{J)t, 'I' R 2(0, a) = 0. Thus R2 t=O is a better choice than R 1 with respect to the discounted cost criterion. For more on this subject see the bibliographical notes of Chapter 6. Suppose we define <i>R(i) =lim inf SR,r(i)/(T + 1). As a consequence of Theorem 2 we have: CoROLLARY 1. T here exists a policy R* (the same policy as m Theorem 2) such that </>R.(i) = inf </>R(i), i E J, R e C when 4>R(i) is defined as lim inf SR ,r(i)/T + l. Expected Average Cost Problem 27 Proof: If R is such that lim SR. r(i)/T + 1 exists for every is I, then by Theorem 2 <I>R•(i) 4>R(i), i E /. Suppose that for every i and j there exists a policy R1 i E CD such that PR1,{ Y, = i for some t l I Y0 = i} = l , and m1; , the mean first-passage time from j to i under R1i, is finite. Starting with an arbitrary policy R, define R as follows: use policy R for times t = 0, ... , T; then if Y0 = i and Yr = j, use policy R1, until Y, = i for the first time after T, after which use policy R as if starting from t = 0. Repeat this construction; that is, each time policy R 1,, depending on the state j, returns the process to state i, use policy R as if starting from t =0 for T + l units of time, then use policy R 1 i where j is the state of the process at the time of the switch in policies. The process {Y,} under R is a recurrent event process (see Appendix B) and by Theorem (i, Appendix B, lim SR,r(i)/(T + l) exists. Hence, <I>R•(i) ¢R(i). Now let us suppose there exists a policy R such that 4> R(i) < 4> R .(i) for some i. Then there will be an e > 0 and a subsequence {Tv, v = l, 2, . . . } of T values such that SR, ru(i) < .(i)- e T + 1 'f'R ' v = 1, 2, .... v Let us adjoin to K1 for each j # i an action a such that q1;(a) = l with cost w 1a = w to be assigned. Let R be defined as above with R1, the policy which takes action a in state j and T = Tv for some v to be assigned. By Theorem 6, Appendix B, 4> R( i) = lim SR., rC i) T + 1 Choose w large enough so that SR, ru(i) +w Tv + 2 </>R'(i) =min </>R(i); R eCo that is, if w is large enough so that any poli cy R E CD using action a is to o costly, the policy R* being originally optimal will again be optimal 28 3 Some Existence Theorems among all policies in the enlarged class CD. But also choose v large enough so that after which we have ·' l . a contradiction of Theorem 2. This proves the corollary. Remark:. That Theorem 2 and its corollary are true might seem intuitively obvious. This, however,, does not stem alone from the Markovian structure we have assumed. For when the state space I is allowed to be countable, Theorem 2 not hold ; moreover, optimal policies may not exist or, when they do exist, they may not be members of CD or C5 . First-Passage P roblem We now consider the optimal first-passage problem. Let us first assume that wia for all a E K 1 and i E J. We let j = 0 denote a given target state. Without loss of generality, we can take w 0 • = 0 andq00 (a) = 1 for all a E K0 . For since}= 0 is the target state and only costs associa- ted with reaching the target state are relevant, this assumption will not affect the solution to the problem. Then, if Y0 =i is the initial state and r denotes the smallest positive integer t such that Y\" = 0, then where W, a re nonnegative random variables. By setting w 0• = 0 and First P assage Problem l9 q00 (a) = 1, we are able to remove the random variable • in the upper limit of the definition of CJ R(i). We now prove: T HEOREM 3. If {wi.} are nonnegative, then there exists an R * E CD such that CJR•(i) = inf CJR(i), i E I. ReC Proof: Consider first 0) \\}'R(i, ex) = L ex'ER W. r = O 0) = ER L ex'W, . t = O The interchange of expectation and summation is justified smce T oo L ex'W, converges unifo rmly to L ex'W, fo r 0 < 1. By Corollary 1 r= O t=O to Theorem 1 there exists a policy R E CD such that \\}' A(i, R(i, ex) for all i E J, R E C, and ex near enough to 1. Since for every R E C, 0) lim \\}' R(i, ex) = L ER W, (the limit may equal oo ), we have from a-}-1 t =O Theorem 4(a) of Appendix B th at 0) CJA(i) = EA L w; r = O which in turn is The theorem is proved. 30 3 Some Existence Theorems In relaxing the condition of nonnegativity on all costs, we need an hypothesis of the sort that P R{ Y0 = 0 for some t ;:;; 1 I Y0 = i} = 1 for i E- I for all R E CD. Such a condition guarantees that oAi) is well defined .. Without such a condition an infinite path through positive and negative cost states could give rise to an indeterminate cost criterion. We now state and prove Theorem 4, a version of Theorem 3 without the nonnegativity assumption. In Chapter 7 a proof by other methods will also be given for this case. THEOREM 4. If PR{Y1 = 0 for some t :S 11 Y0 = i} = 1 for all i E I and R E C0 , then the conclusion of Theorem 3 holds. Proof: Under the hypothesis, the mean first passage mt from state i to 0 is finite for every i E I and R E CD (see' Theorem 6 of Appendix A). Now let w;a = lw;al and, by Theorem 4(a) of Appendix B, co (JR'(i) = ER L W,' r = o co = LER W,'' r =O where W,' = w;a if Y, = i, A, = a, t = 0, 1, .. . . By Corollary 1 of Theorem 1 (with maximum replacing minimum) there exists an R* E C0 such that a) ;:;; a), i E I, R E C for all a sufficiently close to 1. H owever, co (JR'(i) = ER L W,' r =O co = lim L a'ER Wr' (equation continued) a -+ 1 t = O First Passage as a Discounted Cost Problem 31 co L a'ER. a--+1 t=O i , a <co, for every i E I and R E C. Thus, since 1 W,l = W,', it follows fro m Theorem 4(b) of Appendix B that co CJR(i) = ER L w, t= O for every i E I, R E C. Now, again using Corollary 1 to Theorem l , there exists an R* E C0 such that fo r all i E I, R E C, and a near t l n1 c· ) > n1 (i a) Hence using Theorem l (a) of Append1x B, 0 , T R L, a = T R • , • , co CJR.(i) = lim L a'ER. W, a -o r= O co L a'ER W, a - 1 r=O i #j, R E C . This proves the theorem. First Passage as a Discounted Cost Problem Consider the special case where q;0(a) = 1 -a for all a E K ;, i E 1 _ {0}, where again q00(a) = 1, Woa = 0. Then, for any R E C, t = 1 , 2, .. . , i E I - { 0}. 32 3 Some Existence Theorems But ao T - 1 = 2: L ER(W,Ir=T, T=1 t =O ao ao =I I EROV,I• = T, t=O T=t+ 1 Y0 = i)ar- 1(1 ,- a) ao -=LERUV,Ir>t, Y0 =i }P{r>t } t = O ao = L ER{W, I r > t, Y0 = i}o:', t= O i E I- {0} . However, E{W, I r > t, Y 0 = i} =I I WjaPR{Y, =j, A, = al Y0 = i} Ia', j*O a iEI-{0}. Thus, (JR(i),i E I-{0}, is equivalent to 'I'R(i, a), iEI - {0}, for a Markovian decision process based on the state space I' '::\" I _ {0}, with laws of motion q;/a)=qi/a)/a,aE K 1 , i,jEI', and costs w;a=W- a E Kt, i E I'. Or, let '¥ R(i, a) be a discounted cost criterion over a space I, with laws of motion {qJa)}. Define a fictitious state \"0\" and adjoin it to I. la_ws of motion by q i0(a) = 1 - a, i E I, q i/a) = cxqij(a), a E K 1, l, J E I , With qoo(a) = 1. Then, '¥ R(i, a) is equivalent to (JR(i), i E I, where (JR(i) is the expected cost of a first passage to the state \"0\" using the laws of motion {qi/a)}. Bibliographical Remarks Using essentially the same method, Derman [17] proved Theorem 1 for the case of a denumerable state space I. The idea of using Tychonov's theorem in this connection is due to K arlin [35]. Another proof of Problems 33 Theorem 1 holding for a denumerable state space is given by Black- well [7]. Corollary 1 to Theorem 1 is due to Blackwell [6]. The proof of Theorem 2 employs techniques used in Derman [15] and Gillette [33]. G illette [33] employs an incorrect (see Liggett and Lippman [42]) extension of a theorem by Hardy and Littlewood while working with ¢ R(i) defined by lim inf SR,rCi)fT. The proof of Corollary T->ao 1 to Theorem 2 given here is a modification of the one given by Derman [18]. For more on the remark following Theorem 2 see Derman [19] and Fisher and Ross [32]. Theorem 3 was proved by Derman [15]. In connection with Problem 2 following, Veinott [53] provides an alternative proof. His proof shows that the limit converges geometri- cally. This result in Problem 2 is used in the proof of Theorem 1 of Chapter 5. Problems (1) Is ¢R(i) always a continuous function of R? (2) Suppose q00(a) = 1 for every a E K 0 and that PR{Y, = 0 for some t > 0 I Y0 = i} > 0, i E I , R E CD. Show that lim supPR{Y, -:/- 0 1 Y0 = i} = 0, i El. t-> ao Re C S olution: By Theorem 4 and T heorem 6 of Appendix A, on maximizing the expected first-passage cost with W 10 = 1, a E K 1 , i -:/- 0, there exists an R* E C 0 such that R• R m 1 = max m 1 < w , ReC where m/ denotes the mean firs t-passage time to 0 from 34 3 Some Existence Theorems i. But by Chebychev's inequality, (P{X > c} < EX(c 1f X IS a nonnegative random variable) for every R E C, i E J, PR{ Y, of. 0 I Yo = i} = PR{(first-passage time toO) > t 1 y 0 = i} Thus, for i E J, and m.R <-' = t R• m <- = t R• sup PRO'; of. 01 Y0 = i} ReC t .. ; ' lim sup PR{Y; of. 01 Y0 = i} = 0, i E I. t-+ co ReC 4 Computational Methods for the Discounted Cost Problem Introduction and Summary In the previous chapter we showed that solutions exist to the problems of minimizing the expected discounted cost and the expected average cost criterion as well as to the problem of minimizing the mean first-passage cost. In this chapter and Chapters 5 and 6, we present methods by which the optimal policies actually can be obtained. 35 36 4 The Discounted Cost Problem Although it was shown that optimal policies for each problem exist in Cn, the problem of finding them is nontrivial. In spite of the fact that, given R E Cn, the cost criteria ¢1R, and aR can be evaluated, the number of policies in C n may be astronomically large. For example if I contains N states and K 1 = 2, i E I, then Cn contains 2N different policies. For small values of N the method of simple enumeration is feasible; however, for N moderately large, complete enumeration is virtually impossible. N evertheless, modern computational metpods have over- come problems of this sort. For example, in solving ... the linear pro- gramming problem, the space of possible solutions thaf'would have to be searched if the problem were to be solved by enumeration would usually be a large finite number; however, computational methods (the simplex method is one) have been developed which select an optimal · solution without the need for complete enumeration. W e shall show here that comparable methods exist for obtaining optimal policies. In fact, linear programm ing computational procedures, among others, can be employed. In this chapter we discuss the problem of minimizing the expected discounted cost criterion R. We present three approaches to obtaining the optimal policy: the method of successive approximations, policy improvement, and linear programming. The first is the -classical method used in solving differential and integral equations. In itself, it does not provide a method for dbtaining a solution in a finite number of itera- tions ; however, slightly modi fied, it can. M ore significantly, this method has some uses in determining structure of optimal policies. Both the policy improvement and linear programming methods are finite algorithms and are feasible, provided the size of I is not too large. M ethod of Successive Approximations Let {v0(i), i E I} denote an arbitrary set of values. Define for n = 0, 1, ... , i E I' (1) Method of Successive Approximations 37 where 0 a.1 is fixed . W e have : THEOREM 1. If { v\"(i), i E I, n = 1, . .. } is defined by the transforma- tion (1) with {v0(i), i E I} arbitrary, then lim v\"(i) = Ro(i, a.), i E I, independent of {v0(i), i E I}, where R 0 is a policy that minimizes R(i, a.), i E [. Proof: Let {v0(i), i E I} be arbitrary and v0'(i) = Ro(i, a.), i E /. We first show that maxlvn+ 1(i)- maxlvn(i)- v\"'(i)l, n = 0, 1, .... lEI i E I Let a;', i E I, be the action which minimizes the right-hand side of Eq. (1), where in (1) v\"(i) is the nth iterate of v0' (i). Then vn+ 1(i)- 1(i) W 1a,· +a. I qu(a;')v\"(j)- Wia,·- a. I qu(a/)vn'(j) j j I %(a/) maxlvn(j)- vn' (j)l j j Similarly, on Jetting ai, i E I, be the action which minimizes the right- hand side of Eq. (1), where in (1) v\"(i) is the nth iterate of v0 (i), we obtain vn+ 1(i) maxlvn(j)- vn'(j)l, i E I. H ence, we ha ve shown the inequality. Now, by iteration, we obtain that i E I, n = 0, 1, . . .. Thus, lim vn(i)) = 0, i E J. H owever , from T heorem 1 of Chapter 3 (without using uniqueness) we have that v;(i) = Ro(i, a.), i E /, n = 0, 1, ... . Hence, lim v\"(i) = Ro(i, a), i E I . Since v0(i), i E I, was arbitrarily selected, the theorem is proven. 38 4 The Discounted Cost Problem The proof of the uniqueness part of Theorem 1 of Chapter 3 was postponed. Essentially, we now have shown this and summarize it in COROLLARY 1. Equation (2) of Chapter 3* has one and only one solution, namely, 'l'R 0 (i, a), i E /, where R 0 is a policy that minimizes 'l' R(i, a), i E I. Proof\" If v0 H(i), i E I, is a second solution, then v,:;(i) = v0\"(i), i E I, n = 1, · .. . ; and thus, lim v/(i) = v0 \"(i). However, Tfom Theorem 1, n-+ \"' lim v_\"(i) has been evaluated to be I' Ro(i, a). n-+ ro The method of successive approximations based on Theorem 1 con- sists of selecting an initial arbitrary function and transforming it suc- cessively according to the transformation defined by (1). The limiting function will satisfy Eq. (3.2) and the optimal policy is obtained by taking that action in state i, i E I, which minimizes the right-hand side of (3.2). In practice, the limiting function will be approximated only. An approximation to the optimal policy is obtained by treating 'l' Ro(i, a) and its approximation as if they were equal. Actually, if the approximation is close enough to 'l' Ro(i, a), which will be the case for large n, the exact optimal policy will be obtained . H owever, within the procedure, no formal stopping method is given. One might modify the method by occasionally evaluating for various values of n, 'l' R\"(i, a), i E I, from Eq. (2) following, where R \" E Cn is the policy defined by taking that action in state i, i E I, which minimizes the right-hand side of (1). If {'l'di, a), i E I} satisfies (3 .2) then R, is optimal. See also Problems 2, 3, and 4 at the end of this chapter. In general, the functions {v.(i), i E I}, n = 0, 1, ... have no tangible interpretation . H owever, if v0 (i) = min{w;0 } , i E I, then using methods of Chapter 2, we have v,(i) =inf ER( f on.v, I Yo = i), n = 0, 1, ... ' R e C r=O * Hereafter this equation will be referred to as (3.2). Polley Improvement Procedure 39 the optimal expected discounted cost criterion over the periods 0, 1, ... , n. Otherwise, if v0 (i) is interpreted as the terminal cost of being in state i at time n if the process is terminated at time n, then v,(i) is the minimal expected discounted cost plus terminal cost over periods 0, 1, . . . , n given that the initial state is i. In practice, the method of successive approximations may be used when an approximation or guess to an expected discounted cost criter- ion corresponding to a heuristic policy (one arrived at by respected intuition) is available. T hen several iterations will hopefully improve it. In any case, use of the method of successive approximations never necessitates the computation of an exact discounted cost criterion; thus, one is spared the work of solving the system of simultaneous equations (2) following. This latter feature makes the method an attractive com- putational procedure, particularly if the computations are done by hand or with a desk calculator. Perhaps the method of successive approximations is of greatest value in a more theoretical context. That is, certain mathematical pro- perties can be ascertained. For example, suppose I is the set of integers 0, 1, ... , Land the laws of motion are such that v1 (i) is a non decreasing function of i whenever v0(i) is a nondecreasing function. Then it follows that 'l' Ro(i, a) is a nondecreasing function. From this property the structure of an optimal policy may sometimes be deduced. (See Chapter 9, Section 1.) P olicy Improvement P rocedure This is an iterative procedure that improves on each iteration and terminates after a finite number of iterations with an optimal policy. Let Rl E Cn be arbitrary . Then {'l'R,(i, a), i E I} satisfies, uniquely, the equations 'l'R,(i, a)= W;R, +a L qij(R 1)'l'R,(j, a), j (2) Uniqueness follows from the fact (using Theorem 3 of Appendix A) that the matrix of system (2) is I - aQ (I is the identity matrix and 40 4 The Discounted Cost Problem co Q = {qij(R 1 )}) which has an inverse {I- aQ} -l = I a\"Q\". Let £ 1 n=O denote the set of actions i for which w1a +a I %(a)'¥ R,(J, a) is strictly j less than the right-hand side of (2). Define R 2 E CD as follows: For one or more states i for which E 1 is nonempty, prescribe action a in E 1 • For all other states, take the action prescribed by R1. We refer to the derivation of R2 from R 1 as a policy improvement iteration. The fact that the iteration is an improvement is established' in the following. '· ,__· THEOREM 2. If E; is nonempty for at least one state i, then 'l'R 2 (i, 'I'RJi, a), i E I, with strict inequality holding at every i for which R 2 t= R 1 • Proof· By definition of the policy iteration, 'I'R ,(i, W;R 2 + a L qij(R 2 )'I'RJj, a), i E I, j (3) with strict inequality holding at each i for which R 2 t= R 1. Let {qf?CR2)} (t = 0, 1, .. . ) denote the t-step transition probabilities under R 2 • Then from (3), on premultiplying by a'qj'/(R 2 ) and summing over i, we can write for t = 0, 1, . .. , a' L q)'/(R2)1I'R,(i, a) i za' \" q(-'.l(R )w. + a'+ 1 \" q(-'+ 1l(R )'I' (l ) - ;• 2 1R2 1 1 2 R 1 , a , (4) ' l For t 0, Eqs. (3) and ( 4) are identical. On summing ( 4) over t = 0, 1, ... , co we obtain, since '¥R 2 (J, a)= I a' I r=O i co La' L qj;l(R 2)'I'R,(i, a) t =O I co 'f' R,(j, a)+ L a' L qj/ (R 2)'J! Ro(l, a), j E J , t = 1 l Linear Programming 41 with strict inequality holding, because of terms at t = 0, at each j for which R 2 t= R 1 . On subtracting the second term on the right-hand side from the left term, and because the two differ only when j = i (since qfJl = b 1), we have '¥ R, (j, a) 'J! R 2(j, a), j E I, with strict inequality holding for eachj for which R 1 t= R 2 • Thus the theorem is proved. We refer to a sequence of policy improvement iterations as the policy improvement procedure. We can state CoROLLARY 1. The policy improvement procedure terminates, after a finite number of iterations, at an optimal policy. Proof: CD contains only a finite number of policies. Since each iteration is accompanied by a strict improvement, no repetitions will occur. Thus, at some point no improvements will be possible, at which time (3.2) will hold and the corollary is proved. In summary, the policy improvement procedure provides a mono- tone (always improving) convergent sequence of policies and attains in a finite number of iterations the optimal policy. Its drawback is that the discounted cost function for each policy R in the sequence must be calculated. This involves solving the linear system (2). Linear Programming Both the methods of successive approximation and policy improve- ment may be regarded as methods of dynamic programming. Thus, it is somewhat surprising that the method of linear programming can also be brought to bear. For this, consider the linear programming problem : M aximize 42 4 The Discounted Cost Problem subject to Vt W 1a +ct. L q,j(a)vj, j a E K 1, i E l , where fJj > O, j E I, and }2 fJj = 1 are given numbers. The dual linear j programming problem is: Minimize subject to aEK 1 , iEI, and j E I, where b i j = 0 if i i= j, and 1 if i = j. We first discuss the dual problem. THEOREM 3. Let R E C5 be defined by {D1a}: Then CXl X;a = L f3t L a'PR{Y, = il Y0 = l}D1a, a e K 1 , i E I, I t=O is a feasible solution to the dual problem . On the other hand, if {x 1a} is any feasible solution to the dual problem, then {D1a} = {x 1aiL X;q•} a' defines a policy R E C5 and xia == x1a, a E K;, i E I. That is, {D1a} = {x;a/L X;a · } is a one-to-one mapping of the feasible solutions to the dual a ' problem o nto C5 . Proof: It can be readily verified that {x 1a} satisfies the feasibility constraints. Now let {x1a} be a ny feasible solution to the dual p r oblem. Linear Programming 43 The feasibility equations can be written j E J 1 from which it follows that L. x 1a > 0, i E J, since it is assumed that fJj > O,j E J. Thus {D 1a} = {x 1a/L x 1a} is well defined. However, treating a L x 1a, i E J, as variables in the above representation of the feasibility a equations, it follows from Theorem 3, A ppendix A that i E I . But i E I' which completes the proof of the theorem . CoROLLARY 1. An optimal policy R 0 E C5 is obtained by solving the dual problem and setting Dfao = x 1aiL x 1a, a E K;, i E I, where a {x 1.} is an optimal solution to the dual problem. Proof: Since the objective function of the dual pr oblem is in fact I f31 lPR.(/, a), this expression is minimized. However, since fJ1 > 0 and l e i a single R E Cv minimizes '¥ R.(l, a) for every lEI, it follows that 'I'R(/, a) is minimized for each I E I. CoROLLARY 2. If the simplex method is used to solve the dual p roblem, an optimal policy R 0 E Cv is obtained. 4 The Discounted Cost Problem Proof: T he simplex method obtains only extreme point solutions. It follows that L x 1a > 0, i E J, and from Theorem 3 of Appendix C, a x 1a > 0 for exactly one a E K 1 for each i E /. T his then implies = 1 or 0, i E J. CoROLLARY 3. For every optimal solution to the primal problem, (3 .2) must hold. Proof: It follows from the complementary slackness property of pnmal and dual linear programming problems (Theorem 5, Appendix C) that if { v j, j E I} is optimal for the primal problem, then V1 = W 1a +ex. I q 1j(a)vj j ' for those values of i and a where x 1a > 0. However, we have seen in the proof of Theorem 3 that for each i E /, x 1a > 0 for some a. Therefore, if we consider the constraints of the primal problem, (3.2) must hold. We can now prove THEOREM 4. If{v/} is an optimal solution to the primal problem, t hen {v/} satisfies (i2) and consequently v/ = lP R0 (}, cx.),jE I, where R 0 E Cv is an optimal policy. Proof: From Corollary 3 we have that (3.2) must be satisfied by an optimal solution to the primal problem. Since by corollary 1 to Theorem 1, Eq. (3.2) has a unique solution {'P Ro(j, ex.), j E I}, the equality must follow. COROLLARY 1. A n optimal policy R 0 E Cv can be obtained from the optimal solution to the primal problem by letting R 0 be the policy that takes action a = a1 at state i which achieves equality in the con- straints of the primal lin ear progra mm ing problem. If more than one action a achieves equality at any state then either action may be taken. Linear Programming 45 Proof: Once a solution to (3.2) is obtained by any means, an optimal policy is prescribed according to Theorem 1 of Chapter 3. Theorems 3 and 4 and their corollaries provide the linear program- ming machinery for obtaining optimal policies. We point out that the variables {x1a} of the dual problem have policy and expected frequency interpretations for every feasible solution to the problem. 1 For x 1a is, in a discounted sense, an average probability of being in state i and making decision a when P{ Y0 =!} = {31 , l E J, and policy R E C8 is used, where R is given by D 1a = x 1afi x 1a. Thus, in a sense, the simplex algorithm a for solving the dual problem, which is a procedure that has the property that successive iterations provide improving solutions, is a special type of policy improvement method. On the other hand, for the primal problem, it is the equalities (3.2) obtained only in an optimal solution that yield an interpretation with respect to policies. Of course, the optimal values have their interpretation in terms of being the optimal discounted costs 'PR(i, ex.), iE!. In the proof of Theorem 3, the assumption that {31 > 0, l E /, is used in showing that I x 1a > 0, i E /. If we allow a subset S of states a . such that {31 = 0, l E S, then it is possible that L x 1a = 0 for some i E S. a In this case, suppose we define R = {D 10} by setting D 1a = xtaf''i x 1a if a I x 1a > 0 and choosing D 1a arbitrarily if L x 1a = 0. T hen, for every i a a such that I x ia = 0, we can assert that P R { Y1 = II Y0 = !} = 0, a t = 0, 1, ... for every l such that {31 > 0. M oreover, R is optimal with respect to minimizing \\f' R(l, ex.) for every I for which /31 > 0. H owever, R may not be optimal with respect to minimizing 'P R(i, ex.) for every i such that I x 1a = 0. a 1 For this reason we should have perhaps referred to the problem involving the variables {x,.} as the primal linear programming problem and to the other as dual. However, since the problem involving the variables {v,} arises first we have called it primal. 4 The Discounted Cost Problem Computational Examples Suppose, as in Chapter 2, that I = {0, 1 }, Ki = 2, i = 0, I, where { Wol W 0 z} = {1 0} w11 Wcz 2 2 and We take ex = !. Let us first employ the method of approximations in order to obtain an optimal policy. Let v0 (0) = v0 (1) = 0. Then using (1), and, similarly, Then, v1(0) = m!n{w0 • q0 j( a)v0(j)} = min{ l, 0} = 0, v1(0) = min{2, 2} =2. v2 (0) = mm 1 + - · - · 2 0 + - · ·_ · 2 · { 1 1 13} 2 2 ' 2 4 2' 4 3 4 Computational Example!! and v2 (l) = min{2 +! ·! · 2 2 +! · · 2} 2 3 ' 2 3 = min(i, 7 =- 3 Iteration once again yields and = min{37 93} 24'96 31 32 95 36 47 The policy R• approximating the optimal policy is the one that takes action a = 2 at state 0 since and takes action a = 1 at state 1 since 48 4 The Discounted Cost Problem Let us check whether this policy is, in fact, optimal. We have that and O n solving, we obtain 36 'P R.(O) = 29' We' can now check whether '., · 1 + . 36 + . 84) > . 36 + . 84) 2 2 29 2 29 2 4 29 4 29 and The inequalities hold so that R* is, in fact, optimal. Let us now obtain an optimal policy using the policy improvement procedure. Let R 1 be the p olicy that takes action a;;, 1 at state 0 and action a = I at state a = I. Then 1J1 R/0) = 1 + ±(fiJI RJO) + fiJI R,(l)), IJ'R,(l) = 2 + f(tiJ'R,(O) + ! IJ'R/1)), from which we see that At state 0, 0 + . 32 + . 44) < 32 . 2 4 13 4 13 13' hence, the policy R 2 , action a = 2 at state 0, and actio n a= 1 at state 1, is better. Since we a lready know R 2 is optimal, no further policy improvement iterations will be possible. Bibliographical Remarks The primal linear progra mming problem, letting {30 = /31 = -!-, is: To maximize subject to ivo - i v1 1, i v1 0, - tvo + i v1 2, - -!v0 + f v1 2. . The dual p roblem is to minimize subject t o and Xol 0, ix ol + t x11 - -!x12 = -!-, -ix ol - ixo2 + + tx1 2 =-!-. 49 We leave it t o the reader to numerically solve each of the linear pro- grammmg problems and determine the optimal policy from each solution. Bibliographical Remarks The proof of Theorem 1 essentially involves establishing that .the transformation defined by (1) is a contraction. Since we know a fixed point of (1) already exists; namely {'¥R0(i), i E I} , the remainder of the proof is somewhat simpler than the classic proof of the P icard- Ba nach fixed p oint theorem. M aitra's proof [45] for the denumerable state case motivated o ur approach. However, the theorem fo r t he fin ite case should be credited to Shapley [48] who also used the contractio n method . 4 The Discounted Cost Problem That the policy improvement procedures are associated with dynamic programming can be seen in the writings of Bellman (see, for example, [3]). The explicit procedure for the Markovian decision process with discounted cost criterion appears in Howard [34]. See also Blackwell [6]. That linear programming can be used for the discounted cost criterion is due to D'Epenoux [14]. Problems (1) Using the data provided for Problem 1 of Chapter 2, find the minimal expected discounted cost policy using each of the computational metpods. (2) In the method of successive approximations with v0 (i) = 0, i E J, show that for each i E J, where R 0 is optimal. (3) Assume min {!w;.- wia' l} > 0. How large must n be i·, a, a' a':i:-a in order that the method of successive approximations yields an optimal policy? ( 4) If in the method of successive approximations, v1(i) v0(i), i E I, show that v.+ 1(i) v.(i), i E J; that is, that { v.(i), n = 0, 1, .. . } converges monotonically to 'P Ro(i, a) from below. (5) By direct argument, that is, without resorting to the dual problem, prove that the optimal solution { v1 , i E J} to the primal problem must satisfy Problelll!l (6) Define the transformation TR of a vector v = {v(i), i E J} by (TR v)(i) = w1R +a L % (R)v(j), i E I. j Show that if v'(i) v(i), i E J, then (TR v')(i) (TR v)(i), i E ]. (7) Prove the assertions in the last paragraph of the linear programming section. 51 54 S The Optimal First-Passage P roblem some t>OIY0 =i}=l,iEI, and q00(R)=I for all RECD. Recall from Theorem 4, Chapter 3 that some policy R E CD is optimal and thus we need only consider the rules in CD. However, as in the previous chapter, it will be convenient, in the linear programming formulation, to consider the class Cs of policies. Method of Successive Approximations We first discuss the method of successive approximations in the present context. Let {v0(i), i E I- {0}} be arbitrary, and define i E I- {0}. (1) We shall prove: THEOREM 1. If {vn(i), i E I- {0}, n = 0, 1, ... } are defined by transformation (1), then lim vn(i) = (JRo(i), i E I- {0}, independent of n--+oo {v 0(i), i E I- {0}, where R0 E Cn minimizes uR(i), i E I - {0}. Proof\" Let {v0(i), i E I- {0}} be arbitrary and v 0 '(i) = uRo(i), i E I- {0}. Since R 0 ,.is optimal and is a member of CD we have that v/(i), the nth iterate of v0 '(i) in (1), is equal to v0 '(i) for n = 1, 2, .... Let a1 , i E I - {0} denote the actions minimizing the right-hand side of (1) where vn(i) is the nth iterate of v0(i) in (1). On subtraction, we have for n = 0, 1, ... , i E I- {0}. Similarly, if a/, i E I- {0}, denote the actions minimizing the right- hand side of (1), where v\"(i) of (I) is the nth iterate of v 0'(i). Then for n = 0, 1, ... , i E I- {0}. Method of Successive Approximations ss Putting the two inequalities together we obtain lvn+ 1(i)- l(i)l L %(aa lvnU)- vn'(j)l, i*O for each n = 0, 1, ... and i E I- {0}. Consequently, letting Rn denote the policy in CD which takes action ai or a/ at state i depending upon which yields the larger value for L %(a) I vn(j)- vn'(J)I, we obtain i E I- {0}, for each n = 0, 1, .... Repeated iteration yields lvn+ 1(i)- 1(i)l L P .dYn = j I Yo = i} lvo(j)- Vo'U)I i*O PlfJYn =1- 01 Y0 = i} maxlv (j)- Vo'Ci)}, j i E I - {0}, where ft\" is the policy in CM that takes action at time t according to policy Rn-r (0 t n). From Problem 2, Chapter 3, it follows that i E I- {0}. n--+oo Therefore lim lvn(i)- vn'(O)I = 0, i E I- {0}. Since we have that lim vn(i)= n--+oo uR0 (i), i E I - {0}, and the theorem is proved. The remarks regarding the method of successive approximations in Chapter 4 hold here as well. In particular we have: COROLLARY I. The function uR0(i) , i E I- {0}, uniquely satisfies i E I - {0}. (2) 56 5 The Optimal First-Passage Problem Proof: Same as for Corollary 1 to Theorem 1, Chapter 4. Thus, we start the method of successive approximations with an arbitrary function v0 (i), i E I- {0}, and iterate it according to (1). In the limit, we get (2) with R 0 E CD as that policy determined by those actions which minimize the right-hand side of (2). In practice, the limit is not attained, but a large number of iterations of (1) should in most cases yield the optimal policy or a good approxima,tion. Policy Improvement Procedure We turn now to the policy improvement procedure for obtaining an optimal policy. Let R denote an arbitrary policy in CD. Then {uR(i)} satisfies uniquely (Theorem 2 of Appendix A) the system i E I - {0} . For each i E I- {0} let E, denote those actions a for which wia + I q,i(a)uR(j) < uR(i) . i*O (3) Define R' E CD by ch<?osing an action in E, for at least one i where E, is not empty. At all other states Jet R' = R. If E, is nonempty fo r at least one i we call the transformation of R to R ' a policy improvement iteration. A sequence of policy improvement iterations that leads to an optimal p olicy is called the policy improvement procedure. That, in fact, every policy improvement procedure leads to an optimal policy is summarized in the following theorem and corollary. THEOREM 2. If R.' is obtained from R by a policy improvement iteration, then uR'(i) uR(i), i E I - {0}, with strict inequality holding for at least one i E I - {0}. Proof: From (3) on substituting the inequalities of the policy Linear Programming FormUlations improvement iteration, we have uR(i) wiR' + I qii(R')uR(j), i *O 57 i E I- {0}' with strict inequality holding for at least one i E I- {0}, namely, for i where R' -f:. R. On iterating the inequality we obtain uR(i) W;R' +I q,/R')[wiR' +I qiL(R')uR(l)] joFO !oF O T = 1, 2, . ... On letting T oo we obtain uR(i) uR.( i) +lim I q\\J+ 1l(R ')uR(j) T--< oo joFO i E I - {0}, since lim qfJl(R') = 0. Strict equality holds, at least for those i where T--< oo R ' -/:- R. The theorem is proved. CoROLLARY 1. The policy improvement procedure converges , within a finite number of policy imp rovement iterations, to an optimal policy. Proof: Each iteration yields a strictly better policy. Only a finite number of policies are in CD . Thus, at some point, no policy iteration is possible and (2) is satisfied by the final policy; by the cor ollary to Theorem 1 it must be optimal. Linear Programming Formulations We allude now to the linear programming formulations, the primal and dual, of the optimal first-passage problem. 58 S The Optimal First-Passage Problem Consider first what we call the primal problem: To maximize subject to V; W;a + L %(a)vi, a E K;, i - {0} , where the {{Ji} are known positive numbers such that L {Ji = 1. The , dual problem is: to minimize subject to i E I- {0}' and j E I- {0}' By the same methods of Theorem 3, Chapter 4 we can assert that there is a one-to-one s:orrespondence between any solution {x;.} to the dual problem, and R E Cs given by and R Xia L. X ;a \"\" a E K;, i E I- {0}, X;a = I /J1 L = i, Yn # 0, 0 n t I Y0 = le l -{0) t=O \"\" I /31 L le l -{0) t=O a E K;, i E I - {0} . In words, X;a is equal to the expected number of times under the policy R that the process is in state i and action a is taken before the process Computational Examples 59 enters state 0 given that P{ Y0 = /} = /31, IE I- {0}. That x;., in fact, is finite for R E Cs, follows from the assumption that each i, i E I- {0}, is transient for every R E CD and Theorem 2 of Appendix A. Thus, Theorems 3 and 4 of Chapter 4 and their corollaries have their counterpart for the optimal first-passage problem with Eq. (2) of this chapter replacing (3.2) of those discussions. When some fJ/s are equal to zero the remark in Chapter 4 holds here as well. Computational Examples In order to keep the computations extremely simple we shall consider a two-state problem with one of the states as the target state. Clearly, for such a simple case, the optimal policy can be seen by inspection. However, we shall formally go through the steps of the various procedures. Suppose I= {0, 1}; 0 is the target state; K 1 = 2, q11 (1) = !, q11 (2) = !; Wu = 3, w12 = 1. We first use the method of successive approximations. Let v0 (1) = 0. Then v1(1) = min{3 + -!v0(1), 1 + 1v0 (1)} = min{3, I} =1. v2 (1) = min{3 + 1- · 1, l + 1· 1} -2.. - 3' v3 (1) = min{3 + 1- · f, 1 + 1 · f} - 10 - 9• On the basis of v3 (1), we have that 110 2 10 3+-->1+-- ' 2 9 3 9 ' hence, the approximation to the optimal policy is to take action a = 2 60 5 The Optimal First-Passage Problem at state 1. In this case the approximation is, in fact, the optimal action. Using policy improvement, suppose R 1 is the policy which pre- scribes action a = 1 at state 1. Then O\"Rt(1) = 3 + kRt(l)' and, therefore O\"RJ1) = 6. Since 6 > 1 + t(JRJ1) .. I ' = 5, / R 2 , which prescribes action a= 2, is the policy obtained by the policy iteration; Cn contains only the policies C1 and C2 ; therefore R 2 is optimal. To evaluate O\"R 2(1), we have that O\"R,(1) = 1 + !O\"R 2(1), or O\"R/1) = 3. The primal linear programming problem looks like: Maximize subject to and Clearly the solution is v1 = 3, and since equality is obtained at the second constraint, action a =2 is optimal. The dual problem takes the form: Minimize subject to !xu +tx12=1. The solution is x 12 =3 which yields D12 = 1, as the optimal policy. The Finite Horizon Problem 61 The Finite Horizon Problem as a First-Passage Problem The finite horizon problem of Chapter 2 can be viewed as an optimal first-passage problem. Let I' be the state space consisting of all pairs i' = (i, t), i E J, t = 0, 1, . .. , T and an adjoined state 0 (say); let K; · = K i , i' E I' - {0}, K 0 = 1, q;·r(a) =%(a) if i' = (i, t),j' = (}, t + 1) for i,j E J, t = 0, 1, • .. , T- 1, q i•o(a) = 1 if i' E {(i, T ), i E I}, q00(a) = 1 and q;·r(a) = 0 otherwise; W; ·a = W;a, i E J, t = 0, ... , T, and w0 a = 0. Denote by C' the class of all policies. This is merely an enlargement of the original state space to one where the new state designation includes the time of observation as well as the original state ; state 0 denotes time T + 1 without concern for the original state at time T + 1. Within this conception the state 0 is an absorbing state and a first passage from any state in {(i, 0), i E I} to state 0 takes exactly T + 1 units of time. Thus, it should be clear, that to find RECto minimize SR ,r(i), i E J, is equivalent to finding R E C' to minimize O\"R((i, 0)), i E J, where the \"target\" is the state 0. This observation coupled with the contents of this chapter point out that S R y(i) can be minimized by the method of successive approxi-. ' mations, the policy improvement procedure, and by linear programming. This is not to say that any of these methods would be superior to the method of Chapter two. The simple dynamic programming algorithm is the natural and highly efficient way to solve the problem. However, when certain types of additional constraints are imposed the dual linear programming approach may prove useful. For example, suppose, for a given initial state i, we wish to find R E C8 to minimize SR,r(i) subject to the constraint that SR,r(i) s (a given constant). Translated to the first-passage problem this would be equivalent to finding R E C8 ' (the stationary Markovia n subclass of C') such that O\"R((i, 0)) is minimized subject to O\"R((i, 0)) s. Letting fJ(i,O) = 1, /1;• = 0, i' i: (i, 0), this problem can then be formulated as finding {X;·a}, to minimize 62 5 Tbe Optimal First-Passage Problem subject to X;•a 0, a E K1., i' E /' - {0}, j' E J'- {0} and If {x;•a} is the optimal to this linear programming problem, set D; ·a= X; ·JL X;·a if L X; •a > 0 and let D; •a be arbitrary if L X;•a = 0. a a a We point out that the policy so obtained will not in general be a member of Cv' since for at least one state i' a random mechanism will be used for deciding on which action to take. Bibliographical Remarks The optimal first-passage problem was formulated by Eaton and Zadeh [30]; they called it a \" pursuit problem.\" The transformation (1) is not, in general, a contraction for the /\"' norm. However, all states except 0 are transient, the proof of the convergence of the method of successive approximations proceeds along the lines of the previous chapter. Other norms are given by Veinott [53] for which (1) is a contraction. A different linear programming formulation involving the minimiza- tion of the ratio of two linear forms (a problem called a fractional linear programming problem which can be readily transformed into a linear programming problem) was first given by Derman [15]. The one given here circumvents the need for the fractional linear programming form. The remark regarding the formulation of the finite horizon problem as a first-passage problem with application to constrained optimal policies appears in Derman and Klein [22]. Problems Problems (1) Is it possible to put a bound on lvn(i) - aRo(i)l, i E I- {0}? (2) For the data given in Problem 1, Chapter 2, find the optimal policy using each method . 63 6 Expected Average Cost Criterion Computational Procedures Summary In Chapters 4 and 5, it was shown that the method of successive approximations, the policy improvement procedure, and linear pro- gramming provide general methods for obtaining optimal policies for the discounted cost criterion and for the first-passage problem. In this chapter, which is dev oted to the expected average cost criterion, we shall see that a special kind of policy improvement procedure and the 65 66 6 Expected Average Cost Criterion methods of linear programming provide general algorithms for obtain- ing optimal policies. Policy Improvement Procedure We first consider the policy improvement procedure. Let R E Cn be arbitrary. We let ' 1 T nii(R) = lim -- I qijl(R), T->oo T+1t =O i,j E I, the limit always existing (Theorem of Appendix A). We also have (Theorem 1 of Appendix A): na(R) =I nii(R)qj?(R) =I qljl(R)ni1(R) ' j j = L n,j(R)nit(R), j relations which we shall use throughout. Consider the equations in { 4\\, v,, ... i E I}: and ¢, + v, = w,R +I %(R)vi, j i,jEI, t=0,1, . .. , i e i (1) I n,iR)vi = 0, j i E J. (2) Equations (1) and (2) are the essence of the policy improvement pro- cedure. We shall construct a solution to (1) and (2). Let E 1 , E 2 , ... , Ek be the recurrent classes of I under R . Let E = {j1 , .. . , jk} be a set of selected states from E 1 , • • . , Ek; that is, in E E., n = 1, ... , k . D efine w;R = w,R - ¢R(i), i E I; let W/ = w}R, if <- 1 Y, = j, and set r = min{t I Y, E E, t 1}. Let uR(i) = ER{ I W/ I Y0 = i}, t=O i E I ; that is, uR(i) is the expected cost under R and the cost structure { w;R} of going from state i to any of the states in E not counting the cost at the time of arrival. Policy Improvement Procedilre 67 In constructing a solution to (1) and (2), we first demonstrate that { ¢R(i), uR(i), i E I} satisfies the system (1). T hen by a suitable m odifica- tion we can construct a solution to (1) and (2). By its definition we clearly have that i E J. However, from Theorem 5 of Appendix A, for any i E E (say i E E., where E\" is one of the recurrent classes E1 , ... , Ek), using the fact that nii = 0 if j smce Therefore, = - 1 - I nii(R)( wiR - ¢R(j)) nii(R) j E En = ··( 1 R) (¢R(i) - .I n,j(R)¢R(j )) nn ) E E n =0 = I na(R)w1R l e En = w,R - ¢ R(i) + I qii(R )uR(j), i E I' je l and { ¢ R(i), uR(i) , i E I} is a solution to (1). Define where 70 6 Expected Average Cost Criterion Using (2) and the Abelian theorem l(b) of Appendix B on the last term of the right-hand side, the lemma follows. Let R E Cv be arbitrary. For each i E I, define Ei to be the set of actions at state i for which L %(a)¢R(j) < cPR(i), j or, if no actions satisfy the inequality, the set that satisfies and Wia + L qij(a)vR(j) < wiR + qiJ(R)vR(j) j = cPR(i) +- VR(i) · Define R' E CD as the policy which takes an action a E Ei in at least one state i for which Ei is nonempty; otherwise, the action taken is the one dictated by R. Of course, if Ei is empty for all i, then R = R'. If R' =I= R, then either L qij(R')c/JR(j) cPR(i), j i E I' ' (6) with strict inequalitY,., holding for at least one i and qiJ(R') = %(R), wiR = wiw j E I, for each i where equality holds, or and L qij(R')cjJR(i) = cPR(i), j i EI' cPR(i) + vR(i) wiR' + L %(R')vR(j), j (7) i EI, (8) with strict inequality in (8) holding for at least one i and %(R') = qu(R), wiR = wiR' j E I, for each i where equality in (8) holds. L EMMA 3. If R' =I= R, then c/JR'(i) cPR(i), i E J , (9) P olicy Improvement Procedure 71 and iEI, IX near 1, (10) with strict inequality holding in (10) for at least one i. Proof: From the representation (5) we can write 'I'R(i, IX)= WiR + VR(j) + eR(j, IX)}, i E J. If (6) holds, for some IXo near enough to 1 we can write for all IX IX0 , \" ' {c/J R(j) ( ') (j' )} l-IX +VR) +eR ,IX = wiR' + cc L %(R' )'I' R(j, IX), i E I, j with strict inequality holding for that i where strict inequality holds in (6). Thus, Theorem 2 of Chapter 4 applies; that is, policy improvement for the discounted cost criterion takes place in going from R to R ' for every o:0 . If (7) and (8) hold, the same can be said. Thus (10) holds. From the fact that (10) holds and using (5), one sees that (9) also holds. Thus the lemma is proven. Let us define the transformation from R to R' as a policy improve- ment iteration. Thus, the policy improvement iteration takes a policy R E CD to R' E CD such that ( 6) is satisfied or (7) and (8) is satisfied. Thus, the policy improvement iteration is analogous to those discussed in Chapters 4 and 5 though somewhat more complicated. We refer to a sequence of policy improvement iterations as the policy improvement procedure. We have: THEOREM I. The policy improvement procedure leads to an optimal policy within a finite number of iterations. Proof : Let R 1 , R 2 , . • • be the policies obtained from a sequence of policy improvement iterations with R 1 arbitrary. Since there are only a 72 6 Expected Average Cost Criterion finite number of policies in Cv and {\\f\" RJi, a), i E I} is a strictly decreas- ing sequence as long as a policy improvement iteration can be effected, there is an n for which Rn = Rm+ 1 = R (say); that is, a policy iteration on R results in no change of policy. The fact that {\\f\" RJi, a), v = 1, ... , n} is strictly decreasing prevents cycling from occurring within the sequence R 1 , ... , Rn. Then we must have and i E /, (12) where K;' in (11) is the subset of actions at i such that equality is achieved in (12). We now show that whenever ,R is such that both (11) and (12) hold, then R must be optimal; that is, R is not a local minimum but is, in fact, an absolute minimum. Suppose R is an arbitrary policy in CD. By virtue of (11) and (i2) holding together with the argument employing (5) and its expansion used to prove Lemma 3, we now conclude that for all a sufficiently c!ose.to one. By the method of proof used in proving Theorem 2, Chapter 4, we obtain the fact that \\f\" R(i, a) \\f\" R.(i, a), i E I , for all a sufficiently near 1. From (5) we then conclude that Since R is arbitrary this proves the theorem. To spell out the policy improvement procedure, we first start with an arbitrary R1 E CD. W e then solve for {¢R,(i), vR/i), i E /}. Given { rPR,(i), i E I}, we obtain { vRJi), i E I} algebraically by virtue of Lemma 1. For any R E CD, {¢ R(i),i E I} is calculated from {n ij(R),i,j EI} , Linear Programming Formulations where { n;j(R) = nn i , i, j E En} is the unique solution to j E En, k (Theorem 4 of Appendix A), and for i ¢ U En, n = l 73 k where a;n = P{Y 1 E En for some t l l Y0 = i} . {cx;n , i ¢ U E\"} uniquely · n =1 satisfies the system k ain = L qij(R) + j E En i E U En . n= 1 k j ¢ V En n = 1 Hence {¢R,(i), vR,(i), iE / } can be obtained algebraically. Having solved for {¢R,(i), vR,(i), i E I}, R 2 is obtained by a policy improvement iteration; that is, at one or more i, where possible, an action a is taken which satisfies either (6) or (7) and (8) with R 1 = R 2 at .. all other states. This process is repeated until (1 1) and (1 2) are satisfied, at which point an optimal policy is on hand. U nfortunately, ¢R(i) a nd vR(i) must be obtained a new at each iteration. Linear Programming Formulations W e now turn to the linear programming approach to obtaining an optimal policy. First consider the linear programming (primal) problem . To determine values of the variables { ¢;, v, , i E I} to maximize 74 6 Expected AYerage Cost Criterion subject to I (b,i- %(a))¢i 0, j a E K,, i E I, a E K ,, i E I, (13) (14) where [Ji > 0, I [Ji = 1 are known constants. The dual problem is to find j values of the variables {x,a, Yia, a E K ,, i E I} to minimize subject to Yia 0, (15) 1 a I X j a +I IY ia(bij- Yij(a)) = [Jj, j E I. a i a (16) We consider the primal problem. In what follows, R* E CD is an optimal policy. LEMMA. 4. Let {¢,, v,, iEI} be any optimal solution to the primal problem; then'¢;= ¢R.(i), i E I. Proof: From (13) we obtain that I nc;(R*)¢, nli(R*)w;R• i i =rf>R,(l), lEI . From (14) we have that fort= 1, 2, I qfY(R*)¢i ¢ 1 , IE I. j Hence, Linear Programming Fonnulations 75 Thus ¢R.(l) ¢ 1 , lEI. However, from Eqs. (11) and (12), we see that {¢R.(i), vR,(i) + c¢R.(i), i E I} for c large enough is a feasible solution to the primal linear programming problem from which it follows that ¢i = ¢R.(j),jEI. LEMMA 5. Let RECD be any optimal policy and {¢,,v,,iEI} an optimal solution to the primal problem; then I vj(bii- qii(R)) + ¢, = w,R j for every i that is recurrent with respect to R, and I (bij- %(R))¢j = 0 j for every i E I. Proof: The first assertion follows from (13) on premultiplying the appropriate inequality in (13) by nu(R) and summing over i. If equality fails to hold, we have I n 1,(R)¢, < ¢R(l) for some lEI; hence, i L nu(R)¢, <I nli(R)¢R(i), contradicting Lemma 4 and the assumption i i that R is optimal. The second assertion must hold since, by Lemma 4, ¢, = ¢R,(i) = ¢R(i), i E I, and therefore, I qij(R)¢j = L %(R)¢RU) j j = rp, , i E J. Lemma 5 asserts that in any optimal solution to the primal problem one can always select actions a = a, for each i E I such that l:Cbii- qii(a;))¢i = 0, i E I, and L v/bii- qii(a;)) + ¢, = w,a, for j j all i in a nonempty subset A of I. Let R E CD denote the policy that takes action a, for i E I. THEOREM 2. If the states i E I- A are transient with respect to R then the policy R is optimal. 76 6 Expected Average Cost Criterion Proof: By hypc;>thesis, the states i for which L v/bii- qii(R)) j + ¢, < w,R are transient with respect to R . From (14) we have L nli(R)¢, = ¢R(l), lEI, i and from (14) with equality holding, ¢L = L nlj(R)¢ i, l E I. Hence ¢ R(l) = ¢ 1 = ¢ R .([), l E I, and R is optimal. COROLLARY 1. If for some R there exists { v,, i E I} satisfying L vj(bu- qu(R)) + ¢R•(i) = wiR, i E I j - then R is optimal. L (bii- qu(R))¢R.0) = o,' j The linear programming method as suggested by Theorem 2 is to solve the primal linear programming problem and choose R E Cv, if possible, by taking those actions for which equality in (13) and (14) are simultaneously obtained. There may be some states where equality in (13) is not attained for any action. If we are fortunate in our selection of actions, then the states where equality is not attained will be transient, in which case R is optimal. However, we may not be so fortunate in our selection of actions as to have the states transient where equality is not attained. We now show that the solution of a second linear programming leads to an optimal policy. The idea behind the method is that of finding new values of the variables {v;} which will force equality in (13) and (14) for at least one decision at every state. Let { ¢,, v,, i E I} be an optimal solution to the primal problem (we refer to this problem as Problem !). Let A denote the states for which equality in (13) and (14) is achieved for at least one action. Then by Lemma 5, I - A must consist entirely of transient states under every optimal policy. Let A' be the largest subset of A such that for some action a, satisfying the equality in (13) and (14), we have qii(a,) = 0 for Linear Programming Formulations 77 all j E I -A . By Lemma 5 the states in A -A ' must also be under an optimal R, because if i E A - A' is recurrent under an opt1mal policy, then there exists an action a, (viz. the action in the optimal policy) for which the equality in ( 13) and (14) must hold and, smce the states of I - A are transient, q ii(a,) = 0, j E I - A, which is contrary to the definition of A' . LetT= I- A' and K, denote the actions at state i, i E T, for which equality in (14) holds. Consider Problem 2: To find {u,, i E T } to maximize 2: u, i.E T subject to = b;(a), aEK;, iET. We shall show that by solving for {u,, i E T} and replacing v, by u, for i E T, we shall be in a position to make use of Corollary l of Theorem 2. LEMMA 6. A finite optimal solution to Problem 2 exists. Proof: From Lemma 5 it follows that there is at least one set of actions a, E R,, i E T, for which the states i E T are transient. Let {a · i E T} be such a set of actions. Then L ui811 - %(a.)) = b;(a,), P jE T i E T, has a unique solution (Theorem 2 of Appendix A), say {ui *, jET}. Let {ui, jET} be any solution to the inequalities of Problem 2. In particular we must have L ui bii - qjj(a;)) b,(a;), iE T , jeT On subtracting, we get L (u/ - u)( bu - qii(a.)) 0, i E T , jeT 78 6 Expected Average Cost Criterion from which (since the inverse of I- Q consists of all positive terms) u/;:;; u1 , and, hence, I u/;:;; I u1 . jeT jeT jET, LEMMA 7. Let { u1 , j E T} be any optimal solution to Problem 2 and {a,, i E T} be the actions from which l1 1(a) = b;(a)- I u1(<;,.i- %Ca)) jeT ., is,rninirnized; then 11 1(a;) = 0, i E T. Proof: If 11,(a;) > 0, then u1 + e withe > 0 and small enough would imply { u/ = u1 , j =/= i, u;' = u1 + e} is also a feasible solution. However, {u) would not then be optimal. We now have THEOREM 3. Let R be the policy obtained by taking the actions dictated by the solutions to Problems 1 and 2; then R is optimal. Proof: The theorem follows from Lemma 4, L emma 7, and Corollary 1 of Theorem 2. Policy Improvement, Linear Programming under Irreducibility Assumption We now discuss the problem of finding the optimal policy under the assumption (A) : I is irreducible for every R E C0 ; that is, for every R E C 0 , every i is recurrent and every pair of states i and j communicate. LEMMA 8. If (A) holds, then I is irreducible for every R E Cs. Proof: Let R E C5 be arbitrary and piJ =I i,jE I. F or a Policy Improvement, Linear Programming 79 some where Let li=min{D; 0 .}. i Hence, p,1 ;:;; (;q 1j(a;); i,j E I. Let R 0 E C0 be defined as the policy that takes action a = a 1 in state i, i E I. Since I is irreducible under R 0 , for each i,j E I there exists an n such that q}j>(R0 ) > 0. Consequently, P!]> > 0 for the same value of n. Thus under Revery state communicates with every other state from which it follows that I is irreducible under R. If (A) holds, ¢ R(i) = ¢ R independent of i for every R E C 0 (Theorem 4 of Appendix A). Consequently, (7) always holds so that the policy improvement procedure is involved only with inequalities (8) . Also since niJ(R) = n1(R) independent of i,Eqs.(2) reduce to a single equation. We can then alternatively replace (2) by the equation; for example, vR(j) = 0 for some given j E I. Thus, the policy improvement procedure takes the form of starting with an initial policy R 1 E C0 and solving for {¢R,, vR,(i), i E I}. Then R2 is taken to be any policy in C0 that takes action a for at least one state i which reduces W;a + IqiJ(a)vRJj). Where re- i duction cannot or is not effected, the action a under R 1 is taken . We then repeat this process to obtain R 2 , R 3 , ... until for some n, R\" = Rn+l> at which point R\" is optimal. The linear programming formulation (the primal problem) is to find variables { ¢, v1 , i E I} to maximize subject to I v/(;tj- qij(a)) + ¢ W;a, j a E K,, i E I. (17) An optimal solution { ¢, v1, i E I} will have ¢ = ¢R• where R* is an optimal policy and an optimal policy is obtained from the linear pro- gramming solution by taking an action a= a1 in state i where equality holds in (17). Since, by assumption (A) there will be no transient states, an action a = a 1 in state i where equality holds in (17) , will exist for every i E J. 80 6 Expected Average Cost Criterion The dual problem becomes that of finding variables {x;a, a E K;, i E I} which minimize I Lxia Wia i a subject to a e Kt , i e I, · I , ,. I I X;a(c\\;- qij(a))= 0, (18) i a Since under (A), { rr. j(R), j E.!} satisfy uniqt!ely the steady-state equations I rr.,(R)(bij- qij(R)) = 0,' j E I' i . I rr.j(R) = 1' j where rr. /R) > O,j E I, by arguments similar to those employed in Theorem 3, Chapter 4, we can assert that there is a one-to-one corres- pondence between the solutions {x;a} to (18) a nd poFcies R E Cs given by a E K;, i E I, and X;a = a E K;, i E I. T hus, every solution {x;a} to the dual problem is capable of a policy interpretation; in particular, ¢R =I I X;a W;a . If, in fact, an optimal i a solution is obtained by the simplex method, then the corresponding policy R will be a member of C v since at least one of the equations in (18) is redundant and I X;a > 0, i E I . Hence, for exactly one a= a;, a X;a > 0 for each state i. Computational Example 81 In the next chapter we shall be concerned with obtaining optimal policies under certain types of additional constraints. Here optimal policies will be outside the class C v . It will turn out to be most useful to approach this kind of problem from the point of view given by the correspondence between X;a and D 1a as suggested by the dual problem. Computational Example Suppose we have I= {0, 1 }, K; = 2, i = 0, 1, where {Wol Woz} = {I 3} w11 w12 4 0 and First, we use the policy improvement procedure to find the optimal policy. Let R 1 be the policy th at takes action a = I at state 0 and action a = I at state I . The transition matrix under R 1 is { qol(Rl) qol(R 1)} = {1 0} qu(R l) q11(R 1) 1 0 · Clearly rr.00 (R 1) = rr.10(R1) = 1 a nd rr.0 1(R1 ) = rr.11(R 1) = 0, from which ¢R ,(O) = ¢R,(l) = 1. Equations (1) become 1 + v0 = 1 + v0 , 1 + v 1 = 4 + v0 , from which v0 = v1 - 3. From (2) we have v0 = 0; hence, v1 = 3. Now 3 + ±v0 + -!-v1 > 1 and v1 = 3 < 4; therefore R 2 , action a= 1 at state 0 and action a = 2 at state 1, is an improvement over R 1• T he matrix of transition probabilities under R 2 82 6 Expected Average Cost Criterion is This time n 00(R 2) = 1, n01 (R 2) = 0, n 10(R 2 ) = 0, n!l(R2) = 1, where ¢R 2(0) = 1, ¢R2(1) = 0. Now qoo(a2)cpR,(0) + qo!(a2(¢R 2(1) = 1-c/JR,(O) = 1-¢R 2(1) = -!- < ¢Rl(0) = 1; hence R 3 , action a= 2 at state 0 and action a= 2 at state 1, is an improvement over R 2 . The matrix of transition probabilities under R3 is Here we must have n 00(R 3 ) = n 10(R3 ) = 0 since state 0 is transient under R 3 , while n 01 (R 3 ) = n11(R 3) = l. Thus, ¢R,(O) = 0, ¢R,(I) = 0. Equations ( 1) become hence, v0 = 6 + v1 . From Eq. (2), v1 = 0; thus v0 = 6. Now and Wo1 + qoo(1)v0 + qo1(1)v1 = 1 + 6 > ¢R,(0) + Vo = 6 Wu + q!o(Ova + qu(l)vl = 4 > ¢R,(1) + vl = 0. Therefore, R 3 is optimal. Bibliographical Remarks 83 We can write the primal linear programming problem: to find ¢ 0 , ¢ 1 , v1 , and v2 to maximize subject to ¢ o 1' ±v0 - 1-v1 + ¢ 0 3 , ¢1 4, ¢1 0, ±¢o - -}¢1 0, ¢ o 0 . By inspection, it is seen that an optimal solution is Equality in (13) is achieved at state 1 with action a= 2. Eq uality in (14) is achieved by actions a = 1 and 2 at state 0 and by action a = 2 in state 1. Problem 2 becomes : choose u0 to maximize subject to tu0 3 + -}; that is, u 0 = 7. Setting v 0 ' = 7, v1 ' = 1, we get from C or ollary 1 of Theorem 2 that R, action a = 2 at both states 0 and 1, is optimal. Bibliographical Remarks T he policy improvement procedure is d ue t o H oward [34]. The proof of the existence and u niqueness of solutio ns t o Eqs. (I) and (2) given here stems from the approach taken by Derman and Veinott [26]. Blackwell [6] originally gave a different developm ent of 84 6 Expected Average Cost Criterion representation (5) involving solutions to (1) and (2). Equations like (11) and (12) appear in Bellman [2]. Miller and Veinott [47] and Veinott (53] generalize the representation (5); that is they are able to express 'I' R(i, a) in a Laurent expansion in p = (1 - a)/a (the interest rate when a is the discount factor) . J ust as (5) is crucial to the policy improvement procedure, the Laurent expansion in pis used to obtain algorithms for finding policies optimal to more sensitive criteria. I n particular, an algorithm can be given to obtain a policy optimal in the sense of Corollary 1 to Theorem 1 of Chapter 3. Also '[53] a more efficient method for { <f>R(i), vR(v), i E I} is which does not require computation of the quantities {nii(R)}. The linear programming method in the dual form was given first by Manne [46] for the case where all states belong to one irreducible class for every policy R E Cn. The multiple class case is due to Denardo and Fox [13], although the first attempted linear programming approach utilizing the first of the two problems goes back to Balinski [1]. No satisfactory treatment of the dual problem for the multiple class case has been published. Problems (1) is any stochastic matrix and 1 N IT =lim - L P\". N- coN n=1 (a) Show that the rank of er/l is equal to the number of rows of P. (b) Prove that / - P +IT has an inverse . . (c) (Veinott [53]). Show that if B is a square matrix for N which lim L B\"/(N + 1) = 0, then I - B is non- N-+oo n=O N n singular and (I- B) - 1 = lim L L Bk/(N + 1). N- co n=O k=O N Hint: (I- B) L B\" =I- BN + 1 for every N. n= O Problems (d) As an alternative approach use (c) to prove (b) giving a representation for (I- P + II) - 1 . (2) For the data provided in Chapter 2, p. 17, solve for the optimal average cost policy by policy improvement and by linear programming. (3) Let the costs be as in Problem 2. However, suppose ( (qoo(l), qoo(2)) (q1o(l), q1o(2)) (q2o(1), q2o(2)) (qoo(l), qo1(2)) (qo2(1), qo2Cl))} (q 11 (1), qu(2)) (qdl), qd2)) (q21(1), q21(2)) (q22(1), q22(2)) \\ 0:, t) (t, t) (0, t)} = (t, t) (t, 0) (0, t) ; (0, t) (0, t) (1, t) find the optimal average cost policy using the policy im- provement procedure and linear programming. (4) Show that Lemma 1 does not necessarily hold if </>i =I= <f>R(i) when i is transient. (5) Show that the optimal set of actions to be taken in the states of T (defined for Problem 2 in the linear pro- gramming formulation) are any set which make all the states of T transient. (6) Given any subset I' of I, for which I' is inaccessible from the states of I - I', construct an algorithm that will find a policy R E Cn , if one exists, which has the pro- perty that all the states in I' are transient. 85 7 State-Aetion Frequencies and Pr·oblems with (A)nstt·aints Introduction and Summary Most problems encountered involve cost criteria expressible in terms of the frequencies of the occurrence of the various combinations of states and actions as the decision process evolves over time. I n seeking an optimal policy, we have seen in the cases studied that not all policies have to be considered. In the problems of Chapters 4, 5, and 6 it is sufficient to limit consideration to the policies of Cv although for com- putational purposes, we did allow policies from Cs . The aim of this 87 88 7 State-Action Frequencies, Problems with Constraints chapter is to show that the expected long range state-action frequencies generated by a decision process under an arbitrary policy can be reproduced by a policy belonging to CM , or, under certain conditions, to C5 or CD. Once having shown this, we will possess a most useful tool for enabling us to assert that optimal policies for other problems exist some- where in the class CM, C5 , or CD. An analogous result will be obtained for the long range state-action frequencies without taking expectations. This approach becomes particularly useful when optimization problems involving nonlinear functions or side constr .. aints are con- sidered. In such problems, it is not obvious (nor necessarily true) that optimal policies exist at all, or if they do that one can be found in the class CD or C5 . F or example, suppose 0 is the initial state of a dynamic system and j is a state to be avoided if practicable; for example, j may represent a state at which the system is inoperable. Only one action is available in state j and qj 0(1) = 1. In other words, when the system fails it takes a unit of time to make it operable, after which the system is always returned to its initial state 0. However, there are other states from which the system may be returned to 0. Each return to 0 is costly, although the costs may not be assessable. All that can be said is that returns to 0 from failure are more costly than returns from nonfailure states. Under these circumstances a not unreasonable problem can be formulated: to find the policy R E C that maximizes the expected time until a recurrence of state 0 takes place, subject to the constraint, that P{Y, = }, t < -rl Y0 = 0} where-r = min{tl Y, = 0, t 1}, and a is a given number between 0 and 1. At this point, it is not obvious that an optimal policy must exist for this problem which is a member of the class CD or C5 . It will be shown later that the expected recurrence time of state 0, E-r, and the constraint can be expressed in terms of expected state-action freq uencies and that an optimal policy exists in Cs. Expected State-Action Frequencies With some apologies for the notation, let 1 T = T + 1 J/R{Y; = j, A, = a I Y0 = i}; Some Examples 89 in words, under policy R and given Y0 = i, is the expected frequency, up to timeT, of entrances into state} where action a is taken. Let XrR(i) denote the matrix (we assume, with no loss of generality, Ki = K for all i) of Xf ji) over all a E K, , j E I. For a given policy R, let HR(i) denote the set of all limit points of the sequence {XrR(i), T= 0, 1, ... }. Under some policies, at least those in Cs, HR(i) will consist of one point. However, for the larger classes this may not be true. Let H(i) = U HR(i) , ReC HM(i ) = u HR(i)' ReCM H 5(i) = U HR(i), R e Cs HD(i) = u HR(i) . Re Co Thus, for example, H(i) is the totality of limit points of {XrR(i), T = 0, 1, ... } obtainable as R assumes all possible policies. Further, we let H 5(i ) and H D(i) denote the respective closed convex hulls of Hs(i) and H D(i). Some Examples The following are examples of problems, where the cost criterion and constraints are expressible as functions of points in H(i) or subsets thereof. Example 1. To minimize cpR(i), the expected average cos( We need only note that ¢R(i) = lim inf (or lim sup) I I xL(i)wja. j a Thus, fo r some point XR(i) = E I!R(i), cfJR(i) = L L j a 90 7 State-Action Frequencies, Problems with Constraints and for some point {Xja(i)} E H(i) (or, as we have shown in HD(i)), ifJ R is minimized. Example 2. Suppose Y0 = i. Let r = min{t I Y, = i, t 1}. Assume ERr< co for every R E C. Let t CJR(i) = ER L w; = L L 1'/ja Wja' c= 1 j a where 1'/ja is the expected number of t such that Y, = j and A, = a for 1 t r.The problem is to minimize CJR(i). Since the cost criterion(]\" R (i) is involved with the decision only from t = 0 until t = r, the first time of reentry to state i, we need only consider the class of policies that\" begin again\" every time i is entered; we call these\" renewal policies.\" Under such policies { Y,} is a \"recurrent event,\" process (see Appendix B), entry into state i being the recurrent For such processes, from Theorem 6 of Appendix B, l . c·) 1'/ja Jill Xrja l =-, T...,ao Et if Er <co. However, I 11 ia = I since there is precisely one entry into i for I t r. a Hence, Er: = (lim L Xr;a(i)) - 1 T-+oo a (Note, the equation extends to the case where Er = co; that IS, if Er = co, then lim I Xy;.(i) = 0). Therefore, T-+oo a L L = lim -=-i---=\"=---=---- r .... co L xL(i) so that CJR(i) is expressible as a function of the points in H(i). In par- ticular, the problem is to find the point {xf.,(i)} in U H R(i) which R e C' minimizes CJ R(i), where C ' is the class of renewal policies. The Main Theorems and Applications 91 Example 3. Suppose Y0 = i and r is as in Example 2. Letj\" ... ,j, # i be such that qi. 1(a) = I, v = 1, ... , r. Let v = 1, ... , r. Consider the problem of maximizing Er subject to constraints v = 1, . . . , r. In Example 2 it was shown that Er is expressible as a function of points in H(i). Since Y, = j\", 1 t r, for at most one t, it follows that v = 1, ... , r. However, since 1'/ja are expressible as functions of points in H(i), the problem can be expressed as finding that point in H(i) which maximizes Er subject to the given constraints. The Main Theorems and Applications We now proceed to the main theorems. n THEOREM 1. Let Yo = i, R1, ... , Rn E C, {[JJ 3 f3u 0, L f3u = 1. u=1 Then there exists an R E CM satisfying n X R(·) \" {J ' .-·Rv(·) T l = L., u T l ' for all T . u=1 · Proof\" Consider a policy generated by selecting R \" (v = 1, ... , n) at random according to selection probabilities {30(v = I, 2, .. . , n); that is, introduce an initial randomization over the policies R 1 , . . . , R\". Denote this policy by Jt Strictly speaking, R is outside of the class C of all rules which we are considering since R not only depends on the history of the process but also the outcome of the initial random- ization. However, R is a policy from the use of which { Y,, A 1 , t = 0, 92 7 State-Action Frequencies, Problems with Constraints 1, ... } is a stochastic process. Define = PR{A, =a I Y; = j} =PA:{AK=aiY;=j, Y0 =i} PA:{Y; =j, At= al Y0 = i} P A:{ Y; = i I Y0 = i} n L fJuPR{Y; =j, At= a I Yo= i} u=l . n L fJuPR,{Yt=fl Yo=i} u=1 We shall show for every t = 0, 1, ... that P R { Y; = j, At = a I Y0 = i} =PR{Y;=j, A,=aiY0 =i} n = L f3uPRJY; =j, A,= a I Yo= i}, u=1 For t = 0 and j i= i, Eq. (1) holds trivially since both sides vanish. For t = 0 and j = i, n PR{Yo = i, Ao =a I Yo= i} = L fJvPRJY0 = i, A 0 ==a I Y0 = i} u = 1 n L fJvPR)Yo = i, Ao = al Y0 = i} v =1 n L fJuPR)Yo = il Yo= i} v= 1 = PR{A 0 =a I Y0 = i} = PR{Yo = i, A 0 = i I Y0 = i}. Hence (1) is true for t = 0. Assume (1) is true for t = 0, . .. , T- 1. Now PR{YT = j, AT= a I Y0 = i} =PA:{YT=iiY0 =i} PR{AT=ai YT =j, Y0 =i} = PR{ YT = j I Yo= The Main Theorems and Applications However, by the induction hypothesis PR{ YT =JI Y0 = i} = L L Pll:{YT- 1 = l, AT-1 =a I Yo= i}qlj(a) l a = L L PR{YT- 1 = l, AT- 1 =a I Y0 = i}%(a) l a Hence, Pll:{YT =j, AT= a I Y0 = i} = P.R{YT=il Y0 = i}Dfa(t) = PR{YT = j I Y0 = =PR{YT=j, AT=aiYo=i}, and the induction argument is complete. 93 COROLLARY 1. Let HR.(i) be the set of limit points of {X/:(i), T = 0, 1, . .. } ; then there exists an R E eM such that HR(i) = HR(i). Proof: Since XT.R(i) = XTR(i) for every Tif R is the policy constructed in Theorem 1, the corollary is evident. The significance of Theorem 1 and its corollary is that for any optimization problem involving expected frequencies of state and decision in its cost criterion and constraints, only policies in eM need be considered. That is, if R 1 (say) is any other policy that is optimal it can always be replaced by a M arkovian policy R which is also optimal. However, more can be said along these lines. Proof: First we prove that H(i) c HD(i) . Suppose the contrary; that is, there exists a point X' = {xj.(i)} E H(i) not contained in HD(i) . Since H D(i) is a closed convex set there exists (Theorem 1 of Appendix C) 94 7 State-Action Frequencies, Problems with Constraints a set of numbers {wjk} such that w}ax}a < inf I I w}axjaU). J a XEHD(i) j a H owever, X' is a limit point of some policy R E C. Hence, I I w}ax}a(i) rPR'(i) = lim inf I L wj/rja(i). 1 a T-hXJ j a However, by Corollary I of Theorem 2, Chapter 3, there exists an R* E Cv such that Thus we have a. contradiction. We now prove that H 0 (i) =H 5(i). Clearly H 5(i) ::::J H 0 (i) ::::J H D(i). H 5(i) c H(i) and consequently, H 5(i) c H(i), the closed convex hull of H(i). But since H(i) c H 0 (i) we must have H(i) c H 0 (i). Therefore H 5(i) c H 0 (i) and the equality is proved. The fact that H(i) = HM(i) follows from Theorem 1 and its corollary with n = 1 · M · -v. .' ' H (1) ::::J H (1) follows from Theorem I apphed.to the policies R1, •.• , R. being selected from C 0 . The theorem is proved. As an application of Theorem 2 suppose f( ·) is a continuous concave function defined over the closure of the points XrR(i), R E c, T = 0, I, .... With Y0 = i, we wish to choose an R E C to minimize lim infj(XTR(i)). T-oo Let X* E H(i) be such thatf(X*) = min f(X). Since H(i) = H 0 (i), XEH(i) and f(X) is concave, it assumes its minimum (Theorem 2, Appendix B and Theorem 2a, Chapter 3) at an extreme point of H 0 (i) . H owever, every extreme point of H 0 (i) is equal to HR'(i) for some R* E C 0 . Thus for some R * E C0 , lim XrR(i) = XR'(i) =X*, andsincefiscontinuous T-+oo ' lim f( X r(i)) = f( X R*(i)) T--+ oo = f(X*) =min f(X) XEH(i) =min lim inf f(XrR(i)). RECT-+oo Therefore, f can be minimized by a policy R* E C 0 . The Main Theorems and Applications 95 THEOREM 3. If I has at most one ergodic class for every R E C0 , then H(i) = H 5(i). Proof: In much the same way as Lemma 8 of Chapter 6 was proven we can show that if I has at most one ergodic class for every R E C 0 , this also holds for every R E C5 . We shall show that under the hypo- thesis, H 5(i) is closed and convex, in which case, H 5(i) = H 5(i) and the theorem follows using Theorem 2. For each .R E C5 , let X= {xja} = { n / D7a} where n / is the steady-state probability or long term expected frequency of state j under policy R. Let R* and R** be any two policies in C5 with X* and X** being their corresponding matrices. Let X= f3X* + (1 - f3)X** where 0 f3 s 1 is arbitrary. In order to show that H 5(i) is convex, we need to show that X corresponds tc some R E C5 . We note that X satisfies the system (the steady-state system of equations and inequalities) Xja 0, Xja 0, L L X;a qij(a) = L Xia 0, i a a j E I, since X* and X** satisfy the system. But we know that there corres- ponds a policy R E Cs yielding X= {n/ D7a}; namely Dja =X jaiL Xja, a if I xja > 0, and Dja arbitrary, if L xja = 0. Hence H 5(i) is convex. To show that H 5(i) is closed, let {Ru, v = 1, 2, ... } be a sequence of policies in C5 such that {Xu, v = 1, 2, ... }, the sequence of correspond- ing X matrices, converges to X. We need to show that X E H 5(i). Since C5 is compact we can assume that {R\", v = 1, 2, ... } converges to R (say) E Cs, for otherwise we can select a convergent subsequence of {Ru} that does converge. H owever, the transition probabilities {p;) are continuous functions of the policies in C5 and since the steady-state has a unique solution (Theorems 2 and 4, Appendix A), we must have 96 7 State-Action Frequencies, Problems with Constraints that X corresponds to R. Thus H 5(i) is closed as well as convex and the theorem is proved. As in the application of Theorem 2, let f( ·) be a continuous func- tion over the closure of the possible points XrR(i) for all R E C, T = 0, 1, ... , for a given Y0 = i. Suppose the problem is to minimize lim infj(XrR(i)) over R E C subject to the constraint that H R(i) c G, T..,oo a given closed subset of H (i). Since f is continuous, and G is closed, an optimal policy will exist. Let R* denote an optirrial policy with lim jnf f(Xf(i)) = f(X*). Then by Theorem 3, there exists an R** E Cs T..,oo · such that X**= X*, where X** is the X matrix corresponding toR**. That is, R** is also optimal; consequently, in seeking an optimal policy, we need only consider those policies- in class Cs. Let us return to the problem described at the beginning of this chapter which is also Example 3 with i = 0 and r = I. It was shown in Example 3 that E-r:, the expected recurrence time, and the probability under constraint are expressible as continuous functions of points in H(i) . Under reasonable conditions on the laws of motion the hypothesis of Theorem 3 will hold. Thus, it is possible in accordance with the above remark to restrict consideration to policies in Cs. However, for policies in Cs it is readily seen (Theorem 5 of Appendix A) that ER-r= (noR)- 1 and PR{Y, = j, 1 t -r I Y0 = 0} = n//n/. Thus, we can state the problem as that of minimizing n 0 R subject to the constraint that n / n 0 RIX, where IX is a given number 0 I. This problem can now be put into the linear programming form : M inimize I Xoa a subject to Xia 0, i E I, I Xia = L Xla qli(a), i E I, a a The Main Theorems and Applications 97 i a I X ia IX I X o a • a a Letting Dia = x ;./'f x ia if I Xia > 0 or D ia arbitrary otherwise, yields a a the optimal policy R E Cs. In Theorem 4 of Chapter 3 we proved that (JR(i ) is minimized by a policy R E CD. We now provide an alternative proof. Repeating the statement of the theorem : THEOREM 4. Let j be the target state. If PR{Y, = j for some t Y0 = i} = 1 for every R E CD, then there exists an R E CD such that (JR(i) =min (JR(i), for i # j . REC Proof: Define qji(a) = fJ when i # j, where 1 + 1/fJ is equal to the number of states in I. Since wja = 0, we can define for every R E C, (Note, in each case -r denotes min{t I Y, = j, t 1 }.) Clearly, if R minimizes (JR(j ), it will also minimize (JR(i ) for each i # j. We first argue that ER{ -r I Y0 = j} < oo fo r every R E C. In Example 2 of this chapter, we have that ER{r l Y0 = j} = (lim where R is any T-+c:o a policy in the. class of\" renewal\" policies. However, under the hypothesis of the theorem and the definition of qj i(a), the state space I is irreducible for every R E CD. Hence, by Theorem 3 if an R existed such that ER{-r:l Y0 = j} = oo, then there must exist an RE C5 such that ER{-r: I Y0 = j} = oo . But from Markov chain theory (Theorem 6 of Appendix A) this cannot be the case. H ence ER { r I Y0 = j } < co for all R. Now from Example 2 we also have t hat (JR(j) is expressible as a con- tinuous function of points in H (i). In fact it can be shown that this function assumes its minimum at the extreme points of H(i) = HD(i ) (the 98 7 State-Action Frequencies, Problems with Constraints equality of these two sets given by Theorem 2). Thus by the argument used in the application follo wing Theorem 2, there exists a policy R E Cv that minimizes aR(i). This proves the theorem. State-Action Frequencies The first three theorem s of this chapter deal' with expected state- action frequencies. In some applications it is desirable to have similar st,atements concerning the · sample frequencies: th'at is, the actual frequencies of state-action combinations without taking expectations. If R E Cs , the long-run frequencies and expected frequencies coincide with probability 1. H owever, if R ¢ Cs this may not be the case. Let Let Z tja = I, = 0, if Y, = j , At = a, otherwise. and Zr denote the matrix of quantities {Zrja }. For a fixed R, denote by w a sample sequence of the joint process { Yu At, t = O, I, . . . }. Let UR(w) be the set of limit points of {ZrR, T = 0, 1, .. . }. W e have THEOREM 5. For each R E C, PR{ UR(w) c: H } = 1, where H is the closed convex hull of U HD(j). · jel Before proceeding to the proof of Theorem 5, we shall need a preliminary inequality. U sing the notation of Chapter 2, we set Vr*(i) = minE{£ W, I Y0 = i}. We state: R e C r= O LEMMA 1. lim infmin Vr*U)/T min ¢R(i). T-+co iel ie/ R eC State-Action Frequencies 99 Proof: If the ineq uality were not t o hold , t here would exist a sufficiently large T and state i, recurrent with respect t o some p olicy such tha t Vr *( i) < T min min ¢ R(i), i e/ ReC from which one could construct a policy R with a ¢n (i ) sma ller than min min ¢ R (i ). W e leave the details to t he rea der. ie I R e C Proof of Theorem 5: Suppose the theorem is false. Let R be a policy such that PR{ UR(w) c: H } < 1. Then there exists a sphere S with posi- tive radius such that S n H = 0 , the null set, and PR{UR(w)n S # 0} > 0. This is so since H c , the complement of H, can be covered by a denumerable number of such spheres S v, u = 1, 2 .. . and <XJ PR{ UR(w) n He# 0 } I Pn{ UR(w) n Su} . u = l Since H is a closed and bounded co nvex set and Sis convex and the two sets are disj oint, the two sets can be separated by a hyper-plane; that is, by Theorem 1 of Appendix C there exists a set of numbers { w ia}, a E K, , i E I such tha t I I w,a r,a >I L wiasia fo r all r = {r,.} E H and i a i a s = {s,a } E S. Let Wt = w,a when Y, = i, At =a and note tha t 1 T - - - I W. =I I w,a ZTia• T +l t=O i a so that lim inf - 1 - I w. =I I w,a z ,a T + 1 r = o i a for some point Z = {Z,.} in UR(w). We intend to show that the set of o/s such that li m inf- 1 - £ W, < min I I w ia ria has at most pro b- T +1r= O r e H a bility 0. If this is the case we must have PR {UR(w) n s # 0 } = 0, a 100 7 State-Action Frequencies, Problems with Constraints contradiction proving the theorem. For a fixed N let vN Bv = L Jift;, v =1 , .. . ,[T/N], r=(v-J. )N+1 T B' = L rt;, if [T /N] < T/N, r=[T/N).¥+ 1 =0, if [T /N]=TJN, where [T/N] is the greatest integer less than or 'equal to T/N. Clearly IBvl, v = 1, ... , [T/N] an d IB'I are bounded and ' ER{Bv I B1, • · ·' Bv-1} vN ;::=;;min L L Wja L PR{Y, = 0, A,= a 1Ycv-1 )N = i} i e l j a r=(v-1 )N+1 }'; = mini I: wja L PR{Y, =j, A,= il Y0 = i} iel j a t = 1 j E I o eKt ;::=;;min VN*(i)- m ax{w,a}. i E I By Lemma 1, for any 8 > 0, there exists anN such that max lw,al i, a 6 and -'---<- 2 N min VN*(i) i E I . . ,/, ( \") 8 - -- -;::=;; mmmm '+'R! -- N ielReC 2 = L I wi a - i a 2 where r* = {r;:} is such that L L =m in min ¢R(i). T hus, for the i a ie l R eC value of Nand for v = 1, .. . , [T/N] (the greatest integer less than or equal to T/ N), we have ER{Bv I B1, ... , Bv- 1 } ;::=;; N L L W ;a N8 . i a State-Action Frequencies However, by Theorem 5 of Appendix B, we have [T / N ] lim [ T /Nr 1 I {Bv -E (Bvi B1, .. . , Bv- 1)} =0 T--+ ro u== 1 with probability 1. Consequently, ;::=;; c) with probability 1. But then, with probability 1, . . f 1 f hm m -- 6 rt; T- oo T +1 r=O 1 { [T / N ] } = lim inf -- W0 + I Bv + B' r-oo T + 1 v = t 1 [T /N] 1 ;::=;; lim inf -- I Bv + lim inf - -(B' + W0 ) r - oo T + 1 v = ! T- oo T +1 1 (T / N] = lim inf-- L Bv r-oo T + 1 v = 1 ( T / N] =N- 1 lim inf[T/N r 1 I Bv T-+ co v=l ;::=;; L L W ;a - 8. i a Since 8 is arbitrary, we have with probability 1, and the theorem is proved. 101 As an application, suppose / ( ·) is a continuous function defined over the closure of the possible values of Zr , T = 0, 1, ... and H, and it is of interest to find R E C which minimizes E lim inf j( Zr) (as distinct from minimizing lim inf/(Xr) =lim inf /(E Zr)). Assume that, T -+oo T-+co for each R E CD, I is irreducible. Then H = H 8(i) = H 8 independent of 102 7 State-Action Frequencies, Problems with Constraints i by Theorem 3 and Theorem 4 of Appendix A. Since f( ·) is continuous, then lim inf/(Zr) minf(r) = f(r 0 ) with probability 1. Hence, T__.co rei1 E lim infj(Zr) minf(r0). Since H = H 5 , there exists a policy R E Cs such that lim Z r = r0 with T-+co probability 1. Therefore, under this policy, ER lim inf/(Zr) = f(r 0 ). T-+oo ·' ' ' Bib_!iograpbical Remarks Theorem 1 is a slightly more general form of a result obtained by D erman and Strauch (25]. The general form was given by Strauch and Veinott [50], from which follows the equali-ty of H(i ) with H M(i) in Theorem 2. The remaining results of this chapter are due to the author [16, 18]. 8 Optimal Stopping of a Markov Chain Statement of the Problem Let us suppose {Y 1 , t = 0, 1, . . . } is a finite state Markov chain with stationary transition probabilities {p,j). Let us suppose there exists an absorbing state 0 (that is, p 0 0 = 1) in the state space I such that P{ Y 1 = 0 for some t 1 j Y0 = i} = 1 for every iE I. Let {w,, i E I} denote nonnegative numerical values associated with each state. When the chain is absorbed at state 0, we can think of the process as having been stopped at that point in time and we receive the value w 0 associated 103 104 8 Optimal Stopping of a Markov Chain with the state 0. H owever, we can also think of stopping the process at any point in time prior to absorption and receiving the value wi if i is the state of the chain when the process is stopped. If our aim is to receive the highest possible numerical value and if w 0 < max{wJ , then iel clearly we would not necessarily wait for absorption before stopping the process. By a stopping time -r, we mean a rule that ,prescribes the time to stop the process; ' = t means that the process is stopped at time t and the information for stopping the process at time t must be confined to the of the variables Y0 , . .. , Y, . We shall assume for all stopping times -r considered that if-r1 = min{tl Y, = 0, t 1}, then -r -r1 . By a stopped process { Y, t = 0, I , ... } determined by a stopping time -r we mean The problem of this chapter is to determine the stopping time -r such that E{wy, I Y0 = i}, i E I- {0}, is maximized .. Stopping Problem as an Expected Average Gain Problem W e first remark that an optimal stopping time does exist and in fact is of the form that the prescription as to when to stop the process need only be a function of the state of the process at the time of stopping; that is, I will be dichotimized into states where the process is stopped and states where the process is not stopped. To see this we need only to observe that the problem can be reformulated so as to be of the form discussed in Chapter 6. At each state there are two possible actions. Action I continues the process according to the transition probabilities {pij}; action 2 at state i transforms i into an absorbing state. At state 0 the two actions coincide. That is, Set Wn = 0, wi 2 = wi, i E I - {0}, A Different Approach lOS and Consider the problem of maximizing ¢ R(i), the expected average cost per unit time, over all possible policies in C. Notice that the class of stopping times is a subclass of the class C of all policies. This is the subclass of policies such that whenever action 2 is dictated at a state i and time t = -r, then action 2 is dictated for all t > -r. Notice also, that for such a policy R = -r (say), ¢ R(i) = E{w Y, I Y0 = i} . Thus max ¢R(i) REC max E{ w y, I Y0 = i}. However, by Theorem 2, Chapter 3, or its r Corollary I, ¢R(i) is maximized for each i E I by a policy R E C0 . But each R E C 0 is a stopping time since, if action 2 is prescribed at state i, the process will remain in state i and continue to prescribe action 2. Thus, max ¢R(i) = max ¢ R(i) REC RECD =max E {wy, I Y0 = i}, i E I' where the optimal stopping time -r is the policy R E C 0 that maximizes cpR(i), i E J. Of course, it follows that the computational method s of Chapter 6 can be used to obtain an optimal stopping time. A Different Approach We return to the original problem fo rmulation of this chapter and offer another approach . Let M(i) =m axE {wy,IY0 =i}, iEJ . By the remarks of the previous section, the optimal stopping time -r need only be a time invariant function of the state of t he process, so 106 8 Optimal Stopping of a Markov Chain that we have the dynamic programming functional equations M(O) = w0 M(i) = max{w;, pijM(j)}, i E I- {0}; (1) thus, the optimal stopping time takes the form of stopping the process at those values of i where w, LP ;j M(j), i E I. If M(i), i E I, were a j known function, the optimal stopping time would. be known. The following discussion is intended to provide methods fo r determining M(i), i E I. By a super-regular function f(i), i E I, with respect to {P;), we mean a nonnegative function satisfying L pijf(j) f(i), 'i E I . (2) We first prove : LEMMA I. Let r be any stopping time. Iff(i), i E I, is any function such that f(i) w,, i E I, then iEI. Proof: E{f(Y,) I Y0 = i} = L E{f(Y,) I Y0 = i, Y, = j} P{Y, = j I Y0 = i} j then = IJCJ)P { Y, =jl Y0 = i} j wjP{Y, =j l Y0 = i} =IE{wr,IYo=i, Y,=j}P{Y,=jiY0 =i} j = E{wyJ Y0 = i} . LEMMA 2. Let r be any stopping time. l ff(i) , i E I, is super-regular, E{f( Y,) I Y0 = i} A Different Approach 107 Proof: Let .Q be t he space of all sequences cv = {ik, k = 0, 1, ... }, where the range of each coordinate of cv is I. Because { Y,} is eventually stopped or absorbed at 0, all the probability mass on .Q is concentrated on a denumerable subset of .Q. I n what follows, P {cv} is to be interpreted as P{Yk = ik, k = 0, 1, . . . }. Let En denote any subset of .Q determined by conditions on i 0 , i 1 , ... , in (that is, on the first n + 1 coordinates of cv). Since f is super-regular, we have L P{cvl Yo= i}f(Yn+ 1(cv)) weEn = I LLP{cv, Yn = l, Yn+ l =j l Yo= i}j(j) we En j l =I L:I P{cv, Y,=l i Y0 =i }p!jf(j) W E En j I I I P{cv, Yn = ll Yo = i }f (l) OJ E En l = I P(cv I Y0 = i)f( Yn(cv) ). (J) E En Recall that {Y\", n = 0, 1, .. . } denotes the stopped process. W e now show that for each n = 0, 1, . .. , (3) N oting that {cv I r n} and {cv I r > n} are both subsets of .Q of t he form En , we can write using (3) E{f(Yn+ 1) I Yo = i} I f(Yn+l( cv))P{cv l Yo= i} + I f(Yn +l(cv))P{w l Y0 = i} (wlt> n} (w)<;:!n } = I j ( Y,+ 1(cv))P{cv l Y0 = i} + I f(Yn (cv))P{cvl Y0 = i} {w l<>n) {w )t;:! n } I f( Y/cv))P{w I Y0 = i} + L f(Yn(cv ))P{w I Y0 = i} (w)<> n} (w )<;:!n} = I j( f, (cv))P{cv I Y0 = i} + L f(Yn(cv)) P{cv I Y0 = i} {w )<>n } (w )<;:!n} Since E{f('t0 ) I Y0 = i} = f(i), Eq. (4) holds on iterating the above 108 8 Optimal Stopping of a Markov Chain inequality. Since 't < oo with probability 1 (because 't is less than or equal to the time of absorption at state 0), we have that lim Yn(w) = Ylw) with probability l. Since interchange of limit and expectation are valid here, we have I Yo = i} =lim E{f(Yn) I Yo = i} f(i), i E ] '. , and the lemma is proved. We now define the smallest super-regular function dominating {w,, i E I} as that function {s(i), i E I} satisfying the conditions that (i) s is super-regular, (ii) , (iii) s(i) i E J, whenever /is super-regular andf(i) w,, i E J. Iff and g are two super-regular functions then h =min(/, g) is also super-regular since L pij min(f(j), g(j)) (2:: pij fU) , L pij g(j)) J J J min(f(j), g(j)), i E I. Thus, s can be defined as the lower envelope of all superregular func- tions dominating {w,, i E I} . Clearly, one and only one such function exists. The main result relating M(i) to the notion of the smallest super- regular function dominating { w;, i E I} is THEOREM l. T he function {M(i), i E J} of (I) is equivalent to the smallest super-regular function dominating { w,, i E I}. Proof: Clearly M(i) satisfies condition (ii). Also, from Eq. (I), if M( i) = w,, then M (i) LP;jM(j); otherwise, M(i) = L PijM(j). J j H ence, condition (i) holds. T o show that condition (iii) holds, suppose/ is super-regular andf(i) w, , i E J. Let 't be the optimal stopping time. A Different Approach Then for each i E J, by Lemma 1 and 2, M(i) = E{wr, I Y 0 = i} E{f( Y,) I Y0 = i} Thus, condition (iii) holds and the theorem is proved. 109 W e now can determine {M(i), i E I} by solving a linear programming problem as given in: THEOREM 2. If {v;*, i E I} is an optimal solution to the linear programming problem to mm1mtze subject to then M(i) = v,*, i E J. L pij vj v,, j i E I' Proof: From the constraints of linear programming problem, the function { v;*, i E I} is super-regular and dominates { w,, i E I}. By Theorem 2, s(i) = M(i), i E J. If {M(i), i E I} is not equal to {v;*, i E I} then M(i) v,*, i E J, with strict inequality holding for at least one i. However, then { v;*, i E I} cannot be the optimal solution to the linear programming problem, a contradiction proving the theorem. Another method for calculating {M(i), i E J} is a method of suc- cessive approximations. Let {/0(i), i E J} be a given function. Define 110 8 OptitnaJ Slopping of a M arkov Chain recursively, fn(i) = rnax{/0(i), L Piifn_ 1(j) }, i E I for n = 1, 2, . . .. We j have: THEOREM 3. If f 0 (i) = W;, i E I, then M(i) = lim fr(i), i E I. Proof: It is easily established that f n(i) = e{ tv y I Yo = i}' i E I, . 'n where -r\" is the optimal stopping time among the class, of all stopping such that -r ;:;:; n with probability 1. As {};,} is a nondecreasing sequence with fn(i) ;:;:; M(i), i E I , then f(i) = limfn(i) ;:;:; M(i). We also have iEI, and clearly f is a super-regular function that dominates {w;, i E I}. However, by T heorem 2, we must have thatf(i) = M(i), i E I, so that the theorem is proved. A third method for calculating {M(i), i E I} is to solve the system ( 1). T hat this is true follows from: THEOREM 4. If {f(i), i E J} satisfies f(O) = W 0 f(i) = max{w,, Piif(j)}, thenf(i) = M(i ), i E I . i E I- {0}, P roof: If {f(i), i E I} satisfies (1) then it is super-regular with f(i) W;, i E I. Since M(i) is the unique smallest super-regular function dominating {w;, i E J }, then 11, = f(i) - M(i) 0, i E I. A Different Approacb On subtracting M(i) from f(i ) in (1), we obtain il;;;:; L fiiLli, iEI. j However, on iterating, we obtain that A < '\\' ( I) A Llt = 1.... Pii Lli, j i E I' 111 and since j = 0 is an absorbing state with all other states being transient, limpfY = 0 for j f= 0. Thus i E I' which proves the theorem. It is sometimes possible, without knowing {M (i), i E 1}, to determine that a state i as one at which the process is stopped under an optimal stopping time. More explicitly we state : THEOREM 5. Let {f(j ), j E J} be any super-regular functio n that dominates {wi , j E I} (that is,J(j) wi, j E I). Iff or some i , I Piif(j ) = j w;, then M (i) = w;; that is, i is a state where the process is stopped under an optimal policy. Proof: Since w; ;:;:; M(i) ;:;:; f(i ), M (i) = max{w;, p,iM(j) } ;:;:; max{ w,, Pii f(j)} hence, equality must hold. See P roblem 3 for an application of Theorem 5. 112 Computational Example · Suppose I = 0, 1, 2, where { Poo Pot Pto P11 P2o P21 8 Optimal Stopping of a Markov Chllin Po2) {1 0 0) P12 = : t , P22 1\" 2 t , and (w 0 , w1 , W 2 ) = (0, 2, 1), We want to find r to 'mmlmlZe Ewy . Let us solve for M(i), i = 0, I, 2, by linear Since ..;e know that M(O) = 0, the linear programming problem can be stated as finding those variables V;, v2 to minimize subject to and V2 1 . Solving, we find v;* = 2, v,* = 1 as an optimal solution. Thus, M(O) = 0, M(l) = 2, M(2) = 1, where > !(2 + 1) and w2 = 1 < i(2 + 1). Thus, the optimal stopping time is always r = 0. Dual Linear Progranuning Problem 113 The Dual Linear Programming Problem Let us consider the dual linear programming problem for obtaining {M(i), i E I }. First, it is convenient to rewrite the primal problem : Minimize subject to u,:? 0, i E I ' I (oij - pij)uj :? - .L (oij - pij)wj j j = LP ij Wj - w, j = y, , iEI (say) . This problem was obtained by setting u, = v, - w, in the original primal problem; the constant term in the objective functio n has been dropped . The dual problem is : Maximize subject to X ; :? 0, i E I ' j E I. From Theorem 2 of Appendix A, one can argue that for every possible stopping setS, the values {.X;}, equal to the expected number of entries into state i from time t = 0 up to but not including the time of entry into S, where P(Y0 = i) = {J i , i E I , are feasible solutions to the dual problem. H owever, the objective function under one of these solutions for -r given by the stopping set S, can be seen to be Ewy, - I {J, w, . Thus if Sis the optimal stopping set, the objective function must equal I {J, u,, where {u;} is the optimal solution to the primal i 114 8 Optimal Stopping of a Markov Chain problem. By the duality theorem (Theorem 4 of Appendix C), at least one optimal solution of the dual problem must be {.X;}, where Sis the optimal stopping set. In any case, using Theorem 4 of Appendix C, part of the optimal stopping set can easily be extracted from the dual problem solution; namely, set u1 = 0 when the jth constraint in the dual problem solution holds with strict inequality prevailing. However, u1 = 0 implies that j E S. Some Other Forms of the Stopping Problem The problem formulation of this chapter includes the case where a cost is incurred for each period that the process continues. The cost can be a function of the state of the process; that is, there is a cost c, each time the process is in state i, i E I. The pro-blem is to maximize E{wy,- Icy, I Y0 = i}, i E I , t=O by selecting the best stopping number T. Let where T 1 denotes the. stopping time that waits until Y, = 0 for the first time ; that is, T1 = min{t I Y, = 0, t:;:;; 1}. For any stopping time T, Jet us note that E{ I Cr,l Y0 = i\\ r=r+ 1 J co { ,, =I IE I Cy, I Y, = j, j n= O r=r+ 1 Y0 = i, x P{ Y, = j, T = n I Y0 = i} co = I I C(j)P { Y, = j, T = n I Y0 = i} j n= O =I C(j)P{Y, = j I Y0 = i}, i E I. j Some Other Forms of the Stopping Problem Then, for any stopping time T, E{wr, - I Cy, I Y0 = i}· t = O = E {wr, l Y0 = i} + E{ I cr,l Y0 = i}- C(i) r= r+ 1 =I wiP {Y, = j I Y0 = i} + L C(j)P{Y, = i} - C(i) j . =I (wj + C(j))P{Y, = j I Y0 = i }- C(i) j = E{wy, + C(Y,) I Y0 = i} - C(i). 115 Therefore, on Jetting w;' = w; + C(i), i E I , and solving the original stopping problem with respect to the values {w;', i E I}, we will obtain an optimal stopping time fo r the problem with costs. The problem formulation also includes the problem of finding a stopping time T to maximize E {cc'wy, I Y0 = i}, i E I' where cc is a number between 0 and 1. It is well to note that for this problem, it is not necessary to have an absorbing state in order to have a nontrivial problem. The presence of the factor cc' makes it imperative that we do not wait too long before stopping the process. However, we now show that by introducing an additional state, the P}Oblem can be reverted to its original form . Let us consider a related Markov chain { Y/, t = 0, 1, ... } over the state space I' = I u {0} where 0 is an absorbing state of { Y/, t = 0, 1, . . . }. M ore specifically, we let p 00 = 1; p;0 = 1 - cc, i E J; p;i = ccpii, i,j E I ; w0 ' = 0, w;' = W;, i E I. For any stopping time <' to stop the process, {Y/ , t = 0, 1, . .. }, which is a function only of the state of the process, relate the stopping time ' to stop { Y, , t = 0, 1, .. . } by defining ' to stop { Y1} at those states i =I= 0 at which <' stops { Y/}. 116 8 Optimal Stopping of a Markov Chain However, for any stopping time-r', we have Y0 ' = i} 00 =I Iw/ P{ Y/=j, -r'=tiYo=i} jei r=O co =I L;w/P {Y;'=j , Y,'t=O, -r'=t i Y0 =i} jeir= O co =I L; wj cx'P{Y;=j, Y\"t=O, 'Fti Y0 = i } j e ir=O = E {cx'wr, I Y0 = i}. Thus, if -r' is optimal for stopping {Y,' , t = 0, 1, ... } in order to maxi- mize. E{w' r ·, I Y0 ' = i }, i E / , then -r is for maximizing E {cx' w r,l Y0 = i}, i E J. We point out that just as the maximization of E { w r, I Y0 = i} can be viewed as an expected average gain maximization, the maximization of E {cx'wr, I Y0 = i} can be viewed as an expected discounted gain maximi- zation. The constant ex being the discount factor . We leave the details of establishing the equivalence to the reader. Of course, then, the computational methods of Chapter 4 are applicable. Bibliographical Remarks A substantial literature xists with respect to the problem of when to stop a stochastic process. M uch of the literature involves the methods of martingales. Papers by Snell [49], Derman and Sachs [24) , and Chow and Robbins [10] are early works along these lines. T he methods presented in this chapter are due to D ynkin [29]. See B:reiman[8]. The proof of Lemma 2 is one encountered in the stu dy of martin- gales (see Doob [28], p. 300) ; it differs fro m the proof given by Dynkin . T aylor [5 1, 52] has exploited the D ynkin approach in connection with continuous time and space stopping problems. Problems 117 The optimal policy of Pr oblem 4 follows from a general theorem proved by Derman and Sachs [24] and also by Chow and R obbins [10]. Breiman [8] refers to the set of conditions as the absolutely mono- tone case. The result with proof'of Problem 5 also appears in Breiman [8]. He refers to this as the monotone case. Our modified primal problem in the discussion of the dual linear programming p roblem is obtained by Breiman [8] by other means. That which we denote by Yi is the negative of that which Breiman calls entrance fees for our stopping problem. These entrance fees also appear in Problems 4 and 5. Problems (1) Supp ose I = {0, 1, 2, 3}, (w0 , w1 , w2 , w3 ) = (0, 1, 2, 1), ;::) P2o P21 P22 Pz3 t t t 0 P3o P31 P31 P3 3 t i- ! i- Find -r that maximizes E {wyJ. (2) If f(i), i E I , is such that I Puf(j) = f(i), i E I, then j show that E {f(Y,) I Y0 = i} = /(i) , i E I , for any stopping time -r. (3) Suppose there are n objects with associated distinct values v1 , v 2 , . .. , v\". We define the following sele<:;tion process : A n object is selected at random. If its value is acceptable, then the p rocess of selection terminates with the value of the selected object given to the decision m aker. If the value of the obj ect is unaccept- able, then the object is discarded and another random selection is made fro m the remaining n - 1 objects. 118 8 Optimal Stopping of a Markov Chain The selection process continues in this manner until an object is accepted. If all n objects have been rejected then the value received is zero. Determine a stopping procedure that maximizes the probability of choice of the most valu able object. Consider only stopping procedures that do not accept an object whose value is less than the value of one already rejected. Solution (D ynkin [29]): Consider, the state space I = { I, 2, . . . , n, 0}, with Y1 = I. Let Y2 i if the ith object selected is the first to have a value greater than the first selected; Y3 = j if the jth object selected is the first object to have a value greater than that associated with the value of the ith object selected. In general, Y, is the number of the object selected which has value exceeding the value of the (Y[.:. 1)th object selected; Y, = 0 when n objects have been rejected. Clearly, pij=Oifl F or i<j, Pii = j (j - I)' n Pw = 1 - I Pij · j= i+ 1 Also, whenever Y, = i and the object is accepted, the probability that the object is most valuable is i/n. Therefore, we set w, = i/n, i = I , ... , n. Let i* be n-1 the largest integer for which I i/j > 1. One can verify j =i.• that the function f(j) = max(i* jn, j /n) is a super- regular function which dominates {wi, jE I }. Also, LPiif(j) = W; i*. Thus, by Theorem 5, states i ) for which i i* are states at which a n optimal stopping timer stops and M(i) = i/n i *. Since p ij = 0 for j i, { i* - 1 n i* - 1 } i* - 1 M(i* - 1) = -- , I -:- 0 1 )J/n > -- ; n j = i J - n Problems hence, at state i * - 1 an op timal stopping time does not stop. R epeating the argument successively, the same holds for state i* - 2, .. . , 1. Summarizing, r stops at states i*, ... , n and does n ot stop at states i, ... , i* - 1. ( 4) Suppose E is a set of states such that Pij = 0 for every iEE and for every i¢ E , and j \" P·. w . < w. for every i E E. Then show that a n lJ J = t j optimal policy stops for all i E E, and continues for all i ¢ E. (5) Suppose f( i) = w,- I Piiwi, i E I , is a nonincreasing j function and I P;d(J), i E I , is nonincreasing whenever j g(i), i E I, is n onincreasing. Show that the op timal policy is of the form: stop for all i i* and continue for all i < i* where i* is a state that must be deter mined. Proof (Breiman [8]) : Let H(i) = M (i) - w, = m ax{o, pii M(j)- w,} =m ax{o, iE I . U sing Theorem 3, show that H(i) is nonincreasing from which it will follow that H(i ) = 0 fo r all i i* and H (i) < 0 for i < i* . 119 9 Some Applications 1 A Replacement Model A common activity is the periodic inspection of some system, or one of its components, as part of a procedure for keeping it operative. After each inspection, an action must be taken as to whether or not to alter the system at that time. The problem is that of determining, according to some appropriate cost criterion, the optimal policy for taking actions. M ore specifically, suppose a unit (a system, a component of a system, a piece of operating equipment, etc.) is inspected at equally spaced points in time and that after each inspection it is classified into 121 122 9 Some Applications one of L + 1 states 0, 1, .. . , L. Then { Y,} is the sequence of states. A unit is in state 0 if and only if it is new; a unit is in state L if and only if it is inoperative. W e assume that at states 1, .. . , L -1, there are two possible actions: a = 1 is not to replace the unit, a = 2 is to replace the unit. At state 0 only one action is possible, not to replace. At state L only one action is possible, to replace. Accordingly, we set qii(l ) = P ii , i=O, .. . ,L,j=O, ... ,L with P;0 = 0,i=O, ... ,L, and PLo = l ; q ;0(2) = 1, i = 1, . . . , L - 1. We assume the {p ii} are that for every i (i = 0, . .. , L - l)pfr > 0 for some t 1. This implie,s that a unit not replaced will eventually become inoperative with probability 1. W e assume two types of cost, the cost to replace an operative unit and the cost to replace an inoperative unit. That is, we set WLl = c +A, i = 0, ... , L- 1, i = 1, .. -, L - 1 , where c > 0, A > 0. Thus, A is the additional cost incurred if the unit is allowed to become inoperative before being replaced; {We} is the sequence of costs. Either the discounted expected cost criterion \\lf R(i, a) for some given a (0 <a< 1) or the expected average cost criterioh <I>R may be of interest. The methods of Chapters 4 and 6 can be employed to find optimal replacement policies, depending on which criterion is selected. H owever, in practice, it is frequently desirable to use simple replace- ment policies. For example, we speak of a control limit policy as one which always replaces the unit whenever the observed state is i0 , i 0 + 1, .. . , L and never replaces the unit in states 0, 1, ... , i0 - 1; state i 0 is the control limit. We shall sho w under certain conditions on {p ij} that there always exists a control limit policy that is optimal. We state CONDITION A: The transition probabilities {pii} are such that for every nondecreasing func tionf(j), j = 0, ... , L, the function L g(i) = I. Pij f(J), i = 0, ... , L - 1 j = O is also nondecreasing. 1 A Replacement Mod ·I 123 We also state CONDITION B : The transition probabilities {p,) are such that for each k = 0, 1, ... , L, the function L rk(i ) = L Pii • i = 0, . .. , L- 1 , j=k is '•'/'' . Let us first show LEMMA 1. Conditions A and B are equivalent. · Proof: Assume Condition A. Then, in particular, the function is nondecreasing. But we have L j <k, g(i) = L Pii fk(j) j = O L = L Pii j = k =rk( i), and, hence, Condition B holds. Assume Condition B holds. Any non- decreasing function f (j) can be expressed in the fo rm L f(i) = L ck fii) k = O where ck 0, k = 0, ... , L and fk(i) is defined above. Then, L g(i ) = I. Pij fcn j=O L L = '[ Pii '[ ck fi j ) (equation continued) j = O k = O 124 9 Some Applications L L = I ck I Plj fkCJ) k = O j=O L L = L cki Pij· k=O j = k L Since ck 0, and by hypothesis, I p ij is nondecreasing for each k, it j=k follows that g(i) is nondecreasing, proving the lem,ma. T he significance of Lemma 1 is that Condition A' becomes a verifi- ab1e condition through the verification of condition B. We now state: THEOREM 1. If Condition A (or B) holds, then there exists a control limit policy R(a) such that ll'Rca/i, a) = min IJ'R( i, a), REC i= 0, . . . , L . N Proof. Let IJ'(i, a, N ) = min I a'E(W, I Y0 = i), N = 0, ... , L. REC t = O Clearly, 'P(i, a, 0) is a nondecreasing function of i., Assume IJ'(i, 0, n) is nondecreasing in i for 0 n N. Then since 'P(i, a, N + 1) = min{aJ/ij 'f'(j, a, N), c + aJ/oj 1!'(0, a, N)}, it= L L = c + A + a L Poj 1!'(0, a, N), j=O i = L, from the induction hypothesis and Condition A, it follows that there exists an i* such that L 'I'(i, a, N + 1) = a L pij 'P(j, a, N), j = O L = c +a L Poj IJ'(j, a, N), j= O L = c +A +a I Poj 'I'(j, a , N), j = O . ·* z < z, ·* < . z = z < L, i = L. 2 A Surveillance-Maintenance-Replacement Model 125 where IJ'(i, a, N + 1) is a nondecreasing function of i. Therefo re, 'P(i, a, N) is nondecreasing in i for N = 0, I, .... F rom Chapter 4, Theorem 1, we know that lim 'P(i, a, N ) = min I¥ R(i, a) is also non- N .... oo R eC decreasing in i. On repeating the argument using Condition A again, the theorem follows. THEOREM 2. If Condition A (or B) holds, then there exists a control-limit policy R* such that i = 0, .. . , L . Proof. From Theorem 2, Chapter 3 and Corollary 1 to T heorem 1, Chapter 6, we need only consider policies in CD. By Theorem 1 fo r each a (0 <a < 1) there exists a control-limit policy R (a) that minimizes I¥ R(i, a). Let {au , v = 1, 2, . .. } be any sequence of discount factors such that lim au= 1 and R (a 1) = R (a 2 ) = · · · = R*. Since there are at most a finite number of different control-limit policies, such a sequence exists. Let R be any policy in CD. Since R* = R (au) is optimal fo r au , we have v = I, 2, .. .. Letting v-+ oo and using Theorems 'l , Appendix A, and l(b), Appendix B, we obtain that = IJ' R*( i), i = 0, . . . , L. T herefore, R* is optimal and the theorem is proved. 2 A Surveillance-Maintenance-Replacement Model Consider a system, in use or in storage, which is deteriorati ng. Suppose that the deterioration occurs stochastically and that the con- dition of the system is kno wn only if it is inspected, which is costly. 126 9 Some Applications After inspection the manager of the system has two basic alternatives : (a) to replace the system or (b) to keep it. Under the second alternative he must decide the extent of repairs to be made and when to make the next inspection. If inspection is put off too long the system may fail in the interim, the consequence of which is an incurred cost which is a function of how long the system has been inoperative. Let us suppose that the uninspected system evolves according to a Markov chain through the states 0, l, . .. , L. 'l he state 0, as before denotes a new system and L an inoperative system. {p;) denote the of transition probabilities with PLL = I anct·p,L > 0 for each i. Assume that when a replacement is made an instantaneous transition to state 0 takes place; when a repair is made an instantaneous transition takes place to one of the states, I, . .. , L - I depending on the extent of the repairs. R eplacements or repairs are only made at the time of inspections. Assume that M < oo denotes the upper bound on the number of periods that can elapse without an inspection. Let c 1 denote the cost of inspection when, in fact, the system is in state i. Let r1j, i = l, ... , L ,j = 0, ... , L- I denote the cost to place the system in state j after observing the system to be in state i. In par- ticular, r10 is the cost to replace the system from state i. In addition we let r L(m) .i , m = I, ... , M, denote the cost to place the system in state j from state L when prior to discovering the system in state L, the system has been in state £ . for m uninspected periods. This cost represents, in addition to the repair or replacement costs, the cost associated with undetected failure . F or a criterion, we shall be interested in minimizing the expected average cost per unit time attributed to the surveillance- replacement-maintenance policy. The M arkovian decision process we shall work with is the process {Y,, Aut= 0, I, .. . }, Y0 = 0, where {Y;, t = 0, l, ... } is the sequence of observed states and {A\" t = 0, I, ... } is the sequence of actions taken. T he state space I will consist of the states 0, I, . .. , L, L (I ), ... , L(M), where L(m), m = I, ... , M are additional states with L(m) de- noting the fact that the system is observed to be in state Land has been in state L for m uninspected periods. At each state i E /,an action A, =aim consists in placing the system in state j, j = 0, I, ... , L and deciding to 2 A Surveillance-Maintenance-Replacement Model . 127 skip m (m M ) time periods before observing the system again. If the system is observed in one of the states L , L (I), . .. , L (M ) we assume that ai 0 ,j = 0, . .. , L- I. are the only possible actions. We have as transition probabilities for the observed process %Calm) = Pir 1 ) for each i, }, !, and m. Let and Z,i jm = 1, = 0, if Y, = i, A ,= aim, otherwise, If JT is the average cost up to time T (in real time) then, for each i,j E J, t(T) I I I I c ci + r ij)z , ,jm J _ t=O i j m T- t(T) _, I I I I C1 + m )Zti j m + e t = O i j m where t(T) is the largest value of t such that the real time is less than or equal to T and e iri' some positive integer less than or equal to M . We also have I I I (c, + r ,)Zr (T) ijm J _ i j m T - I L I (1 + m) Zt(T) ijm + ti(t(T))- 1 . ' J m From the application to Theorem 5, Chapter 7 and since t(T ) --. oo when T --> oo, it is possible to select a policy R E Cs that minimizes E lim inf JT (o rE lim sup Jr) over all R E C. Since for any R E Cs T-+ oo T-+ oo lim ZTijm = lim - 1 - I P{ Y, = i, A, = a j m} T-+ao T -+ ao T + 1 c= O U8 9 Some Applications with probability 1 (combine Theorems 4, Appendix A, and 5, Appendix B), the problem can, using the methods of Chapter 6, ultimately be put into the form: Choose {x;a 1J to minimize to for all i, j , m and The optimal policy is obtained by putting for each i and ajm . The above problem involves minimizing a ratio of linear functions subject to linear constraints where the lower linear form is always positive. Any problem of this form can always be transformed to a linear programming problem. Namely, suppose we wish to minimize n L C; X; i=l n Ld ;X; i = 1 2 A Surveillance-Maintenance-Replacement Model subject to 0, i = 1, .. . , n, \" L a;jx j = 0, i= 1 i = 1, . . . , m, \" LX= 1, i = 1 \" where L d;X ; > 0 for all feasible (x1, ... ' xn)· Set i=1 X; Z· =--- ' n L d,x,' i= 1 1 zn+1 = _\" _ _ _ L d; X; i = l i = 1, . .. , n, T hen we can write the linear programming problem in z1, . .. , zn + 1 to minimize subject to n 0, n L au zj = 0, j = 1 L Z; - zn+ 1 = 0' i = 1 n L d;Z ; = 1. i=1 i = 1, ... , n + 1, i = 1, .. . , m, 129 Clearly a one-to-one correspondence exists between the feasible solu- tions of the two problems.","libVersion":"0.2.3","langs":""}