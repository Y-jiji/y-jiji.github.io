{"path":"1-项目/1-科研/_assets/ai-neural-morphic-chip.pdf","text":"1 A Survey of Neuromorphic Computing and Neural Networks in Hardware Catherine D. Schuman, Member, IEEE, Thomas E. Potok, Member, IEEE, Robert M. Patton, Member, IEEE, J. Douglas Birdwell, Fellow, IEEE, Mark E. Dean, Fellow, IEEE, Garrett S. Rose, Member, IEEE, and James S. Plank, Member, IEEE Abstract—Neuromorphic computing has come to refer to a variety of brain-inspired computers, devices, and models that contrast the pervasive von Neumann computer architecture. This biologically inspired approach has created highly connected synthetic neurons and synapses that can be used to model neu- roscience theories as well as solve challenging machine learning problems. The promise of the technology is to create a brain- like ability to learn and adapt, but the technical challenges are signiﬁcant, starting with an accurate neuroscience model of how the brain works, to ﬁnding materials and engineering breakthroughs to build devices to support these models, to creating a programming framework so the systems can learn, to creating applications with brain-like capabilities. In this work, we provide a comprehensive survey of the research and motivations for neuromorphic computing over its history. We begin with a 35-year review of the motivations and drivers of neuromor- phic computing, then look at the major research areas of the ﬁeld, which we deﬁne as neuro-inspired models, algorithms and learning approaches, hardware and devices, supporting systems, and ﬁnally applications. We conclude with a broad discussion on the major research topics that need to be addressed in the coming years to see the promise of neuromorphic computing fulﬁlled. The goals of this work are to provide an exhaustive review of the research conducted in neuromorphic computing since the inception of the term, and to motivate further work by illuminating gaps in the ﬁeld where new research is needed. Index Terms—neuromorphic computing, neural networks, deep learning, spiking neural networks, materials science, digital, analog, mixed analog/digital I. INTRODUCTION T HIS paper provides a comprehensive survey of the neu- romorphic computing ﬁeld, reviewing over 3,000 papers from a 35-year time span looking primarily at the motivations, neuron/synapse models, algorithms and learning, applications, advancements in hardware, and brieﬂy touching on materials and supporting systems. Our goal is to provide a broad and historic perspective of the ﬁeld to help further ongoing C.D. Schuman is with the Computational Data Analytics Group, Oak Ridge National Laboratory, Oak Ridge, TN, 37831 USA e-mail: schu- mancd@ornl.gov Notice: This manuscript has been authored by UT-Battelle, LLC under Contract No. DE-AC05-00OR22725 with the U.S. Department of Energy. The United States Government retains and the publisher, by accepting the article for publication, acknowledges that the United States Government retains a non-exclusive, paid-up, irrevocable, world-wide license to publish or reproduce the published form of this manuscript, or allow others to do so, for United States Government purposes. The Department of Energy will provide public access to these results of federally sponsored research in accordance with the DOE Public Access Plan (http://energy.gov/downloads/doe-public- access-plan). Fig. 1. Areas of research involved in neuromorphic computing, and how they are related. research, as well as provide a starting point for those new to the ﬁeld. Devising a machine that can process information faster than humans has been a driving forces in computing for decades, and the von Neumann architecture has become the clear standard for such a machine. However, the inevitable comparisons of this architecture to the human brain highlight signiﬁcant differences in the organizational structure, power requirements, and processing capabilities between the two. This leads to a natural question regarding the feasibility of creating alternative architectures based on neurological models, that compare favorably to a biological brain. Neuromorphic computing has emerged in recent years as a complementary architecture to von Neumann systems. The term neuromorphic computing was coined in 1990 by Carver Mead [1]. At the time, Mead referred to very large scale integration (VLSI) with analog components that mimicked biological neural systems as “neuromorphic” systems. More recently, the term has come to encompass implementations that are based on biologically-inspired or artiﬁcial neural networks in or using non-von Neumann architectures. These neuromorphic architectures are notable for being highly connected and parallel, requiring low-power, and col- locating memory and processing. While interesting in their own right, neuromorphic architectures have received increased attention due to the approaching end of Moore’s law, the increasing power demands associated with Dennard scaling, and the low bandwidth between CPU and memory known as the von Neumann bottleneck [2]. Neuromorphic computers have the potential to perform complex calculations faster, more power-efﬁciently, and on a smaller footprint than traditionalarXiv:1705.06963v1 [cs.NE] 19 May 2017 2 von Neumann architectures. These characteristics provide compelling reasons for developing hardware that employs neuromorphic architectures. Machine learning provides the second important reason for strong interest in neuromorphic computing. The approach shows promise in improving the overall learning performance for certain tasks. This moves away from hardware beneﬁts to understanding potential application beneﬁts of neuromorphic computing, with the promise of developing algorithms that are capable of on-line, real-time learning similar to what is done in biological brains. Neuromorphic architectures appear to be the most appropriate platform for implementing machine learning algorithms in the future. The neuromorphic computing community is quite broad, including researchers from a variety of ﬁelds, such as materials science, neuroscience, electrical engineering, computer engi- neering, and computer science (Figure 1). Materials scientists study, fabricate, and characterize new materials to use in neuromorphic devices, with a focus on materials that exhibit properties similar to biological neural systems. Neuroscientists provide information about new results from their studies that may be useful in a computational sense, and utilize neuromor- phic systems to simulate and study biological neural systems. Electrical and computer engineers work at the device level with analog, digital, mixed analog/digital, and non-traditional circuitry to build new types of devices, while also determining new systems and communication schemes. Computer scientists and computer engineers work to develop new network models inspired by both biology and machine learning, including new algorithms that can allow these models to be trained and/or learn on their own. They also develop the supporting software necessary to enable the use of neuromorphic computing sys- tems in the real world. The goals of this paper are to give a thirty-year survey of the published works in neuromorphic computing and hardware implementations of neural networks and to discuss open issues for the future of neuromorphic computing. The remainder of the paper is organized as follows: In Section II, we present a historical view of the motivations for developing neuromorphic computing and how they have changed over time. We then break down the discussion of past works in neuromorphic computing into models (Section III), algorithms and learning (Section IV), hardware implementations (Section V), and supporting components, including communication schemes and software systems (Section VI-B). Section VII gives an overview of the types of applications to which neuromorphic computing systems have been applied in the literature. Finally, we conclude with a forward-looking perspective for neuromor- phic computing and enumerate some of the major research challenges that are left to tackle. II. MOTIVATION The idea of using custom hardware to implement neurally- inspired systems is as old as computer science and computer engineering itself, with both von Neumann [3] and Turing [4] discussing brain-inspired machines in the 1950’s. Computer scientists have long wanted to replicate biological neural Fig. 2. Neuromorphic and neural network hardware works over time. systems in computers. This pursuit has led to key discoveries in the ﬁelds of artiﬁcial neural networks (ANNs), artiﬁcial intelligence, and machine learning. The focus of this work, however, is not directly on ANNs or neuroscience itself, but on the development of non-von Neumann hardware for simulating ANNs or biological neural systems. We discuss several reasons why neuromorphic systems have been developed over the years based on motivations described in the literature. Figure 2 shows the number of works over time for neuromorphic computing and indicates that there has been a distinct rise in interest in the ﬁeld over the last decade. Figure 3 shows ten of the primary motivations for neuromorphic in the literature and how those motivations have changed over time. These ten motivations were chosen because they were the most frequently noted motivations in the literature, each speciﬁed by at least ﬁfteen separate works. Much of the early work in neuromorphic computing was spurred by the development of hardware that could perform parallel operations, inspired by observed parallelism in bio- logical brains, but on a single chip [5]–[9]. Although there were parallel architectures available, neuromorphic systems emphasized many simple processing components (usually in the form of neurons), with relatively dense interconnections between them (usually in the form of synapses), differentiating them from other parallel computing platforms of that time. In works from this early era of neuromorphic computing, the inherent parallelism of neuromorphic systems was the most popular reason for custom hardware implementations. Another popular reason for early neuromorphic and neural network hardware implementations was speed of computation [10]–[13]. In particular, developers of early systems empha- sized that it was possible to achieve much faster neural network computation with custom chips than what was pos- sible with traditional von Neumann architectures, partially by exploiting their natural parallelism as mentioned above, but also by building custom hardware to complete neural-style computations. This early focus on speed foreshadowed a future of utilizing neuromorphic systems as accelerators for machine learning or neural network style tasks. Real-time performance was also a key motivator of early neuromorphic systems. Enabled by natural parallelism and speed of computation, these devices tended to be able to complete neural network computations faster than implemen- tations on von Neumann architectures for applications such as real-time control [14], real-time digital image reconstruction [15], and autonomous robot control [16]. In these cases, the need for faster computation was not driven by studying the neural network architectures themselves or for training, but 3 0 10 20 30 40 50 60 70 80 90 100 Percentage of papers in each field 1988-1997 1998-2005 2006-2009 2010-2011 2012 2013 2014 2015 (103) (108) (112) (104) (102) (127) (123) (150) Neuroscience Online Learning Faster Fault Tolerance Footprint Low Power Scalability Von Neumann Bottleneck Parallelism Real-Time Performance Fig. 3. Ten different motivations for developing neuromorphic systems, and over time, the percentage of the papers in the literature that have indicated that motivation as one of the primary reasons they have pursued the development of neuromorphic systems. was more driven by application performance. This is why we have differentiated it from speed and parallelism as a motivation for the development of neuromorphic systems. Early developers also started to recognize that neural net- works may be a natural model for hardware implementation because of their inherent fault tolerance, both in the massively parallel representation and in potential adaptation or self- healing capabilities that can be present in artiﬁcial neural network representations in software [5], [17]. These were and continue to be relevant characteristics for fabricating new hardware implementations, where device and process variation may lead to imperfect fabricated devices, and where utilized devices may experience hardware errors. The most popular motivation in present-day literature and discussions of neuromorphic systems in the referenced articles is the emphasis on their potential for extremely low power operation. Our major source of inspiration, the human brain, requires about 20 watts of power and performs extremely complex computations and tasks on that small power budget. The desire to create neuromorphic systems that consume similarly low power has been a driving force for neuromorphic computing from its conception [18], [19], but it became a prominent motivation about a decade into the ﬁeld’s history. Similarly, creating devices capable of neural network-style computations with a small footprint (in terms of device size) also became a major motivation in this decade of neuromor- phic research. Both of these motivations correspond with the rise of the use of embedded systems and microprocessors, which may require a small footprint and, depending on the application, very low power consumption. In recent years, the primary motivation for the development of neuromorphic systems is low-power consumption. It is, by far, the most popular motivation for neuromorphic computers, as can be seen in Figure 3. Inherent parallelism, real-time- performance, speed in both operation and training, and small device footprint also continue to be major motivations for the development of neuromorphic implementations. A few other motivations became popular in this period, including a rise of approaches that utilize neural network-style architectures (i.e., architectures made up of neuron and synapse-like components) because of their fault tolerance characteristics or reliability in the face of hardware errors. This has become an increasingly popular motivation in recent years in light of the use of novel materials for implementing neuromorphic systems (see Section V-C). Another major motivation for building neuromorphic sys- tems in recent years has been to study neuroscience. Cus- tom neuromorphic systems have been developed for sev- eral neuroscience-driven projects, including those created as part of the European Union’s Human Brain Project [20], because simulating relatively realistic neural behavior on a traditional supercomputer is not feasible, in scale, speed, or power consumption [21]. As such, custom neuromorphic implementations are required in order to perform meaningful neuroscience simulations with reasonable effort. In this same vein, scalability has also become an increasingly popular 4 motivation for building neuromorphic systems. Most major neuromorphic projects discuss how to cascade their devices together to reach very large numbers of neurons and synapses. A common motivation not given explicitly in Figure 3 is the end of Moore’s Law, though most of the other listed motivations are related to the consideration of neuromorphic systems as a potential complementary architecture in the beyond Moore’s law computing landscape. Though most re- searchers do not expect that neuromorphic systems will replace von Neumann architectures, “building a better computer” is one of their motivations for developing neuromorphic devices; though this is a fairly broad motivation, it encompasses issues associated with traditional computers, including the looming end of Moore’s law and the end of Dennard scaling. Another common motivation for neuromorphic computing development is solving the von Neumann bottleneck [22], which arises in von Neumann architectures due to the separation of memory and processing and the gap in performance between processing and memory technologies in current systems. In neuromorphic systems, memory and processing are collocated, mitigating issues that arise with the von Neumann bottleneck. On-line learning, deﬁned as the ability to adapt to changes in a task as they occur, has been a key motivation for neuromorphic systems in recent years. Though on-line learn- ing mechanisms are still not well understood, there is still potential for the on-line learning mechanisms that are present in many neuromorphic systems to perform learning tasks in an unsupervised, low-power manner. With the tremendous rise of data collection in recent years, systems that are capable of processing and analyzing this data in an unsupervised, on-line way will be integral in future computing platforms. Moreover, as we continue to gain an understanding of biological brains, it is likely that we will be able to build better on-line learning mechanisms and that neuromorphic computing systems will be natural platforms on which to implement those systems. III. MODELS One of the key questions associated with neuromorphic computing is which neural network model to use. The neural network model deﬁnes what components make up the network, how those components operate, and how those components interact. For example, common components of a neural net- work model are neurons and synapses, taking inspiration from biological neural networks. When deﬁning the neural network model, one must also deﬁne models for each of the components (e.g., neuron models and synapse models); the component model governs how that component operates. How is the correct model chosen? In some cases, it may be that the chosen model is motivated by a particular application area. For example, if the goal of the neuromorphic device is to utilize the device to simulate biological brains for a neuroscience study on a faster scale than is possible with traditional von Neumann architectures, then a biologically realistic and/or plausible model is necessary. If the application is an image recognition task that requires high accuracy, then a neuromorphic system that implements convolutional neural networks may be best. The model itself may also be shaped by the characteristics and/or restrictions of a particular device or material. For example, memristor-based systems (discussed further in Section V-B1) have characteristics that allow for spike-timing dependent plasticity-like mechanisms (a type of learning mechanism discussed further in Section IV) that are most appropriate for spiking neural network models. In many other cases, the choice of the model or the level of complexity for the model is not entirely clear. A wide variety of model types have been implemented in neuromorphic or neural network hardware systems. The models range from predominantly biologically-inspired to predominantly computationally driven. The latter models are inspired more by artiﬁcial neural network models than by bio- logical brains. This section discusses different neuron models, synapse models, and network models that have been utilized in neuromorphic systems, and points to key portions of the literature for each type of model. A. Neuron Models A biological neuron is usually composed of a cell body, an axon, and dendrites. The axon usually (though not al- ways) transmits information away from the neuron, and is where neurons transmit output. Dendrites usually (though not always) transmit information to the cell body and are typically where neurons receive input. Neurons can receive information through chemical or electrical transmissions from other neurons. The juncture between the end of an axon of one neuron and the dendrite of another neuron that allows information or signals to be transmitted between the two neurons is called a synapse. The typical behavior of a neuron is to accumulate charge through a change in voltage potential across the neuron’s cell membrane, caused by receiving signals from other neurons through synapses. The voltage potential in a neuron may reach a particular threshold, which will cause the neuron to “ﬁre” or, in the biological terminology, generate an action potential that will travel along a neuron’s axon to affect the charge on other neurons through synapses. Most neuron models implemented in neuromorphic systems have some concept of accumulation of charge and ﬁring to affect other neurons, but the mechanisms by which these processes take place can vary signiﬁcantly from model to model. Simi- larly, models that are not biologically plausible (i.e. artiﬁcial models that are inspired by neuroscience rather than mimicking neuroscience) typically do not implement axons or dendrites, although there are a few exceptions (as noted below). Figure 4 gives an overview of the types of neuron models that have been implemented in hardware. The neuron models are given in ﬁve broad categories: • Biologically-plausible: Explicitly model the types of be- havior that are seen in biological neural systems. • Biologically-inspired: Attempt to replicate behavior of biological neural systems but not necessarily in a biologically-plausible way. • Neuron+Other: Neuron models including other biologically-inspired components that are not usually included in other neuromorphic neuron models, such as axons, dendrites, or glial cells. 5 Fig. 4. A hierarchy of neuron models that have hardware implementations. The size of the boxes corresponds to the number of implementations for that model, and the color of the boxes corresponds to the “family” of neuron models, which are labeled either above or below the group of same-colored boxes. • Integrate-and-ﬁre: A simpler category of biologically- inspired spiking neuron models. • McCulloch-Pitts: Neuron models that are derivatives of the original McCulloch-Pitts neuron [23] used in most artiﬁcial neural network literature. For this model, the output of neuron j is governed by the following equation: yj = f ( N∑ i=0 wi,jxi ) , (1) where yj is the output value, f is an activation function, N is the number of inputs into neuron j, wi,j is the weight of the synapse from neuron i to neuron j and xi is the output value of neuron i. A variety of biologically-plausible and biologically-inspired neuron models have been implemented in hardware. Com- ponents that may be included in these models may include: cell membrane dynamics, which govern factors such as charge leakage across the neuron’s cell membrane; ion channel dy- namics, which govern how ions ﬂow into and out of a neuron, changing the charge level of the neuron; axonal models, which may include delay components; and dendritic models, which govern how many pre- and post-synaptic neurons affect the current neuron. A good overview of these types of spiking neuron models is given by Izhikevich [24]. 1) Biologically-Plausible: The most popular biologically- plausible neuron model is the Hodgkin-Huxley model [25]. The Hodgkin-Huxley model was ﬁrst introduced in 1952 and is a relatively complex neuron model, with four-dimensional nonlinear differential equations describing the behavior of the neuron in terms of the transfer of ions into and out of the neuron. Because of their biological plausibility, Hodgkin- Huxley models have been very popular in neuromorphic implementations that are trying to accurately model biological neural systems [26]–[54]. A simpler, but still biologically- plausible model is the Morris Lecar model, which reduces the model to a two-dimensional nonlinear equation [55]. It is a commonly implemented model in neuroscience and in neuromorphic systems [27], [56]–[61]. 2) Biologically-Inspired: There are a variety of neuron models that are simpliﬁed versions of the Hodgkin-Huxley model that have been implemented in hardware, including Fitzhugh-Nagumo [62]–[64] and Hindmarsh-Rose [65]–[69] models. These models tend to be both simpler computationally and simpler in terms of number of parameters, but they become more biologically-inspired than biologically-plausible because they attempt to model behavior rather than trying to emulate physical activity in biological systems. From the perspective of neuromorphic computing hardware, simpler computation can lead to simpler implementations that are more efﬁcient and can be realized with a smaller footprint. From the algorithms and learning method perspective, a smaller number of parameters can be easier to set and/or train than models with a large number of parameters. The Izhikevich spiking neuron model was developed to produce similar bursting and spiking behaviors as can be elicited from the Hodgkin-Huxley model, but do so with much simpler computation [70]. The Izhikevich model has been very popular in the neuromorphic literature because of its simultaneous simplicity and ability to reproduce biologically accurate behavior [27], [71]–[82]. The Mihalas¸-Niebur neuron is another popular neuron model that tries to replicate bursting and spiking behaviors, but it does so with a set of linear differential equations [83]; it also has neuromorphic imple- mentations [84], [85]. The quartic model has two non-linear differential equations that describe its behavior, and also has an implementation for neuromorphic systems [86]. 3) Neuron + Other Biologically-Inspired Mechanism: Other biologically-inspired models are also prevalent that do not fall into the above categories. They typically contain a much higher level of biological detail than most models from the machine learning and artiﬁcial intelligence literature, such as the inclusion of membrane dynamics [87]–[91], modeling ion-channel dynamics [92]–[98], the incorporation of axons and/or dendrite models [99]–[110], and glial cell or astrocyte interactions [111]–[116]. Occasionally, new models are devel- oped speciﬁcally with the hardware in mind. For example, a neuron model with equations inspired by the Fitzhugh- Nagumo, Morris Lecar, Hodgkin-Huxley, or other models have been developed, but the equations were updated or the models abstracted in order to allow for ease of implementation in low- power VLSI [117], [118], on FPGA [119], [120], or using static CMOS [121]–[123]. Similarly, other researchers have updated the Hodgkin-Huxley model to account for new hard- ware developments, such as the MOSFET transistor [124]– [130] or the single-electron transistor [131]. 4) Integrate-and-Fire Neurons: A simpler set of spiking neuron models belong to the integrate-and-ﬁre family, which is a set of models that vary in complexity from relatively simple (the basic integrate-and-ﬁre) to those approaching complexity levels near that of the Izhikevich model and other more com- plex biologically-inspired models [132]. In general, integrate- and-ﬁre models are less biologically realistic, but produce enough complexity in behavior to be useful in spiking neural systems. The simplest integrate-and-ﬁre model maintains the 6 current charge level of the neuron. There is also a leaky integrate-and-ﬁre implementation that expands the simplest implementation by including a leak term to the model, which causes the potential on a neuron to decay over time. It is one of the most popular models used in neuromorphic systems [58], [133]–[164]. The next level of complexity is the general nonlinear integrate-and-ﬁre method, including the quadratic integrate-and-ﬁre model that is used in some neuromorphic systems [165], [166]. Another level of complexity is added with the adaptive exponential integrate-and-ﬁre model, which is similar in complexity to the models discussed above (such as the Izhikevich model). These have also been used in neuromorphic systems [167], [168]. In addition to the previous analog-style spiking neuron mod- els, there are also implementations of digital spiking neuron models. The dynamics in a digital spiking neuron model are usually governed by a cellular automaton, as opposed to a set of nonlinear or linear differential equations. A hybrid analog/digital implementation has been created for neuromor- phic implementations [169], as well as implementations of resonate-and-ﬁre [170] and rotate-and-ﬁre [171], [172] digital spiking neurons. A generalized asynchronous digital spiking model has been created in order to allow for exhibition of nonlinear response characteristics [173], [174]. Digital spiking neurons have also been utilized in pulse-coupled networks [175]–[179]. Finally, a neuron for a random neural network has been implemented in hardware [180], In the following sections, the term spiking neural network will be used to describe full network models. These spiking networks may utilize any of the above neuron models in their implementation; we do not specify which neuron model is being utilized. Moreover, in some hardware implementations, such as SpiNNaker (see Section V-A1), the neuron model is programmable, so different neuron models may be realized in a single neuromorphic implementation. 5) McCulloch-Pitts Neurons: Moving to more traditional artiﬁcial neural network implementations in hardware, there is a large variety of implementations of the traditional McCulloch-Pitts neuron model [23]. The perceptron is one implementation of the McCulloch-Pitts model, which uses a simple thresholding function as the activation function; because of its simplicity, it is commonly used in hardware implementations [181]–[191]. There has also been signiﬁcant focus to create implementations of various activation func- tions for McCulloch-Pitts-style neurons in hardware. Different activation functions have had varying levels of success in neural network literature, and some activation functions can be computationally intensive. This complexity in computation can lead to complexity in hardware, resulting in a variety of different activation functions and implementations that are attempting to trade-off complexity and overall accuracy and computational usefulness of the model. The most popular implementations are the basic sigmoid function [192]–[212] and the hyperbolic tangent function [213]–[216], but other hardware-based activation functions that have been imple- mented include the ramp-saturation function [194], linear [200], piecewise linear [217], step function [194], [218], multi-threshold [219], radial basis function [208], the tan- Fig. 5. A qualitative comparison of neuron models in terms of biological inspiration and complexity of the neuron model. gent sigmoid function [208], and periodic activation functions [220]. Because the derivative of the activation function is utilized in the back-propagation training algorithm [221], some circuits implement both the function itself and its derivative, for both sigmoid [222]–[227] and hyperbolic tangent [224]. A few implementations have focused on creating neurons with programmable activation functions [228] or on creating building blocks to construct neurons [229]. Neuron models for other traditional artiﬁcial neural net- work models have also been implemented in hardware. These neuron models include binary neural network neurons [230], fuzzy neural network neurons [231], and Hopﬁeld neural network neurons [232]–[234]. On the whole, there have been a wide variety of neuron models implemented in hardware, and one of the decisions a user might make is a tradeoff between complexity and biological inspiration. Figure 5 gives a qualitative comparison of different neuron models in terms of those two factors. B. Synapse Models Just as some neuromorphic work has focused particularly on neuron models, which occasionally also encapsulate the synapse implementation, there has also been a focus on devel- oping synapse implementations independent of neuron models for neuromorphic systems. Once again, we may separate the synapse models into two categories: biologically-inspired synapse implementations, which include synapses for spike- based systems, and synapse implementations for traditional artiﬁcial neural networks, such as feed-forward neural net- works. It is worth noting that synapses are typically going to be the most abundant element in neuromorphic systems, or the element that is going to require the most real estate on a particular chip. For many hardware implementations and especially for the development and use of novel materials for neuromorphic, the focus is typically on optimizing the synapse implementation. As such, synapse models tend to be relatively simple, unless they are attempting to explicitly model biological behavior. One popular inclusion for more complex synapse models is a plasticity mechanism, which causes the neuron’s strength or weight value to change over 7 time. Plasticity mechanisms have been found to be related to learning in biological brains. For more biologically-inspired neuromorphic networks, synapse implementations that explicitly model the chemical interactions of synapses, such as the ion pumps or neuro- transmitter interactions, have been utilized in some neuromor- phic systems [67], [235]–[245]. Ion channels have also been implemented in neuromorphic implementations in the form of conductance-based synapse models [246]–[251]. For these implementations, the detail goes above and beyond what one might see with the modeling of ion channels in neuron models such as Hodgkin-Huxley. Implementations for spiking neuromorphic systems focus on a variety of characteristics of synapses. Neuromorphic synapses that exhibit plasticity and learning mechanisms in- spired by both short-term and long-term potentiation and depression in biological synapses have been common in biologically-inspired neuromorphic implementations [252]– [262]. Potentiation and depression rules are speciﬁc forms of spike-timing dependent plasticity (STDP) [263] rules. STDP rules and their associated circuitry are extremely common in neuromorphic implementations for synapses [252], [256], [259], [264]–[300]. More information on STDP as a learning algorithm and its implementations in neuromorphic systems is provided in Section IV. Synaptic responses can also be relatively complex in neuromorphic systems. Some neuromor- phic synapse implementations focus on synaptic dynamics, such as the shape of the outgoing spike from the synapse or the post-synaptic potential [301]–[305]. Synapses in spiking neuromorphic systems have also been used as homeostasis mechanisms to stabilize the network’s activity, which can be an issue in spiking neural network systems [306], [307]. A variety of neuromorphic synaptic implementations for non-spiking neural networks have also been implemented. These networks include feed-forward multi-layer networks [308]–[314], winner-take-all [315], and convolutional neu- ral networks [316]. A focus on different learning rules for synapses in artiﬁcial neural network-based neuromorphic sys- tems is also common, as it is for STDP and potentiation and depression rules in spike-based neuromorphic systems. Common learning rules in artiﬁcial neural network-based systems include Hebbian learning [310], [312], [317], [318] and least mean-square [11], [319]. Gaussian synapses have also been implemented in order to help with back-propagation learning rules [320]–[322]. C. Network Models Network models describe how different neurons and synapses are connected and how they interact. As may be intuited from the previous sections, there are a wide variety of neural network models that have been developed for neuromor- phic systems. Once again, they range from trying to replicate biological behavior closely to much more computationally- driven, non-spiking neural networks. There are a variety of factors to consider when selecting a network model. One of the factors is clearly biological inspiration and complexity of neuron and synapse models, as discussed in previous sections. Another factor to consider is the topology of the network. Figure 6 shows some examples of network topologies that might be used in various network models, including biologically-inspired networks and spiking networks. Depend- ing on the hardware chosen, the connectivity might be rela- tively restricted, which would restrict the topologies that can be realistically implemented. A third factor is the feasibility and applicability of existing training or learning algorithms for the chosen network model, which will be discussed in more detail in Section IV. Finally, general applicability of that network model to a set of applications may also play a role in choosing the appropriate network model. There are a large variety of general spiking neural network implementations in hardware [21], [86], [169], [323]–[570]. These implementations utilize a variety of neuron models such as the various integrate-and-ﬁre neurons listed above or the more biologically-plausible or biologically-inspired models. Spiking neural network implementations also typically include some form of STDP in the synapse implementation. Spiking models have been popular in neuromorphic implementations in part because of their event-drive nature and improved energy efﬁciency relative to other systems. As such, implementations of other neural network models have been created using spiking neuromorphic systems, including spiking feed-forward networks [571]–[575], spiking recurrent networks [576]–[581], spiking deep neural networks[582]–[592], spiking deep belief networks [593], spiking Hebbian systems [594]–[602], spiking Hopﬁeld networks or associative memories [603]–[605], spik- ing winner-take-all networks [606]–[611], spiking probabilistic networks [612], [613], and spiking random neural networks [614]. In these implementations a spiking neural network architecture in neuromorphic systems has been utilized for another neural network model type. Typically, the training for these methods is done on the traditional neural network type (such as the feed-forward network), and then the resulting network solution has been adapted to ﬁt the spiking neuro- morphic implementation. In these cases, the full properties of the spiking neural network may not be utilized. A popular biologically-inspired network model that is often implemented using spiking neural networks is central pattern generators (CPGs). CPGs generate oscillatory motion, such as walking gaits or swimming motions in biological systems. A common use of CPGs have been in robotic motion. There are several neuromorphic systems that were built speciﬁcally to operate as CPGs [615]–[619], but CPGs are also often an application built on top of existing spiking neuromorphic systems, as is discussed in Section VII. The most popular implementation by far is feed-forward neural networks, including multi-layer perceptrons [8], [9], [16]–[19], [192], [194], [200], [202], [206], [207], [530], [620]–[1023]. Extreme learning machines are a special case of feed-forward neural networks, where a number of the weights in the network are randomly set and never updated based on a learning or training algorithm; there have been several neuromorphic implementations of extreme learning machines [1024]–[1029]. Another special case of feed-forward neural networks are multi-layer perceptrons with delay, and those have also been implemented in neuromorphic systems [1030]– 8 Fig. 6. Different network topologies that might be desired for neuromorphic systems. Determining the level of connectivity that is required for a neuromorphic implementation and then ﬁnding the appropriate hardware that can accommodate that level of connectivity is often a non-trivial exercise. [1034]. Probabilistic neural networks, yet another special-case of feed-forward neural networks that have a particular type of functionality that is related to Bayesian calculations, have several neuromorphic implementations [1035]–[1043]. Single- layer feed-forward networks that utilize radial basis functions as the activation function of the neurons have also been used in neuromorphic implementations [530], [747], [825], [826], [1044]–[1053]. In recent years, with the rise of deep learning, convolutional neural networks have also seen several implementations in neuromorphic systems [1054]–[1075]. Recurrent neural networks are those that allow for cycles in the network, and they can have differing levels of connec- tivity, including all-to-all connections. Non-spiking recurrent neural networks have also been implemented in neuromorphic systems [7], [813], [829], [857], [1076]–[1099]. Reservoir computing models, including liquid state machines, have be- come popular in neuromorphic systems [1100]–[1106]. In reservoir computing models, recurrent neural networks are utilized as a “reservoir”, and outputs of the reservoir are fed into simple feed-forward networks. Both spiking and non-spiking implementations exist. Winner-take-all networks, which utilize recurrent inhibitory connections to force a single output, have also been implemented in neuromorphic systems [1107]–[1109]. Hopﬁeld networks were especially common in earlier neuromorphic implementations, as is consistent with neural network research at that time [5], [6], [13], [15], [759], [764], [813], [1110]–[1148], but there are also more recent implementations [838], [1149]–[1159]. Similarly, asso- ciative memory based implementations were also signiﬁcantly more popular in earlier neuromorphic implementations [1053], [1160]–[1182]. Stochastic neural networks, which introduce a notion of ran- domness into the processing of a network, have been utilized in neuromorphic systems as well [1183]–[1192]. A special case of stochastic neural networks, Boltzmann machines, have also been popular in neuromorphic systems. The general Boltzmann machine was utilized in neuromorphic systems primarily in the early 1990’s [12], [1193]–[1199], but it has seen occasional implementations in more recent publications [1200]–[1203]. A more common use of the Boltzmann model is the restricted Boltzmann machine, because the training time is signiﬁcantly reduced when compared with a general Boltz- mann machine. As such, there are several implementations of the restricted Boltzmann machine in neuromorphic imple- mentations [1201]–[1211]. Restricted Boltzmann machines are an integral component to deep belief networks, which have become more common with increased interest in deep learn- ing and have been utilized in neuromorphic implementations [1212]–[1214]. Neural network models that focus on unsupervised learning rules have also been popular in neuromorphic implementa- tions, beyond the STDP rules implemented in most spiking neural network systems. Hebbian learning mechanisms, of which STDP is one type in spiking neural networks, are common in non-spiking implementations of neuromorphic networks [1215]–[1231]. Self-organizing maps are another form of artiﬁcial neural network that utilize unsupervised learning rules, and they have been utilized in neuromorphic implementations [759], [1053], [1160], [1232]–[1248]. More discussion on unsupervised learning methods such as Hebbian learning or STDP is provided in Section IV. The visual system has been a common inspiration for artiﬁcial neural network types, including convolutional neural networks. Two other visual system-inspired models, cellular neural networks [1249] and pulse-coupled neural networks [1250], have been utilized in neuromorphic systems. In partic- ular, cellular neural networks were common in early neuromor- phic implementations [1251]–[1260] and have recently seen a resurgence [1261]–[1268], whereas pulse-coupled networks were popular in the early 2000’s [1269]–[1277]. Other, less common neural network and neural network- adjacent models implemented in neuromorphic systems in- clude cellular automata [1278]–[1282], fuzzy neural networks [1283], which combine fuzzy logic and artiﬁcial neural net- works, and hierarchical temporal memory [1284], a network model introduced by Hawkins in [1285]. Figure 7 gives an overview of the network models im- plemented in neuromorphic systems. Figure 8 shows how some of the most frequently used models in neuromorphic implementations have evolved over time. As can be seen in the ﬁgures, spiking and feed-forward implementations are by far the most common, with spiking implementations seeing a rise in the last decade. General feed-forward networks had begun to taper off, but the popularity and success of convolutional neural networks in deep learning has increased in activity in the last ﬁve years. D. Summary and Discussion In terms of model selection for neuromorphic implementa- tions, it is clear that there are a wide variety of options, and 9 Fig. 7. A breakdown of network models in neuromorphic implementations, grouped by overall type and sized to reﬂect the number of associates papers. much of the ground of potential biological and artiﬁcial neural network models has been tread at least once by previous work. The choice of model will be heavily dependent on the intent of the neuromorphic system. With projects whose goal it is to produce useful results for neuroscience, the models usually err on the side of biologically-plausible or at least biologically- inspired. For systems that have been moved to hardware for a particular application, such as image processing on a remote sensor or autonomous robots, more artiﬁcial neural network- like systems that have proven capabilities in those arenas may be most applicable. It is also the case that the model is chosen or adapted to ﬁt within some particular hardware characteristics (e.g., selecting models that utilize STDP for memristors), or that the model is chosen for efﬁciency’s sake, as is often the case for event-driven spiking neural network systems. On the whole, it is clear that most neural network models have, at some point in their history, been implemented in hardware. IV. ALGORITHMS AND LEARNING Some of the major open questions for neuromorphic systems revolve around algorithms. The chosen neuron, synapse, and network models have an impact on the algorithm chosen, as certain algorithms are speciﬁc to certain network topolo- gies, neuron models, or other network model characteristics. Beyond that, a second issue is whether training or learning for a system should be implemented on chip or if net- works should be trained off-chip and then transferred to the neuromorphic implementation. A third issue is whether the algorithms should be on-line and unsupervised (in which case they would necessarily need to be on-chip), whether off-line, supervised methods are sufﬁcient, or whether a combination of the two should be utilized. One of the key reasons neu- romorphic systems are seen as a popular post-Moore’s law era complementary architecture is their potential for on-line learning; however, even the most well-funded neuromorphic systems struggle to develop algorithms for programming their hardware, either in an off-line or on-line way. In this section, we focus primarily on on-chip algorithms, chip-in-the-loop algorithms, and algorithms that are tailored directly for the hardware implementation. A. Supervised Learning The most commonly utilized algorithm for programming neuromorphic systems is back-propagation. Back-propagation is a supervised learning method, and is not typically thought of as an on-line method. Back-propagation and its many variations can be used to program feed-forward neural net- works, recurrent neural networks (usually back-propagation through time), spiking neural networks (where often feed- forward neural networks are adapted to spiking systems), and convolutional neural networks. The simplest possible approach is to utilize back-propagation off-line on a traditional host machine, as there are many available software implementations that have been highly optimized. We omit citing these ap- proaches, as they typically utilize basic back-propagation, and that topic has been covered extensively in the neural network literature [1286]. However, there are also a large variety of implementations for on-chip back-propagation in neuromor- phic systems [217], [575], [623], [628]–[630], [633], [641]– [644], [646], [647], [650], [657]–[659], [661], [662], [671], [691]–[697], [721], [726]–[728], [730], [738], [741], [745], [746], [759], [764], [775], [778]–[780], [784], [786], [788]– [790], [793], [795], [796], [798]–[803], [840], [843], [857], [872], [877], [896], [902], [941]–[965], [968], [970], [971], [973]–[975], [986], [987], [1010], [1022], [1032], [1034], [1052], [1089], [1128], [1287]–[1296]. There have been sev- eral works that adapt or tailor the back-propagation method to their particular hardware implementation, such as coping with memristive characteristics of synapses [634], [689], [1297]– [1299]. Other gradient descent-based optimization methods have also been implemented on neuromorphic systems for training, and they tend to be variations of back-propagation that have been adapted or simpliﬁed in some way [639], [645], [709], [716], [718], [719], [723], [792], [812], [844], [1030], [1122], [1300]–[1303]. Back-propagation methods have also been developed in chip-in-the-loop training methods [686], [702], [732], [815], [859]; in this case, most of the learning takes place on a host machine or off-chip, but the evaluation of the solution network is done on the chip. These methods can help to take into account some of the device’s characteristics, such as component variation. There are a variety of issues associated with back- propagation, including that it is relatively restrictive on the type of neuron models, networks models, and network topolo- gies that can be utilized in an efﬁcient way. It can also be difﬁcult or costly to implement in hardware. Other approaches for on-chip supervised weight training have been utilized. These approaches include the least-mean-squares algorithm 10 Fig. 8. An overview of how models for neuromorphic implementations have changed over time, in terms of the number of papers published per year. [750], [787], [1025], [1026], weight perturbation [19], [625], [655], [669], [682], [698], [699], [708], [710], [712], [713], [715], [736], [834], [835], [841], [845]–[847], [856], [1078]– [1080], [1098], [1099], [1148], [1304], training speciﬁcally for convolutional neural networks [1305], [1306] and others [169], [220], [465], [714], [804], [864], [865], [1029], [1049], [1307]–[1320]. Other on-chip supervised learning mechanisms are built for particular model types, such as Boltzmann ma- chines, restricted Boltzmann machines, or deep belief net- works [12], [627], [1135], [1193], [1196], [1201], [1202], [1207] and hierarchical temporal memory [1284]. A set of nature-based or evolution-inspired algorithms have also been also been implemented for hardware. These implementations are popular because they do not rely on particular characteristics of a model to be utilized, and off- chip methods can easily utilize the hardware implementa- tions in the loop. They can also be used to optimize within the characteristics and peculiarities of a particular hardware implementation (or even the characteristics and peculiarities of a particular hardware device instance). Off-chip nature- based implementations include differential evolution [1321]– [1324], evolutionary or genetic algorithms [484]–[487], [512], [570], [680], [700], [1076], [1082]–[1085], [1092], [1325]– [1337], and particle swarm optimization [1338]. We explicitly specify these off-chip methods because all of the nature-based implementations rely on evaluations of a current network solution and can utilize the chip during the training process (as a chip-in-the-loop method). There have also been a variety of implementations that include the training mechanisms on the hardware itself or in a companion hardware implementation, including both evolutionary/genetic algorithms [524], [554]– [556], [560], [622], [626], [1339]–[1350] and particle swarm optimization [1351]–[1354]. B. Unsupervised Learning There have been several implementations of on-chip, on- line, unsupervised training mechanisms in neuromorphic sys- tems. These self-learning training algorithms will almost cer- tainly be necessary to realize the full potential of neuromorphic implementations. Some early neuromorphic implementations of unsupervised learning were based on self-organizing maps Fig. 9. An overview of on-chip training/learning algorithms. The size of the box corresponds to the number of papers in that category. or self-organizing learning rules [759], [1022], [1053], [1197], [1198], [1233]–[1235], [1241], [1244], [1245], [1247], [1271], [1273], [1274], though there have been a few implementations in more recent years [1237], [1242], [1248], [1272]. Hebbian- type learning rules, which encompass a broad variety of rules, have been very popular as on-line mechanisms for neuromorphic systems, and there are variations that encompass both supervised and unsupervised learning [323], [334], [341], [342], [355], [362], [366], [385], [426], [427], [496], [573], [576], [580], [594]–[596], [598]–[602], [607], [612], [642], [660], [800], [918], [1053], [1114], [1126], [1143], [1144], [1146], [1149], [1151]–[1154], [1157], [1166], [1181], [1184], [1195], [1215], [1217]–[1219], [1221]–[1223], [1225]–[1231], [1355]–[1363]. Finally, perhaps the most popular on-line, unsupervised learning mechanism in neuromorphic systems is spike-timing dependent plasticity [21], [254], [328], [329], [340], [343], [347], [348], [352], [353], [357], [358], [365], 11 TABLE I ALGORITHMS PROS AND CONS Algorithm Class Any Model Device Quirks Complex to Implement On-Line Fast Time to Solution Demonstrated Broad Applicability Biologically-Inspired or Plausible Back-Propagation No No Yes No Yes Yes No Evolutionary Yes Yes No No No Yes Maybe Hebbian No Yes No Yes Maybe No Yes STDP No Yes Maybe Yes Maybe No Yes [368]–[371], [375]–[377], [380], [384], [386], [388], [393], [396]–[399], [402], [406], [407], [410]–[412], [414], [423], [425], [432], [435]–[437], [442], [444], [446]–[449], [452], [456], [458]–[463], [473]–[475], [477], [478], [497], [502], [507], [516], [518], [519], [549], [552], [558], [561], [566], [571], [578], [579], [586], [597], [603], [838], [1036], [1064], [1169], [1307], [1330]–[1333], [1364]–[1453], which is a form of Hebbian-like learning that has been observed in real biological systems [1454]. The rule for STDP is generally that if a pre-synaptic neuron ﬁres shortly before (after) the post-synaptic neuron, the synapse’s weight will be increased (decreased) and the less time between the ﬁres, the higher the magnitude of the change. There are also custom circuits for depression [351], [356], [409], [440], [1455], [1456] and potentiation [1457] in synapses in more biologically-inspired implementations. It is worth noting that, especially for STDP, wide applicability to a set of applications has not been fully demonstrated. C. Summary and Discussion Spiking neural network-based neuromorphic systems have been popular for several reasons, including the power and/or energy efﬁciency of their event-driven computation and their closer biological inspiration relative to artiﬁcial neural net- works in general. Though there have been proposed methods for training spiking neural networks that usually utilize STDP learning rules for synaptic weight updates, we believe that the full capabilities of spiking neuromorphic systems have not yet been realized by training and learning mechanisms. As noted in Section III-C, spiking neuromorphic systems have been frequently utilized for non-spiking network models. These models are attractive because we typically know how to train them and how to best utilize them, which gives a set of applica- tions for spiking neuromorphic systems. However, we cannot rely on these existing models to realize the full potential of neuromorphic systems. As such, the neuromorphic computing community needs to develop algorithms for spiking neural network systems that can fully realize the characteristics and capabilities of those systems. This will require a paradigm shift in the way we think about training and learning. In particular, we need to understand how to best utilize the hardware itself in training and learning, as neuromorphic hardware systems will likely allow us to explore larger-scale spiking neural networks in a more computationally and resource efﬁcient way than is possible on traditional von Neumann architectures. An overview of on-chip learning algorithms is given in Figure 9. When choosing the appropriate algorithm for a neuromorphic implementation, one must consider several fac- tors: (1) the chosen model, (2) the chosen material or device type, (3) whether learning should be on-chip, (4) whether learning should be on-line, (5) how fast learning or training needs to take place, (6) how successful or broadly applicable the results will be, and (7) whether the learning should be biologically-inspired or biologically-plausible. Some of these factors for various algorithms are considered in Table I. For example, back-propagation is a tried and true algorithm, has been applied to a wide variety of applications and can be relatively fast to converge to a solution. However, if a device is particularly restrictive (e.g., in terms of connectivity or weight resolution) or has a variety of other quirks, then back-propagation requires signiﬁcant adaptation to work cor- rectly and may take signiﬁcantly longer to converge. Back- propagation is also very restrictive in terms of the types of models on which it can operate. We contrast back-propagation with evolutionary-based methods, which can work with a vari- ety of models, devices, and applications. Evolutionary methods can also be relatively easier to implement than more analytic approaches for different neuromorphic systems. However, they can be slow to converge for complex models or applications. Additionally, both back-propagation and evolutionary methods require feedback, i.e., they are supervised algorithms. Both Hebbian learning and STDP methods can be either super- vised or unsupervised; they are also biologically-inspired and biologically-plausible, making them attractive to developers who are building biologically-inspired devices. The downside to choosing Hebbian learning or STDP is that they have not been demonstrated to be widely applicable. There is still a signiﬁcant amount of work to be done within the ﬁeld of algorithms for neuromorphic systems. As can be seen in Figure 8, spiking network models are on the rise. Currently, STDP is the most commonly used algorithm proposed for training spiking systems, and many spiking systems in the literature do not specify a learning or training rule at all. It is worth noting that algorithms such as back-propagation and the associated network models were developed with the von Neumann architecture in mind. Moving forward for neuromorphic systems, algorithm devel- opers need to take into account the devices themselves and have an understanding of how these devices can be utilized most effectively for both learning and training. Moreover, algorithm developers need to work with hardware developers to discover what can be done to integrate training and learning directly into future neuromorphic devices, and to work with neuroscientists in understanding how learning is accomplished in biological systems. 12 V. HARDWARE Here we divide hardware implementations of neuromorphic implementations into three major categories: digital, analog, and mixed analog/digital platforms. These are examined at a high-level with some of the more exotic device-level com- ponents utilized in neuromorphic systems explored in greater depth. For the purposes of this survey, we maintain a high- level view of the neuromorphic system hardware considered. A. High-Level There have been many proposed taxonomies for neuromor- phic hardware systems [1458], but most of those taxonomies divide the hardware systems at a high-level into analog, digital or mixed analog/digital implementations. Before diving into the neuromorphic systems themselves, it is worthwhile to note the major characteristics of analog and digital systems and how they relate to neuromorphic systems. Analog systems utilize native physical characteristics of electronic devices as part of the computation of the system, while digital systems tend to rely on Boolean logic-based gates, such as AND, OR, and NOT, for building computation. The biological brain is an analog system and relies on physical properties for computation and not on Boolean logic. Many of the computa- tions in neuromorphic hardware lend themselves to the sorts of operations that analog systems naturally perform. Digital systems rely on discrete values while analog systems deal with continuous values. Digital systems are usually (but not always) synchronous or clock-based, while analog systems are usually (but not always) asynchronous; in neuromorphic, however, this rule of thumb is often not true, as even the digital systems tend to be event-driven and analog systems sometimes employ clocks for synchronization. Analog systems tend to be signiﬁcantly more noisy than digital systems; however, there have been some arguments that because neural networks can be robust to noise and faults, they may be ideal candidates for analog implementation [1459]. Figure 10 gives an overall summary breakdown of high-level breakdown of different neuromorphic hardware implementations. 1) Digital: Two broad categories of digital systems are addressed here. The ﬁrst is ﬁeld programmable gate arrays or FPGAs. FPGAs have been frequently utilized in neuromorphic systems [86], [169], [192], [194], [200], [202], [206], [207], [217], [481], [489]–[570], [573], [575], [587], [588], [605], [617]–[619], [805], [806], [871]–[940], [966]–[1023], [1029], [1032]–[1034], [1037]–[1043], [1049]– [1053], [1067]–[1075], [1095]–[1099], [1104]–[1106], [1139], [1141]–[1148], [1155]–[1159], [1179]–[1182], [1187]–[1192], [1209]–[1211], [1213], [1214], [1245]–[1248], [1267], [1268], [1277], [1279]–[1282], [1287], [1289], [1293], [1304], [1339]–[1348], [1350]–[1352], [1354], [1360], [1361], [1460]–[1525]. For many of these implementations, the use of the FPGA is often utilized as a stop-gap solution on the way to a custom chip implementation. In this case, the programmability of the FPGA is not utilized as part of the neuromorphic implementation; it is simply utilized to program the device as a neuromorphic system that is then evaluated. However, it is also frequently the case that the Fig. 10. An overview of hardware implementations in neuromorphic com- puting. These implementations are relatively basic hardware implementations and do not contain the more unusual device components discussed in Section V-B. FPGA is utilized as the ﬁnal implementation, and in this case, the programmability of the device can be leveraged to realize radically different network topologies, models, and algorithms. Because of their relative ubiquity, most researchers have access to at least one FPGA and can work with languages such as VHDL or Verilog (hardware description languages) to implement circuits in FPGAs. If the goal of developing a neuromorphic system is to achieve speed-up over software simulations, then FPGAs can be a great choice. However, if the goal is to achieve a small, low-power system, then FPGAs are probably not the correct approach. Liu and Wang point out several advantages of FPGAs over both digital and analog ASIC implementations, including shorter design and fabrication time, reconﬁgurability and reusability for different applications, optimization for each problem, and easy interface with a host computer [1526]. Full custom or application speciﬁc integrated circuit (ASIC) chips have also been very common for neuro- morphic implementations [6], [9], [348], [350], [389]– [404], [438], [477], [677], [683], [749]–[764], [764]–[808], [1047], [1048], [1054]–[1060], [1100], [1101], [1118]–[1130], [1170], [1183]–[1185], [1204], [1205], [1212], [1236]–[1242], [1260], [1275], [1283], [1363], [1410], [1527]–[1561]. IBM’s TrueNorth, one of the most popular present-day neuromorphic implementations, is a full custom ASIC design [1562]–[1569]. The TrueNorth chip is partially asynchronous and partially synchronous, in that some activity does not occur with the clock, but the clock governs the basic time step in the system. A core in the TrueNorth system contains a 256x256 crossbar conﬁguration that maps incoming spikes to neurons. The behavior of the system is deterministic, but there is the ability to generate stochastic behavior through pseudo-random source. This stochasticity can be exactly replicated in a software 13 simulation. SpiNNaker, another popular neuromorphic implementation, is also a full custom digital, massively parallel system [1570]– [1593]. SpiNNaker is composed of many small integer cores and a custom interconnect communication scheme which is optimized for the communication behavior of a spike-based network architecture. That is, the communication fabric is meant to handle a large number of very small messages (spikes). The processing unit itself is very ﬂexible and not custom for neuromorphic, but the conﬁguration of each SpiNNaker chip includes instruction and data memory in order to minimize access time for frequently used data. Like TrueNorth, SpiNNaker supports the cascading of chips to form larger systems. TrueNorth and SpiNNaker provide good examples of the extremes one can take with digital hardware implementa- tions. TrueNorth has chosen a ﬁxed spiking neural network model with leaky integrate-and-ﬁre neurons and limited pro- grammable connectivity, and there is no on-chip learning. It is highly optimized for the chosen model and topology of the network. SpiNNaker, on the other hand, is extremely ﬂexible in its chosen neuron model, synapse model, and learning algorithm. All of those features and the topology of the network are extremely ﬂexible. However, this ﬂexibility comes at a cost in terms of energy efﬁciency. As reported by Furber in [1594], TrueNorth consumes 25 pJ per connection, whereas SpiNNaker consumes 10 nJ per connection. 2) Analog: Similar to the breakdown of digital systems, we separate analog systems into programmable and custom chip implementations. As there are FPGAs for digital systems, there are also ﬁeld programmable analog arrays (FPAAs). For many of the same reasons that FPGAs have been utilized for digital neuromorphic implementations, FPAAs have also been utilized [481], [483]–[488], [604], [869], [1028], [1595]. There have also been custom FPAAs speciﬁcally developed for neu- romorphic systems, including the ﬁeld programmable neural array (FPNA) [482] and the NeuroFPAA [870]. These circuits contain programmable components for neurons, synapses, and other components, rather than being more general FPAAs for general analog circuit design. It has been pointed out that custom analog integrated circuits and neuromorphic systems have several characteristics that make them well suited for one another. In particular, factors such as conservation of charge, ampliﬁcation, thresholding and integration are all characteristics that are present in both analog circuitry and biological systems [1]. In fact, the original term neuromorphic was used to refer to analog designs. Moreover, taking inspiration from biological neural systems and how they operate, neuromorphic based implementations have the poten- tial to overcome some of the issues associated with analog circuits that have prevented them from being widely accepted. Some of these issues are dealing with global asynchrony and noisy, unreliable components [1459]. For both of these cases, systems such as spiking neural networks are natural applications for analog circuitry because they can operate asynchronously and can deal with noise and unreliability. One of the common approaches for analog neuromorphic systems is to utilize circuitry that operates in subthreshold mode, typically for power efﬁciency purposes [101], [325], [330], [334], [373], [576], [577], [641], [647], [654], [665], [684], [700], [704], [704], [720], [721], [727], [737], [1045], [1079], [1080], [1084], [1089], [1160]–[1163], [1274], [1394], [1411], [1596]–[1619]. In fact, the original neuromorphic deﬁ- nition by Carver Mead referred to analog circuits that operated in subthreshold mode [1]. There are a large variety of other neuromorphic analog implementations [7], [8], [12], [14], [17]–[19], [99], [100], [323], [324], [326]–[329], [331]–[333], [335]–[372], [374]–[387], [516], [571], [572], [578]–[580], [594]–[603], [606]–[610], [612]–[614], [620]–[640], [642]– [646], [648]–[653], [655]–[664], [666]–[683], [685]–[699], [701]–[703], [705]–[719], [722]–[726], [728]–[736], [738]– [747], [747], [748], [1030], [1031], [1035], [1044], [1046], [1076]–[1078], [1081]–[1083], [1085]–[1088], [1090]–[1093], [1110]–[1117], [1164]–[1169], [1193]–[1196], [1215]–[1220], [1232]–[1235], [1251]–[1259], [1269]–[1273], [1313], [1320], [1324], [1359], [1395], [1419], [1429], [1455], [1457], [1597], [1620]–[1720]. 3) Mixed Analog/Digital: Mixed analog/digital systems are also very common for neuromorphic systems [5], [15], [16], [21], [408], [409], [428], [430]–[432], [440]–[444], [450], [451], [456], [457], [459]–[467], [471], [473], [477]–[479], [585], [593], [809], [810], [812], [818], [830], [833], [839]– [841], [843], [848], [850], [852], [853], [858], [860], [1061], [1062], [1094], [1102], [1130], [1132], [1135], [1136], [1173]– [1176], [1199], [1203], [1244], [1367], [1434], [1444], [1721]– [1745]. Because of its natural similarity to biological systems, analog circuitry has been commonly utilized in mixed ana- log/digital neuromorphic systems to implement the processing components of neurons and synapses. However, there are several issues with analog systems that can be overcome by utilizing digital components, including unreliability. In some neuromorphic systems, it has been the case that synapse weight values or some component of the memory of the system are stored using digital components, which can be less noisy and more reliable than analog-based memory components [405], [410], [411], [413], [422], [425], [445], [458], [814]–[817], [819]–[821], [821], [823], [824], [828], [834], [835], [837], [842], [844]–[846], [849], [851], [856], [857], [859], [861], [1107], [1131], [1137], [1138], [1186], [1202], [1223], [1243], [1746]–[1760]. For example, synaptic weights are frequently stored in digital memory for analog neuromorphic systems. Other neuromorphic platforms are primarily analog, but utilize digital communication, either within the chip itself, to and from the chip, or between neuromorphic chips [13], [133], [406], [412], [413], [415], [417]–[421], [424], [429], [434]–[437], [446]–[449], [452]– [455], [468]–[470], [472], [480], [586], [615], [616], [813], [826], [827], [829], [854], [855], [1024], [1108], [1109], [1133], [1134], [1171], [1172], [1198], [1221], [1276], [1372], [1452], [1453], [1761]–[1776]. Communication within and between neuromorphic chips is usually in the form of digital spikes for these implementations. Using digital components for programmability or learning mechanisms has also been common in mixed analog/digital systems [407], [412], [416], [423], [433], [439], [825], [831], [832], [1197], [1201], [1201], [1261], [1318], [1777]–[1781]. 14 Fig. 11. Device-level components and their relative popularity in neuromor- phic systems. The size of the boxes corresponds to the number of works referenced that have included those components. Two major projects within the mixed analog/digital family are Neurogrid and BrainScaleS. Neurogrid is a primarily analog chip that is probably closest in spirit to the origi- nal deﬁnition of neuromorphic as coined by Mead [1782]– [1785]. Both of these implementation fall within the mixed analog/digital family because of their digital communication framework. BrainScaleS is a wafer-scale implementation that has analog components [363], [372], [444], [459], [1631], [1770], [1786]. Neurogrid operates in subthreshold mode, and BrainScaleS operates in superthreshold mode. The developers of BrainScaleS chose superthreshold mode because it allows BrainScaleS chips to operate at a much higher rate than is possible with Neurogrid, achieving a 10,000x speed-up [1594]. B. Device-Level Components In this section, we cover some of the non-standard device level or circuit level components that are being utilized in neuromorphic systems. These include a variety of components that have traditionally been used as memory technologies, but they also include elements such as optical components. Figure 11 gives an overview of the device-level components and also shows their relative popularity in the neuromorphic literature. 1) Memristors: Perhaps the most ubiquitous device-level component in neuromorphic systems is the “memory resistor” or the memristor. Memristors were a theoretical circuit element proposed by Leon Chua is 1971 [1787] and “found” by researchers at HP in 2008 [1788]. The key characteristic of memristive devices is that the resistance value of the memristor is dependent upon its historical activity. One of the major reasons that memristors have become popular in neuromorphic computing is their relationship to synapses; namely, circuits that incorporate memristors can exhibit STDP-like behavior that is very similar to what occurs in biological synapses. In fact, it has been proposed that biological STDP can be explained by memristance [1789]. Memristors can be and have been made from a large variety of materials, some of which will be discussed in Section V-C, and these different materials can exhibit radically different characteristics. Another reason for utilizing memristors in neuromorphic systems is their potential for building energy efﬁcient circuitry, and this has been studied extensively, with several works focused entirely on evaluating energy consumption of memristive circuitry in neuromorphic systems [1790]–[1797]. It has also been observed that neuromorphic implementations are a good ﬁt for memristors because the inherent fault tolerance of neural network models can help mitigate effects caused by memristor device variation [1798]–[1802]. A common use of memristors in neuromorphic implemen- tations is as part of or the entire synapse implementation (de- pending on the type of network) [862]–[868], [1025]–[1027], [1036], [1063]–[1065], [1103], [1149]–[1154], [1177], [1178], [1200], [1206]–[1208], [1262]–[1264], [1284], [1356], [1357], [1456], [1803]–[1820]. Sometimes the memristor is simply used as a synaptic weight storage element. In other cases, because of their plasticity-like properties, memristors have been used to implement synaptic systems that include Heb- bian learning in general [1224]–[1231] or STDP in particular [254], [1364], [1365], [1369]–[1371], [1373]–[1377], [1381]– [1389], [1391]–[1393], [1397]–[1400], [1403]–[1405], [1409], [1412], [1413], [1416], [1420]–[1422], [1424], [1430]–[1432], [1436]–[1438], [1441]–[1443], [1445], [1450], [1451]. Per- haps the most common use of a memristor in neuromor- phic systems is to build a memristor crossbar to represent the synapses in the network [475], [476], [478], [1288], [1294], [1296], [1301], [1302], [1307]–[1312], [1316], [1317], [1319], [1358], [1366], [1368], [1379], [1380], [1401], [1402], [1406], [1407], [1414], [1415], [1417], [1418], [1423], [1425], [1426], [1428], [1435], [1440], [1446]–[1449], [1512], [1513], [1821]–[1882]. Early physical implementations of memristors have been in the crossbar conﬁguration. Crossbar realizations are popular in the literature mainly due to their density advan- tage by also because physical crossbars have been fabricated and shown to perform well. Because a single memristor cannot represent positive and negative weight values for a synapse (which may be required over the course of training for that synapse), multi-memristor synapses have been proposed, in- cluding memristor-bridge synapses, which can realize positive, negative and zero weight values [1883]–[1893]. Memristor- based synapse implementations that include forgetting effects have also been studied [1894], [1895]. Because of their relative ubiquity in neuromorphic systems, a set of training algorithms have been developed speciﬁcally with characteristics of mem- ristive systems in mind, such as dealing with non-ideal device characteristics [220], [1295], [1303], [1314], [1329]–[1333], [1362], [1378], [1386], [1388], [1427], [1433], [1896]–[1905]. Memristors have also been utilized in neuron implementa- tions [1416], [1906]–[1913]. For example, memristive circuits have been used to generate complex spiking behavior [1914]– [1916]. In another case, a memristor has been utilized to add stochasticity to an implemented neuron model [1208]. Memris- tors have also been used to implement Hodgkin-Huxley axons in hardware as part of a neuron implementation [1917], [1918]. It is worth noting that there are a variety of issues asso- ciated with using memristors for neuromorphic implemen- tations. These include issues with memristor behavior that can seriously affect the performance of STDP [1919]–[1921], sneak paths [1922], and geometry variations [1923]. It is also 15 worth noting that a fair amount of theory about memristive neural networks has been established, including stabiliza- tion [1924]–[1982], synchronization [1927], [1930], [1947], [1974], [1976], [1981], [1983]–[2046], and passivity [2047]– [2063] criteria for memristive neural networks. However, these works are typically done with respect to ideal memristor models and may not be realistic in fabricated systems. 2) CBRAM and Atomic Switches: Conductive-bridging RAM (CBRAM) has also been utilized in neuromorphic systems. Similar to resistive RAM (ReRAM), which is im- plemented using metal-oxide based memristors or memristive materials, CBRAM is a non-volatile memory technology. CBRAM has been used to implement synapses [299], [1027], [1384], [1390], [1408], [1439], [2064]–[2069] and neurons [457], [2070]. CBRAM differs from resistive RAM in that it utilizes electrochemical properties to form and dissolve connections. CBRAM is fast, nanoscale, and has very low power consumption [2064]. Similarly, atomic switches, which are nano-devices related to resistive memory or memristors, control the diffusion of metal ions to create and destroy an atomic bridge between two electrodes [2071]. Atomic switches have been fabricated for neuromorphic systems. Atomic switches are typically utilized to implement synapses and have been shown to implement synaptic plasticity in a similar way to approaches with memristors [2072]–[2079]. 3) Phase Change Memory: Phase change memory elements have been utilized in neuromorphic systems, usually to achieve high density. Phase change memory elements have commonly been used to realize synapses that can exhibit STDP. Phase change memory elements are usually utilized for synapse im- plementations [261], [2080]–[2099] or synapse weight storage [2100]–[2103], but they have also been used to implement both neurons and synapses [2104]–[2106]. 4) Spin Devices: One of the proposed beyond-CMOS tech- nologies for neuromorphic computing is spintronics (i.e., mag- netic devices). Spintronic devices and components have been considered for neuromorphic implementation because they allow for a variety of tunable functionalities, are compatible with CMOS, and can be implemented at nanoscale for high density. The types of spintronic devices utilized in neuromor- phic systems include spin-transfer torque devices, spin-wave devices, and magnetic domain walls [2107]–[2111]. Spintronic devices have been used to implement neurons [1853], [1880], [2112]–[2118], synapses that usually incorporate a form of on- line learning such as STDP [2119]–[2126], and full networks or network modules [2127]–[2144]. 5) Floating Gate Transistors: Floating-gate transistors, commonly used in digital storage elements such as ﬂash memory [2145], have been utilized frequently in neuromorphic systems. As Aunet and Hartmann note, ﬂoating-gate transistors can be utilized as analog ampliﬁers, and can be used in analog, digital, or mixed-signal circuits for neuromorphic implementa- tion [2146]. The most frequent uses for ﬂoating-gate transistors in neuromorphic systems have been either as analog memory cells for synaptic weight and/or parameter storage [2147]– [2155] or as a synapse implementation that usually includes a learning mechanism such as STDP [259], [281], [285], [2156]–[2168]. However, ﬂoating gate transistors have also been used to implement a linear threshold element that could be utilized for neurons [2146], a full neuron implementation [2169], dendrite models [2170], and to estimate ﬁring rates of silicon neurons [2171]. 6) Optical: Optical implementations and implementations that include optical or photonic components are popular for neuromorphic implementations [2172]–[2181]. In the early days of neuromorphic computing, optical implementations were considered because they are inherently parallel, but it was also noted that the implementation of storage can be difﬁcult in optical systems [2182], so their implementations became less popular for several decades. In more recent years, optical implementations and photonic platforms have reemerged because of their potential for ultrafast operation, relatively moderate complexity and programmability [2183], [2184]. Over the course of development of neuromorphic systems, optical and/or photonic components have been uti- lized to build different components within neuromorphic im- plementations. Optical neuromorphic implementations include optical or opto-electronic synapse implementations in early neuromorphic implementations [2185], [2186] and more recent optical synapses, including using novel materials [2187]– [2191]. There have been several proposed optical or photonic neuron implementations in recent years [150], [2192]–[2197]. C. Materials for Neuromorphic Systems One of the key areas of development in neuromorphic computing in recent years have been in the fabrication and characterization of materials for neuromorphic systems. Though we are primarily focused on the computing and system components of neuromorphic computing, we also want to emphasize the variety of new materials and nano-scale devices being fabricated and characterized for neuromorphic systems by the materials science community. Atomic switches and CBRAM are two of the common nano-scale devices that have been fabricated with different materials that can produce different behaviors. A review of different types of atomic switches for neuromorphic systems is given in [2078], but common materials for atomic switches are Ag2S [2072], [2073], [2076], Cu2S [2074], Ta2O5 [2077], and WO3 – x [2079]. Different materials for atomic switches can exhibit different switching behavior under different con- ditions. As such, the selection of the appropriate material can govern how the atomic switch will behave and will likely be application-dependent. CBRAM has been implemented using GeS2/Ag [299], [457], [1027], [1384], [1439], [2066], [2068], HfO2/GeS2 [2067], Cu/Ti/Al2O3 [2070], Ag/Ge0.3Se0.7 [1408], [2069], [2198], Ag2S [2199]–[2201] and Cu/SiO2 [2069]. Similar to atomic switches, the switching behavior of CBRAM devices is also dependent upon the material selected; the stability and reliability of the device is also dependent upon the material chosen. There are a large variety of implementations of mem- ristors. Perhaps the most popular memristor implementa- tions are based on transition metal-oxides (TMOs). For metal-oxide memristors, a large variety of different materi- als are used, including HfOx [2202]–[2210], TiOx [2211]– [2216], WOx [2217]–[2221], SiOx [2222], [2223], TaOx/TiOx 16 [2224], [2225], NiOx [2226]–[2228], TaOx [2229]–[2231], FeOx [2232], AlOx [2233], [2234], TaOx/TiOx [2224], [2225], HfOx/ZnOx [2235], and PCMO [2236]–[2241] . Different metal oxide memristor types can produce different numbers and types of resistance states, which govern the weight values that can be “stored” on the memristor. They also have different endurance, stability, and reliability characteristics. A variety of other materials for memristors have also been proposed. For example, spin-based magnetic tunnel junction memristors based on MgO have been proposed for imple- mentations of both neurons and synapses [2242], though it has been noted that they have a limited range of resistance levels that make them less applicable to store synaptic weights [2231]. Chalcogenide memristors [2243]–[2245] have also been used to implement synapses; one of the reasons given for utilizing chalcogenide-based memristors is ultra-fast switching speeds, which allow for processes like STDP to take place at nanosecond scale [2243]. Polymer-based memristors have been utilized because of their low cost and tunable per- formance [2211], [2246]–[2254]. Organic memristors (which include organic polymers) have also been proposed [2211], [2255]–[2266]. Ferroelectric materials have been considered for build- ing analog memory for synaptic weights [2267]–[2272], and synaptic devices [2273]–[2276], including those based on ferroelectric memristors [2277]–[2279]. They have primar- ily been investigated as three-terminal synaptic devices (as opposed other implementations that may be two-terminal). Three-terminal synaptic devices can realize learning processes such as STDP in the device itself [2273], [2278], rather than requiring additional circuitry to implement STDP. Graphene has more recently been incorporated in neuromor- phic systems in order to achieve more compact circuits. It has been utilized for both transistors [2280]–[2282] and resistors [2283] for neuromorphic implementations and in full synapse implementations [2284], [2285]. Another material considered for some neuromorphic imple- mentations is the carbon nanotube. Carbon nanotubes have been proposed for use in a variety of neuromorphic compo- nents, including dendrites on neurons [2286]–[2290], synapses [235], [2291]–[2307], and spiking neurons [139], [2308]– [2310]. The reasons that carbon nanotubes have been utilized are that they can produce both the scale of neuromorphic systems (number of neurons and synapses) and density (in terms of synapses) that may be required for emulating or sim- ulating biological neural systems. They have also been used to interact with living tissue, indicating that carbon-nanotube based systems may be useful in prosthetic applications of neuromorphic systems [2297]. A variety of synaptic transistors have also been fabricated for neuromorphic implementations, including silicon-based synaptic transistors [2311], [2312] and oxide-based synaptic transistors [2313]–[2325]. Organic electrochemical transistors [2326]–[2331] and organic nanoparticle transistors [2332]– [2335] have also been utilized to build neuromorphic com- ponents such as synapses. Similar to organic memristors, organic transistors are being pursued because of their low- cost processing and ﬂexibility. Moreover, they are natural for implementations of brain-machine interfaces or any kind of chemical or biological sensor [2326]. Interestingly, groups are pursuing the development of transistors within polymer based membranes that can be used in neuromorphic applications such as biosensors [2336]–[2340]. There is a very large amount of fascinating work being done in the materials science community to develop devices for neuromorphic systems out of novel materials in order to build smaller, faster, and more efﬁcient neuromorphic devices. Different materials for even a single device implementation can have wildly different characteristics. These differences will propagate effects through the rest of the community, up through the device, high-level hardware, supporting software, model and algorithms levels of neuromorphic systems. Thus, as a community, it is important that we understand what implications different materials may have on functionality, which will almost certainly require close collaborations with the materials science community moving forward. D. Summary and Discussion In this section, we have looked at hardware implementations at the full device level, at the device component level, and at the materials level. There is a signiﬁcant body of work in each of these areas. At the system level, there are fully functional neuromorphic systems, including both programmable architec- tures such as FPGAs and FPAAs, as well as custom chip im- plementations that are digital, analog, or mixed analog/digital. A wide variety of novel device components beyond the basic circuit elements used in most device development have been utilized in neuromorphic systems. The most popular new component that is utilized is the memristor, but other device components are becoming popular, including other memory technologies such as CBRAM and phase change memory, as well as spin-based components, optical components, and ﬂoating gate transistors. There are also a large variety of materials being used to develop device components, and the properties of these materials will have fundamental effects on the way future neuromorphic systems will operate. VI. SUPPORTING SYSTEMS In order for neuromorphic systems to be feasible as a complementary architecture for future computing, we must consider the supporting tools required for usability. Two of the key supporting systems for neuromorphic devices are communication frameworks and supporting software. In this section, we brieﬂy discuss some of the work in these two areas. A. Communication Communication for neuromorphic systems includes both intra-chip and inter-chip communication. Perhaps the most common implementation of inter-chip communication is ad- dress event representation (AER) [1785], [2343]–[2350]. In AER communication, each neuron has a unique address, and when a spike is generated that will traverse between chips, the address speciﬁes to which chip it will go. Custom PCI 17 (a) High-Level View (b) Low-Level View Fig. 12. Example neuromorphic visualization tools, giving a high-level view of a spiking neural network model [2341] and a low-level view of a network layout on a particular neuromorphic implementation [2342]. boards for AER have been implemented to optimize perfor- mance [2351], [2352]. Occasionally, inter-chip communication interfaces will have their own hardware implementations, usually in the form of FPGAs [2353]–[2357]. SpiNNaker’s interconnect system is one of its most innovative components; the SpiNNaker chips are interconnected in a toroidal mesh, and an AER communication strategy for inter-chip communication is used [1585], [1587], [2358]–[2366]. It is the communication framework for SpiNNaker that enables scalability, allowing tens of thousands of chips to be utilized together to simulate activity in a single network. A hierarchical version of AER utilizing a tree structure has also been implemented [2367], [2368]. One of the key components of AER communication is that it is asynchronous. In contrast, BrainScaleS utilizes an isynchronous inter-chip communication network, which means that events occur regularly [2369], [2370]. AER communication has also been utilized for on-chip communication [2371], [2372], but it has been noted that there are limits of AER for on-chip communication [2373]. As such, there are several other approaches that have been used to op- timize intra-chip communication. For example, in early work with neuromorphic systems, buses were utilized for some on- chip communication systems [2374], [2375]. In a later work, one on-chip communication optimization removed buses as part of the communication framework to improve performance [879]. Vainbrand and Ginosaur examined different network- on-chip architectures for neural networks, including mesh, shared bus, tree, and point-to-point, and found network-on- chip multicast to give the highest performance [2376], [2377]. Ring-based communication for on-chip communication has also been utilized successfully [2378], [2379]. Communication systems speciﬁcally for feed-forward networks have also been studied [2380]–[2382]. One of the common beyond Moore’s law era technologies to improve performance in communication that is being utilized across a variety of computing platforms (including traditional von Neumann computer systems) is three-dimensional (3D) integration. 3D integration has been utilized in neuromorphic systems even from the early days of neuromorphic, especially for pattern recognition and object recognition tasks [2383], [2384]. In more recent applications, 3D integration has been used in a similar way as it would be for von Neumann architec- tures, where memory is stacked with processing [1058]. It has also been utilized to stack neuromorphic chips. Through sili- con vias (TSVs) are commonly used to physically implement 3D integration approaches for neuromorphic systems [1058], [2066], [2385]–[2387], partially because utilizing TSVs in neuromorphic systems help mitigate some of the issues that arise with using TSVs, such as parasitic capacitance [2388]; however, other technologies have also been utilized in 3D integration, such as microbumps [2389]. 3D integration is commonly used in neuromorphic systems with a variety of other technologies, such as memristors [866], [2390]–[2393], phase change memory [2093], and CMOS-molecular (CMOL) systems [2394]. B. Supporting Software Supporting software will be a vital component in order for neuromorphic systems to be truly successful and accepted both within and outside the computing community. However, there has not been much focus on developing the appropriate tools for these systems. In this section, we discuss some efforts in developing supporting software systems for different neuromorphic implementations and use-cases. One important set of software tools consist of custom hardware synthesis tools [628], [871], [2395]–[2401]. These synthesis tools typically take a relatively high level description and convert it to very low level representations of neural circuitry that can be used to implement neuromorphic systems. They tend to generate application speciﬁc circuits. That is, these tools are meant to work within the conﬁnes of a par- ticular neuromorphic system, but also generate neuromorphic systems for particular applications. A second set of software tools for neuromorphic systems are tools that are meant for programming existing neuromorphic systems. These fall into two primary categories: mapping and programming. Mapping tools are usually meant to take an existing neural network model representation, probably trained 18 ofﬂine using existing methods such as back-propagation, and convert or map that neural network model to a particu- lar neuromorphic architecture [414], [1588], [1589], [2402]– [2413]. These tools typically take into account restrictions associated with the hardware, such as connectivity restrictions or parameter value bounds, and make appropriate adaptations to the network representation to work within those restrictions. Programming tools, in contrast to mapping tools, are built so that a user can explicitly program a particular neuromorphic architecture [566], [1515], [2342], [2414]–[2424]. These can allow the user to program at a low level by setting differ- ent parameter and topology conﬁgurations, or by utilizing custom training methods built speciﬁcally for a particular neuromorphic architecture. The Corelet paradigm used in TrueNorth programming ﬁts into this category [2425]. Corelets are pre-programmed modules that accomplish different tasks. Corelets can be used as building blocks to program networks for TrueNorth that solve more complex tasks. There have also been some programming languages for neuromorphic systems such as PyNN [2426], [2427], PyNCS [2428], and even a neuromorphic instruction set architecture [2429]. These languages have been developed to allow users to describe and program neuromorphic systems at a high-level. Software simulators have also been key in developing usable neuromorphic systems [1515], [1587], [2065], [2342], [2407], [2417], [2430]–[2440]. Software-based simulators are vital for verifying hardware performance, testing new potential hardware changes, and for development and use of training algorithms. If the hardware has not been widely deployed or distributed, software simulators can be key to developing a user base, even if the hardware has not been fabricated beyond simple prototypes. Visualization tools that show what is happening in neuromorphic systems can also be key to allowing users to understand how neuromorphic systems solve problems and to inspire further development within the ﬁeld [2341], [2342], [2416], [2419], [2441]. These visualization tools are often used in combination with software simulations, and they can provide detailed information about what might be occurring at a low-level in the hardware. Figure 12 provides two examples of visualizations for neuromorphic systems. C. Summary When building a neuromorphic system, it is extremely important to think about how the neuromorphic system will actually be used in real computing systems and with real users. Supporting systems, including communication on-chip and between chips and supporting software, will be necessary to enable real utilization of neuromorphic systems. Compared to the number of hardware implementations of neuromorphic systems there are very few works that focus on the develop- ment of supporting software that will enable ease-of-use for these systems. There is signiﬁcantly more work to be done, especially on the supporting software side, within the ﬁeld of neuromorphic computing. It is absolutely necessary that the community develop software tools alongside hardware moving forward. VII. APPLICATIONS The “killer” applications for neuromorphic computing, or the applications that best showcase the capabilities of neuro- morphic computers and devices, have yet to be determined. Obviously, various neural network types have been applied to a wide variety of applications, including image [2442], speech [2443], and data classiﬁcation [2444], control [2445], and anomaly detection [2446]. Implementing neural networks for these types of applications directly in hardware can poten- tially produce lower power, faster computation, and a smaller footprint than can be delivered on a von Neumann architecture. However, many of the application spaces that we will discuss in this section do not actually require any of those characteris- tics. In addition, spiking neural networks have not been studied to their full potential in the way that artiﬁcial neural networks have been, and it may be that physical neuromorphic hardware is required in order to determine what the killer applications for spiking neuromorphic systems will be moving forward. This goes hand-in-hand with the appropriate algorithms being developed for neuromorphic systems, as discussed in Section IV. Here, we discuss a variety of applications of neuromorphic systems. We omit one of the major application areas, which is utilizing neuromorphic systems in order to study neuro- science via faster, more efﬁcient simulation than is possible on traditional computing platforms. Instead, we focus on other real-world applications to which neuromorphic systems have been applied. The goal of this section is to provide a scope of the types of problems neuromorphic systems have successfully tackled, and to provide inspiration to the reader to apply neuromorphic systems to their own set of applications. Figure 13 gives an overview of the types of applications of neuromorphic systems and how popular they have been. There are a broad set of neuromorphic systems that have been developed entirely based on particular sensory sys- tems, and applied to those particular application areas. The most popular of these “sensing” style implementations are vision-based systems, which often have architectures that are entirely based on replicating various characteristics of biological visual systems [299], [324], [344]–[346], [416], [469], [510], [584], [1054], [1057], [1161], [1179], [1186], [1432], [1439], [1456], [1459], [1521]–[1525], [1597], [1598], [1607], [1608], [1612], [1613], [1615], [1643], [1644], [1710], [1711], [1715], [1717], [1718], [1723], [1725], [1727], [1738], [1762], [1767], [1774], [1881], [2160], [2165], [2292], [2351], [2371], [2447]–[2513]. Though vision-based systems are by far the most popular sensory-based systems, there are also neuromorphic auditory systems [127], [299], [335], [374], [1269], [1283], [1418], [1434], [1439], [1460]–[1462], [1649], [1744], [2393], [2441], [2463], [2514]–[2521], olfactory sys- tems [720], [1514], [1515], [2522]–[2531], and somatosensory or touch-based systems [1520], [2532]–[2537]. Another class of applications for neuromorphic systems are those that either interface directly with biological systems or are implanted or worn as part of other objects that are usually used for medical treatment or monitoring [2538]. A key feature of all of these applications is that they require devices that can be made very small and require very low- 19 Fig. 13. Breakdown of applications to which neuromorphic systems have been applied. The size of the boxes corresponds to the number of works in which a neuromorphic system was developed for that application. power. Neuromorphic systems have become more popular in recent years in brain-machine interfaces [237], [412], [590], [1625], [2538]–[2545]. By their very nature, spike-based neuromorphic models communicate using the same type of communication as biological systems, so they are a natural choice for brain-machine or brain-computer interfaces. Other wearable or implantable neuromorphic systems have been developed for pacemakers or deﬁbrillator systems [18], [602], [651]–[653], retina implants [2546], wearable fall detectors for elderly users [1019], and prosthetics [2547]. Robotics applications are also very common for neuromor- phic systems. Very small and power efﬁcient systems are often required for autonomous robots. Many of the requirements for robotics, including motor control, are applications that have been successfully demonstrated in neural networks. Some of the common applications of neuromorphic systems for robotics include learning a particular behavior [2548], [2549], locomotion control or control of particular joints to achieve a certain motion [67]–[69], [361], [663], [1082], [1083], [1261], [1279], [1527], [1784], [1885], [2550], [2551], social learning [2552], [2553], and target or wall following [498], [606], [1576], [1682], [2554]. Thus far, in terms of robotics, the most common use of neuromorphic implementations is for autonomous navigation tasks [16], [195], [393], [486], [532], [547], [735], [1077], [1329]–[1333], [1339], [1503], [1539], [1563], [1671], [1716], [2379], [2555]–[2560]. In the same application space as robotics is the generation of motion through central pattern generators (CPGs). CPGs generate oscillatory motions, such as those used to generate swimming motions in lampreys or walking gaits. There are a variety of implementations of CPGs in neuromorphic systems [615]– [619], [1084], [1313], [1335], [1561], [1609], [1622], [1637], [1647], [1729], [1731]–[1734], [1739], [1775]. Control tasks have been popular for neuromorphic systems because they typically require real-time performance, are of- ten deployed in real systems that require small volume and low power, and have a temporal processing component, so they beneﬁt from models that utilize recurrent connections or delays on synapses. A large variety of different control applications have utilized neuromorphic systems [466], [687], [688], [863], [872], [895], [902], [903], [920], [932], [962], [1017], [1051], [1085], [1660], [2561], [2562], but by far the most common control test case is the cart-pole problem or the inverted pendulum task [487], [512], [792], [902], [903], [1008], [1045], [1337], [1340], [2563]. Neuromorphic systems have also been applied to video games, such as Pong [1563], PACMAN [2405], and Flappy Bird [1337]. An extremely common use of both neural networks and neu- romorphic implementations has been on various image-based applications, including edge detection [220], [339], [520], [783], [829], [922], [1260], [1873], [2107], [2118], [2141], [2287], [2564], image compression [641], [721], [875], [960], [1243], [1263], image ﬁltering [15], [338], [1112], [1255], [1267], [1492], [1516], [1551], [1779], [1886], [1887], [2565], [2566], image segmentation [141], [490], [541], [921], [1272]– [1274], [1277], [1278], [1679], [1712], [1713], [2567]–[2569], and feature extraction [388], [392], [608], [827], [854], [1165], [1269], [1270], [1502], [1606], [1611], [1658], [1719], [2065], [2132], [2570]–[2572]. Image classiﬁcation, detection, or recognition is an extremely popular application for neural networks and neuromorphic systems. The MNIST data set, subsets of the data set, and variations of the data set has been used to evaluate many neuromorphic implementations [211], [264], [267], [290], [297], [316], [395], [491], [557], [584], [585], [593], [790], [862], [936], [941], [967], [1055], [1061], [1063]–[1065], [1183], [1184], [1205], [1207], [1208], [1212]–[1214], [1296]–[1298], [1301], [1305]–[1307], [1371], [1378], [1379], [1390], [1427], [1433], [1563], [1567], [1593], [1796], [1797], [1800], [1801], [1818], [1827], [1829], [1831], [1833], [1835], [1840], [1842], [1845], [1846], [1848], [1852], [1853], [1855], [1856], [1870], [1873], [1877], [1903], [1904], [1906], [1907], [1922], [2080], [2086], [2100]–[2103], [2115], [2116], [2122], [2126], [2130], [2137], [2143], [2209], [2215], [2224], [2241], [2300], [2342], [2389], [2407], [2413], [2418], [2429], [2433], [2568], [2573]–[2594]. Other digit recognition tasks [448], [706], [731], [788], [798], [813], [852], [853], [1130], [1149], [1380], [1415], [1446], [1798], [1819], [1852], [1878], [1895], [2283], [2395], [2396], [2593], [2595]–[2599] and general character recognition tasks [197], [231], [438], [478], [545], [559], [665], [670], [714], [730], [757], [758], [768], [831], [832], [888], [899], [946], [989], [991], [992], [1022], [1157], [1158], [1172], [1304], [1312], [1316], [1317], [1349], [1356], [1381], [1401], [1402], [1435], [1543], [1552], [1558], [1714], [1830], [1858], [1860], [1891], [1892], [1902], [1923], [2070], [2128], [2135], [2140], [2141], [2144], [2350], [2411], [2585], [2600]–[2612] have also been very popular. Recognition of other patterns such as simple shapes or pixel patterns have also been used as applications for neuromor- phic systems [119], [191], [217], [308], [375], [446], [528], [533], [549], [580], [586], [598], [599], [603], [689], [765], [785], [805]–[808], [821], [953], [955], [1113], [1152], [1153], 20 Fig. 14. Examples from different image data sets (MNIST [2618], CIFAR10 [2619], and SVHN [2620]) to which neuromorphic systems have been applied for classiﬁcation purposes. [1167], [1180], [1257], [1287], [1289], [1318], [1398], [1399], [1420], [1423], [1426], [1441], [1445], [1455], [1702], [1795], [1839], [1851], [1857], [1867], [1868], [1879], [1880], [1957], [2085], [2093], [2096], [2097], [2112], [2202], [2277], [2392], [2613], [2614]. Other image classiﬁcation tasks that have been demon- strated on neuromorphic systems include classifying real world images such as trafﬁc signs [1018], [1065], [1305], face recognition or detection [582], [1067], [1204], [1284], [1885], [2130], [2585], [2591], [2615], car recognition or detection [1883], detecting air pollution in images [2616], detection of manufacturing defects or defaults [1692], hand gesture recognition [558], [1038], human recognition [1422], object texture analysis [2617], and other real world image recog- nition tasks [825], [1037], [1058], [2127]. There are several common real-world image data sets that have been evaluated on neuromorphic systems, including the CalTech-101 data set [867], [1068], the Google Street-View House Number (SVHN) data set [316], [2580], [2585], [2590], [2591], the CIFAR10 data set [587], [592], [1066], [1069], [2130], [2590], [2591], and ImageNet [1827]. Evaluations of how well neuromorphic systems implement AlexNet, a popular convolutional neural network architecture for image classiﬁcation, have also been conducted [1056], [1074]. Examples of images from the MNIST data set, the CIFAR10, and the SVHN data set are given in Figure 14 to demonstrate the variety of images that neuromorphic systems have been used to successfully classify or recognize. A variety of sound-based recognition tasks have also been considered with neuromorphic systems. Speech recognition, for example, has been a common application for neuromorphic systems [502], [591], [628], [754], [755], [812], [868], [976], [978], [984], [1004]–[1006], [1030], [1031], [1048], [1104], [1105], [1206], [1463], [1546], [1665], [1859], [1861], [2448], [2621]–[2624]. Neuromorphic systems have also been applied to music recognition tasks [584], [588], [2425]. Both speech and music recognition tasks may require the ability to process temporal components, and may have real-time constraints. As such, neuromorphic systems that are based on recurrent or spiking neural networks that have an inherent temporal processing component are natural ﬁts for these applications. Neuromorphic systems have also been used for other sound- based tasks, including speaker recognition [584], distinguish- ing voice activity from noise [592], and analyzing sound for identiﬁcation purposes [336], [337]. Neuromorphic systems have also been applied to noise ﬁltering applications as well to translate noisy speech or other signals into clean versions of the signal [624], [644], [648], [649]. Applications that utilize video have also been common uses of neuromorphic systems. The most common example for video is object recognition within video frames [794], [1073], [1242], [1439], [1562], [1565], [1568], [2082], [2088]–[2090], [2416], [2625]–[2631]. This application does not necessarily require a temporal component, as it can analyze video frames as images. However, some of the other applications in video do require a temporal component, including motion detection [383], [572], [1768], [1769], [2632], motion estimation [1031], [1704], [2633], motion tracking [905], [917], [2634], [2635], and activity recognition [916], [2636]. Neuromorphic systems have also been applied to natural language processing (NLP) tasks, many of which require recurrent networks. Example applications in NLP that have been demonstrated using neuromorphic systems include sen- tence construction [388], sentence completion [1097], [2411], [2637], question subject classiﬁcation [581], sentiment analy- sis [583], and document similarity [1239]. Tasks that require real-time performance, ability to deploy into an environment with a small footprint, and/or low power are common use cases for neuromorphic systems. Smart sensors are one area of interest, including humidty sensors [756], light intensity sensors [1009], and sensors on mobile devices that can be used to classify and authenticate users [2638]. Similarly, anomaly detectors are also applications for neuromorphic systems, including detecting anomalies in trafﬁc [1825], biological data [2639], and industrial data [2639], ap- plications in cyber security [1807], [2640], and fault detection in diesel engines [931] and analog chips [1686], [1687]. General data classiﬁcation using neuromorphic systems has also been popular. There are a variety of different and diverse application areas in this space to which neuromorphic sys- tems have been applied, including accident diagnosis [1015], cereal grain identiﬁcation [657], [659], computer user analysis [2641], [2642], driver drowsiness detection [2434], gas recog- nition or detection [622], [943], [972], product classiﬁcation [781], hyperspectral data classiﬁcation [750], stock price pre- diction [1537], wind velocity estimation [939], solder joint classiﬁcation [1050], solar radiation detection [929], climate prediction [966], and applications within high energy physics [982], [1018]. Applications within the medical domain have also been popular, including coronary disease diagnosis [954], pulmonary disease diagnosis [887], deep brain sensor moni- toring [404], DNA analysis [1146], heart arrhythmia detection [2434], analysis of electrocardiogram (ECG) [900], [922], [965], [1007], electroencephalogram (EEG) [403], [1100], [1103], [1850], and electromyogram (EMG) [1039], [1103] results, and pharmacology applications [2643]. A set of bench- marks from the UCI machine learning repository [2644] and/or the Proben1 data set [710], [908], [974], [2444] have been popular in both neural network and neuromorphic systems, including the following data sets: building [998], [1840], con- nect4 [1842], [1845], gene [998], [1840], [1842], [1845], glass [951], [1055], [1338], heart [998], [2645], ionosphere [1029], [1055], [2645], iris [215], [305], [530], [573], [575], [612], [952], [959], [983], [1050], [1055], [1295], [1303], [1336], 21 [1342], [1345], [1346], [1504], [2646], [2647], lymphography [1842], [1845], mushroom [1840], [1842], [1845], phoneme [952], Pima Indian diabetes [998], [1027], [1336], [1338], [1504], [2648], semeion [983], thyroid [998], [1840], [1842], [1845], wine [1055], [1303], [1336], [2646], and Wisconsin breast cancer [305], [573], [612], [959], [998], [1029], [1288], [1295], [1303], [1336], [1338], [1340], [1842], [1845], [2645]. These data sets are especially useful because they have been widely used in the literature and can serve as points of com- parison across both neuromorphic systems and neuromorphic models. There are a set of relatively simple testing tasks that have been utilized in evaluating neuromorphic system behaviors, especially in the early development of new devices or archi- tectures. In the early years of development, the two spirals problem was a common benchmark [1315], [2641], [2642], [2649], [2650]. N -bit parity tasks have also been commonly used [209], [217], [625], [657]–[659], [661], [680], [696], [697], [737], [855], [1092], [1193], [1346], [1349], [1883]– [1885], [2640], [2650], [2651]. Basic function approximation has also been a common task for neuromorphic systems, where a mathematical function is speciﬁed and the neuromorphic system is trained to replicate its output behavior [196], [419]– [421], [514], [518], [519], [614], [628], [643], [715], [819], [873], [874], [971], [983], [984], [1024], [1053], [1078], [1079], [1098], [1148], [1304], [1354], [1538], [2214], [2397], [2652]–[2657]. Temporal pattern recall or classiﬁcation [341], [351], [355], [390], [407], [411], [584], [595], [605], [1377], [1622], [2104], [2401], [2658], [2659] and spatiotemporal pattern classiﬁcation [104], [382], [447], [473], [561], [1101], [1370], [1416], [1661], [1771], [1899], [2294], [2416], [2419], [2660]–[2666] have also been popular with neuromorphic systems, because they demonstrate the temporal processing characteristics of networks with recurrent connections and/or delays on the synapses. Finally, implementing various simple logic gates have been common tests for neural networks and neuromorphic systems, including AND [746], [834], [835], [896], [1224], [1289], [1400], [1805], [1902], [2255], [2295], [2667], [2668], OR [910], [1289], [1805], [1862], [1902], [2255], [2295], [2668], NAND [181], [864], [910], [1902], [1909], [1912], [2211], [2255], [2295], [2301], [2669], [2670], NOR [181], [864], [910], [1400], [1902], [1909], [1912], [2211], [2255], [2295], [2301], [2669], [2670], NOT [2667], [2670], and XNOR [712], [1400], [1902], [1912], [2671]. XOR has been especially popular because the results are not linearly separable, which makes it a good test case for more complex neural network topologies [97], [223], [227], [234], [276], [304], [536], [620], [628], [629], [633], [642], [647], [655], [670], [679], [682], [693], [698], [699], [702]– [704], [712], [713], [723], [726], [737], [741], [746], [799], [834], [835], [841], [844]–[848], [865], [928], [946], [949], [952], [954], [956], [957], [959], [1012], [1026], [1052], [1089], [1128], [1271], [1289], [1291], [1304], [1310], [1334], [1340], [1343], [1347], [1348], [1362], [1387], [1400], [1504], [1507], [1517], [1614], [1805], [1902], [1912], [2163], [2278], [2379], [2395], [2396], [2434], [2624], [2640]–[2642], [2668], [2671]–[2681]. In all of the applications discussed above, neuromorphic architectures are utilized primarily as neural networks. How- ever, there are some works that propose utilizing neuromorphic systems for non-neural network applications. Graph algorithms have been one common area of application, as most neuromor- phic architectures can be represented as graphs. Example graph algorithms include maximum cut [1200], minimum graph coloring [1156], [1698], traveling salesman [1147], [2682], and shortest path [1131]. Neuromorphic systems have also been used to simulate motion in ﬂocks of birds [1570] and for optimization problems [1697]. Moving forward, we expect to see many more use cases of neuromorphic architectures in non-neural network applications, as neuromorphic computers become more widely available and are considered simply as a new type of computer with speciﬁc characteristics that are radically different from the characteristics traditional von Neumann architecture. VIII. DISCUSSION: NEUROMORPHIC COMPUTING MOVING FORWARD In this work, we have discussed a variety of components of neuromorphic systems: models, algorithms, hardware in terms of full hardware systems, device-level components, new materials, supporting systems such as communication infras- tructure and supporting software systems, and applications of neuromorphic systems. There has clearly been a tremendous amount of work up until this point in the ﬁeld of neuromorphic computing and neural networks in hardware. Moving forward, there are several exciting research directions in a variety of ﬁelds that can help revolutionize how and why we will use neuromorphic computers in the future (Figure 15). From the machine learning perspective, the most intrigu- ing question is what the appropriate training and/or learn- ing algorithms are for neuromorphic systems. Neuromorphic computing systems provide a platform for exploring different training and learning mechanisms on an accelerated scale. If utilized properly, we expect that neuromorphic computing devices could have a similar effect on increasing spiking neural network performance as GPUs had for deep learning networks. In other words, when the algorithm developer is not reliant on a slow simulation of the network and/or training method, there is much faster turn around in developing effective methods. However, in order for this innovation to take place, algorithm developers will need to be willing to look beyond traditional algorithms such as back-propagation and to think outside the von Neumann box. This will not be an easy task, but if successful, it will potentially revolutionize the way we think about machine learning and the types of applications to which neuromorphic computers can be applied. From the device level perspective, the use of new and emerging technologies and materials for neuromorphic devices is one of the most exciting components of current neuromor- phic computing research. With today’s capabilities in fabrica- tion of nanoscale materials, many novel device components are certain to be developed. Neuromorphic computing researchers at all levels, including models and algorithms, should be collaborating with materials scientists as these new materials are developed in order to customize them for use in a variety 22 Fig. 15. Major neuromorphic computing research challenges in different ﬁelds. of neuromorphic use cases. Not only is there potential for extremely small, ultra-fast neuromorphic computers with new technologies, but we may be able to collaborate to build new composite materials that elicit behaviors that are speciﬁcally tailored for neuromorphic computation. From the software engineering perspective, neuromorphic computers represent a new challenge in how to develop the supporting software systems that will be required for neuro- morphic computers to be usable by non-experts in the future. The neuromorphic computing community would greatly ben- eﬁt from the inclusion of more software engineers as we con- tinue to develop new neuromorphic devices moving forward, both to build supporting software for those systems, but also to inform the design themselves. Once again, neuromorphic computers require a totally different way of thinking than tradi- tional von Neumann architectures. Building out programming languages speciﬁcally for neuromorphic devices wherein the device is not utilized as a neural network simulator but as a special type of computer with certain characteristics (e.g., massive parallelism and collocated memory and computation elements) is one way to begin to attract new users, and we believe such languages will be extremely beneﬁcial moving forward. From the end-user and applications perspective, there is much work that the neuromorphic community needs to do to develop and communicate use cases for neuromorphic systems. Some of those use cases include as a neuromorphic co-processor in a future heterogeneous computer, as smart sensors or anomaly detectors in Internet of Things applica- tions, as extremely low power and small footprint intelligent controllers in autonomous vehicles, as in situ data analysis platforms on deployed systems such as satellites, and many other application areas. The potential to utilize neuromorphic systems for real-time spatiotemporal data analysis or real-time control in a very efﬁcient way needs to be communicated to the community at large, so that those that have these types of applications will think of neuromorphic computers as one solution to their computing needs. There are clearly many exciting areas of development for neuromorphic computing. It is also clear that neuromorphic computers could play a major role in the future computing landscape if we continue to ramp up research at all levels of neuromorphic computing, from materials all the way up to algorithms and models. Neuromorphic computing research would beneﬁt from coordination across all levels, and as a community, we should encourage that coordination to drive innovation in the ﬁeld moving forward. IX. CONCLUSION In this work, we have given an overview of past work in neuromorphic computing. The motivations for building neuromorphic computers have changed over the years, but the need for a non-von Neumann architecture that is low- power, massively parallel, can perform in real time, and has the potential to train or learn in an on-line fashion is clear. We discussed the variety of neuron, synapse and network models that have been used in neuromorphic and neural network hardware in the past, emphasizing the wide variety of selections that can be made in determining how the neuromorphic system will function at an abstract level. It is not clear that this wide variety of models will ever be narrowed down to one all encompassing model in the future, as each model has its own strengths and weaknesses. As such, the neuromorphic computing landscape will likely continue to encompass everything from feed-forward neural networks to highly-detailed biological neural network emulators. We discussed the variety of training and learning algorithms that have been implemented on and for neuromorphic systems. Moving forward, we will need to address building training and learning algorithms speciﬁcally for neuromorphic systems, rather that adapting existing algorithms that were designed with an entirely different architecture in mind. There is great potential for innovation in this particular area of neuromorphic computing, and we believe that it is one of the areas for which innovation will have the most impact. We discussed high-level hardware views of neuromorphic systems, as well as the novel device-level components and materials that are being used to implement them. There is also signiﬁcant room for continued development in this area moving forward. We brieﬂy discussed some of the supporting systems for neuromorphic computers, such as supporting software, of which there is relatively little and from which the community would greatly beneﬁt. Finally, we discuss some of the applications to which neuromorphic computing systems have been successfully applied. The goal of this paper was to give the reader a full view of the types of research that has been done in neuromorphic computing across a variety of ﬁelds. As such, we have included all of the references in this version. We hope that this work will inspire others to develop new and innovative systems to 23 ﬁll in the gaps with their own research in the ﬁeld and to consider neuromorphic computers for their own applications. ACKNOWLEDGMENT This material is based upon work supported in part by the U.S. Department of Energy, Ofﬁce of Science, Ofﬁce of Advanced Scientiﬁc Computing Reserach, under contract number DE-AC05-00OR22725. Research sponsored in part by the Laboratory Directed Research and Development Program of Oak Ridge National Laboratory, managed by UT-Battelle, LLC, for the U. S. Department of Energy. REFERENCES [1] C. Mead, “Neuromorphic electronic systems,” Proceedings of the IEEE, vol. 78, no. 10, pp. 1629–1636, Oct 1990. [2] D. Monroe, “Neuromorphic computing gets ready for the (really) big time,” Communications of the ACM, vol. 57, no. 6, pp. 13–15, 2014. [3] J. Von Neumann and R. Kurzweil, The computer and the brain. Yale University Press, 2012. [4] A. M. Turing, “Computing machinery and intelligence,” Mind, vol. 59, no. 236, pp. 433–460, 1950. [5] A. F. Murray and A. V. Smith, “Asynchronous vlsi neural networks using pulse-stream arithmetic,” Solid-State Circuits, IEEE Journal of, vol. 23, no. 3, pp. 688–697, 1988. [6] F. Blayo and P. Hurat, “A vlsi systolic array dedicated to hopﬁeld neural network,” in VLSI for Artiﬁcial Intelligence. Springer, 1989, pp. 255–264. [7] F. Salam, “A model of neural circuits for programmable vlsi im- plementation,” in Circuits and Systems, 1989., IEEE International Symposium on. IEEE, 1989, pp. 849–851. [8] S. Bibyk, M. Ismail, T. Borgstrom, K. Adkins, R. Kaul, N. Khachab, and S. Dupuie, “Current-mode neural network building blocks for analog mos vlsi,” in Circuits and Systems, 1990., IEEE International Symposium on. IEEE, 1990, pp. 3283–3285. [9] F. Distante, M. Sami, and G. S. Gajani, “A general conﬁgurable architecture for wsi implementation for neural nets,” in Wafer Scale Integration, 1990. Proceedings.,[2nd] International Conference on. IEEE, 1990, pp. 116–123. [10] J. B. Burr, “Digital neural network implementations,” Neural net- works, concepts, applications, and implementations, vol. 3, pp. 237– 285, 1991. [11] M. Chiang, T. Lu, and J. Kuo, “Analogue adaptive neural network circuit,” IEE Proceedings G (Circuits, Devices and Systems), vol. 138, no. 6, pp. 717–723, 1991. [12] K. Madani, P. Garda, E. Belhaire, and F. Devos, “Two analog counters for neural network implementation,” Solid-State Circuits, IEEE Journal of, vol. 26, no. 7, pp. 966–974, 1991. [13] A. F. Murray, D. Del Corso, and L. Tarassenko, “Pulse-stream vlsi neural networks mixing analog and digital techniques,” Neural Networks, IEEE Transactions on, vol. 2, no. 2, pp. 193–204, 1991. [14] P. Hasler and L. Akers, “Vlsi neural systems and circuits,” in Com- puters and Communications, 1990. Conference Proceedings., Ninth Annual International Phoenix Conference on. IEEE, 1990, pp. 31– 37. [15] J.-C. Lee and B. J. Sheu, “Parallel digital image restoration using adaptive vlsi neural chips,” in Computer Design: VLSI in Computers and Processors, 1990. ICCD’90. Proceedings, 1990 IEEE Interna- tional Conference on. IEEE, 1990, pp. 126–129. [16] L. Tarassenko, M. Brownlow, G. Marshall, J. Tombs, and A. Murray, “Real-time autonomous robot navigation using vlsi neural networks,” in Advances in neural information processing systems, 1991, pp. 422– 428. [17] L. Akers, M. Walker, D. Ferry, and R. Grondin, “A limited- interconnect, highly layered synthetic neural architecture,” in VLSI for artiﬁcial intelligence. Springer, 1989, pp. 218–226. [18] P. H. Leong and M. A. Jabri, “A vlsi neural network for morphology classiﬁcation,” in Neural Networks, 1992. IJCNN., International Joint Conference on, vol. 2. IEEE, 1992, pp. 678–683. [19] G. Cairns and L. Tarassenko, “Learning with analogue vlsp mlps,” in Microelectronics for Neural Networks and Fuzzy Systems, 1994., Proceedings of the Fourth International Conference on. IEEE, 1994, pp. 67–76. [20] H. Markram, “The human brain project,” Scientiﬁc American, vol. 306, no. 6, pp. 50–55, 2012. [21] J. Schemmel, D. Bruderle, A. Grubl, M. Hock, K. Meier, and S. Millner, “A wafer-scale neuromorphic hardware system for large- scale neural modeling,” in Circuits and Systems (ISCAS), Proceedings of 2010 IEEE International Symposium on. IEEE, 2010, pp. 1947– 1950. [22] J. Backus, “Can programming be liberated from the von neumann style?: a functional style and its algebra of programs,” Communica- tions of the ACM, vol. 21, no. 8, pp. 613–641, 1978. [23] W. S. McCulloch and W. Pitts, “A logical calculus of the ideas im- manent in nervous activity,” The bulletin of mathematical biophysics, vol. 5, no. 4, pp. 115–133, 1943. [24] E. M. Izhikevich, “Which model to use for cortical spiking neurons?” IEEE transactions on neural networks, vol. 15, no. 5, pp. 1063–1070, 2004. [25] A. L. Hodgkin and A. F. Huxley, “A quantitative description of membrane current and its application to conduction and excitation in nerve,” The Journal of physiology, vol. 117, no. 4, p. 500, 1952. [26] A. Basu, C. Petre, and P. Hasler, “Bifurcations in a silicon neuron,” in Circuits and Systems, 2008. ISCAS 2008. IEEE International Symposium on. IEEE, 2008, pp. 428–431. [27] A. Basu, “Small-signal neural models and their applications,” Biomed- ical Circuits and Systems, IEEE Transactions on, vol. 6, no. 1, pp. 64–75, 2012. [28] F. Castanos and A. Franci, “The transition between tonic spiking and bursting in a six-transistor neuromorphic device,” in Electrical Engineering, Computing Science and Automatic Control (CCE), 2015 12th International Conference on. IEEE, 2015, pp. 1–6. [29] ——, “Implementing robust neuromodulation in neuromorphic cir- cuits,” Neurocomputing, 2016. [30] S. P. DeWeerth, M. S. Reid, E. A. Brown, and R. J. Butera Jr, “A comparative analysis of multi-conductance neuronal models in silico,” Biological cybernetics, vol. 96, no. 2, pp. 181–194, 2007. [31] D. Dupeyron, S. Le Masson, Y. Deval, G. Le Masson, and J.-P. Dom, “A bicmos implementation of the hodgkin-huxley formalism,” in Microelectronics for Neural Networks, 1996., Proceedings of Fifth International Conference on. IEEE, 1996, pp. 311–316. [32] F. Grassia, T. L´evi, S. Sa¨ıghi, and T. Kohno, “Bifurcation analysis in a silicon neuron,” Artiﬁcial Life and Robotics, vol. 17, no. 1, pp. 53–58, 2012. [33] M. Grattarola, M. Bove, S. Martinoia, and G. Massobrio, “Silicon neuron simulation with spice: tool for neurobiology and neural net- works,” Medical and Biological Engineering and Computing, vol. 33, no. 4, pp. 533–536, 1995. [34] A. M. Hegab, N. M. Salem, A. G. Radwan, and L. Chua, “Neuron model with simpliﬁed memristive ionic channels,” International Jour- nal of Bifurcation and Chaos, vol. 25, no. 06, p. 1530017, 2015. [35] K. M. Hynna and K. Boahen, “Thermodynamically equivalent silicon models of voltage-dependent ion channels,” Neural computation, vol. 19, no. 2, pp. 327–350, 2007. [36] S. Kanoh, M. Imai, and N. Hoshimiya, “Analog lsi neuron model inspired by biological excitable membrane,” Systems and Computers in Japan, vol. 36, no. 6, pp. 84–91, 2005. [37] T. Kohno and K. Aihara, “Mathematical-model-based design of silicon burst neurons,” Neurocomputing, vol. 71, no. 7, pp. 1619–1628, 2008. [38] ——, “A design method for analog and digital silicon neurons- mathematical-model-based method-,” in Collective Dynamics: Topics on Competition and Cooperation in the Biosciences: A Selection of Papers in the Proceedings of the BIOCOMP2007 International Conference, vol. 1028, no. 1. AIP Publishing, 2008, pp. 113–128. [39] C. H. Lam, “Neuromorphic semiconductor memory,” in 3D Systems Integration Conference (3DIC), 2015 International. IEEE, 2015, pp. KN3–1. [40] S. Le Masson, A. Laﬂaquiere, T. Bal, and G. Le Masson, “Analog circuits for modeling biological neural networks: design and applica- tions,” Biomedical Engineering, IEEE Transactions on, vol. 46, no. 6, pp. 638–645, 1999. [41] Q. Ma, M. R. Haider, V. L. Shrestha, and Y. Massoud, “Bursting hodgkin–huxley model-based ultra-low-power neuromimetic silicon neuron,” Analog Integrated Circuits and Signal Processing, vol. 73, no. 1, pp. 329–337, 2012. [42] ——, “Low-power spike-mode silicon neuron for capacitive sensing of a biosensor,” in Wireless and Microwave Technology Conference (WAMICON), 2012 IEEE 13th Annual. IEEE, 2012, pp. 1–4. [43] M. Mahowald and R. Douglas, “A silicon neuron,” 1991. 24 [44] M. PARODI and M. STORACE, “On a circuit representation of the hodgkin and huxley nerve axon membrane equations,” International journal of circuit theory and applications, vol. 25, no. 2, pp. 115–124, 1997. [45] F. Pelayo, E. Ros, X. Arreguit, and A. Prieto, “Vlsi implementation of a neural model using spikes,” Analog Integrated Circuits and Signal Processing, vol. 13, no. 1-2, pp. 111–121, 1997. [46] C. Rasche, R. Douglas, and M. Mahowald, “Characterization of a silicon pyramidal neuron,” PROGRESS IN NEURAL PROCESSING, pp. 169–177, 1998. [47] S. Sa¨ıghi, L. Buhry, Y. Bornat, G. N’Kaoua, J. Tomas, and S. Renaud, “Adjusting the neurons models in neuromimetic ics using the voltage- clamp technique,” in Circuits and Systems, 2008. ISCAS 2008. IEEE International Symposium on. IEEE, 2008, pp. 1564–1567. [48] M. Sekikawa, T. Kohno, and K. Aihara, “An integrated circuit design of a silicon neuron and its measurement results,” Artiﬁcial Life and Robotics, vol. 13, no. 1, pp. 116–119, 2008. [49] J. Shin and C. Koch, “Dynamic range and sensitivity adaptation in a silicon spiking neuron,” Neural Networks, IEEE Transactions on, vol. 10, no. 5, pp. 1232–1238, 1999. [50] M. F. Simoni and S. P. DeWeerth, “Adaptation in an avlsi model of a neuron,” in Circuits and Systems, 1998. ISCAS’98. Proceedings of the 1998 IEEE International Symposium on, vol. 3. IEEE, 1998, pp. 111–114. [51] M. F. Simoni, G. S. Cymbalyuk, M. Q. Sorensen, R. L. Calabrese, and S. P. DeWeerth, “Development of hybrid systems: Interfacing a silicon neuron to a leech heart interneuron,” Advances in neural information processing systems, pp. 173–179, 2001. [52] M. F. Simoni, G. S. Cymbalyuk, M. E. Sorensen, R. L. Calabrese, and S. P. DeWeerth, “A multiconductance silicon neuron with biologically matched dynamics,” Biomedical Engineering, IEEE Transactions on, vol. 51, no. 2, pp. 342–354, 2004. [53] J. Tomas, S. Sa¨ıghi, S. Renaud, J. Silver, and H. Barnaby, “A conductance-based silicon neuron with membrane-voltage dependent temporal dynamics,” in NEWCAS Conference (NEWCAS), 2010 8th IEEE International. IEEE, 2010, pp. 377–380. [54] C. Toumazou, J. Georgiou, and E. Drakakis, “Current-mode analogue circuit representation of hodgkin and huxley neuron equations,” Elec- tronics Letters, vol. 34, no. 14, pp. 1376–1377, 1998. [55] A. Borisyuk, “Morris–lecar model,” in Encyclopedia of Computa- tional Neuroscience. Springer, 2015, pp. 1758–1764. [56] M. Gholami and S. Saeedi, “Digital cellular implementation of morris- lecar neuron model,” in Electrical Engineering (ICEE), 2015 23rd Iranian Conference on. IEEE, 2015, pp. 1235–1239. [57] M. Hayati, M. Nouri, S. Haghiri, and D. Abbott, “Digital multi- plierless realization of two coupled biological morris-lecar neuron model,” Circuits and Systems I: Regular Papers, IEEE Transactions on, vol. 62, no. 7, pp. 1805–1814, July 2015. [58] K. Nakada, K. Miura, and T. Asai, “Silicon neuron design based on phase reduction analysis,” in Soft Computing and Intelligent Systems (SCIS) and 13th International Symposium on Advanced Intelligent Systems (ISIS), 2012 Joint 6th International Conference on. IEEE, 2012, pp. 1059–1062. [59] G. Patel and S. DeWeerth, “Analogue vlsi morris-lecar neuron,” Electronics letters, vol. 33, no. 12, pp. 997–998, 1997. [60] G. N. Patel, G. S. Cymbalyuk, R. L. Calabrese, and S. P. DeWeerth, “Bifurcation analysis of a silicon neuron,” in Advances in Neural Information Processing Systems, 2000, pp. 731–737. [61] M. Sekerli and R. Butera, “An implementation of a simple neu- ron model in ﬁeld programmable analog arrays,” in Engineering in Medicine and Biology Society, 2004. IEMBS’04. 26th Annual International Conference of the IEEE, vol. 2. IEEE, 2004, pp. 4564– 4567. [62] S. Binczak, S. Jacquir, J.-M. Bilbault, V. B. Kazantsev, and V. I. Nekorkin, “Experimental study of electrical ﬁtzhugh–nagumo neurons with modiﬁed excitability,” Neural Networks, vol. 19, no. 5, pp. 684– 693, 2006. [63] J. Cosp, S. Binczak, J. Madrenas, and D. Fern´andez, “Implementation of compact vlsi ﬁtzhugh-nagumo neurons,” in Circuits and Systems, 2008. ISCAS 2008. IEEE International Symposium on. IEEE, 2008, pp. 2370–2373. [64] B. Linares-Barranco, E. S´anchez-Sinencio, A. Rodr´ıguez-V´azquez, and J. L. Huertas, “A cmos implementation of ﬁtzhugh-nagumo neuron model,” Solid-State Circuits, IEEE Journal of, vol. 26, no. 7, pp. 956–965, 1991. [65] M. Hayati, M. Nouri, D. Abbott, and S. Haghiri, “Digital multipli- erless realization of two-coupled biological hindmarsh–rose neuron model,” IEEE Transactions on Circuits and Systems II: Express Briefs, vol. 63, no. 5, pp. 463–467, 2016. [66] Y. J. Lee, J. Lee, Y.-B. Kim, J. Ayers, A. Volkovskii, A. Selverston, H. Abarbanel, and M. Rabinovich, “Low power real time electronic neuron vlsi design using subthreshold technique,” in Circuits and Systems, 2004. ISCAS’04. Proceedings of the 2004 International Symposium on, vol. 4. IEEE, 2004, pp. IV–744. [67] J. Lu, Y.-B. Kim, and J. Ayers, “A low power 65nm cmos electronic neuron and synapse design for a biomimetic micro-robot,” in Circuits and Systems (MWSCAS), 2011 IEEE 54th International Midwest Symposium on. IEEE, 2011, pp. 1–4. [68] J. Lu, J. Yang, Y.-B. Kim, and K. K. Kim, “Implementation of cmos neuron for robot motion control unit,” in SoC Design Conference (ISOCC), 2013 International. IEEE, 2013, pp. 9–12. [69] J. Lu, J. Yang, Y.-B. Kim, J. Ayers, and K. K. Kim, “Implementation of excitatory cmos neuron oscillator for robot motion control unit,” JOURNAL OF SEMICONDUCTOR TECHNOLOGY AND SCIENCE, vol. 14, no. 4, pp. 383–390, 2014. [70] E. M. Izhikevich, “Simple model of spiking neurons,” IEEE Transac- tions on neural networks, vol. 14, no. 6, pp. 1569–1572, 2003. [71] O. O. Dutra, G. D. Colleta, L. H. Ferreira, and T. C. Pimenta, “A sub- threshold halo implanted mos implementation of izhikevich neuron model,” in SOI-3D-Subthreshold Microelectronics Technology Uniﬁed Conference (S3S), 2013 IEEE. IEEE, 2013, pp. 1–2. [72] N. Mizoguchi, Y. Nagamatsu, K. Aihara, and T. Kohno, “A two- variable silicon neuron circuit based on the izhikevich model,” Ar- tiﬁcial Life and Robotics, vol. 16, no. 3, pp. 383–388, 2011. [73] N. Ning, G. Li, W. He, K. Huang, L. Pan, K. Ramanathan, R. Zhao, L. Shi et al., “Modeling neuromorphic persistent ﬁring networks,” International Journal of Intelligence Science, vol. 5, no. 02, p. 89, 2015. [74] S. Ozoguz et al., “A low power vlsi implementation of the izhikevich neuron model,” in New Circuits and Systems Conference (NEWCAS), 2011 IEEE 9th International. IEEE, 2011, pp. 169–172. [75] E. Radhika, S. Kumar, and A. Kumari, “Low power analog vlsi implementation of cortical neuron with threshold modulation,” in Advances in Computing, Communications and Informatics (ICACCI), 2015 International Conference on. IEEE, 2015, pp. 561–566. [76] V. Rangan, A. Ghosh, V. Aparin, and G. Cauwenberghs, “A subthresh- old avlsi implementation of the izhikevich simple neuron model,” in Engineering in Medicine and Biology Society (EMBC), 2010 Annual International Conference of the IEEE. IEEE, 2010, pp. 4164–4167. [77] O. Shariﬁpoor and A. Ahmadi, “An analog implementation of biolog- ically plausible neurons using ccii building blocks,” Neural Networks, vol. 36, pp. 129–135, 2012. [78] H. Soleimani, A. Ahmadi, M. Bavandpour, and O. Shariﬁpoor, “A generalized analog implementation of piecewise linear neuron models using ccii building blocks,” Neural Networks, vol. 51, pp. 26–38, 2014. [79] J. H. Wijekoon and P. Dudek, “Simple analogue vlsi circuit of a cor- tical neuron,” in Electronics, Circuits and Systems, 2006. ICECS’06. 13th IEEE International Conference on. IEEE, 2006, pp. 1344–1347. [80] ——, “Compact silicon neuron circuit with spiking and bursting behaviour,” Neural Networks, vol. 21, no. 2, pp. 524–534, 2008. [81] ——, “Integrated circuit implementation of a cortical neuron,” in Cir- cuits and Systems, 2008. ISCAS 2008. IEEE International Symposium on. IEEE, 2008, pp. 1784–1787. [82] ——, “A cmos circuit implementation of a spiking neuron with bursting and adaptation on a biological timescale,” in Biomedical Circuits and Systems Conference, 2009. BioCAS 2009. IEEE. IEEE, 2009, pp. 193–196. [83] S¸ . Mihalas¸ and E. Niebur, “A generalized linear integrate-and-ﬁre neu- ral model produces diverse spiking behaviors,” Neural computation, vol. 21, no. 3, pp. 704–718, 2009. [84] F. Folowosele, R. Etienne-Cummings, and T. J. Hamilton, “A cmos switched capacitor implementation of the mihalas-niebur neuron,” in Biomedical Circuits and Systems Conference, 2009. BioCAS 2009. IEEE. IEEE, 2009, pp. 105–108. [85] F. Folowosele, T. J. Hamilton, and R. Etienne-Cummings, “Silicon modeling of the mihalas¸–niebur neuron,” Neural Networks, IEEE Transactions on, vol. 22, no. 12, pp. 1915–1927, 2011. [86] F. Grassia, T. Levi, T. Kohno, and S. Sa¨ıghi, “Silicon neuron: digital hardware implementation of the quartic model,” Artiﬁcial Life and Robotics, vol. 19, no. 3, pp. 215–219, 2014. [87] J. V. Arthur and K. Boahen, “Silicon neurons that inhibit to synchro- nize,” in Circuits and Systems, 2006. ISCAS 2006. Proceedings. 2006 IEEE International Symposium on. IEEE, 2006, pp. 4–pp. 25 [88] ——, “Silicon-neuron design: A dynamical systems approach,” Cir- cuits and Systems I: Regular Papers, IEEE Transactions on, vol. 58, no. 5, pp. 1034–1043, 2011. [89] R. Wang, T. J. Hamilton, J. Tapson, and A. van Schaik, “A gener- alised conductance-based silicon neuron for large-scale spiking neural networks,” in Circuits and Systems (ISCAS), 2014 IEEE International Symposium on. IEEE, 2014, pp. 1564–1567. [90] R. Wang, C. S. Thakur, T. J. Hamilton, J. Tapson, and A. van Schaik, “A compact avlsi conductance-based silicon neuron,” in Biomedical Circuits and Systems Conference (BioCAS), 2015 IEEE. IEEE, 2015, pp. 1–4. [91] J. H. Wittig and K. Boahen, “Silicon neurons that phase-lock,” in Circuits and Systems, 2006. ISCAS 2006. Proceedings. 2006 IEEE International Symposium on. IEEE, 2006, pp. 4–pp. [92] A. Basu and P. E. Hasler, “Nullcline-based design of a silicon neuron,” Circuits and Systems I: Regular Papers, IEEE Transactions on, vol. 57, no. 11, pp. 2938–2947, 2010. [93] A. Basu, C. Petre, and P. E. Hasler, “Dynamics and bifurcations in a silicon neuron,” Biomedical Circuits and Systems, IEEE Transactions on, vol. 4, no. 5, pp. 320–328, 2010. [94] K. Hynna and K. Boahen, “Space-rate coding in an adaptive silicon neuron,” Neural Networks, vol. 14, no. 6, pp. 645–656, 2001. [95] K. M. Hynna and K. Boahen, “Neuronal ion-channel dynamics in silicon,” in Circuits and Systems, 2006. ISCAS 2006. Proceedings. 2006 IEEE International Symposium on. IEEE, 2006, pp. 4–pp. [96] ——, “Silicon neurons that burst when primed,” in Circuits and Systems, 2007. ISCAS 2007. IEEE International Symposium on. IEEE, 2007, pp. 3363–3366. [97] J. L. Meador and C. S. Cole, “A low-power cmos circuit which emulates temporal electrical properties of neurons,” in Advances in neural information processing systems, 1989, pp. 678–686. [98] C. Rasche and R. Douglas, “An improved silicon neuron,” Analog integrated circuits and signal processing, vol. 23, no. 3, pp. 227–236, 2000. [99] J. G. Elias, H.-H. Chu, and S. M. Meshreki, “Silicon implementation of an artiﬁcial dendritic tree,” in Neural Networks, 1992. IJCNN., International Joint Conference on, vol. 1. IEEE, 1992, pp. 154–159. [100] J. G. Elias, H.-H. Chu, and S. Meshreki, “A neuromorphic impulsive circuit for processing dynamic signals,” in Circuits and Systems, 1992. ISCAS’92. Proceedings., 1992 IEEE International Symposium on, vol. 5. IEEE, 1992, pp. 2208–2211. [101] J. G. Elias and D. P. Northmore, “Programmable dynamics in an analog vlsi neuromorph,” in Neural Networks, 1994. IEEE World Congress on Computational Intelligence., 1994 IEEE International Conference on, vol. 4. IEEE, 1994, pp. 2028–2033. [102] V. Gorelik, “Silicon approximation to biological neuron,” in Neural Networks, 2003. Proceedings of the International Joint Conference on, vol. 2. IEEE, 2003, pp. 965–970. [103] P. Hasler, S. Kozoil, E. Farquhar, and A. Basu, “Transistor channel dendrites implementing hmm classiﬁers,” in Circuits and Systems, 2007. ISCAS 2007. IEEE International Symposium on. IEEE, 2007, pp. 3359–3362. [104] S. Hussain and A. Basu, “Morphological learning in multicompart- ment neuron model with binary synapses,” in Circuits and Systems (ISCAS), 2016 IEEE International Symposium on. IEEE, 2016, pp. 2527–2530. [105] U. Koch and M. Brunner, “A modular analog neuron-model for research and teaching,” Biological cybernetics, vol. 59, no. 4-5, pp. 303–312, 1988. [106] B. A. Minch, P. Hasler, C. Diorio, and C. Mead, “A silicon axon,” Advances in neural information processing systems, pp. 739–746, 1995. [107] C. Rasche and R. J. Douglas, “Forward-and backpropagation in a silicon dendrite,” Neural Networks, IEEE Transactions on, vol. 12, no. 2, pp. 386–393, 2001. [108] C. Rasche, “An a vlsi basis for dendritic adaptation,” Circuits and Systems II: Analog and Digital Signal Processing, IEEE Transactions on, vol. 48, no. 6, pp. 600–605, 2001. [109] R. Wang, C. T. Jin, A. L. McEwan, and A. Van Schaik, “A pro- grammable axonal propagation delay circuit for time-delay spiking neural networks.” in ISCAS, 2011, pp. 869–872. [110] R. Wang, G. Cohen, T. J. Hamilton, J. Tapson, and A. van Schaik, “An improved avlsi axon with programmable delay using spike timing dependent delay plasticity,” in Circuits and Systems (ISCAS), 2013 IEEE International Symposium on. IEEE, 2013, pp. 1592–1595. [111] M. Hayati, M. Nouri, S. Haghiri, and D. Abbott, “A digital realization of astrocyte and neural glial interactions,” Biomedical Circuits and Systems, IEEE Transactions on, vol. PP, no. 99, pp. 1–1, 2015. [112] Y. Irizarry-Valle, A. C. Parker, and J. Joshi, “A cmos neuromorphic approach to emulate neuro-astrocyte interactions,” in Neural Networks (IJCNN), The 2013 International Joint Conference on. IEEE, 2013, pp. 1–7. [113] Y. Irizarry-Valle and A. C. Parker, “An astrocyte neuromorphic circuit that inﬂuences neuronal phase synchrony.” IEEE transactions on biomedical circuits and systems, vol. 9, no. 2, pp. 175–187, 2015. [114] M. Ranjbar and M. Amiri, “An analog astrocyte–neuron interaction circuit for neuromorphic applications,” Journal of Computational Electronics, vol. 14, no. 3, pp. 694–706, 2015. [115] ——, “Analog implementation of neuron–astrocyte interaction in tripartite synapse,” Journal of Computational Electronics, pp. 1–13, 2015. [116] H. Soleimani, M. Bavandpour, A. Ahmadi, and D. Abbott, “Digital implementation of a biological astrocyte model and its application,” Neural Networks and Learning Systems, IEEE Transactions on, vol. 26, no. 1, pp. 127–139, 2015. [117] O. Erdener and S. Ozoguz, “A new neuron model suitable for low power vlsi implementation,” in 2015 9th International Conference on Electrical and Electronics Engineering (ELECO). IEEE, 2015, pp. 15–19. [118] ¨O. Erdener and S. Ozoguz, “A new neuron and synapse model suitable for low power vlsi implementation,” Analog Integrated Circuits and Signal Processing, vol. 89, no. 3, pp. 749–770, 2016. [119] A. Upegui, C. A. Pe˜na-Reyes, and E. Sanchez, “A functional spiking neuron hardware oriented model,” in Computational Methods in Neural Modeling. Springer, 2003, pp. 136–143. [120] A. Upegui, C. A. Pe˜na-Reyes, and E. S´anchez, “A hardware imple- mentation of a network of functional spiking neurons with hebbian learning,” in Biologically Inspired Approaches to Advanced Informa- tion Technology. Springer, 2004, pp. 233–243. [121] T. Kohno, J. Li, and K. Aihara, “Silicon neuronal networks towards brain-morphic computers,” Nonlinear Theory and Its Applications, IEICE, vol. 5, no. 3, pp. 379–390, 2014. [122] T. Kohno and K. Aihara, “A qualitative-modeling-based low-power silicon nerve membrane,” in Electronics, Circuits and Systems (ICECS), 2014 21st IEEE International Conference on. IEEE, 2014, pp. 199–202. [123] T. Kohno, M. Sekikawa, and K. Aihara, “A conﬁgurable qualitative- modeling-based silicon neuron circuit,” Nonlinear Theory and Its Applications, IEICE, vol. 8, no. 1, pp. 25–37, 2017. [124] E. Farquhar and P. Hasler, “A bio-physically inspired silicon neuron,” Circuits and Systems I: Regular Papers, IEEE Transactions on, vol. 52, no. 3, pp. 477–488, 2005. [125] T. Kohno and K. Aihara, “A mathematical-structure-based avlsi silicon neuron model,” in Proceedings of the 2010 International Symposium on Nonlinear Theory and its Applications, 2010, pp. 261–264. [126] G. Massobrio, P. Massobrio, and S. Martinoia, “Modeling and simu- lation of silicon neuron-to-isfet junction,” Journal of Computational Electronics, vol. 6, no. 4, pp. 431–437, 2007. [127] K. Saeki, R. Iidaka, Y. Sekine, and K. Aihara, “Hardware neuron mod- els with cmos for auditory neural networks,” in Neural Information Processing, 2002. ICONIP’02. Proceedings of the 9th International Conference on, vol. 3. IEEE, 2002, pp. 1325–1329. [128] K. Saeki, Y. Hayashi, and Y. Sekine, “Extraction of phase information buried in ﬂuctuation of a pulse-type hardware neuron model using stdp,” in Neural Networks, 2006. IJCNN’06. International Joint Conference on. IEEE, 2006, pp. 1505–1510. [129] A. Tete, A. Deshmukh, P. Bajaj, and A. Keskar, “Design of cortical neuron circuits with vlsi design approach,” J. Soft Computing, vol. 2, no. 4, 2011. [130] J. H. Wijekoon and P. Dudek, “Spiking and bursting ﬁring patterns of a compact vlsi cortical neuron circuit,” in Neural Networks, 2007. IJCNN 2007. International Joint Conference on. IEEE, 2007, pp. 1332–1337. [131] L. Wen-peng, C. Xu, and L. Hua-xiang, “A new hardware-oriented spiking neuron model based on set and its properties,” Physics Procedia, vol. 22, pp. 170–176, 2011. [132] W. Gerstner and W. M. Kistler, Spiking neuron models: Single neurons, populations, plasticity. Cambridge university press, 2002. [133] S. A. Aamir, P. M¨uller, A. Hartel, J. Schemmel, and K. Meier, “A highly tunable 65-nm cmos lif neuron for a large scale neuromor- phic system,” in European Solid-State Circuits Conference, ESSCIRC Conference 2016: 42nd. IEEE, 2016, pp. 71–74. 26 [134] T. Asai, Y. Kanazawa, and Y. Amemiya, “A subthreshold mos neuron circuit based on the volterra system,” Neural Networks, IEEE Transactions on, vol. 14, no. 5, pp. 1308–1312, 2003. [135] A. Bindal and S. Hamedi-Hagh, “The design of a new spiking neuron using dual work function silicon nanowire transistors,” Nanotechnol- ogy, vol. 18, no. 9, p. 095201, 2007. [136] ——, “An integrate and ﬁre spiking neuron using silicon nano-wire technology,” Proc. of Nano Sci. and Technol. Inst.(NSTI), San Jose, California, 2007. [137] J. A. Bragg, E. A. Brown, P. Hasler, and S. P. DeWeerth, “A silicon model of an adapting motoneuron,” in Circuits and Systems, 2002. ISCAS 2002. IEEE International Symposium on, vol. 4. IEEE, 2002, pp. IV–261. [138] Y. Chen, S. Hall, L. McDaid, O. Buiu, and P. M. Kelly, “Analog spiking neuron with charge-coupled synapses.” in World Congress on Engineering. Citeseer, 2007, pp. 440–444. [139] C. Chen, K. Kim, Q. Truong, A. Shen, Z. Li, and Y. Chen, “A spiking neuron circuit based on a carbon nanotube transistor,” Nanotechnol- ogy, vol. 23, no. 27, p. 275202, 2012. [140] P.-Y. Chen, J.-s. Seo, Y. Cao, and S. Yu, “Compact oscillation neuron exploiting metal-insulator-transition for neuromorphic computing,” in Proceedings of the 35th International Conference on Computer-Aided Design. ACM, 2016, p. 15. [141] G. Crebbin, “Image segmentation using neuromorphic integrate-and- ﬁre cells,” in Information, Communications and Signal Processing, 2005 Fifth International Conference on. IEEE, 2005, pp. 305–309. [142] L. Deng, D. Wang, G. Li, Z. Zhang, and J. Pei, “A new computing rule for neuromorphic engineering,” in 2015 15th Non-Volatile Memory Technology Symposium (NVMTS). IEEE, 2015, pp. 1–3. [143] F. Folowosele, A. Harrison, A. Cassidy, A. G. Andreou, R. Etienne- Cummings, S. Mihalas, E. Niebur, and T. J. Hamilton, “A switched capacitor implementation of the generalized linear integrate-and- ﬁre neuron,” in Circuits and Systems, 2009. ISCAS 2009. IEEE International Symposium on. IEEE, 2009, pp. 2149–2152. [144] T. J. Hamilton and A. Van Schaik, “Silicon implementation of the generalized integrate-and-ﬁre neuron model,” in Intelligent Sensors, Sensor Networks and Information Processing (ISSNIP), 2011 Seventh International Conference on. IEEE, 2011, pp. 108–112. [145] T. Iguchi, A. Hirata, and H. Torikai, “Integrate-and-ﬁre-type digital spiking neuron and its learning for spike-pattern-division multiplex communication,” in Neural Networks (IJCNN), The 2010 Interna- tional Joint Conference on. IEEE, 2010, pp. 1–8. [146] G. Indiveri, “A low-power adaptive integrate-and-ﬁre neuron circuit,” in ISCAS (4), 2003, pp. 820–823. [147] G. Indiveri, F. Stefanini, and E. Chicca, “Spike-based learning with a generalized integrate and ﬁre silicon neuron,” in Circuits and Systems (ISCAS), Proceedings of 2010 IEEE International Symposium on. IEEE, 2010, pp. 1951–1954. [148] G. Lecerf, J. Tomas, S. Boyn, S. Girod, A. Mangalore, J. Grollier, and S. Saighi, “Silicon neuron dedicated to memristive spiking neural networks,” in Circuits and Systems (ISCAS), 2014 IEEE International Symposium on. IEEE, 2014, pp. 1568–1571. [149] V. Kornijcuk, H. Lim, J. Y. Seok, G. Kim, S. K. Kim, I. Kim, B. J. Choi, and D. S. Jeong, “Leaky integrate-and-ﬁre neuron circuit based on ﬂoating-gate integrator,” Frontiers in neuroscience, vol. 10, 2016. [150] K. Kravtsov, M. P. Fok, D. Rosenbluth, and P. R. Prucnal, “Ultra- fast all-optical implementation of a leaky integrate-and-ﬁre neuron,” Optics express, vol. 19, no. 3, pp. 2133–2147, 2011. [151] M.-Z. Li, P. Ping-Wang, K.-T. Tang, and W.-C. Fang, “Multi-input silicon neuron with weighting adaptation,” in Life Science Systems and Applications Workshop, 2009. LiSSA 2009. IEEE/NIH. IEEE, 2009, pp. 194–197. [152] H. Lim, V. Kornijcuk, J. Y. Seok, S. K. Kim, I. Kim, C. S. Hwang, and D. S. Jeong, “Reliability of neuronal information conveyed by unreliable neuristor-based leaky integrate-and-ﬁre neurons: a model study,” Scientiﬁc reports, vol. 5, 2015. [153] S.-C. Liu and B. A. Minch, “Homeostasis in a silicon integrate and ﬁre neuron,” Advances in Neural Information Processing Systems, pp. 727–733, 2001. [154] S.-C. Liu, “A wide-ﬁeld direction-selective avlsi spiking neuron,” in Circuits and Systems, 2003. ISCAS’03. Proceedings of the 2003 International Symposium on, vol. 5. IEEE, 2003, pp. V–829. [155] P. Livi and G. Indiveri, “A current-mode conductance-based silicon neuron for address-event neuromorphic systems,” in Circuits and systems, 2009. ISCAS 2009. IEEE international symposium on. IEEE, 2009, pp. 2898–2901. [156] M. Mattia and S. Fusi, “Modeling networks with vlsi (linear) integrate-and-ﬁre neurons,” in Losanna, Switzerland: Proceedings of the 7th international conference on artiﬁcial neural networks. Citeseer, 1997. [157] T. Morie, “Cmos circuits and nanodevices for spike based neural computing,” in Future of Electron Devices, Kansai (IMFEDK), 2015 IEEE International Meeting for. IEEE, 2015, pp. 112–113. [158] D. B.-d. Rubin, E. Chicca, and G. Indiveri, “Characterizing the ﬁring proprieties of an adaptive analog vlsi neuron,” in In MM Auke Jan Ijspeert & N. Wakamiya (Eds.), Biologically Inspired Approaches to Advanced Information Technology First International Workshop, Bioadit 2004. Citeseer, 2004. [159] A. Russell and R. Etienne-Cummings, “Maximum likelihood param- eter estimation of a spiking silicon neuron,” in Circuits and Systems (ISCAS), 2011 IEEE International Symposium on. IEEE, 2011, pp. 669–672. [160] A. Russell, K. Mazurek, S. Mihalas, E. Niebur, and R. Etienne- Cummings, “Parameter estimation of a spiking silicon neuron,” Biomedical Circuits and Systems, IEEE Transactions on, vol. 6, no. 2, pp. 133–141, 2012. [161] S. Srivastava and S. Rathod, “Silicon neuron-analog cmos vlsi imple- mentation and analysis at 180nm,” in Devices, Circuits and Systems (ICDCS), 2016 3rd International Conference on. IEEE, 2016, pp. 28–32. [162] O. Torres, J. Eriksson, J. M. Moreno, and A. Villa, “Hardware optimization of a novel spiking neuron model for the poetic tissue.” in Artiﬁcial Neural Nets Problem Solving Methods. Springer, 2003, pp. 113–120. [163] R. Wang, J. Tapson, T. J. Hamilton, and A. Van Schaik, “An avlsi programmable axonal delay circuit with spike timing dependent delay adaptation,” in Circuits and Systems (ISCAS), 2012 IEEE Interna- tional Symposium on. IEEE, 2012, pp. 2413–2416. [164] S. Wolpert and E. Micheli-Tzanakou, “A neuromime in vlsi,” Neural Networks, IEEE Transactions on, vol. 7, no. 2, pp. 300–306, 1996. [165] E. J. Basham and D. W. Parent, “An analog circuit implementation of a quadratic integrate and ﬁre neuron,” in Engineering in Medicine and Biology Society, 2009. EMBC 2009. Annual International Conference of the IEEE. IEEE, 2009, pp. 741–744. [166] ——, “Compact digital implementation of a quadratic integrate-and- ﬁre neuron,” in Engineering in Medicine and Biology Society (EMBC), 2012 Annual International Conference of the IEEE. IEEE, 2012, pp. 3543–3548. [167] S. Abbas and C. Muthulakshmi, “Neuromorphic implementation of adaptive exponential integrate and ﬁre neuron,” in Communication and Network Technologies (ICCNT), 2014 International Conference on. IEEE, 2014, pp. 233–237. [168] S. Millner, A. Gr¨ubl, K. Meier, J. Schemmel, and M.-O. Schwartz, “A vlsi implementation of the adaptive exponential integrate-and- ﬁre neuron model,” in Advances in Neural Information Processing Systems, 2010, pp. 1642–1650. [169] S. Hashimoto and H. Torikai, “A novel hybrid spiking neuron: Bifurcations, responses, and on-chip learning,” Circuits and Systems I: Regular Papers, IEEE Transactions on, vol. 57, no. 8, pp. 2168–2181, 2010. [170] T. Hishiki and H. Torikai, “Bifurcation analysis of a resonate-and- ﬁre-type digital spiking neuron,” in Neural Information Processing. Springer, 2009, pp. 392–400. [171] ——, “Neural behaviors and nonlinear dynamics of a rotate-and- ﬁre digital spiking neuron,” in Neural Networks (IJCNN), The 2010 International Joint Conference on. IEEE, 2010, pp. 1–8. [172] ——, “A novel rotate-and-ﬁre digital spiking neuron and its neuron- like bifurcations and responses,” Neural Networks, IEEE Transactions on, vol. 22, no. 5, pp. 752–767, 2011. [173] T. Matsubara and H. Torikai, “Dynamic response behaviors of a generalized asynchronous digital spiking neuron model,” in Neural Information Processing. Springer, 2011, pp. 395–404. [174] ——, “A novel asynchronous digital spiking neuron model and its various neuron-like bifurcations and responses,” in Neural Networks (IJCNN), The 2011 International Joint Conference on. IEEE, 2011, pp. 741–748. [175] H. Torikai, H. Hamanaka, and T. Saito, “Reconﬁgurable digital spiking neuron and its pulse-coupled network: Basic characteristics and potential applications,” Circuits and Systems II: Express Briefs, IEEE Transactions on, vol. 53, no. 8, pp. 734–738, 2006. [176] H. Torikai, Y. Shimizu, and T. Saito, “Various spike-trains from a digital spiking neuron: Analysis of inter-spike intervals and their 27 modulation,” in Neural Networks, 2006. IJCNN’06. International Joint Conference on. IEEE, 2006, pp. 3860–3867. [177] H. Torikai, A. Funew, and T. Saito, “Approximation of spike-trains by digital spiking neuron,” in Neural Networks, 2007. IJCNN 2007. International Joint Conference on. IEEE, 2007, pp. 2677–2682. [178] ——, “Digital spiking neuron and its learning for approximation of various spike-trains,” Neural Networks, vol. 21, no. 2, pp. 140–149, 2008. [179] H. Torikai and S. Hashimoto, “A hardware-oriented learning algorithm for a digital spiking neuron,” in Neural Networks, 2008. IJCNN 2008.(IEEE World Congress on Computational Intelligence). IEEE International Joint Conference on. IEEE, 2008, pp. 2472–2479. [180] C. Cerkez, I. Aybay, and U. Halici, “A digital neuron realization for the random neural network model,” in Neural Networks, 1997., International Conference on, vol. 2. IEEE, 1997, pp. 1000–1004. [181] S. Aunet, B. Oelmann, S. Abdalla, and Y. Berg, “Reconﬁgurable sub- threshold cmos perceptron,” in Neural Networks, 2004. Proceedings. 2004 IEEE International Joint Conference on, vol. 3. IEEE, 2004, pp. 1983–1988. [182] S. Aunet, B. Oelmann, P. A. Norseng, and Y. Berg, “Real-time re- conﬁgurable subthreshold cmos perceptron,” Neural Networks, IEEE Transactions on, vol. 19, no. 4, pp. 645–657, 2008. [183] V. Bohossian, P. Hasler, and J. Bruck, “Programmable neural logic,” Components, Packaging, and Manufacturing Technology, Part B: Advanced Packaging, IEEE Transactions on, vol. 21, no. 4, pp. 346– 351, 1998. [184] S. Draghici, D. Miller et al., “A vlsi neural network classiﬁer based on integer-valued weights,” in Neural Networks, 1999. IJCNN’99. International Joint Conference on, vol. 4. IEEE, 1999, pp. 2419– 2424. [185] B. Taheri et al., “Proposed cmos vlsi implementation of an elec- tronic neuron using multivalued signal processing,” in Multiple- Valued Logic, 1991., Proceedings of the Twenty-First International Symposium on. IEEE, 1991, pp. 203–209. [186] V. Varshavsky, “Cmos artiﬁcial neuron on the base of β-driven threshold element,” in Systems, Man, and Cybernetics, 1998. 1998 IEEE International Conference on, vol. 2. IEEE, 1998, pp. 1857– 1861. [187] V. Varshavsky and V. Marakhovsky, “Learning experiments with cmos artiﬁcial neuron,” in Computational Intelligence. Springer, 1999, pp. 706–707. [188] ——, “Beta-cmos artiﬁcial neuron and implementability limits,” in Engineering Applications of Bio-Inspired Artiﬁcial Neural Networks. Springer, 1999, pp. 117–128. [189] ——, “Implementability restrictions of the beta-cmos artiﬁcial neu- ron,” in Electronics, Circuits and Systems, 1999. Proceedings of ICECS’99. The 6th IEEE International Conference on, vol. 1. IEEE, 1999, pp. 401–405. [190] V. Varshavsky, V. Marakhovsky, and H. Saito, “Cmos implementation of an artiﬁcial neuron training on logical threshold functions,” WSEAS Transaction on Circuits and Systems, no. 4, pp. 370–390, 2009. [191] B. Zamanlooy and M. Mirhassani, “Efﬁcient hardware implementation of threshold neural networks,” in New Circuits and Systems Confer- ence (NEWCAS), 2012 IEEE 10th International. IEEE, 2012, pp. 1–4. [192] S. A. Al-Kazzaz and R. A. Khalil, “Fpga implementation of artiﬁcial neurons: Comparison study,” in Information and Communication Technologies: From Theory to Applications, 2008. ICTTA 2008. 3rd International Conference on. IEEE, 2008, pp. 1–6. [193] S. Azizian, K. Fathi, B. Mashouﬁ, and F. Derogarian, “Implementation of a programmable neuron in 0.35µm cmos process for multi- layer ann applications,” in EUROCON-International Conference on Computer as a Tool (EUROCON), 2011 IEEE. IEEE, 2011, pp. 1–4. [194] M. Ba˜nuelos-Saucedo, J. Castillo-Hern´andez, S. Quintana-Thierry, R. Dami´an-Zamacona, J. Valeriano-Assem, R. Cervantes, R. Fuentes- Gonz´alez, G. Calva-Olmos, and J. P´erez-Silva, “Implementation of a neuron model using fpgas,” Journal of Applied Research and Technology, vol. 1, no. 03, 2003. [195] M. Acconcia Dias, D. Oliva Sales, and F. S. Osorio, “Automatic gen- eration of luts for hardware neural networks,” in Intelligent Systems (BRACIS), 2014 Brazilian Conference on. IEEE, 2014, pp. 115–120. [196] M. Al-Nsour and H. S. Abdel-Aty-Zohdy, “Implementation of pro- grammable digital sigmoid function circuit for neuro-computing,” in Circuits and Systems, 1998. Proceedings. 1998 Midwest Symposium on. IEEE, 1998, pp. 571–574. [197] A. Basaglia, W. Fornaciari, and F. Salice, “Correct implementation of digital neural networks,” in Circuits and Systems, 1995., Proceedings., Proceedings of the 38th Midwest Symposium on, vol. 1. IEEE, 1995, pp. 81–84. [198] X. Chen, G. Wang, W. Zhou, S. Chang, and S. Sun, “Efﬁcient sigmoid function for neural networks based fpga design,” in Intelligent Computing. Springer, 2006, pp. 672–677. [199] I. del Campo, R. Finker, J. Echanobe, and K. Basterretxea, “Controlled accuracy approximation of sigmoid function for efﬁcient fpga-based implementation of artiﬁcial neurons,” Electronics Letters, vol. 49, no. 25, pp. 1598–1600, 2013. [200] S. Jeyanthi and M. Subadra, “Implementation of single neuron using various activation functions with fpga,” in Advanced Communication Control and Computing Technologies (ICACCCT), 2014 International Conference on. IEEE, 2014, pp. 1126–1131. [201] G. Khodabandehloo, M. Mirhassani, and M. Ahmadi, “Analog imple- mentation of a novel resistive-type sigmoidal neuron,” Very Large Scale Integration (VLSI) Systems, IEEE Transactions on, vol. 20, no. 4, pp. 750–754, 2012. [202] D. E. Khodja, A. Kheldoun, and L. Refouﬁ, “Sigmoid function approximation for ann implementation in fpga devices,” in Proc. of the 9th WSEAS Int. Conf. On Circuits, Systems, Electronics, Control, and Signal Processing, Stevens point, WI, 2010. [203] D. Larkin, A. Kinane, V. Muresan, and N. O?Connor, “An efﬁcient hardware architecture for a neural network activation function gener- ator,” in Advances in Neural Networks-ISNN 2006. Springer, 2006, pp. 1319–1327. [204] A. Mishra, K. Raj et al., “Implementation of a digital neuron with nonlinear activation function using piecewise linear approximation technique,” in Microelectronics, 2007. ICM 2007. Internatonal Con- ference on. IEEE, 2007, pp. 69–72. [205] D. Myers and R. Hutchinson, “Efﬁcient implementation of piecewise linear activation function for digital vlsi neural networks,” Electronics Letters, vol. 25, p. 1662, 1989. [206] F. Ortega-Zamorano, J. M. Jerez, G. Juarez, J. O. Perez, and L. Franco, “High precision fpga implementation of neural network activation functions,” in Intelligent Embedded Systems (IES), 2014 IEEE Sym- posium on. IEEE, 2014, pp. 55–60. [207] M. Panicker and C. Babu, “Efﬁcient fpga implementation of sigmoid and bipolar sigmoid activation functions for multilayer perceptrons,” IOSR Journal of Engineering (IOSRJEN), pp. 1352–1356, 2012. [208] I. Sahin and I. Koyuncu, “Design and implementation of neural networks neurons with radbas, logsig, and tansig activation functions on fpga,” Elektronika ir Elektrotechnika, vol. 120, no. 4, pp. 51–54, 2012. [209] V. Saichand, D. Nirmala, S. Arumugam, and N. Mohankumar, “Fpga realization of activation function for artiﬁcial neural networks,” in Intelligent Systems Design and Applications, 2008. ISDA’08. Eighth International Conference on, vol. 3. IEEE, 2008, pp. 159–164. [210] T. Szabo, G. Horv et al., “An efﬁcient hardware implementation of feed-forward neural networks,” Applied Intelligence, vol. 21, no. 2, pp. 143–158, 2004. [211] C.-H. Tsai, Y.-T. Chih, W. Wong, and C.-Y. Lee, “A hardware- efﬁcient sigmoid function with adjustable precision for neural network system,” 2015. [212] B. M. Wilamowski, J. Binfet, and M. Kaynak, “Vlsi implementation of neural networks,” International journal of neural systems, vol. 10, no. 03, pp. 191–197, 2000. [213] D. Baptista and F. Morgado-Dias, “Low-resource hardware implemen- tation of the hyperbolic tangent for artiﬁcial neural networks,” Neural Computing and Applications, vol. 23, no. 3-4, pp. 601–607, 2013. [214] C. W. Lin and J. S. Wang, “A digital circuit design of hyperbolic tan- gent sigmoid function for neural networks,” in Circuits and Systems, 2008. ISCAS 2008. IEEE International Symposium on. IEEE, 2008, pp. 856–859. [215] P. Santos, D. Ouellet-Poulin, D. Shapiro, and M. Bolic, “Artiﬁcial neural network acceleration on fpga using custom instruction,” in Electrical and Computer Engineering (CCECE), 2011 24th Canadian Conference on. IEEE, 2011, pp. 000 450–000 455. [216] B. Zamanlooy and M. Mirhassani, “Efﬁcient vlsi implementation of neural networks with hyperbolic tangent activation function,” Very Large Scale Integration (VLSI) Systems, IEEE Transactions on, vol. 22, no. 1, pp. 39–48, 2014. [217] H. Hikawa, “A digital hardware pulse-mode neuron with piecewise linear activation function,” Neural Networks, IEEE Transactions on, vol. 14, no. 5, pp. 1028–1037, 2003. 28 [218] J. Faridi, M. S. Ansari, and S. A. Rahman, “A neuromorphic majority function circuit with o (n) area complexity in 180 nm cmos,” in Proceedings of the International Conference on Data Engineering and Communication Technology. Springer, 2017, pp. 473–480. [219] X. Zhu, J. Shen, B. Chi, and Z. Wang, “Circuit implementation of multi-thresholded neuron (mtn) using bicmos technology,” in Neural Networks, 2005. IJCNN’05. Proceedings. 2005 IEEE International Joint Conference on, vol. 1. IEEE, 2005, pp. 627–632. [220] C. Merkel, D. Kudithipudi, and N. Sereni, “Periodic activation func- tions in memristor-based analog neural networks,” in Neural Networks (IJCNN), The 2013 International Joint Conference on. IEEE, 2013, pp. 1–7. [221] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, “Learning rep- resentations by back-propagating errors,” Cognitive modeling, vol. 5, no. 3, p. 1, 1988. [222] C. Lu and B. Shi, “Circuit design of an adjustable neuron activation function and its derivative,” Electronics Letters, vol. 36, no. 6, pp. 553–555, 2000. [223] ——, “Circuit realization of a programmable neuron transfer function and its derivative,” in ijcnn. IEEE, 2000, p. 4047. [224] A. Armato, L. Fanucci, E. P. Scilingo, and D. De Rossi, “Low- error digital hardware implementation of artiﬁcial neuron activation functions and their derivative,” Microprocessors and Microsystems, vol. 35, no. 6, pp. 557–567, 2011. [225] K. Basterretxea, J. Tarela, and I. Del Campo, “Approximation of sigmoid function and the derivative for hardware implementation of artiﬁcial neurons,” IEE Proceedings-Circuits, Devices and Systems, vol. 151, no. 1, pp. 18–24, 2004. [226] V. Beiu, J. Peperstraete, J. Vandewalle, and R. Lauwereins, “Closse approximations of sigmoid functions by sum of step for vlsi imple- mentation of neural networks,” Sci. Ann. Cuza Univ., vol. 3, pp. 5–34, 1994. [227] P. Murtagh and A. Tsoi, “Implementation issues of sigmoid function and its derivative for vlsi digital neural networks,” IEE Proceedings E (Computers and Digital Techniques), vol. 139, no. 3, pp. 207–214, 1992. [228] K. Al-Ruwaihi, “Cmos analogue neurone circuit with programmable activation functions utilising mos transistors with optimised pro- cess/device parameters,” IEE Proceedings-Circuits, Devices and Sys- tems, vol. 144, no. 6, pp. 318–322, 1997. [229] S. Lee and K. Lau, “Low power building block for artiﬁcial neural networks,” Electronics Letters, vol. 31, no. 19, pp. 1618–1619, 1995. [230] A. Deshmukh, J. Morghade, A. Khera, and P. Bajaj, “Binary neural networks–a cmos design approach,” in Knowledge-Based Intelligent Information and Engineering Systems. Springer, 2005, pp. 1291– 1296. [231] T. Yamakawa, “Silicon implementation of a fuzzy neuron,” Fuzzy Systems, IEEE Transactions on, vol. 4, no. 4, pp. 488–501, 1996. [232] M. Abo-Elsoud, “Analog circuits for electronic neural network,” in Circuits and Systems, 1992., Proceedings of the 35th Midwest Symposium on. IEEE, 1992, pp. 5–8. [233] P. W. Hollis and J. J. Paulos, “An analog bicmos hopﬁeld neuron,” in Analog VLSI Neural Networks. Springer, 1993, pp. 11–17. [234] B. Liu, S. Konduri, R. Minnich, and J. Frenzel, “Implementation of pulsed neural networks in cmos vlsi technology,” in Proceedings of the 4th WSEAS International Conference on Signal Processing, Robotics and Automation. World Scientiﬁc and Engineering Academy and Society (WSEAS), 2005, p. 20. [235] A. K. Friesz, A. C. Parker, C. Zhou, K. Ryu, J. M. Sanders, H.-S. P. Wong, and J. Deng, “A biomimetic carbon nanotube synapse circuit,” in Biomedical Engineering Society (BMES) Annual Fall Meeting, vol. 2, no. 8, 2007, p. 29. [236] C. Gordon, E. Farquhar, and P. Hasler, “A family of ﬂoating-gate adapting synapses based upon transistor channel models,” in Circuits and Systems, 2004. ISCAS’04. Proceedings of the 2004 International Symposium on, vol. 1. IEEE, 2004, pp. I–317. [237] C. Gordon, A. Preyer, K. Babalola, R. J. Butera, and P. Hasler, “An artiﬁcial synapse for interfacing to biological neurons,” in Circuits and Systems, 2006. ISCAS 2006. Proceedings. 2006 IEEE International Symposium on. IEEE, 2006, pp. 4–pp. [238] A. Kazemi, A. Ahmadi, S. Alirezaee, and M. Ahmadi, “A modiﬁed synapse model for neuromorphic circuits,” in 2016 IEEE 7th Latin American Symposium on Circuits & Systems (LASCAS). IEEE, 2016, pp. 67–70. [239] H. You and D. Wang, “Neuromorphic implementation of attractor dynamics in decision circuit with nmdars,” in Circuits and Systems (ISCAS), 2016 IEEE International Symposium on. IEEE, 2016, pp. 369–372. [240] E. Lazaridis, E. M. Drakakis, and M. Barahona, “A biomimetic cmos synapse,” in Circuits and Systems, 2006. ISCAS 2006. Proceedings. 2006 IEEE International Symposium on. IEEE, 2006, pp. 4–pp. [241] S. Thanapitak and C. Toumazou, “A bionics chemical synapse,” IEEE transactions on biomedical circuits and systems, vol. 7, no. 3, pp. 296–306, 2013. [242] M. Noack, C. Mayr, J. Partzsch, and R. Schuffny, “Synapse dynamics in cmos derived from a model of neurotransmitter release,” in Circuit Theory and Design (ECCTD), 2011 20th European Conference on. IEEE, 2011, pp. 198–201. [243] S. Pradyumna and S. Rathod, “Analysis of cmos inhibitory synapse with varying neurotransmitter concentration, reuptake time and spread delay,” in VLSI Design and Test (VDAT), 2015 19th International Symposium on. IEEE, 2015, pp. 1–5. [244] ——, “Analysis of cmos synapse generating excitatory postsynaptic potential using dc control voltages,” in Communication Technologies (GCCT), 2015 Global Conference on. IEEE, 2015, pp. 433–436. [245] J. H. Wijekoon and P. Dudek, “Analogue cmos circuit implementation of a dopamine modulated synapse,” in Circuits and Systems (ISCAS), 2011 IEEE International Symposium on. IEEE, 2011, pp. 877–880. [246] C. Bartolozzi and G. Indiveri, “Synaptic dynamics in analog vlsi,” Neural computation, vol. 19, no. 10, pp. 2581–2603, 2007. [247] B. V. Benjamin, J. V. Arthur, P. Gao, P. Merolla, and K. Boahen, “A superposable silicon synapse with programmable reversal potential,” in Engineering in Medicine and Biology Society (EMBC), 2012 Annual International Conference of the IEEE. IEEE, 2012, pp. 771– 774. [248] M. Noack, M. Krause, C. Mayr, J. Partzsch, and R. Schuffny, “Vlsi implementation of a conductance-based multi-synapse using switched- capacitor circuits,” in Circuits and Systems (ISCAS), 2014 IEEE International Symposium on. IEEE, 2014, pp. 850–853. [249] C. Rasche and R. J. Douglas, “Silicon synaptic conductances,” Journal of computational neuroscience, vol. 7, no. 1, pp. 33–39, 1999. [250] R. Z. Shi and T. K. Horiuchi, “A summating, exponentially-decaying cmos synapse for spiking neural systems,” in Advances in neural information processing systems, 2003, p. None. [251] T. Yu, S. Joshi, V. Rangan, and G. Cauwenberghs, “Subthreshold mos dynamic translinear neural and synaptic conductance,” in Neural Engineering (NER), 2011 5th International IEEE/EMBS Conference on. IEEE, 2011, pp. 68–71. [252] S.-J. Choi, G.-B. Kim, K. Lee, K.-H. Kim, W.-Y. Yang, S. Cho, H.-J. Bae, D.-S. Seo, S.-I. Kim, and K.-J. Lee, “Synaptic behaviors of a single metal–oxide–metal resistive device,” Applied Physics A, vol. 102, no. 4, pp. 1019–1025, 2011. [253] T. Chou, J.-C. Liu, L.-W. Chiu, I. Wang, C.-M. Tsai, T.-H. Hou et al., “Neuromorphic pattern learning using hbm electronic synapse with excitatory and inhibitory plasticity,” in VLSI Technology, Systems and Application (VLSI-TSA), 2015 International Symposium on. IEEE, 2015, pp. 1–2. [254] E. Covi, S. Brivio, M. Fanciulli, and S. Spiga, “Synaptic potentia- tion and depression in al: Hfo 2-based memristor,” Microelectronic Engineering, vol. 147, pp. 41–44, 2015. [255] S. Desbief, A. Kyndiah, D. Guerin, D. Gentili, M. Murgia, S. Lenfant, F. Alibart, T. Cramer, F. Biscarini, and D. Vuillaume, “Low voltage and time constant organic synapse-transistor,” Organic Electronics, vol. 21, pp. 47–53, 2015. [256] R. Gopalakrishnan and A. Basu, “Robust doublet stdp in a ﬂoating- gate synapse,” in Neural Networks (IJCNN), 2014 International Joint Conference on. IEEE, 2014, pp. 4296–4301. [257] S.-C. Liu, “Analog vlsi circuits for short-term dynamic synapses,” EURASIP Journal on Applied Signal Processing, vol. 2003, pp. 620– 628, 2003. [258] M. Noack, C. Mayr, J. Partzsch, M. Schultz, and R. Sch¨uffny, “A switched-capacitor implementation of short-term synaptic dynamics,” in Mixed Design of Integrated Circuits and Systems (MIXDES), 2012 Proceedings of the 19th International Conference. IEEE, 2012, pp. 214–218. [259] S. Ramakrishnan, P. E. Hasler, and C. Gordon, “Floating gate synapses with spike-time-dependent plasticity,” Biomedical Circuits and Sys- tems, IEEE Transactions on, vol. 5, no. 3, pp. 244–252, 2011. [260] M. Suri, V. Sousa, L. Perniola, D. Vuillaume, and B. DeSalvo, “Phase change memory for synaptic plasticity application in neuromorphic systems,” in Neural Networks (IJCNN), The 2011 International Joint Conference on. IEEE, 2011, pp. 619–624. 29 [261] M. Suri, O. Bichler, Q. Hubert, L. Perniola, V. Sousa, C. Jahan, D. Vuillaume, C. Gamrat, and B. DeSalvo, “Interface engineering of pcm for improved synaptic performance in neuromorphic systems,” in Memory Workshop (IMW), 2012 4th IEEE International. IEEE, 2012, pp. 1–4. [262] A. D. Tete, A. Deshmukh, P. Bajaj, and A. G. Keskar, “Design of dynamic synapse circuits with vlsi design approach,” in Emerging Trends in Engineering and Technology (ICETET), 2010 3rd Interna- tional Conference on. IEEE, 2010, pp. 707–711. [263] Y. Dan and M.-m. Poo, “Spike timing-dependent plasticity of neural circuits,” Neuron, vol. 44, no. 1, pp. 23–30, 2004. [264] S. Afshar, L. George, C. S. Thakur, J. Tapson, A. van Schaik, P. de Chazal, and T. J. Hamilton, “Turn down that noise: Synaptic encoding of afferent snr in a single spiking neuron,” Biomedical Circuits and Systems, IEEE Transactions on, vol. 9, no. 2, pp. 188– 196, 2015. [265] S. Ambrogio, S. Balatti, F. Nardi, S. Facchinetti, and D. Ielmini, “Spike-timing dependent plasticity in a transistor-selected resistive switching memory,” Nanotechnology, vol. 24, no. 38, p. 384012, 2013. [266] S. Ambrogio, S. Balatti, V. Milo, R. Carboni, Z.-Q. Wang, A. Calderoni, N. Ramaswamy, and D. Ielmini, “Neuromorphic learn- ing and recognition with one-transistor-one-resistor synapses and bistable metal oxide rram,” IEEE Transactions on Electron Devices, vol. 63, no. 4, pp. 1508–1515, 2016. [267] S. Ambrogio, S. Balatti, V. Milo, R. Carboni, Z. Wang, A. Calderoni, N. Ramaswamy, and D. Ielmini, “Novel rram-enabled 1t1r synapse capable of low-power stdp via burst-mode communication and real- time unsupervised machine learning,” in VLSI Technology, 2016 IEEE Symposium on. IEEE, 2016, pp. 1–2. [268] M. R. Azghadi, O. Kavehei, S. Al-Sarawi, N. Iannella, and D. Abbott, “Novel vlsi implementation for triplet-based spike-timing dependent plasticity,” in Intelligent Sensors, Sensor Networks and Information Processing (ISSNIP), 2011 Seventh International Conference on. IEEE, 2011, pp. 158–162. [269] M. R. Azghadi, S. Al-Sarawi, D. Abbott, and N. Iannella, “A neuromorphic vlsi design for spike timing and rate based synaptic plasticity,” Neural Networks, vol. 45, pp. 70–82, 2013. [270] M. R. Azghadi, S. Al-Sarawi, N. Iannella, and D. Abbott, “A new compact analog vlsi model for spike timing dependent plasticity,” in Very Large Scale Integration (VLSI-SoC), 2013 IFIP/IEEE 21st International Conference on. IEEE, 2013, pp. 7–12. [271] M. R. Azghadi, N. Iannella, S. Al-Sarawi, and D. Abbott, “Tunable low energy, compact and high performance neuromorphic circuit for spike-based synaptic plasticity,” PloS one, vol. 9, no. 2, p. e88326, 2014. [272] S. A. Bamford, A. F. Murray, and D. J. Willshaw, “Spike-timing- dependent plasticity with weight dependence evoked from physical constraints,” Biomedical Circuits and Systems, IEEE Transactions on, vol. 6, no. 4, pp. 385–398, 2012. [273] C. Bartolozzi and G. Indiveri, “Global scaling of synaptic efﬁcacy: Homeostasis in silicon synapses,” Neurocomputing, vol. 72, no. 4, pp. 726–731, 2009. [274] M. Boegerhausen, P. Suter, and S.-C. Liu, “Modeling short-term synaptic depression in silicon,” Neural Computation, vol. 15, no. 2, pp. 331–348, 2003. [275] A. Cassidy, A. G. Andreou, and J. Georgiou, “A combinational digital logic approach to stdp,” in Circuits and Systems (ISCAS), 2011 IEEE International Symposium on. IEEE, 2011, pp. 673–676. [276] S. Dytckov, M. Daneshtalab, M. Ebrahimi, H. Anwar, J. Plosila, and H. Tenhunen, “Efﬁcient stdp micro-architecture for silicon spiking neural networks,” in Digital System Design (DSD), 2014 17th Eu- romicro Conference on. IEEE, 2014, pp. 496–503. [277] S. Fusi, M. Annunziato, D. Badoni, A. Salamon, and D. J. Amit, “Spike-driven synaptic plasticity: theory, simulation, vlsi implemen- tation,” Neural Computation, vol. 12, no. 10, pp. 2227–2258, 2000. [278] R. Gopalakrishnan and A. Basu, “Triplet spike time dependent plas- ticity in a ﬂoating-gate synapse,” in Circuits and Systems (ISCAS), 2015 IEEE International Symposium on. IEEE, 2015, pp. 710–713. [279] ——, “On the non-stdp behavior and its remedy in a ﬂoating-gate synapse,” Neural Networks and Learning Systems, IEEE Transactions on, vol. 26, no. 10, pp. 2596–2601, 2015. [280] Y. Hayashi, K. Saeki, and Y. Sekine, “A synaptic circuit of a pulse- type hardware neuron model with stdp,” in International Congress Series, vol. 1301. Elsevier, 2007, pp. 132–135. [281] T. Hindo, “Weight updating ﬂoating-gate synapse,” Electronics Let- ters, vol. 50, no. 17, pp. 1190–1191, 2014. [282] G. Indiveri, “Circuits for bistable spike-timing-dependent plasticity neuromorphic vlsi synapses,” Advances in Neural Information Pro- cessing Systems, vol. 15, 2002. [283] Y. Irizarry-Valle, A. C. Parker, and N. M. Grzywacz, “An adaptable cmos depressing synapse with detection of changes in input spike rate,” in Circuits and Systems (LASCAS), 2014 IEEE 5th Latin American Symposium on. IEEE, 2014, pp. 1–4. [284] V. Kornijcuk, O. Kavehei, H. Lim, J. Y. Seok, S. K. Kim, I. Kim, W.- S. Lee, B. J. Choi, and D. S. Jeong, “Multiprotocol-induced plasticity in artiﬁcial synapses,” Nanoscale, vol. 6, no. 24, pp. 15 151–15 160, 2014. [285] S.-C. Liu and R. Mockel, “Temporally learning ﬂoating-gate vlsi synapses,” in Circuits and Systems, 2008. ISCAS 2008. IEEE Inter- national Symposium on. IEEE, 2008, pp. 2154–2157. [286] C. Mayr, M. Noack, J. Partzsch, and R. Schuffny, “Replicating exper- imental spike and rate based neural learning in cmos,” in Circuits and Systems (ISCAS), Proceedings of 2010 IEEE International Symposium on. IEEE, 2010, pp. 105–108. [287] C. Mayr, J. Partzsch, M. Noack, and R. Sch¨uffny, “Live demon- stration: Multiple-timescale plasticity in a neuromorphic system.” in ISCAS, 2013, pp. 666–670. [288] J. Meador, D. Watola, and N. Nintunze, “Vlsi implementation of a pulse hebbian learning law,” in Circuits and Systems, 1991., IEEE International Sympoisum on. IEEE, 1991, pp. 1287–1290. [289] S. Mitra, G. Indiveri, and R. Etienne-Cummings, “Synthesis of log-domain integrators for silicon synapses with global parametric control,” in Circuits and Systems (ISCAS), Proceedings of 2010 IEEE International Symposium on. IEEE, 2010, pp. 97–100. [290] G. Narasimman, S. Roy, X. Fong, K. Roy, C.-H. Chang, and A. Basu, “A low-voltage, low power stdp synapse implementation using domain-wall magnets for spiking neural networks,” in Circuits and Systems (ISCAS), 2016 IEEE International Symposium on. IEEE, 2016, pp. 914–917. [291] M. Noack, J. Partzsch, C. Mayr, S. Henker, and R. Schuffny, “Biology- derived synaptic dynamics and optimized system architecture for neuromorphic hardware,” in Mixed Design of Integrated Circuits and Systems (MIXDES), 2010 Proceedings of the 17th International Conference. IEEE, 2010, pp. 219–224. [292] G. Rachmuth, H. Z. Shouval, M. F. Bear, and C.-S. Poon, “A biophysically-based neuromorphic model of spike rate-and timing- dependent plasticity,” Proceedings of the National Academy of Sci- ences, vol. 108, no. 49, pp. E1266–E1274, 2011. [293] H. Ramachandran, S. Weber, S. A. Aamir, and E. Chicca, “Neuro- morphic circuits for short-term plasticity with recovery control,” in Circuits and Systems (ISCAS), 2014 IEEE International Symposium on. IEEE, 2014, pp. 858–861. [294] K. Saeki, R. Shimizu, and Y. Sekine, “Pulse-type hardware neural network with two time windows in stdp,” in Advances in Neuro- Information Processing. Springer, 2009, pp. 877–884. [295] A. Shahim-Aeen and G. Karimi, “Triplet-based spike timing depen- dent plasticity (tstdp) modeling using vhdl-ams,” Neurocomputing, vol. 149, pp. 1440–1444, 2015. [296] A. W. Smith, L. McDaid, and S. Hall, “A compact spike-timing- dependent-plasticity circuit for ﬂoating gate weight implementation.” Neurocomputing, vol. 124, pp. 210–217, 2014. [297] G. Srinivasan, A. Sengupta, and K. Roy, “Magnetic tunnel junction based long-term short-term stochastic synapse for a spiking neural network with on-chip stdp learning,” Scientiﬁc Reports, vol. 6, 2016. [298] D. Sumislawska, N. Qiao, M. Pfeiffer, and G. Indiveri, “Wide dynamic range weights and biologically realistic synaptic dynamics for spike- based learning circuits,” in Circuits and Systems (ISCAS), 2016 IEEE International Symposium on. IEEE, 2016, pp. 2491–2494. [299] M. Suri, O. Bichler, D. Querlioz, G. Palma, E. Vianello, D. Vuillaume, C. Gamrat, and B. DeSalvo, “Cbram devices as binary synapses for low-power stochastic neuromorphic systems: Auditory (cochlea) and visual (retina) cognitive processing applications,” in Electron Devices Meeting (IEDM), 2012 IEEE International. IEEE, 2012, pp. 10–3. [300] H. Wu, Z. Xu, S. Hu, Q. Yu, and Y. Liu, “Circuit implementation of spike time dependent plasticity (stdp) for artiﬁcial synapse,” in Electron Devices and Solid State Circuit (EDSSC), 2012 IEEE International Conference on. IEEE, 2012, pp. 1–2. [301] Y. Chen, S. Hall, L. McDaid, O. Buiu, and P. Kelly, “A silicon synapse based on a charge transfer device for spiking neural network application,” in Advances in Neural Networks-ISNN 2006. Springer, 2006, pp. 1366–1373. [302] ——, “On the design of a low power compact spiking neuron cell based on charge-coupled synapses,” in Neural Networks, 2006. 30 IJCNN’06. International Joint Conference on. IEEE, 2006, pp. 1511– 1517. [303] Y. Chen, L. McDaid, S. Hall, and P. Kelly, “A programmable facili- tating synapse device,” in Neural Networks, 2008. IJCNN 2008.(IEEE World Congress on Computational Intelligence). IEEE International Joint Conference on. IEEE, 2008, pp. 1615–1620. [304] A. Ghani, L. J. McDaid, A. Belatreche, P. Kelly, S. Hall, T. Dowrick, S. Huang, J. Marsland, and A. Smith, “Evaluating the training dynamics of a cmos based synapse,” in Neural Networks (IJCNN), The 2011 International Joint Conference on. IEEE, 2011, pp. 1162– 1168. [305] A. Ghani, L. McDaid, A. Belatreche, S. Hall, S. Huang, J. Marsland, T. Dowrick, and A. Smith, “Evaluating the generalisation capability of a cmos based synapse,” Neurocomputing, vol. 83, pp. 188–197, 2012. [306] C. Bartolozzi and G. Indiveri, “Silicon synaptic homeostasis,” Brain Inspired Cognitive Systems 2006, pp. 1–4, 2006. [307] S.-C. Liu and B. A. Minch, “Silicon synaptic adaptation mechanisms for homeostasis and contrast gain control,” Neural Networks, IEEE Transactions on, vol. 13, no. 6, pp. 1497–1503, 2002. [308] U. C¸ ilingiro˘glu, “Capacitive synapses for microelectronic neural net- works,” in Circuits and Systems, 1990., IEEE International Sympo- sium on. IEEE, 1990, pp. 2982–2985. [309] H. Chible, “Analog circuit for synapse neural networks vlsi imple- mentation,” in Electronics, Circuits and Systems, 2000. ICECS 2000. The 7th IEEE International Conference on, vol. 2. IEEE, 2000, pp. 1004–1007. [310] C. Diorio, P. Hasler, B. A. Minch, and C. A. Mead, “A single-transistor silicon synapse,” Electron Devices, IEEE Transactions on, vol. 43, no. 11, pp. 1972–1980, 1996. [311] E. J. Fuller, F. E. Gabaly, F. L´eonard, S. Agarwal, S. J. Plimpton, R. B. Jacobs-Gedrim, C. D. James, M. J. Marinella, and A. A. Talin, “Li- ion synaptic transistor for low power analog computing,” Advanced Materials, 2016. [312] P. Hasler, C. Diorio, B. A. Minch, and C. Mead, “Single transistor learning synapses,” Advances in neural information processing sys- tems, pp. 817–826, 1995. [313] S. Kim, Y.-C. Shin, N. C. Bogineni, and R. Sridhar, “A programmable analog cmos synapse for neural networks,” Analog Integrated Circuits and Signal Processing, vol. 2, no. 4, pp. 345–352, 1992. [314] T. McGinnity, B. Roche, L. Maguire, and L. McDaid, “Novel archi- tecture and synapse design for hardware implementations of neural networks,” Computers & electrical engineering, vol. 24, no. 1, pp. 75–87, 1998. [315] S. Yu, “Orientation classiﬁcation by a winner-take-all network with oxide rram based synaptic devices,” in Circuits and Systems (ISCAS), 2014 IEEE International Symposium on. IEEE, 2014, pp. 1058–1061. [316] E. Vianello, D. Garbin, O. Bichler, G. Piccolboni, G. Molas, B. De Salvo, and L. Perniola, “Multiple binary oxrams as synapses for convolutional neural networks,” in Advances in Neuromorphic Hardware Exploiting Emerging Nanoscale Devices. Springer, 2017, pp. 109–127. [317] H. Card and W. Moore, “Implementation of plasticity in mos synapses,” in Artiﬁcial Neural Networks, 1989., First IEE Interna- tional Conference on (Conf. Publ. No. 313). IET, 1989, pp. 33–36. [318] H. Card, C. Schneider, and W. Moore, “Hebbian plasticity in mos synapses,” in IEE Proceedings F (Radar and Signal Processing), vol. 138, no. 1. IET, 1991, pp. 13–16. [319] V. Srinivasan, J. Dugger, and P. Hasler, “An adaptive analog synapse circuit that implements the least-mean-square learning rule,” in Cir- cuits and Systems, 2005. ISCAS 2005. IEEE International Symposium on. IEEE, 2005, pp. 4441–4444. [320] J. Choi, B. J. Sheu, and J.-F. Chang, “A gaussian synapse circuit for analog vlsi neural networks,” Very Large Scale Integration (VLSI) Systems, IEEE Transactions on, vol. 2, no. 1, pp. 129–133, 1994. [321] K. Lau and S. Lee, “A programmable cmos gaussian synapse for analogue vlsi neural networks,” International journal of electronics, vol. 83, no. 1, pp. 91–98, 1997. [322] S. Lee and K. Lau, “An analog gaussian synapse for artiﬁcial neural networks,” in Circuits and Systems, 1995., Proceedings., Proceedings of the 38th Midwest Symposium on, vol. 1. IEEE, 1995, pp. 77–80. [323] M. Annunziato, D. Badoni, S. Fusi, and A. Salamon, “Analog vlsi implementation of a spike driven stochastic dynamical synapse,” in ICANN 98. Springer, 1998, pp. 475–480. [324] C. Bartolozzi and G. Indiveri, “A neuromorphic selective attention architecture with dynamic synapses and integrate-and-ﬁre neurons,” Proceedings of Brain Inspired Cognitive Systems (BICS 2004), pp. 1–6, 2004. [325] C. Bartolozzi, O. Nikolayeva, and G. Indiveri, “Implementing home- ostatic plasticity in vlsi networks of spiking neurons,” in Electronics, Circuits and Systems, 2008. ICECS 2008. 15th IEEE International Conference on. IEEE, 2008, pp. 682–685. [326] J. Binas, G. Indiveri, and M. Pfeiffer, “Spiking analog vlsi neuron assemblies as constraint satisfaction problem solvers,” in Circuits and Systems (ISCAS), 2016 IEEE International Symposium on. IEEE, 2016, pp. 2094–2097. [327] L. Carota, “Dynamics of vlsi analog decoupled neurons,” Neurocom- puting, vol. 82, pp. 234–237, 2012. [328] E. Chicca and S. Fusi, “Stochastic synaptic plasticity in deterministic avlsi networks of spiking neurons,” ser. Proceedings of the World Congress on Neuroinformatics, F. Rattay, Ed. ARGESIM/ASIM Verlag, 2001, pp. 468–477. [Online]. Available: http://citeseerx.ist. psu.edu/viewdoc/summary?doi=10.1.1.12.2064 [329] E. Chicca, G. Indiveri, and R. Douglas, “An adaptive silicon synapse,” vol. 1, 2003, pp. I–81–I–84. [Online]. Available: http://ieeexplore.ieee.org/xpls/abs all.jsp?arnumber=1205505 [330] C.-H. Chien, S.-C. Liu, and A. Steimer, “A neuromorphic vlsi circuit for spike-based random sampling,” IEEE Transactions on Emerging Topics in Computing, 2015. [331] J. Fieres, J. Schemmel, and K. Meier, “Realizing biological spiking network models in a conﬁgurable wafer-scale hardware system,” in Neural Networks, 2008. IJCNN 2008.(IEEE World Congress on Computational Intelligence). IEEE International Joint Conference on. IEEE, 2008, pp. 969–976. [332] S. Fusi and M. Mattia, “Collective behavior of networks with linear (vlsi) integrate-and-ﬁre neurons,” Neural Computation, vol. 11, no. 3, pp. 633–652, 1999. [333] V. S. Ghaderi, D. Song, J. Choma, and T. W. Berger, “Nonlinear cognitive signal processing in ultralow-power programmable analog hardware,” Circuits and Systems II: Express Briefs, IEEE Transactions on, vol. 62, no. 2, pp. 124–128, 2015. [334] M. Giulioni, P. Camilleri, M. Mattia, V. Dante, J. Braun, and P. Del Giudice, “Robust working memory in an asynchronously spiking neural network realized with neuromorphic vlsi,” Frontiers in neuroscience, vol. 5, 2011. [335] M. A. Glover, A. Hamilton, and L. S. Smith, “An analog vlsi integrate- and-ﬁre neural network for sound segmentation.” in NC, 1998, pp. 86–92. [336] M. Glover, A. Hamilton, and L. S. Smith, “Using analogue vlsi leaky integrate-and-ﬁre neurons in a sound analysis system,” in Microelectronics for Neural, Fuzzy and Bio-Inspired Systems, 1999. MicroNeuro’99. Proceedings of the Seventh International Conference on. IEEE, 1999, pp. 90–95. [337] ——, “Analogue vlsi leaky integrate-and-ﬁre neurons and their use in a sound analysis system,” Analog Integrated Circuits and Signal Processing, vol. 30, no. 2, pp. 91–100, 2002. [338] D. H. Goldberg, G. Cauwenberghs, and A. G. Andreou, “Analog vlsi spiking neural network with address domain probabilistic synapses,” in Circuits and Systems, 2001. ISCAS 2001. The 2001 IEEE Interna- tional Symposium on, vol. 3. IEEE, 2001, pp. 241–244. [339] ——, “Probabilistic synaptic weighting in a reconﬁgurable network of vlsi integrate-and-ﬁre neurons,” Neural Networks, vol. 14, no. 6, pp. 781–793, 2001. [340] F. Grassia, T. L´evi, J. Tomas, S. Renaud, and S. Sa¨ıghi, “A neu- romimetic spiking neural network for simulating cortical circuits,” in Information Sciences and Systems (CISS), 2011 45th Annual Conference on. IEEE, 2011, pp. 1–6. [341] P. H¨aﬂiger, M. Mahowald, and L. Watts, “A spike based learning neuron in analog vlsi,” in Advances in neural information processing systems, 1997, pp. 692–698. [342] D. Hajt´aˇs, D. ˇDuraˇckov´a, and G. Benyon-Tinker, “Switched capacitor- based integrate-and-ﬁre neural network,” in The State of the Art in Computational Intelligence. Springer, 2000, pp. 50–55. [343] J. Huo and A. Murray, “The role of membrane threshold and rate in stdp silicon neuron circuit simulation,” in Artiﬁcial Neural Networks: Formal Models and Their Applications–ICANN 2005. Springer, 2005, pp. 1009–1014. [344] G. Indiveri, “Modeling selective attention using a neuromorphic analog vlsi device,” Neural computation, vol. 12, no. 12, pp. 2857– 2880, 2000. [345] ——, “A 2d neuromorphic vlsi architecture for modeling selective attention,” in Neural Networks, 2000. IJCNN 2000, Proceedings of the 31 IEEE-INNS-ENNS International Joint Conference on, vol. 4. IEEE, 2000, pp. 208–213. [346] ——, “A neuromorphic vlsi device for implementing 2d selective attention systems,” Neural Networks, IEEE Transactions on, vol. 12, no. 6, pp. 1455–1463, 2001. [347] ——, “Neuromorphic bistable vlsi synapses with spike-timing- dependent plasticity,” in NIPS, 2002, pp. 1091–1098. [348] ——, “Synaptic plasticity and spike-based computation in vlsi net- works of integrate-and-ﬁre neurons,” Neural Information Processing- Letters and Reviews, vol. 11, no. 4-61, pp. 135–146, 2007. [349] A. Joubert, B. Belhadj, and R. H´eliot, “A robust and compact 65 nm lif analog neuron for computational purposes,” in New Circuits and Systems Conference (NEWCAS), 2011 IEEE 9th International. IEEE, 2011, pp. 9–12. [350] A. Joubert, B. Belhadj, O. Temam, and R. Heliot, “Hardware spiking neurons design: Analog or digital?” in Neural Networks (IJCNN), The 2012 International Joint Conference on. IEEE, 2012, pp. 1–5. [351] Y. Kanazawa, T. Asai, M. Ikebe, and Y. Amemiya, “A novel cmos circuit for depressing synapse and its application to contrast-invariant pattern classiﬁcation and synchrony detection,” International Journal of Robotics and Automation, vol. 19, no. 4, pp. 206–212, 2004. [352] T. J. Koickal, L. C. Gouveia, and A. Hamilton, “A programmable time event coded circuit block for reconﬁgurable neuromorphic com- puting,” in Computational and Ambient Intelligence. Springer, 2007, pp. 430–437. [353] ——, “A programmable spike-timing based circuit block for recon- ﬁgurable neuromorphic computing,” Neurocomputing, vol. 72, no. 16, pp. 3609–3616, 2009. [354] J. Lazzaro, “Low-power silicon spiking neurons and axons,” in Circuits and Systems, 1992. ISCAS’92. Proceedings., 1992 IEEE International Symposium on, vol. 5. IEEE, 1992, pp. 2220–2223. [355] P. H. M. Mahowald and L. Watts, “A spike based learning neuron in analog vlsi,” Advances in Neural Information Processing Systems, vol. 9, p. 692, 1997. [356] R. Mill, S. Sheik, G. Indiveri, and S. L. Denham, “A model of stimulus-speciﬁc adaptation in neuromorphic analog vlsi,” Biomedical Circuits and Systems, IEEE Transactions on, vol. 5, no. 5, pp. 413– 419, 2011. [357] S. Mitra, S. Fusi, and G. Indiveri, “A vlsi spike-driven dynamic synapse which learns only when necessary,” in Circuits and Systems, 2006. ISCAS 2006. Proceedings. 2006 IEEE International Symposium on. IEEE, 2006, pp. 4–pp. [358] S. Nease, S. Brink, and P. Hasler, “Stdp-enabled learning on a reconﬁgurable neuromorphic platform,” in Circuit Theory and Design (ECCTD), 2013 European Conference on. IEEE, 2013, pp. 1–4. [359] E. Neftci, J. Binas, U. Rutishauser, E. Chicca, G. Indiveri, and R. J. Douglas, “Synthesizing cognition in neuromorphic electronic systems,” Proceedings of the National Academy of Sciences, vol. 110, no. 37, pp. E3468–E3476, 2013. [360] M. Oster, A. M. Whatley, S.-C. Liu, and R. J. Douglas, “A hard- ware/software framework for real-time spiking systems,” in Artiﬁcial Neural Networks: Biological Inspirations–ICANN 2005. Springer, 2005, pp. 161–166. [361] F. Perez-Pe˜na, A. Linares-Barranco, and E. Chicca, “An approach to motor control for spike-based neuromorphic robotics,” in Biomedical Circuits and Systems Conference (BioCAS), 2014 IEEE. IEEE, 2014, pp. 528–531. [362] T. Pfeil, A.-C. Scherzer, J. Schemmel, and K. Meier, “Neuromor- phic learning towards nano second precision,” in Neural Networks (IJCNN), The 2013 International Joint Conference on. IEEE, 2013, pp. 1–5. [363] T. Pfeil, J. Jordan, T. Tetzlaff, A. Gr¨ubl, J. Schemmel, M. Diesmann, and K. Meier, “Effect of heterogeneity on decorrelation mechanisms in spiking neural networks: A neuromorphic-hardware study,” Physi- cal Review X, vol. 6, no. 2, p. 021023, 2016. [364] D. Querlioz and V. Trauchessec, “Stochastic resonance in an analog current-mode neuromorphic circuit,” in Circuits and Systems (ISCAS), 2013 IEEE International Symposium on. IEEE, 2013, pp. 1596–1599. [365] S. Renaud, J. Tomas, Y. Bornat, A. Daouzli, and S. Sa¨ıghi, “Neu- romimetic ics with analog cores: an alternative for simulating spiking neural networks,” in Circuits and Systems, 2007. ISCAS 2007. IEEE International Symposium on. IEEE, 2007, pp. 3355–3358. [366] H. K. Riis and P. Haﬂiger, “Spike based learning with weak multi- level static memory,” in Circuits and Systems, 2004. ISCAS’04. Proceedings of the 2004 International Symposium on, vol. 5. IEEE, 2004, pp. V–393. [367] J. Rodrigues de Oliveira-Neto, F. Duque Belfort, R. Cavalcanti-Neto, and J. Ranhel, “Magnitude comparison in analog spiking neural assemblies,” in Neural Networks (IJCNN), 2014 International Joint Conference on. IEEE, 2014, pp. 3186–3191. [368] S. Sa¨ıghi, T. Levi, B. Belhadj, O. Malot, and J. Tomas, “Hardware system for biologically realistic, plastic, and real-time spiking neural network simulations,” in Neural Networks (IJCNN), The 2010 Inter- national Joint Conference on. IEEE, 2010, pp. 1–7. [369] J. Schemmel, K. Meier, and E. Mueller, “A new vlsi model of neural microcircuits including spike time dependent plasticity,” in Neural Networks, 2004. Proceedings. 2004 IEEE International Joint Conference on, vol. 3. IEEE, 2004, pp. 1711–1716. [370] J. Schemmel, D. Bruderle, K. Meier, and B. Ostendorf, “Modeling synaptic plasticity within networks of highly accelerated i&f neurons,” in Circuits and Systems, 2007. ISCAS 2007. IEEE International Symposium on. IEEE, 2007, pp. 3367–3370. [371] J. Schemmel, J. Fieres, and K. Meier, “Wafer-scale integration of ana- log neural networks,” in Neural Networks, 2008. IJCNN 2008.(IEEE World Congress on Computational Intelligence). IEEE International Joint Conference on. IEEE, 2008, pp. 431–438. [372] J. Schemmel, A. Grubl, S. Hartmann, A. Kononov, C. Mayr, K. Meier, S. Millner, J. Partzsch, S. Schiefer, S. Scholze et al., “Live demon- stration: A scaled-down version of the brainscales wafer-scale neu- romorphic system,” in Circuits and Systems (ISCAS), 2012 IEEE International Symposium on. IEEE, 2012, pp. 702–702. [373] J. Schreiter, U. Ramacher, A. Heittmann, D. Matolin, and R. Sch¨uffny, “Analog implementation for networks of integrate-and-ﬁre neurons with adaptive local connectivity,” in Proceedings of the 2002 12th IEEE Workshop on Neural Networks for Signal Processing. Citeseer, 2002, pp. 657–666. [374] L. S. Smith and A. Hamilton, Neuromorphic systems: engineering silicon from neurobiology. World Scientiﬁc, 1998, vol. 10. [375] H. Tanaka, T. Morie, and K. Aihara, “A cmos spiking neural network circuit with symmetric/asymmetric stdp function,” IEICE transactions on fundamentals of electronics, communications and computer sci- ences, vol. 92, no. 7, pp. 1690–1698, 2009. [376] G. M. Tovar, T. Hirose, T. Asai, and Y. Amemiya, “Precisely-timed synchronization among spiking neural circuits on analog vlsis,” in Proc. the 2006 RISP International Workshop on Nonlinear Circuits amd Signal Processing. Citeseer, 2006, pp. 62–65. [377] G. M. Tovar, E. S. Fukuda, T. Asai, T. Hirose, and Y. Amemiya, “Analog cmos circuits implementing neural segmentation model based on symmetric stdp learning,” in Neural Information Processing. Springer, 2008, pp. 117–126. [378] A. Utagawa, T. HIROSE, and Y. AMEMIYA, “An inhibitory neural- network circuit exhibiting noise shaping with subthreshold mos neu- ron circuits,” IEICE Transactions on Fundamentals of Electronics, Communications and Computer Sciences, vol. 90, no. 10, pp. 2108– 2115, 2007. [379] A. van Schaik, “Building blocks for electronic spiking neural net- works,” Neural networks, vol. 14, no. 6, pp. 617–628, 2001. [380] R. J. Vogelstein, F. Tenore, R. Philipp, M. S. Adlerstein, D. H. Goldberg, and G. Cauwenberghs, “Spike timing-dependent plasticity in the address domain,” in Advances in Neural Information Processing Systems, 2002, pp. 1147–1154. [381] Y. Wang and S.-C. Liu, “Programmable synaptic weights for an avlsi network of spiking neurons,” in Circuits and Systems, 2006. ISCAS 2006. Proceedings. 2006 IEEE International Symposium on. IEEE, 2006, pp. 4–pp. [382] ——, “Input evoked nonlinearities in silicon dendritic circuits,” in Cir- cuits and Systems, 2009. ISCAS 2009. IEEE International Symposium on. IEEE, 2009, pp. 2894–2897. [383] ——, “Motion detection using an avlsi network of spiking neurons.” in ISCAS, 2010, pp. 93–96. [384] J. H. Wijekoon and P. Dudek, “Heterogeneous neurons and plastic synapses in a reconﬁgurable cortical neural network ic,” in Circuits and Systems (ISCAS), 2012 IEEE International Symposium on. IEEE, 2012, pp. 2417–2420. [385] Z. Yang and A. F. Murray, “A biologically plausible neuromorphic system for object recognition and depth analysis.” in ESANN, 2004, pp. 157–162. [386] L. Zhang, Q. Lai, and Y. Chen, “Conﬁgurable neural phase shifter with spike-timing-dependent plasticity,” Electron Device Letters, IEEE, vol. 31, no. 7, pp. 716–718, 2010. [387] C. Zhao, J. Li, L. Liu, L. S. Koutha, J. Liu, and Y. Yi, “Novel spike based reservoir node design with high performance spike delay 32 loop,” in Proceedings of the 3rd ACM International Conference on Nanoscale Computing and Communication. ACM, 2016, p. 14. [388] K. Ahmed, A. Shrestha, Y. Wang, and Q. Qiu, “System design for in-hardware stdp learning and spiking based probablistic inference,” in VLSI (ISVLSI), 2016 IEEE Computer Society Annual Symposium on. IEEE, 2016, pp. 272–277. [389] M. Ambroise, T. Levi, Y. Bornat, and S. Saighi, “Biorealistic spiking neural network on fpga,” in Information Sciences and Systems (CISS), 2013 47th Annual Conference on. IEEE, 2013, pp. 1–6. [390] A. Banerjee, S. Kar, S. Roy, A. Bhaduri, and A. Basu, “A current- mode spiking neural classiﬁer with lumped dendritic nonlinearity,” in Circuits and Systems (ISCAS), 2015 IEEE International Symposium on. IEEE, 2015, pp. 714–717. [391] M. E. Dean and C. Daffron, “A vlsi design for neuromorphic computing,” in VLSI (ISVLSI), 2016 IEEE Computer Society Annual Symposium on. IEEE, 2016, pp. 87–92. [392] J. Georgiou, A. G. Andreou, and P. O. Pouliquen, “A mixed ana- log/digital asynchronous processor for cortical computations in 3d soi-cmos,” in Circuits and Systems, 2006. ISCAS 2006. Proceedings. 2006 IEEE International Symposium on. IEEE, 2006, pp. 4–pp. [393] D. Hu, X. Zhang, Z. Xu, S. Ferrari, and P. Mazumder, “Digi- tal implementation of a spiking neural network (snn) capable of spike-timing-dependent plasticity (stdp) learning,” in Nanotechnology (IEEE-NANO), 2014 IEEE 14th International Conference on. IEEE, 2014, pp. 873–876. [394] N. Imam, K. Wecker, J. Tse, R. Karmazin, and R. Manohar, “Neural spiking dynamics in asynchronous digital circuits,” in Neural Net- works (IJCNN), The 2013 International Joint Conference on. IEEE, 2013, pp. 1–8. [395] J. K. Kim, P. Knag, T. Chen, and Z. Zhang, “A 640m pixel/s 3.65 mw sparse event-driven neuromorphic object recognition processor with on-chip learning,” in VLSI Circuits (VLSI Circuits), 2015 Symposium on. IEEE, 2015, pp. C50–C51. [396] S. R. Kulkarni and B. Rajendran, “Scalable digital cmos architecture for spike based supervised learning,” in Engineering Applications of Neural Networks. Springer, 2015, pp. 149–158. [397] A. Nere, U. Olcese, D. Balduzzi, and G. Tononi, “A neuromorphic architecture for object recognition and motion anticipation using burst- stdp,” PloS one, vol. 7, no. 5, p. e36958, 2012. [398] A. Nere, A. Hashmi, M. Lipasti, and G. Tononi, “Bridging the seman- tic gap: Emulating biological neuronal behaviors with simple digital neurons,” in High Performance Computer Architecture (HPCA2013), 2013 IEEE 19th International Symposium on. IEEE, 2013, pp. 472– 483. [399] D. Roclin, O. Bichler, C. Gamrat, S. J. Thorpe, and J.-O. Klein, “De- sign study of efﬁcient digital order-based stdp neuron implementations for extracting temporal features,” in Neural Networks (IJCNN), The 2013 International Joint Conference on. IEEE, 2013, pp. 1–7. [400] T. Schoenauer, S. Atasoy, N. Mehrtash, and H. Klar, “Simulation of a digital neuro-chip for spiking neural networks,” in Neural Networks, 2000. IJCNN 2000, Proceedings of the IEEE-INNS-ENNS International Joint Conference on, vol. 4. IEEE, 2000, pp. 490–495. [401] ——, “Neuropipe-chip: A digital neuro-processor for spiking neural networks,” Neural Networks, IEEE Transactions on, vol. 13, no. 1, pp. 205–213, 2002. [402] J.-s. Seo and M. Seok, “Digital cmos neuromorphic processor design featuring unsupervised online learning,” in Very Large Scale Inte- gration (VLSI-SoC), 2015 IFIP/IEEE International Conference on. IEEE, 2015, pp. 49–51. [403] J. Shen, D. Ma, Z. Gu, M. Zhang, X. Zhu, X. Xu, Q. Xu, Y. Shen, and G. Pan, “Darwin: a neuromorphic hardware co-processor based on spiking neural networks,” Science China Information Sciences, pp. 1–5, 2016. [404] B. Zhang, Z. Jiang, Q. Wang, J.-s. Seo, and M. Seok, “A neuromorphic neural spike clustering processor for deep-brain sensing and stimula- tion systems,” in Low Power Electronics and Design (ISLPED), 2015 IEEE/ACM International Symposium on. IEEE, 2015, pp. 91–97. [405] B. Abinaya and S. Sophia, “An event based cmos quad bilateral combination with asynchronous sram architecture based neural net- work using low power,” in Electronics and Communication Systems (ICECS), 2015 2nd International Conference on. IEEE, 2015, pp. 995–999. [406] A. Aﬁﬁ, A. Ayatollahi, and F. Raissi, “Cmol implementation of spiking neurons and spike-timing dependent plasticity,” International Journal of Circuit Theory and Applications, vol. 39, no. 4, pp. 357– 372, 2011. [407] J. Arthur and K. Boahen, “Learning in silicon: timing is everything,” Advances in neural information processing systems, vol. 18, p. 75, 2006. [408] J. V. Arthur and K. A. Boahen, “Synchrony in silicon: The gamma rhythm,” Neural Networks, IEEE Transactions on, vol. 18, no. 6, pp. 1815–1825, 2007. [409] T. Asai, “A neuromorphic cmos family and its application,” in International Congress Series, vol. 1269. Elsevier, 2004, pp. 173– 176. [410] M. R. Azghadi, S. Moradi, and G. Indiveri, “Programmable neuro- morphic circuits for spike-based neural dynamics,” in New Circuits and Systems Conference (NEWCAS), 2013 IEEE 11th International. IEEE, 2013, pp. 1–4. [411] M. R. Azghadi, S. Moradi, D. B. Fasnacht, M. S. Ozdas, and G. Indiveri, “Programmable spike-timing-dependent plasticity learn- ing circuits in neuromorphic vlsi architectures,” ACM Journal on Emerging Technologies in Computing Systems (JETC), vol. 12, no. 2, p. 17, 2015. [412] F. Boi, T. Moraitis, V. De Feo, F. Diotalevi, C. Bartolozzi, G. Indiveri, and A. Vato, “A bidirectional brain-machine interface featuring a neuromorphic hardware decoder,” Frontiers in Neuroscience, vol. 10, 2016. [413] D. Bruderle, J. Bill, B. Kaplan, J. Kremkow, K. Meier, E. Muller, and J. Schemmel, “Simulator-like exploration of cortical network architectures with a mixed-signal vlsi system,” in Circuits and Systems (ISCAS), Proceedings of 2010 IEEE International Symposium on. IEEE, 2010, pp. 2784–8787. [414] D. Br¨uderle, M. A. Petrovici, B. Vogginger, M. Ehrlich, T. Pfeil, S. Millner, A. Gr¨ubl, K. Wendt, E. M¨uller, M.-O. Schwartz et al., “A comprehensive workﬂow for general-purpose neural modeling with highly conﬁgurable neuromorphic hardware systems,” Biological cybernetics, vol. 104, no. 4-5, pp. 263–296, 2011. [415] E. Chicca, G. Indiveri, and R. J. Douglas, “An event-based vlsi network of integrate-and-ﬁre neurons,” in Circuits and Systems, 2004. ISCAS’04. Proceedings of the 2004 International Symposium on, vol. 5. IEEE, 2004, pp. V–357. [416] E. Chicca, P. Lichtsteiner, T. Delbruck, G. Indiveri, and R. J. Douglas, “Modeling orientation selectivity using a neuromorphic multi-chip system,” in Circuits and Systems, 2006. ISCAS 2006. Proceedings. 2006 IEEE International Symposium on. IEEE, 2006, pp. 4–pp. [417] E. Chicca, G. Indiveri, and R. J. Douglas, “Context dependent ampliﬁcation of both rate and event-correlation in a vlsi network of spiking neurons,” in Advances in Neural Information Processing Systems 19: Proceedings of the 2006 Conference, vol. 19. Mit Press, 2007, p. 257. [418] S. Choudhary, S. Sloan, S. Fok, A. Neckar, E. Trautmann, P. Gao, T. Stewart, C. Eliasmith, and K. Boahen, “Silicon neurons that compute,” in Artiﬁcial neural networks and machine learning–ICANN 2012. Springer, 2012, pp. 121–128. [419] D. Corneil, D. Sonnleithner, E. Neftci, E. Chicca, M. Cook, G. In- diveri, and R. Douglas, “Real-time inference in a vlsi spiking neural network,” in Circuits and Systems (ISCAS), 2012 IEEE International Symposium on. IEEE, 2012, pp. 2425–2428. [420] ——, “Function approximation with uncertainty propagation in a vlsi spiking neural network,” in Neural Networks (IJCNN), The 2012 International Joint Conference on. IEEE, 2012, pp. 1–7. [421] F. Corradi, C. Eliasmith, and G. Indiveri, “Mapping arbitrary mathe- matical functions and dynamical systems to neuromorphic vlsi circuits for spike-based neural computation,” in Circuits and Systems (ISCAS), 2014 IEEE International Symposium on. IEEE, 2014, pp. 269–272. [422] F. Corradi, H. You, M. Giulioni, and G. Indiveri, “Decision making and perceptual bistability in spike-based neuromorphic vlsi systems,” in Circuits and Systems (ISCAS), 2015 IEEE International Symposium on. IEEE, 2015, pp. 2708–2711. [423] J. Cruz-Albrecht, M. Yung, and N. Srinivasa, “Energy-efﬁcient neu- ron, synapse and stdp integrated circuits,” Biomedical Circuits and Systems, IEEE Transactions on, vol. 6, no. 3, pp. 246–256, 2012. [424] F. Folowosele, F. Tenore, A. Russell, G. Orchard, M. Vismer, J. Tap- son, and R. E. Cummings, “Implementing a neuromorphic cross- correlation engine with silicon neurons,” in Circuits and Systems, 2008. ISCAS 2008. IEEE International Symposium on. IEEE, 2008, pp. 2162–2165. [425] S. Friedmann, N. Fr´emaux, J. Schemmel, W. Gerstner, and K. Meier, “Reward-based learning under hardware constraints?using a risc pro- cessor embedded in a neuromorphic substrate,” Frontiers in neuro- science, vol. 7, 2013. 33 [426] C. Gao and D. Hammerstrom, “Cortical models onto cmol and cmos?architectures and performance/price,” Circuits and Systems I: Regular Papers, IEEE Transactions on, vol. 54, no. 11, pp. 2502– 2515, 2007. [427] C. Gao, M. S. Zaveri, and D. W. Hammerstrom, “Cmos/cmol archi- tectures for spiking cortical column,” in IJCNN, 2008, pp. 2441–2448. [428] D. Hajtas and D. Durackova, “The library of building blocks for an” integrate & ﬁre” neural network on a chip,” in Neural Networks, 2004. Proceedings. 2004 IEEE International Joint Conference on, vol. 4. IEEE, 2004, pp. 2631–2636. [429] D. Hammerstrom and M. S. Zaveri, “Prospects for building cortex- scale cmol/cmos circuits: a design space exploration,” in NORCHIP, 2009. IEEE, 2009, pp. 1–8. [430] I. S. Han, “Mixed-signal neuron-synapse implementation for large- scale neural network,” Neurocomputing, vol. 69, no. 16, pp. 1860– 1867, 2006. [431] ——, “A pulse-based neural hardware implementation based on the controlled conductance by mosfet circuit,” in Neural Networks, 2006. IJCNN’06. International Joint Conference on. IEEE, 2006, pp. 2793– 2799. [432] J. Huo, “A dynamic excitatory-inhibitory network in a vlsi chip for spiking information reregistration.” in NIPS, 2012. [433] S. Hussain, A. Basu, M. Wang, and T. J. Hamilton, “Deltron: Neuromorphic architectures for delay based learning,” in Circuits and Systems (APCCAS), 2012 IEEE Asia Paciﬁc Conference on. IEEE, 2012, pp. 304–307. [434] G. Indiveri, T. Horiuchi, E. Niebur, and R. Douglas, “A competitive network of spiking vlsi neurons,” in World Congress on Neuroinfor- matics. Vienna, Austria: ARGESIM/ASIM Verlag, 2001, pp. 443– 455. [435] G. Indiveri, E. Chicca, and R. Douglas, “A vlsi reconﬁgurable network of integrate-and-ﬁre neurons with spike-based learning synapses,” 2004, pp. 405–410. [Online]. Available: http://citeseerx. ist.psu.edu/viewdoc/summary?doi=10.1.1.4.7558 [436] ——, “A vlsi array of low-power spiking neurons and bistable synapses with spike-timing dependent plasticity,” Neural Networks, IEEE Transactions on, vol. 17, no. 1, pp. 211–221, 2006. [437] G. Indiveri and E. Chicca, “A vlsi neuromorphic device for imple- menting spike-based neural networks,” 2011. [438] Y. Kim, Y. Zhang, and P. Li, “An energy efﬁcient approximate adder with carry skip for error resilient neuromorphic vlsi systems,” in Proceedings of the International Conference on Computer-Aided Design. IEEE Press, 2013, pp. 130–137. [439] B. Linares-Barranco, T. Serrano-Gotarredona, and R. Serrano- Gotarredona, “Compact low-power calibration mini-dacs for neural arrays with programmable weights,” Neural Networks, IEEE Trans- actions on, vol. 14, no. 5, pp. 1207–1216, 2003. [440] S.-C. Liu and R. Douglas, “Temporal coding in a silicon network of integrate-and-ﬁre neurons,” Neural Networks, IEEE Transactions on, vol. 15, no. 5, pp. 1305–1314, 2004. [441] C. G. Mayr, J. Partzsch, M. Noack, and R. Sch¨uffny, “Conﬁgurable analog-digital conversion using the neural engineering framework,” Frontiers in neuroscience, vol. 8, 2014. [442] C. Mayr, J. Partzsch, M. Noack, S. H¨anzsche, S. Scholze, S. H¨oppner, G. Ellguth, and R. Sch¨uffny, “A biological-realtime neuromorphic system in 28 nm cmos using low-leakage switched capacitor circuits,” IEEE transactions on biomedical circuits and systems, vol. 10, no. 1, pp. 243–254, 2016. [443] L. McDaid, J. Harkin, S. Hall, T. Dowrick, Y. Chen, and J. Marsland, “Embrace: emulating biologically-inspired architectures on hardware,” in NN?08: Proceedings of the 9th WSEAS International Conference on Neural Networks, 2008, pp. 167–172. [444] K. Meier, “A mixed-signal universal neuromorphic computing sys- tem,” in 2015 IEEE International Electron Devices Meeting (IEDM). IEEE, 2015, pp. 4–6. [445] K. Minkovich, N. Srinivasa, J. M. Cruz-Albrecht, Y. Cho, and A. No- gin, “Programming time-multiplexed reconﬁgurable hardware using a scalable neuromorphic compiler,” Neural Networks and Learning Systems, IEEE Transactions on, vol. 23, no. 6, pp. 889–901, 2012. [446] S. Mitra, G. Indiveri, and S. Fusi, “Learning to classify complex patterns using a vlsi network of spiking neurons,” in Advances in Neural Information Processing Systems, 2007, pp. 1009–1016. [447] ——, “Robust classiﬁcation of correlated patterns with a neuromor- phic vlsi network of spiking neurons,” in Biomedical Circuits and Systems Conference, 2007. BIOCAS 2007. IEEE. IEEE, 2007, pp. 87–90. [448] S. Mitra, S. Fusi, and G. Indiveri, “Real-time classiﬁcation of complex patterns using spike-based learning in neuromorphic vlsi,” Biomedical Circuits and Systems, IEEE Transactions on, vol. 3, no. 1, pp. 32–42, 2009. [449] S. Mitra and G. Indiveri, “Spike-based synaptic plasticity and classiﬁ- cation on vlsi,” The Neuromorphic Engineer a publication of ine-web. org, vol. 10, no. 1200904.1636, 2009. [450] S. Moradi and G. Indiveri, “A vlsi network of spiking neurons with an asynchronous static random access memory,” in Biomedical Circuits and Systems Conference (BioCAS), 2011 IEEE. IEEE, 2011, pp. 277–280. [451] ——, “An event-based neural network architecture with an asyn- chronous programmable synaptic memory,” Biomedical Circuits and Systems, IEEE Transactions on, vol. 8, no. 1, pp. 98–107, 2014. [452] H. Mostafa, F. Corradi, F. Stefanini, and G. Indiveri, “A hybrid analog/digital spike-timing dependent plasticity learning circuit for neuromorphic vlsi multi-neuron architectures,” in Circuits and Sys- tems (ISCAS), 2014 IEEE International Symposium on. IEEE, 2014, pp. 854–857. [453] E. Neftci, E. Chicca, G. Indiveri, J.-J. E. Slotine, and R. J. Dou- glas, “Contraction properties of vlsi cooperative competitive neural networks of spiking neurons.” in NIPS, 2007. [454] E. Neftci, E. Chicca, G. Indiveri, and R. Douglas, “A systematic method for conﬁguring vlsi networks of spiking neurons,” Neural computation, vol. 23, no. 10, pp. 2457–2497, 2011. [455] E. Neftci, J. Binas, E. Chicca, G. Indiveri, and R. Douglas, “System- atic construction of ﬁnite state automata using vlsi spiking neurons,” Biomimetic and Biohybrid Systems, pp. 382–383, 2012. [456] M. Noack, J. Partzsch, C. G. Mayr, S. H¨anzsche, S. Scholze, S. H¨oppner, G. Ellguth, and R. Sch¨uffny, “Switched-capacitor realiza- tion of presynaptic short-term-plasticity and stop-learning synapses in 28 nm cmos,” Frontiers in neuroscience, vol. 9, 2015. [457] G. Palma, M. Suri, D. Querlioz, E. Vianello, and B. De Salvo, “Stochastic neuron design using conductive bridge ram,” in Nanoscale Architectures (NANOARCH), 2013 IEEE/ACM International Sympo- sium on. IEEE, 2013, pp. 95–100. [458] M. A. Petrovici, B. Vogginger, P. M¨uller, O. Breitwieser, M. Lundqvist, L. Muller, M. Ehrlich, A. Destexhe, A. Lansner, R. Sch¨uffny et al., “Characterization and compensation of network- level anomalies in mixed-signal neuromorphic modeling platforms,” 2014. [459] T. Pfeil, A. Gr¨ubl, S. Jeltsch, E. M¨uller, P. M¨uller, M. A. Petrovici, M. Schmuker, D. Br¨uderle, J. Schemmel, and K. Meier, “Six net- works on a universal neuromorphic computing substrate,” Frontiers in neuroscience, vol. 7, 2013. [460] N. Qiao, H. Mostafa, F. Corradi, M. Osswald, F. Stefanini, D. Sum- islawska, and G. Indiveri, “A reconﬁgurable on-line learning spiking neuromorphic processor comprising 256 neurons and 128k synapses,” Frontiers in neuroscience, vol. 9, 2015. [461] S. Renaud, J. Tomas, N. Lewis, Y. Bornat, A. Daouzli, M. Rudolph, A. Destexhe, and S. Sa¨ıghi, “Pax: A mixed hardware/software simula- tion platform for spiking neural networks,” Neural Networks, vol. 23, no. 7, pp. 905–916, 2010. [462] J. Schemmel, A. Grubl, K. Meier, and E. Mueller, “Implementing synaptic plasticity in a vlsi spiking neural network model,” in Neural Networks, 2006. IJCNN’06. International Joint Conference on. IEEE, 2006, pp. 1–6. [463] M. Schmuker, T. Pfeil, and M. P. Nawrot, “A neuromorphic network for generic multivariate data classiﬁcation,” Proceedings of the Na- tional Academy of Sciences, vol. 111, no. 6, pp. 2081–2086, 2014. [464] L. Shi, J. Pei, N. Deng, D. Wang, L. Deng, Y. Wang, Y. Zhang, F. Chen, M. Zhao, S. Song et al., “Development of a neuromorphic computing system,” in 2015 IEEE International Electron Devices Meeting (IEDM). IEEE, 2015, pp. 4–3. [465] S. Sinha, J. Suh, B. Bakkaloglu, and Y. Cao, “Workload-aware neuromorphic design of low-power supply voltage controller,” in Proceedings of the 16th ACM/IEEE international symposium on Low power electronics and design. ACM, 2010, pp. 241–246. [466] ——, “A workload-aware neuromorphic controller for dynamic power and thermal management,” in Adaptive Hardware and Systems (AHS), 2011 NASA/ESA Conference on. IEEE, 2011, pp. 200–207. [467] J. Tapson and A. Van Schaik, “An asynchronous parallel neuromor- phic adc architecture,” in Circuits and Systems (ISCAS), 2012 IEEE International Symposium on. IEEE, 2012, pp. 2409–2412. [468] R. J. Vogelstein, U. Mallik, and G. Cauwenberghs, “Silicon spike- based synaptic array and address-event transceiver,” in Circuits and 34 Systems, 2004. ISCAS’04. Proceedings of the 2004 International Symposium on, vol. 5. IEEE, 2004, pp. V–385. [469] R. J. Vogelstein, U. Mallik, E. Culurciello, G. Cauwenberghs, and R. Etienne-Cummings, “Saliency-driven image acuity modulation on a reconﬁgurable array of spiking silicon neurons,” in Advances in neural information processing systems, 2004, pp. 1457–1464. [470] R. J. Vogelstein, U. Mallik, J. T. Vogelstein, and G. Cauwenberghs, “Dynamically reconﬁgurable silicon array of spiking neurons with conductance-based synapses,” Neural Networks, IEEE Transactions on, vol. 18, no. 1, pp. 253–265, 2007. [471] Y. Wang, R. J. Douglas, and S.-C. Liu, “Attentional processing on a spike-based vlsi neural network,” in Advances in Neural Information Processing Systems, 2006, pp. 1473–1480. [472] H.-P. Wang, E. Chicca, G. Indiveri, and T. J. Sejnowski, “Reliable computation in noisy backgrounds using real-time neuromorphic hardware,” in Biomedical Circuits and Systems Conference, 2007. BIOCAS 2007. IEEE. IEEE, 2007, pp. 71–74. [473] R. M. Wang, T. J. Hamilton, J. C. Tapson, and A. van Schaik, “A mixed-signal implementation of a polychronous spiking neural network with delay adaptation,” Frontiers in neuroscience, vol. 8, 2014. [474] R. Wang, T. J. Hamilton, J. Tapson, and A. van Schaik, “A compact reconﬁgurable mixed-signal implementation of synaptic plasticity in spiking neurons,” in Circuits and Systems (ISCAS), 2014 IEEE International Symposium on. IEEE, 2014, pp. 862–865. [475] Q. Wang, Y. Kim, and P. Li, “Architectural design exploration for neu- romorphic processors with memristive synapses,” in Nanotechnology (IEEE-NANO), 2014 IEEE 14th International Conference on. IEEE, 2014, pp. 962–966. [476] W. Wang, Z. You, P. Liu, and J. Kuang, “An adaptive neural net- work a/d converter based on cmos/memristor hybrid design,” IEICE Electronics Express, 2014. [477] R. M. Wang, T. J. Hamilton, J. C. Tapson, and A. van Schaik, “A neuromorphic implementation of multiple spike-timing synaptic plas- ticity rules for large-scale neural networks,” Frontiers in Neuroscience, vol. 9, p. 180, 2015. [478] Q. Wang, Y. Kim, and P. Li, “Neuromorphic processors with memris- tive synapses: Synaptic interface and architectural exploration,” ACM Journal on Emerging Technologies in Computing Systems (JETC), vol. 12, no. 4, p. 35, 2016. [479] Y. Xu, C. S. Thakur, T. J. Hamilton, J. Tapson, R. Wang, and A. van Schaik, “A reconﬁgurable mixed-signal implementation of a neuromorphic adc,” in Biomedical Circuits and Systems Conference (BioCAS), 2015 IEEE. IEEE, 2015, pp. 1–4. [480] T. Yu, J. Park, S. Joshi, C. Maier, and G. Cauwenberghs, “65k-neuron integrate-and-ﬁre array transceiver with address-event reconﬁgurable synaptic routing,” in Biomedical Circuits and Systems Conference (BioCAS), 2012 IEEE. IEEE, 2012, pp. 21–24. [481] N. Dahasert, ˙I. ¨Ozt¨urk, and R. Kilic¸, “Implementation of izhikevich neuron model with ﬁeld programmable devices,” in Signal Processing and Communications Applications Conference (SIU), 2012 20th. IEEE, 2012, pp. 1–4. [482] E. Farquhar, C. Gordon, and P. Hasler, “A ﬁeld programmable neural array,” in Circuits and Systems, 2006. ISCAS 2006. Proceedings. 2006 IEEE International Symposium on. IEEE, 2006, pp. 4–pp. [483] B. Marr and J. Hasler, “Compiling probabilistic, bio-inspired circuits on a ﬁeld programmable analog array,” Neuromorphic Engineering Systems and Applications, p. 88, 2015. [484] B. McGinley, P. Rocke, F. Morgan, and J. Maher, “Reconﬁgurable analogue hardware evolution of adaptive spiking neural network controllers,” in Proceedings of the 10th annual conference on Genetic and evolutionary computation. ACM, 2008, pp. 289–290. [485] P. Rocke, J. Maher, and F. Morgan, “Platform for intrinsic evolution of analogue neural networks,” in Reconﬁgurable Computing and FPGAs, 2005. ReConFig 2005. International Conference on. IEEE, 2005, pp. 8–pp. [486] P. Rocke, B. McGinley, F. Morgan, and J. Maher, “Reconﬁgurable hardware evolution platform for a spiking neural network robotics controller,” in Reconﬁgurable computing: Architectures, tools and applications. Springer, 2007, pp. 373–378. [487] P. Rocke, B. McGinley, J. Maher, F. Morgan, and J. Harkin, “In- vestigating the suitability of fpaas for evolved hardware spiking neural networks,” in Evolvable Systems: From Biology to Hardware. Springer, 2008, pp. 118–129. [488] J. Zhao and Y.-B. Kim, “Circuit implementation of ﬁtzhugh–nagumo neuron model using ﬁeld programmable analog arrays,” in Circuits and Systems (MWSCAS?2007), 50th Midwest Symposium, 2007, pp. 772–775. [489] R. Agis, E. Ros, J. Diaz, R. Carrillo, and E. Ortigosa, “Hardware event-driven simulation engine for spiking neural networks,” Interna- tional journal of electronics, vol. 94, no. 5, pp. 469–480, 2007. [490] L.-C. Caron, M. D?Haene, F. Mailhot, B. Schrauwen, and J. Rouat, “Event management for large scale event-driven digital hardware spiking neural networks,” Neural Networks, vol. 45, pp. 83–93, 2013. [491] D. Neil and S.-C. Liu, “Minitaur, an event-driven fpga-based spiking network accelerator,” Very Large Scale Integration (VLSI) Systems, IEEE Transactions on, vol. 22, no. 12, pp. 2621–2628, 2014. [492] J. B. Ahn, “Extension of neuron machine neurocomputing architecture for spiking neural networks,” in Neural Networks (IJCNN), The 2013 International Joint Conference on. IEEE, 2013, pp. 1–8. [493] B. Ahn, “Neuron-like digital hardware architecture for large-scale neuromorphic computing,” in Neural Networks (IJCNN), 2015 Inter- national Joint Conference on. IEEE, 2015, pp. 1–8. [494] ——, “Special-purpose hardware architecture for neuromorphic com- puting,” in 2015 International SoC Design Conference (ISOCC). IEEE, 2015, pp. 209–210. [495] L. Bako, “Real-time clustering of datasets with hardware embedded neuromorphic neural networks,” in High Performance Computational Systems Biology, 2009. HIBI’09. International Workshop on. IEEE, 2009, pp. 13–22. [496] L. Bak´o, S. T. Brassai, L. F. M´arton, and L. Losonczi, “Evolving advanced neural networks on run-time reconﬁgurable digital hardware platform,” in Proceedings of the 3rd International Workshop on Adaptive Self-Tuning Computing Systems. ACM, 2013, p. 3. [497] B. Belhadj, J. Tomas, Y. Bornat, A. Daouzli, O. Malot, and S. Renaud, “Digital mapping of a realistic spike timing plasticity model for real- time neural simulations,” in Proceedings of the XXIV conference on design of circuits and integrated systems, 2009, pp. 1–6. [498] S. Bellis, K. M. Razeeb, C. Saha, K. Delaney, C. O’Mathuna, A. Pounds-Cornish, G. de Souza, M. Colley, H. Hagras, G. Clarke et al., “Fpga implementation of spiking neural networks-an initial step towards building tangible collaborative autonomous agents,” in Field-Programmable Technology, 2004. Proceedings. 2004 IEEE International Conference on. IEEE, 2004, pp. 449–452. [499] M. Bhuiyan, A. Nallamuthu, M. C. Smith, V. K. Pallipuram et al., “Optimization and performance study of large-scale biological net- works for reconﬁgurable computing,” in High-Performance Recon- ﬁgurable Computing Technology and Applications (HPRCTA), 2010 Fourth International Workshop on. IEEE, 2010, pp. 1–9. [500] H. T. Blair, J. Cong, and D. Wu, “Fpga simulation engine for customized construction of neural microcircuits,” in Computer-Aided Design (ICCAD), 2013 IEEE/ACM International Conference on. IEEE, 2013, pp. 607–614. [501] L.-C. Caron, F. Mailhot, and J. Rouat, “Fpga implementation of a spiking neural network for pattern matching,” in Circuits and Systems (ISCAS), 2011 IEEE International Symposium on. IEEE, 2011, pp. 649–652. [502] A. Cassidy, S. Denham, P. Kanold, and A. Andreou, “Fpga based silicon spiking neural array,” in Biomedical Circuits and Systems Conference, 2007. BIOCAS 2007. IEEE. IEEE, 2007, pp. 75–78. [503] A. Cassidy and A. G. Andreou, “Dynamical digital silicon neurons,” in Biomedical Circuits and Systems Conference, 2008. BioCAS 2008. IEEE. IEEE, 2008, pp. 289–292. [504] A. Cassidy, A. G. Andreou, and J. Georgiou, “Design of a one million neuron single fpga neuromorphic system for real-time multimodal scene analysis,” in Information Sciences and Systems (CISS), 2011 45th Annual Conference on. IEEE, 2011, pp. 1–6. [505] B. Chappet de Vangel, C. Torres-Huitzil, and B. Girau, “Spiking dynamic neural ﬁelds architectures on fpga,” in ReConFigurable Computing and FPGAs (ReConFig), 2014 International Conference on. IEEE, 2014, pp. 1–6. [506] K. Cheung, S. R. Schultz, and W. Luk, “A large-scale spiking neural network accelerator for fpga systems,” in Artiﬁcial Neural Networks and Machine Learning–ICANN 2012. Springer, 2012, pp. 113–120. [507] ——, “Neuroﬂow: A general purpose spiking neural network simu- lation platform using customizable processors,” Frontiers in Neuro- science, vol. 9, p. 516, 2015. [508] J. Cong, H. T. Blair, and D. Wu, “Fpga simulation engine for cus- tomized construction of neural microcircuit,” in Field-Programmable Custom Computing Machines (FCCM), 2013 IEEE 21st Annual International Symposium on. IEEE, 2013, pp. 229–229. 35 [509] C. Daffron, J. Chan, A. Disney, L. Bechtel, R. Wagner, M. E. Dean, G. S. Rose, J. S. Plank, J. D. Birdwell, and C. D. Schuman, “Exten- sions and enhancements for the danna neuromorphic architecture,” in SoutheastCon, 2016. IEEE, 2016, pp. 1–4. [510] B. C. de Vangel, C. Torres-Huitzil, and B. Girau, “Event based visual attention with dynamic neural ﬁeld on fpga,” in Proceedings of the 10th International Conference on Distributed Smart Camera. ACM, 2016, pp. 142–147. [511] M. E. Dean, C. D. Schuman, and J. D. Birdwell, “Dynamic adaptive neural network array,” in International Conference on Unconventional Computation and Natural Computation. Springer, 2014, pp. 129– 141. [512] M. E. Dean, J. Chan, C. Daffron, A. Disney, J. Reynolds, G. Rose, J. S. Plank, J. D. Birdwell, and C. D. Schuman, “An application devel- opment platform for neuromorphic computing,” in Neural Networks (IJCNN), 2016 International Joint Conference on. IEEE, 2016, pp. 1347–1354. [513] C. Diaz, G. Sanchez, G. Duchen, M. Nakano, and H. Perez, “An efﬁcient hardware implementation of a novel unary spiking neural network multiplier with variable dendritic delays,” Neurocomputing, 2016. [514] E. Z. Farsa, S. Nazari, and M. Gholami, “Function approximation by hardware spiking neural network,” Journal of Computational Electronics, vol. 14, no. 3, pp. 707–716, 2015. [515] P. J. Fox and S. W. Moore, “Efﬁcient handling of synaptic updates in fpga-based large-scale neural network simulations,” in Workshop on Neural Engineering using Reconﬁgurable Hardware, vol. 2012, 2012. [516] V. Garg, R. Shekhar, and J. G. Harris, “The time machine: A novel spike-based computation architecture,” in Circuits and Systems (ISCAS), 2011 IEEE International Symposium on. IEEE, 2011, pp. 685–688. [517] A. Ghani, T. M. McGinnity, L. P. Maguire, and J. Harkin, “Area efﬁcient architecture for large scale implementation of biologically plausible spiking neural networks on reconﬁgurable hardware,” in Field Programmable Logic and Applications, 2006. FPL’06. Inter- national Conference on. IEEE, 2006, pp. 1–2. [518] B. Glackin, T. M. McGinnity, L. P. Maguire, Q. Wu, and A. Bela- treche, “A novel approach for the implementation of large scale spik- ing neural networks on fpga hardware,” in Computational Intelligence and Bioinspired Systems. Springer, 2005, pp. 552–563. [519] B. Glackin, J. Harkin, T. M. McGinnity, and L. P. Maguire, “A hard- ware accelerated simulation environment for spiking neural networks,” in Reconﬁgurable Computing: Architectures, Tools and Applications. Springer, 2009, pp. 336–341. [520] B. Glackin, J. Harkin, T. M. McGinnity, L. P. Maguire, and Q. Wu, “Emulating spiking neural networks for edge detection on fpga hardware,” in Field Programmable Logic and Applications, 2009. FPL 2009. International Conference on. IEEE, 2009, pp. 670–673. [521] S. Gomar and A. Ahmadi, “Digital multiplierless implementation of biological adaptive-exponential neuron model,” Circuits and Systems I: Regular Papers, IEEE Transactions on, vol. 61, no. 4, pp. 1206– 1219, 2014. [522] S. Gomar, M. Mirhassani, M. Ahmadi, and M. Seif, “A digital neuromorphic circuit for neural-glial interaction,” in Neural Networks (IJCNN), 2016 International Joint Conference on. IEEE, 2016, pp. 213–218. [523] P. Haﬂiger, “Asynchronous event redirecting in bio-inspired commu- nication,” in Electronics, Circuits and Systems, 2001. ICECS 2001. The 8th IEEE International Conference on, vol. 1. IEEE, 2001, pp. 87–90. [524] J. Harkin, F. Morgan, S. Hall, P. Dudek, T. Dowrick, and L. Mc- Daid, “Reconﬁgurable platforms and the challenges for large-scale implementations of spiking neural networks,” in Field Programmable Logic and Applications, 2008. FPL 2008. International Conference on. IEEE, 2008, pp. 483–486. [525] J. Harkin, F. Morgan, L. McDaid, S. Hall, B. McGinley, and S. Cawley, “A reconﬁgurable and biologically inspired paradigm for computation using network-on-chip and spiking neural networks,” International Journal of Reconﬁgurable Computing, vol. 2009, p. 2, 2009. [526] H. H. Hellmich and H. Klar, “An fpga based simulation acceleration platform for spiking neural networks,” in Circuits and Systems, 2004. MWSCAS’04. The 2004 47th Midwest Symposium on, vol. 2. IEEE, 2004, pp. II–389. [527] ——, “See: a concept for an fpga based emulation engine for spiking neurons with adaptive weights,” in 5th WSEAS Int. Conf. Neural Networks Applications (NNA?04), 2004, pp. 930–935. [528] T. Iakymchuk, A. Rosado, J. V. Frances, and M. Batallre, “Fast spiking neural network architecture for low-cost fpga devices,” in Reconﬁgurable Communication-centric Systems-on-Chip (ReCoSoC), 2012 7th International Workshop on. IEEE, 2012, pp. 1–6. [529] T. Iakymchuk, A. Rosado-Munoz, M. Bataller-Mompean, J. Guerrero- Martinez, J. Frances-Villora, M. Wegrzyn, and M. Adamski, “Hardware-accelerated spike train generation for neuromorphic image and video processing,” in Programmable Logic (SPL), 2014 IX Southern Conference on. IEEE, 2014, pp. 1–6. [530] S. Johnston, G. Prasad, L. Maguire, and M. McGinnity, “Comparative investigation into classical and spiking neuron implementations on fpgas,” in Artiﬁcial Neural Networks: Biological Inspirations–ICANN 2005. Springer, 2005, pp. 269–274. [531] D. Just, J. F. Chaves, R. M. Gomes, and H. E. Borges, “An efﬁcient implementation of a realistic spiking neuron model on an fpga.” in IJCCI (ICFC-ICNC), 2010, pp. 344–349. [532] S. Koziol, S. Brink, and J. Hasler, “A neuromorphic approach to path planning using a reconﬁgurable neuron array ic,” Very Large Scale Integration (VLSI) Systems, IEEE Transactions on, vol. 22, no. 12, pp. 2724–2737, 2014. [533] J. Li, Y. Katori, and T. Kohno, “An fpga-based silicon neuronal network with selectable excitability silicon neurons,” Frontiers in neuroscience, vol. 6, 2012. [534] W. X. Li, R. C. Cheung, R. H. Chan, D. Song, and T. W. Berger, “Real-time prediction of neuronal population spiking activity using fpga,” Biomedical Circuits and Systems, IEEE Transactions on, vol. 7, no. 4, pp. 489–498, 2013. [535] A. Makhlooghpour, H. Soleimani, A. Ahmadi, M. Zwolinski, and M. Saif, “High accuracy implementation of adaptive exponential integrated and ﬁre neuron model,” in Neural Networks (IJCNN), 2016 International Joint Conference on. IEEE, 2016, pp. 192–197. [536] S. Maya, R. Reynoso, C. Torres, and M. Arias-Estrada, “Com- pact spiking neural network implementation in fpga,” in Field- Programmable Logic and Applications: The Roadmap to Reconﬁg- urable Computing. Springer, 2000, pp. 270–276. [537] J. L. Molin, T. Figliolia, K. Sanni, I. Doxas, A. Andreou, and R. Etienne-Cummings, “Fpga emulation of a spike-based, stochastic system for real-time image dewarping,” in Circuits and Systems (MWSCAS), 2015 IEEE 58th International Midwest Symposium on. IEEE, 2015, pp. 1–4. [538] S. W. Moore, P. J. Fox, S. J. Marsh, A. Mujumdar et al., “Bluehive- a ﬁeld-programable custom computing machine for extreme-scale real-time neural network simulation,” in Field-Programmable Custom Computing Machines (FCCM), 2012 IEEE 20th Annual International Symposium on. IEEE, 2012, pp. 133–140. [539] C. M. Niu, S. Nandyala, W. J. Sohn, and T. Sanger, “Multi-scale hyper-time hardware emulation of human motor nervous system based on spiking neurons using fpga,” in Advances in Neural Information Processing Systems, 2012, pp. 37–45. [540] M. Nu˜no-Maganda and C. Torres-Huitzil, “A temporal coding hard- ware implementation for spiking neural networks,” ACM SIGARCH Computer Architecture News, vol. 38, no. 4, pp. 2–7, 2011. [541] M. A. Nuno-Maganda, M. Arias-Estrada, C. Torres-Huitzil, H. H. Aviles-Arriaga, Y. Hern´andez-Mier, and M. Morales-Sandoval, “A hardware architecture for image clustering using spiking neural net- works,” in VLSI (ISVLSI), 2012 IEEE Computer Society Annual Symposium on. IEEE, 2012, pp. 261–266. [542] M. Pearson, I. Gilhespy, K. Gurney, C. Melhuish, B. Mitchinson, M. Nibouche, and A. Pipe, “A real-time, fpga based, biologically plausible neural network processor,” in Artiﬁcial Neural Networks: Formal Models and Their Applications–ICANN 2005. Springer, 2005, pp. 1021–1026. [543] M. J. Pearson, C. Melhuish, A. G. Pipe, M. Nibouche, K. Gurney, B. Mitchinson et al., “Design and fpga implementation of an embed- ded real-time biologically plausible spiking neural network processor,” in Field programmable logic and applications, 2005. international conference on. IEEE, 2005, pp. 582–585. [544] M. J. Pearson, A. G. Pipe, B. Mitchinson, K. Gurney, C. Melhuish, I. Gilhespy, and M. Nibouche, “Implementing spiking neural networks for real-time signal-processing and control applications: a model- validated fpga approach,” Neural Networks, IEEE Transactions on, vol. 18, no. 5, pp. 1472–1487, 2007. [545] K. L. Rice, M. A. Bhuiyan, T. M. Taha, C. N. Vutsinas, and M. C. Smith, “Fpga implementation of izhikevich spiking neural networks for character recognition,” in Reconﬁgurable Computing and FPGAs, 2009. ReConFig’09. International Conference on. IEEE, 2009, pp. 451–456. 36 [546] A. Rios-Navarro, J. Dominguez-Morales, R. Tapiador-Morales, D. Gutierrez-Galan, A. Jimenez-Fernandez, and A. Linares-Barranco, “A 20mevps/32mev event-based usb framework for neuromorphic systems debugging,” in Event-based Control, Communication, and Signal Processing (EBCCSP), 2016 Second International Conference on. IEEE, 2016, pp. 1–6. [547] D. Roggen, S. Hofmann, Y. Thoma, and D. Floreano, “Hardware spiking neural network with run-time reconﬁgurable connectivity in an autonomous robot,” in Evolvable hardware, 2003. proceedings. nasa/dod conference on. IEEE, 2003, pp. 189–198. [548] E. Ros, R. Agis, R. R. Carrillo, and E. M. Ortigosa, “Post-synaptic time-dependent conductances in spiking neurons: Fpga implementa- tion of a ﬂexible cell model,” in Artiﬁcial Neural Nets Problem Solving Methods. Springer, 2003, pp. 145–152. [549] A. Rosado-Mu˜noz, A. Fijałkowski, M. Bataller-Mompe´an, and J. Guerrero-Mart´ınez, “Fpga implementation of spiking neural net- works supported by a software design environment,” in Proceedings of 18th IFAC World Congress, 2011. [550] H. Rostro-Gonzalez, B. Cessac, B. Girau, and C. Torres-Huitzil, “The role of the asymptotic dynamics in the design of fpga-based hardware implementations of gif-type neural networks,” Journal of Physiology- Paris, vol. 105, no. 1, pp. 91–97, 2011. [551] H. Rostro-Gonzalez, G. Garreau, A. Andreou, J. Georgiou, J. H. Barron-Zambrano, and C. Torres-Huitzil, “An fpga-based approach for parameter estimation in spiking neural networks,” in Circuits and Systems (ISCAS), 2012 IEEE International Symposium on. IEEE, 2012, pp. 2897–2900. [552] G. S´anchez, J. Madrenas, and J. M. Moreno, “Performance evalua- tion and scaling of a multiprocessor architecture emulating complex snn algorithms,” in Evolvable Systems: From Biology to Hardware. Springer, 2010, pp. 145–156. [553] B. Schrauwen and J. M. Van Campenhout, “Parallel hardware imple- mentation of a broad class of spiking neurons using serial arithmetic.” in ESANN, vol. 2, no. 006, 2006. [554] H. Shayani, P. J. Bentley, and A. M. Tyrrell, “An fpga-based model suitable for evolution and development of spiking neural networks.” in ESANN, 2008, pp. 197–202. [555] H. Shayani, P. Bentley, and A. M. Tyrrell, “A cellular structure for online routing of digital spiking neuron axons and dendrites on fpgas,” in Evolvable Systems: From Biology to Hardware. Springer, 2008, pp. 273–284. [556] H. Shayani, P. J. Bentley, and A. M. Tyrrell, “A multi-cellular developmental representation for evolution of adaptive spiking neural microcircuits in an fpga,” in Adaptive Hardware and Systems, 2009. AHS 2009. NASA/ESA Conference on. IEEE, 2009, pp. 3–10. [557] S. Sheik, S. Paul, C. Augustine, C. Kothapalli, M. M. Khellah, G. Cauwenberghs, and E. Neftci, “Synaptic sampling in hardware spiking neural networks,” in Circuits and Systems (ISCAS), 2016 IEEE International Symposium on. IEEE, 2016, pp. 2090–2093. [558] R. J. Sofatzis, S. Afshar, and T. J. Hamilton, “Rotationally invariant vision recognition with neuromorphic transformation and learning networks,” in Circuits and Systems (ISCAS), 2014 IEEE International Symposium on. IEEE, 2014, pp. 469–472. [559] H. Soleimani, A. Ahmadi, and M. Bavandpour, “Biologically inspired spiking neurons: Piecewise linear models and digital implementation,” Circuits and Systems I: Regular Papers, IEEE Transactions on, vol. 59, no. 12, pp. 2991–3004, 2012. [560] A. Upegui, C. A. Pe˜na-Reyes, and E. Sanchez, “An fpga platform for on-line topology exploration of spiking neural networks,” Micropro- cessors and microsystems, vol. 29, no. 5, pp. 211–223, 2005. [561] R. Wang, G. Cohen, K. M. Stiefel, T. J. Hamilton, J. Tapson, and A. van Schaik, “An fpga implementation of a polychronous spiking neural network with delay adaptation,” Frontiers in neuroscience, vol. 7, 2013. [562] R. Wang, T. J. Hamilton, J. Tapson, and A. van Schaik, “An fpga design framework for large-scale spiking neural networks,” in Circuits and Systems (ISCAS), 2014 IEEE International Symposium on. IEEE, 2014, pp. 457–460. [563] ——, “A compact neural core for digital implementation of the neural engineering framework,” in Biomedical Circuits and Systems Conference (BioCAS), 2014 IEEE. IEEE, 2014, pp. 548–551. [564] ——, “An fpga design framework for large-scale spiking neural networks,” in Circuits and Systems (ISCAS), 2014 IEEE International Symposium on. IEEE, 2014, pp. 457–460. [565] S. Wang, C. Ma, D. Wang, and J. Pei, “Extensible neuromorphic computing simulator based on a programmable hardware,” in 2015 15th Non-Volatile Memory Technology Symposium (NVMTS). IEEE, 2015, pp. 1–3. [566] Q. X. Wu, X. Liao, X. Huang, R. Cai, J. Cai, and J. Liu, “Development of fpga toolbox for implementation of spiking neural networks,” in Communication Systems and Network Technologies (CSNT), 2015 Fifth International Conference on. IEEE, 2015, pp. 806–810. [567] J. M. Xicotencatl and M. Arias-Estrada, “Fpga based high density spiking neural network array,” in Field Programmable Logic and Application. Springer, 2003, pp. 1053–1056. [568] S. Yang, Q. Wu, and R. Li, “A case for spiking neural network simulation based on conﬁgurable multiple-fpga systems,” Cognitive neurodynamics, vol. 5, no. 3, pp. 301–309, 2011. [569] S. Yang and T. M. McGinnity, “A biologically plausible real-time spiking neuron simulation environment based on a multiple-fpga platform,” ACM SIGARCH Computer Architecture News, vol. 39, no. 4, pp. 78–81, 2011. [570] A. Zuppicich and S. Soltic, “Fpga implementation of an evolving spik- ing neural network,” in Advances in Neuro-Information Processing. Springer, 2009, pp. 1129–1136. [571] A. Boﬁll-i Petit and A. F. Murray, “Synchrony detection and ampliﬁ- cation by silicon neurons with stdp synapses,” Neural Networks, IEEE Transactions on, vol. 15, no. 5, pp. 1296–1304, 2004. [572] M. Giulioni, X. Lagorce, F. Galluppi, and R. B. Benosman, “Event- based computation of motion ﬂow on a neuromorphic analog neural platform,” Frontiers in Neuroscience, vol. 10, p. 35, 2016. [573] L. Bako, “Real-time classiﬁcation of datasets with hardware embed- ded neuromorphic neural networks,” Brieﬁngs in bioinformatics, p. bbp066, 2010. [574] D. Mart´ı, M. Rigotti, M. Seok, and S. Fusi, “Energy-efﬁcient neuro- morphic classiﬁers,” Neural Computation, 2016. [575] M. A. Nuno-Maganda, M. Arias-Estrada, C. Torres-Huitzil, and B. Gi- rau, “Hardware implementation of spiking neural network classiﬁers based on backpropagation-based learning algorithms,” in Neural Net- works, 2009. IJCNN 2009. International Joint Conference on. IEEE, 2009, pp. 2294–2301. [576] D. Badoni, M. Giulioni, V. Dante, and P. Del Giudice, “An avlsi recurrent network of spiking neurons with reconﬁgurable and plastic synapses,” in Circuits and Systems, 2006. ISCAS 2006. Proceedings. 2006 IEEE International Symposium on. IEEE, 2006, pp. 4–pp. [577] P. Camilleri, M. Giulioni, M. Mattia, J. Braun, and P. Del Giudice, “Self-sustained activity in attractor networks using neuromorphic vlsi,” in IJCNN, 2010, pp. 1–6. [578] E. Chicca, D. Badoni, V. Dante, M. D’Andreagiovanni, G. Salina, L. Carota, S. Fusi, and P. Del Giudice, “A vlsi recurrent network of integrate-and-ﬁre neurons connected by plastic synapses with long- term memory,” Neural Networks, IEEE Transactions on, vol. 14, no. 5, pp. 1297–1307, 2003. [579] S. Fusi, P. Del Giudice, and D. J. Amit, “Neurophysiology of a vlsi spiking neural network: Lann21,” in Neural Networks, 2000. IJCNN 2000, Proceedings of the IEEE-INNS-ENNS International Joint Conference on, vol. 3. IEEE, 2000, pp. 121–126. [580] M. Giulioni, F. Corradi, V. Dante, and P. Del Giudice, “Real time unsupervised learning of visual stimuli in neuromorphic vlsi systems,” Scientiﬁc reports, vol. 5, 2015. [581] P. U. Diehl, G. Zarrella, A. Cassidy, B. U. Pedroni, and E. Neftci, “Conversion of artiﬁcial recurrent neural networks to spiking neural networks for low-power neuromorphic hardware,” in Rebooting Com- puting (ICRC), IEEE International Conference on. IEEE, 2016, pp. 1–8. [582] D. S. Chevitarese and M. N. Dos Santos, “Real-time face tracking and recognition on ibm neuromorphic chip,” in 2016 IEEE International Symposium on Multimedia (ISM). IEEE, 2016, pp. 667–672. [583] P. U. Diehl, B. U. Pedroni, A. Cassidy, P. Merolla, E. Neftci, and G. Zarrella, “Truehappiness: Neuromorphic emotion recognition on truenorth,” in Neural Networks (IJCNN), 2016 International Joint Conference on. IEEE, 2016, pp. 4278–4285. [584] S. K. Esser, A. Andreopoulos, R. Appuswamy, P. Datta, D. Barch, A. Amir, J. Arthur, A. Cassidy, M. Flickner, P. Merolla et al., “Cog- nitive computing systems: Algorithms and applications for networks of neurosynaptic cores,” in Neural Networks (IJCNN), The 2013 International Joint Conference on. IEEE, 2013, pp. 1–10. [585] B. Han, A. Sengupta, and K. Roy, “On the energy beneﬁts of spiking deep neural networks: A case study,” in Neural Networks (IJCNN), 2016 International Joint Conference on. IEEE, 2016, pp. 971–976. [586] G. Indiveri, F. Corradi, and N. Qiao, “Neuromorphic architectures for spiking deep neural networks,” in Electron Devices Meeting (IEDM), 2015 IEEE International. IEEE, 2015, pp. 4–2. 37 [587] Y. Cao, Y. Chen, and D. Khosla, “Spiking deep convolutional neural networks for energy-efﬁcient object recognition,” International Jour- nal of Computer Vision, vol. 113, no. 1, pp. 54–66, 2015. [588] E. Cerezuela-Escudero, A. Jimenez-Fernandez, R. Paz-Vicente, M. Dominguez-Morales, A. Linares-Barranco, and G. Jimenez- Moreno, “Musical notes classiﬁcation with neuromorphic auditory system using fpga and a convolutional spiking network,” in Neural Networks (IJCNN), 2015 International Joint Conference on. IEEE, 2015, pp. 1–7. [589] W. Murphy, M. Renz, and Q. Wu, “Binary image classiﬁcation using a neurosynaptic processor: A trade-off analysis,” in Circuits and Systems (ISCAS), 2016 IEEE International Symposium on. IEEE, 2016, pp. 1342–1345. [590] E. Nurse, B. S. Mashford, A. J. Yepes, I. Kiral-Kornek, S. Harrer, and D. R. Freestone, “Decoding eeg and lfp signals using deep learning: heading truenorth,” in Proceedings of the ACM International Conference on Computing Frontiers. ACM, 2016, pp. 259–266. [591] W.-Y. Tsai, D. Barch, A. Cassidy, M. Debole, A. Andreopoulos, B. Jackson, M. Flickner, J. Arthur, D. Modha, J. Sampson et al., “Always-on speech recognition using truenorth, a reconﬁgurable, neurosynaptic processor,” IEEE Transactions on Computers, 2016. [592] S. K. Esser, P. A. Merolla, J. V. Arthur, A. S. Cassidy, R. Appuswamy, A. Andreopoulos, D. J. Berg, J. L. McKinstry, T. Melano, D. R. Barch et al., “Convolutional networks for fast, energy-efﬁcient neuromorphic computing,” Proceedings of the National Academy of Sciences, p. 201604850, 2016. [593] E. Stromatias, D. Neil, M. Pfeiffer, F. Galluppi, S. B. Furber, and S.-C. Liu, “Robustness of spiking deep belief networks to noise and reduced bit precision of neuro-inspired hardware platforms,” Frontiers in neuroscience, vol. 9, 2015. [594] A. Boﬁll, D. Thompson, and A. F. Murray, “Circuits for vlsi imple- mentation of temporally asymmetric hebbian learning,” in Advances in Neural Information processing systems, 2001, pp. 1091–1098. [595] A. Boﬁll-i Petit and A. F. Murray, “Learning temporal correlations in biologically-inspired avlsi,” in Circuits and Systems, 2003. ISCAS’03. Proceedings of the 2003 International Symposium on, vol. 5. IEEE, 2003, pp. V–817. [596] P. Camilleri, M. Giulioni, V. Dante, D. Badoni, G. Indiveri, B. Michaelis, J. Braun, and P. Del Giudice, “A neuromorphic avlsi network chip with conﬁgurable plastic synapses,” in Hybrid Intelligent Systems, 2007. HIS 2007. 7th International Conference on. IEEE, 2007, pp. 296–301. [597] M. Giulioni, P. Camilleri, V. Dante, D. Badoni, G. Indiveri, J. Braun, and P. Del Giudice, “A vlsi network of spiking neurons with plastic fully conﬁgurable ?stop-learning? synapses,” in Electronics, Circuits and Systems, 2008. ICECS 2008. 15th IEEE International Conference on. IEEE, 2008, pp. 678–681. [598] M. Giulioni, M. Pannunzi, D. Badoni, V. Dante, and P. D. Giudice, “A conﬁgurable analog vlsi neural network with spiking neurons and self-regulating plastic synapses,” in Advances in Neural Information Processing Systems, 2008, pp. 545–552. [599] M. Giulioni, M. Pannunzi, D. Badoni, V. Dante, and P. Del Giudice, “Classiﬁcation of correlated patterns with a conﬁgurable analog vlsi neural network of spiking neurons and self-regulating plastic synapses,” Neural computation, vol. 21, no. 11, pp. 3106–3129, 2009. [600] C. Gordon and P. Hasler, “Biological learning modeled in an adaptive ﬂoating-gate system,” in Circuits and Systems, 2002. ISCAS 2002. IEEE International Symposium on, vol. 5. IEEE, 2002, pp. V–609. [601] P. H¨aﬂiger and M. Mahowald, “Spike based normalizing hebbian learning in an analog vlsi artiﬁcial neuron,” Analog Integrated Circuits and Signal Processing, vol. 18, no. 2-3, pp. 133–139, 1999. [602] Q. Sun, F. Schwartz, J. Michel, Y. Herve, and R. Dalmolin, “Im- plementation study of an analog spiking neural network for assisting cardiac delay prediction in a cardiac resynchronization therapy de- vice,” Neural Networks, IEEE Transactions on, vol. 22, no. 6, pp. 858–869, 2011. [603] F. L. M. Huayaney, H. Tanaka, T. Matsuo, T. Morie, and K. Aihara, “A vlsi spiking neural network with symmetric stdp and associative memory operation,” in Neural Information Processing. Springer, 2011, pp. 381–388. [604] S. Shapero, C. Rozell, and P. Hasler, “Conﬁgurable hardware integrate and ﬁre neurons for sparse approximation,” Neural Networks, vol. 45, pp. 134–143, 2013. [605] C. H. Ang, C. Jin, P. H. Leong, and A. Van Schaik, “Spiking neural network-based auto-associative memory using fpga interconnect de- lays,” in Field-Programmable Technology (FPT), 2011 International Conference on. IEEE, 2011, pp. 1–4. [606] J. Dungen and J. Brault, “Simulated control of a tracking mobile robot by four avlsi integrate-and-ﬁre neurons paired into maps,” in Neural Networks, 2005. IJCNN ’05. Proceedings. 2005 IEEE International Joint Conference on, vol. 2, July 2005, pp. 695–699 vol. 2. [607] P. Haﬂiger, “Adaptive wta with an analog vlsi neuromorphic learning chip,” Neural Networks, IEEE Transactions on, vol. 18, no. 2, pp. 551–572, 2007. [608] S.-C. Liu and M. Oster, “Feature competition in a spike-based winner- take-all vlsi network,” in Circuits and Systems, 2006. ISCAS 2006. Proceedings. 2006 IEEE International Symposium on. IEEE, 2006, pp. 4–pp. [609] M. Oster and S.-C. Liu, “A winner-take-all spiking network with spiking inputs,” in Electronics, Circuits and Systems, 2004. ICECS 2004. Proceedings of the 2004 11th IEEE International Conference on. IEEE, 2004, pp. 203–206. [610] M. Oster, Y. Wang, R. Douglas, and S.-C. Liu, “Quantiﬁcation of a spike-based winner-take-all vlsi network,” Circuits and Systems I: Regular Papers, IEEE Transactions on, vol. 55, no. 10, pp. 3160– 3169, 2008. [611] J. P. Abrahamsen, P. Haﬂiger, and T. S. Lande, “A time domain winner-take-all network of integrate-and-ﬁre neurons,” in Circuits and Systems, 2004. ISCAS’04. Proceedings of the 2004 International Symposium on, vol. 5. IEEE, 2004, pp. V–361. [612] H.-Y. Hsieh and K.-T. Tang, “Hardware friendly probabilistic spiking neural network with long-term and short-term plasticity,” Neural Networks and Learning Systems, IEEE Transactions on, vol. 24, no. 12, pp. 2063–2074, 2013. [613] ——, “An on-chip learning, low-power probabilistic spiking neural network with long-term memory,” in Biomedical Circuits and Systems Conference (BioCAS), 2013 IEEE. IEEE, 2013, pp. 5–8. [614] H. Abdelbaki, E. Gelenbe, and S. E. El-Khamy, “Analog hardware implementation of the random neural network model,” in Neural Networks, 2000. IJCNN 2000, Proceedings of the IEEE-INNS-ENNS International Joint Conference on, vol. 4. IEEE, 2000, pp. 197–201. [615] E. Donati, F. Corradi, C. Stefanini, and G. Indiveri, “A spiking imple- mentation of the lamprey’s central pattern generator in neuromorphic vlsi,” in Biomedical Circuits and Systems Conference (BioCAS), 2014 IEEE. IEEE, 2014, pp. 512–515. [616] E. Donati, G. Indiveri, and C. Stefanini, “A novel spiking cpg-based implementation system to control a lamprey robot,” in Biomedical Robotics and Biomechatronics (BioRob), 2016 6th IEEE International Conference on. IEEE, 2016, pp. 1364–1364. [617] M. Ambroise, T. Levi, S. Joucla, B. Yvert, and S. Sa¨ıghi, “Real- time biomimetic central pattern generators in an fpga for hybrid experiments,” Neuromorphic Engineering Systems and Applications, p. 134, 2015. [618] J. H. Barron-Zambrano and C. Torres-Huitzil, “Fpga implementation of a conﬁgurable neuromorphic cpg-based locomotion controller,” Neural Networks, vol. 45, pp. 50–61, 2013. [619] S. Joucla, M. Ambroise, T. Levi, T. Lafon, P. Chauvet, S. Sa¨ıghi, Y. Bornat, N. Lewis, S. Renaud, and B. Yvert, “Generation of locomotor-like activity in the isolated rat spinal cord using intraspinal electrical microstimulation driven by a digital neuromorphic cpg,” Frontiers in Neuroscience, vol. 10, 2016. [620] A. Abutalebi and S. Fakhraie, “A submicron analog neural network with an adjustable-level output unit,” in Microelectronics, 1998. ICM’98. Proceedings of the Tenth International Conference on. IEEE, 1998, pp. 294–297. [621] D. Y. Aksin, P. B. Basyurt, and H. U. Uyanik, “Single-ended in- put four-quadrant multiplier for analog neural networks,” in Circuit Theory and Design, 2009. ECCTD 2009. European Conference on. IEEE, 2009, pp. 307–310. [622] M. Al-Nsour and H. S. Abdel-Aty-Zohdy, “Mos fully analog rein- forcement neural network chip,” in Circuits and Systems, 2001. ISCAS 2001. The 2001 IEEE International Symposium on, vol. 3. IEEE, 2001, pp. 237–240. [623] B. A. Alhalabi and M. Bayoumi, “A scalable analog architecture for neural networks with on-chip learning and refreshing,” in VLSI, 1995. Proceedings., Fifth Great Lakes Symposium on. IEEE, 1995, pp. 33– 38. [624] A. Almeida and J. Franca, “Digitally programmable analog building blocks for the implementation of artiﬁcial neural networks,” Neural Networks, IEEE Transactions on, vol. 7, no. 2, pp. 506–514, 1996. [625] J. Alspector, R. Meir, B. Yuhas, A. Jayakumar, and D. Lippe, “A parallel gradient descent method for learning in analog vlsi neural networks,” in Advances in neural information processing systems, 1993, pp. 836–844. 38 [626] J. Amaral, J. Amaral, C. Santini, R. Tanscheit, M. Vellasco, and M. Pacheco, “Towards evolvable analog artiﬁcial neural networks con- trollers,” in Evolvable Hardware, 2004. Proceedings. 2004 NASA/DoD Conference on. IEEE, 2004, pp. 46–52. [627] Y. Arima, M. Murasaki, T. Yamada, A. Maeda, and H. Shinohara, “A refreshable analog vlsi neural network chip with 400 neurons and 40 k synapses,” in Solid-State Circuits Conference, 1992. Digest of Technical Papers. 39th ISSCC, 1992 IEEE International. IEEE, 1992, pp. 132–133. [628] I. Bayraktaro˘glu, A. S. ¨O˘grenci, G. D¨undar, S. Balkır, and E. Al- paydın, “Annsys: an analog neural network synthesis system,” Neural networks, vol. 12, no. 2, pp. 325–338, 1999. [629] Y. Berg, R. L. Sigvartsen, T. S. Lande, and A. Abusland, “An analog feed-forward neural network with on-chip learning,” Analog Integrated Circuits and Signal Processing, vol. 9, no. 1, pp. 65–75, 1996. [630] S. Bibyk and M. Ismail, “Issues in analog vlsi and mos techniques for neural computing,” in Analog VLSI Implementation of Neural systems. Springer, 1989, pp. 103–133. [631] G. Bo, D. Caviglia, and M. Valle, “A current mode cmos multi-layer perceptron chip,” in Microelectronics for Neural Networks, 1996., Proceedings of Fifth International Conference on. IEEE, 1996, pp. 103–106. [632] G. Bo, D. Caviglia, M. Valle, R. Stratta, and E. Trucco, “A re- conﬁgurable analog vlsi neural network architecture with non linear synapses,” in Neural Nets WIRN VIETRI-96. Springer, 1997, pp. 281–288. [633] G. Bo, D. Caviglia, H. Chibl´e, and M. Valle, “A circuit architecture for analog on-chip back propagation learning with local learning rate adaptation,” Analog Integrated Circuits and Signal Processing, vol. 18, no. 2-3, pp. 163–173, 1999. [634] G. Bollano, M. Costa, D. Palmisano, and E. Pasero, “Off-chip training of analog hardware feed-forward neural networks through hyper- ﬂoating resilient propagation,” in Neural Nets WIRN VIETRI-96. Springer, 1997, pp. 289–297. [635] T. Borgstrom, M. Ismail, and S. Bibyk, “Programmable current- mode neural network for implementation in analogue mos vlsi,” IEE Proceedings G (Circuits, Devices and Systems), vol. 137, no. 2, pp. 175–184, 1990. [636] T.-H. Botha, “An analog cmos programmable and conﬁgurable neural network,” in Pattern Recognition, 1992. Vol. IV. Conference D: Architectures for Vision and Pattern Recognition, Proceedings., 11th IAPR International Conference on. IEEE, 1992, pp. 222–224. [637] S. Bridges, M. Figueroa, D. Hsu, and C. Diorio, “A reconﬁgurable vlsi learning array,” in Solid-State Circuits Conference, 2005. ESSCIRC 2005. Proceedings of the 31st European. IEEE, 2005, pp. 117–120. [638] V. Calayir, M. Darwish, J. Weldon, and L. Pileggi, “Analog neu- romorphic computing enabled by multi-gate programmable resistive devices,” in Proceedings of the 2015 Design, Automation & Test in Europe Conference & Exhibition. EDA Consortium, 2015, pp. 928– 931. [639] G. Carvajal, M. Figueroa, D. Sbarbaro, and W. Valenzuela, “Analysis and compensation of the effects of analog vlsi arithmetic on the lms algorithm,” Neural Networks, IEEE Transactions on, vol. 22, no. 7, pp. 1046–1060, 2011. [640] R. C. Chang, B. J. Sheu, J. Choi, and D. C.-H. Chen, “Programmable- weight building blocks for analog vlsi neural network processors,” Analog Integrated Circuits and Signal Processing, vol. 9, no. 3, pp. 215–230, 1996. [641] N. Chasta, S. Chouhan, and Y. Kumar, “Analog vlsi implementation of neural network architecture for signal processing,” International Journal of VLSI Design & Comunication System, vol. 3, no. 2, 2012. [642] J.-W. Cho, “Modular neuro-chip with on-chip learning and adjustable learning parameters,” Neural Processing Letters, vol. 4, no. 1, pp. 45–52, 1996. [643] J.-W. Cho and S.-Y. Lee, “Analog neuro-chips with on-chip learning capability for adaptive nonlinear equalizers,” in Neural Networks Pro- ceedings, 1998. IEEE World Congress on Computational Intelligence. The 1998 IEEE International Joint Conference on, vol. 1. IEEE, 1998, pp. 581–586. [644] ——, “Active noise canceling using analog neuro-chip with on-chip learning capability,” in Advances in Neural Information Processing Systems, 1999, pp. 664–670. [645] M.-R. Choi and F. M. Salam, “Implementation of feedforward artiﬁ- cial neural nets with learning using standard cmos vlsi technology,” in Circuits and Systems, 1991., IEEE International Sympoisum on. IEEE, 1991, pp. 1509–1512. [646] J. Choi and B. J. Sheu, “Vlsi design of compact and high- precision analog neural network processors,” in Neural Networks, 1992. IJCNN., International Joint Conference on, vol. 2. IEEE, 1992, pp. 637–641. [647] Y. K. Choi and S.-Y. Lee, “Subthreshold mos implementation of neural networks with on-chip error backpropagation learning,” in Neural Networks, 1993. IJCNN’93-Nagoya. Proceedings of 1993 International Joint Conference on, vol. 1. IEEE, 1993, pp. 849– 852. [648] J. Choi, S. H. Bang, and B. J. Sheu, “A programmable analog vlsi neural network processor for communication receivers,” Neural Networks, IEEE Transactions on, vol. 4, no. 3, pp. 484–495, 1993. [649] ——, “A programmable vlsi neural network processor for digital communications,” in Custom Integrated Circuits Conference, 1993., Proceedings of the IEEE 1993. IEEE, 1993, pp. 16–5. [650] Y. K. Choi, K.-H. Ahn, and S.-Y. Lee, “Effects of multiplier output offsets on on-chip learning for analog neuro-chips,” Neural Processing Letters, vol. 4, no. 1, pp. 1–8, 1996. [651] R. Coggins and M. A. Jabri, “Wattle: A trainable gain analogue vlsi neural network,” in Advances in Neural Information Processing Systems, 1994, pp. 874–881. [652] R. Coggins, M. Jabri, B. Flower, and S. Pickard, “A hybrid analog and digital vlsi neural network for intracardiac morphology classiﬁcation,” Solid-State Circuits, IEEE Journal of, vol. 30, no. 5, pp. 542–550, 1995. [653] R. Coggins, M. A. Jabri, B. Flower, and S. Pickard, “Iceg morphology classiﬁcation using an analogue vlsi neural network,” in Advances in Neural Information Processing Systems, 1995, pp. 731–738. [654] D. Coue and G. Wilson, “A four-quadrant subthreshold mode multi- plier for analog neural-network applications,” Neural Networks, IEEE Transactions on, vol. 7, no. 5, pp. 1212–1219, 1996. [655] F. Diotalevi, M. Valle, G. Bo, E. Biglieri, and D. Caviglia, “An analog on-chip learning circuit architecture of the weight perturbation algorithm,” in Circuits and Systems, 2000. Proceedings. ISCAS 2000 Geneva. The 2000 IEEE International Symposium on, vol. 1. IEEE, 2000, pp. 419–422. [656] L. Docheva, A. Bekiarski, and I. Dochev, “Analysis of analog neu- ral network model with cmos multipliers,” RADIOENGINEERING- PRAGUE-, vol. 16, no. 3, p. 103, 2007. [657] B. K. Dolenko and H. C. Card, “The effects of analog hardware properties on backpropagation networks with on-chip learning,” in Neural Networks, 1993., IEEE International Conference on. IEEE, 1993, pp. 110–115. [658] B. Dolenko and H. Card, “Neural learning in analogue hardware: Effects of component variation from fabrication and from noise,” Electronics letters, vol. 29, no. 8, pp. 693–694, 1993. [659] B. K. Dolenko and H. C. Card, “Tolerance to analog hardware of on- chip learning in backpropagation networks,” Neural Networks, IEEE Transactions on, vol. 6, no. 5, pp. 1045–1052, 1995. [660] J. Donald and L. A. Akers, “An adaptive neural processing node,” Neural Networks, IEEE Transactions on, vol. 4, no. 3, pp. 413–426, 1993. [661] T. Duong, S. Eberhardt, M. Tran, T. Daud, and A. Thakoor, “Learning and optimization with cascaded vlsi neural network building-block chips,” in Neural Networks, 1992. IJCNN., International Joint Con- ference on, vol. 1. IEEE, 1992, pp. 184–189. [662] S. Eberhardt, T. Duong, and A. Thakoor, “Design of parallel hard- ware neural network systems from custom analog vlsi’building block’chips,” in Neural Networks, 1989. IJCNN., International Joint Conference on. IEEE, 1989, pp. 183–190. [663] S. P. Eberhardt, R. Tawel, T. X. Brown, T. Daud, and A. Thakoor, “Analog vlsi neural networks: Implementation issues and examples in optimization and supervised learning,” Industrial Electronics, IEEE Transactions on, vol. 39, no. 6, pp. 552–564, 1992. [664] E. El-Masry, H.-K. Yang, M. Yakout et al., “Implementations of artiﬁcial neural networks using current-mode pulse width modulation technique,” Neural Networks, IEEE Transactions on, vol. 8, no. 3, pp. 532–548, 1997. [665] M. El-soud, R. AbdelRassoul, H. H. Soliman, L. M. El-ghanam et al., “Low-power cmos circuits for analog vlsi programmable neural networks,” in Microelectronics, 2003. ICM 2003. Proceedings of the 15th International Conference on. IEEE, 2003, pp. 14–17. [666] S. M. Fakhraie, J. Xu, and K. Smith, “Design of cmos quadratic neural networks,” in Proc. IEEE Paciﬁc Rim Conf. Communications, Computers, and Signal Processing, 1995, pp. 493–496. 39 [667] D. B. Feltham and W. Maly, “Physically realistic fault models for analog cmos neural networks,” Solid-State Circuits, IEEE Journal of, vol. 26, no. 9, pp. 1223–1229, 1991. [668] W. Fisher, R. Fujimoto, and M. Okamura, “The lockheed pro- grammable analog neural network processor,” in Neural Networks, 1990., 1990 IJCNN International Joint Conference on. IEEE, 1990, pp. 563–568. [669] B. Foruzandeh and S. F. Quigley, “An analogue multilayer percep- tron circuit with on-chip training,” in Circuits and Systems, 1999. ISCAS’99. Proceedings of the 1999 IEEE International Symposium on, vol. 5. IEEE, 1999, pp. 395–398. [670] B. Foruzandeh and S. Quigley, “An investigation of the effect of synapse transfer characteristic on the performance of analogue neural networks,” in Electronics, Circuits and Systems, 1999. Proceedings of ICECS’99. The 6th IEEE International Conference on, vol. 2. IEEE, 1999, pp. 1017–1020. [671] B. Furman and A. Abidi, “An analog cmos backward error- propagation lsi,” in Signals, Systems and Computers, 1988. Twenty- Second Asilomar Conference on, vol. 2. IEEE, 1988, pp. 645–648. [672] L. Gatet, H. Tap-B´eteille, and M. Lescure, “Design and test of a cmos mlp analog neural network for fast on-board signal processing,” in Electronics, Circuits and Systems, 2006. ICECS’06. 13th IEEE International Conference on. IEEE, 2006, pp. 922–925. [673] ——, “Real-time surface discrimination using an analog neural net- work implemented in a phase-shift laser rangeﬁnder,” Sensors Journal, IEEE, vol. 7, no. 10, pp. 1381–1387, 2007. [674] ——, “Analog neural network implementation for a real-time surface classiﬁcation application,” Sensors Journal, IEEE, vol. 8, no. 8, pp. 1413–1421, 2008. [675] L. Gatet, H. Tap-B´eteille, M. Lescure, D. Roviras, and A. Mallet, “Functional tests of a 0.6 µm cmos mlp analog neural network for fast on-board signal processing,” Analog Integrated Circuits and Signal Processing, vol. 54, no. 3, pp. 219–227, 2008. [676] L. Gatet, H. Tap-B´eteille, D. Roviras, and F. Gizard, “Integrated cmos analog neural network ability to linearize the distorted characteristic of hpa embedded in satellites,” in Electronic Design, Test and Appli- cations, 2008. DELTA 2008. 4th IEEE International Symposium on. IEEE, 2008, pp. 502–505. [677] L. Gatet, H. Tap-B´eteille, and F. Bony, “Comparison between analog and digital neural network implementations for range-ﬁnding appli- cations,” Neural Networks, IEEE Transactions on, vol. 20, no. 3, pp. 460–470, 2009. [678] B. Heruseto, E. Prasetyo, H. Afandi, and M. Paindavoine, “Embedded analog cmos neural network inside high speed camera,” in Quality Electronic Design, 2009. ASQED 2009. 1st Asia Symposium on. IEEE, 2009, pp. 144–147. [679] K. Hirotsu and M. A. Brooke, “An analog neural network chip with random weight change learning algorithm,” in Neural Networks, 1993. IJCNN’93-Nagoya. Proceedings of 1993 International Joint Conference on, vol. 3. IEEE, 1993, pp. 3031–3034. [680] S. G. Hohmann, J. Schemmel, F. Sch¨urmann, and K. Meier, “Explor- ing the parameter space of a genetic algorithm for training an analog neural network.” in GECCO, 2002, pp. 375–382. [681] P. Houselander, J. Taylor, and D. Haigh, “A current mode analogue circuit for implementing artiﬁcial neural networks,” in Electronic Filters, IEE 1988 Saraga Colloquium on. IET, 1988, pp. 14–1. [682] M. Jabri and B. Flower, “Weight perturbation: An optimal architecture and learning technique for analog vlsi feedforward and recurrent multilayer networks,” Neural Networks, IEEE Transactions on, vol. 3, no. 1, pp. 154–157, 1992. [683] V. Kakkar, “Comparative study on analog and digital neural net- works,” Int. J. Comput. Sci. Netw. Secur, vol. 9, no. 7, pp. 14–21, 2009. [684] J.-F. Lan and C.-Y. Wu, “Analog cmos current-mode implementation of the feedforward neural network with on-chip learning and storage,” in Neural Networks, 1995. Proceedings., IEEE International Confer- ence on, vol. 1. IEEE, 1995, pp. 645–650. [685] J. Lansner, T. Lehmann et al., “A neuron-and a synapse chip for artiﬁcial neural networks,” in Solid-State Circuits Conference, 1992. ESSCIRC’92. Eighteenth European. IEEE, 1992, pp. 213–216. [686] T. Lindblad, C. Lindsey, F. Block, and A. Jayakumar, “Using software and hardware neural networks in a higgs search,” Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrom- eters, Detectors and Associated Equipment, vol. 356, no. 2, pp. 498– 506, 1995. [687] J. Liu and M. Brooke, “A fully parallel learning neural network chip for real-time control,” in Neural Networks, 1999. IJCNN’99. International Joint Conference on, vol. 4. IEEE, 1999, pp. 2323– 2328. [688] ——, “Fully parallel on-chip learning hardware neural network for real-time control,” in Circuits and Systems, 1999. ISCAS’99. Proceed- ings of the 1999 IEEE International Symposium on, vol. 5. IEEE, 1999, pp. 371–374. [689] J. B. Lont and W. Guggenb¨yhl, “Analog cmos implementation of a multilayer perceptron with nonlinear synapses,” Neural Networks, IEEE Transactions on, vol. 3, no. 3, pp. 457–465, 1992. [690] J. B. Lont, “High-density analog-eeprom based neural network,” in ICANN?93. Springer, 1993, pp. 1062–1065. [691] C. Lu, B. Shi, and L. Chen, “Hardware implementation of an on-chip bp learning neural network with programmable neuron characteristics and learning rate adaptation,” in Neural Networks, 2001. Proceedings. IJCNN’01. International Joint Conference on, vol. 1. IEEE, 2001, pp. 212–215. [692] ——, “A programmable on-chip bp learning neural network with enhanced neuron characteristics,” in Circuits and Systems, 2001. ISCAS 2001. The 2001 IEEE International Symposium on, vol. 3. IEEE, 2001, pp. 573–576. [693] C. Lu, B.-X. Shi, and L. Chen, “Implementation of an analog self- learning neural network,” in ASIC, 2001. Proceedings. 4th Interna- tional Conference on. IEEE, 2001, pp. 262–265. [694] ——, “An on-chip bp learning neural network with ideal neuron char- acteristics and learning rate adaptation,” Analog Integrated Circuits and Signal Processing, vol. 31, no. 1, pp. 55–62, 2002. [695] C. Lu, B. Shi, and L. Chen, “Hardware implementation of an expand- able on-chip learning neural network with 8-neuron and 64-synapse,” in TENCON’02. Proceedings. 2002 IEEE Region 10 Conference on Computers, Communications, Control and Power Engineering, vol. 3. IEEE, 2002, pp. 1451–1454. [696] ——, “A general-purpose neural network with on-chip bp learning,” in Circuits and Systems, 2002. ISCAS 2002. IEEE International Symposium on, vol. 2. IEEE, 2002, pp. II–520. [697] ——, “An expandable on-chip bp learning neural network chip,” International journal of electronics, vol. 90, no. 5, pp. 331–340, 2003. [698] Y. Maeda, H. Hirano, and Y. Kanata, “An analog neural network circuit with a learning rule via simultaneous perturbation,” in Neural Networks, 1993. IJCNN’93-Nagoya. Proceedings of 1993 Interna- tional Joint Conference on, vol. 1. IEEE, 1993, pp. 853–856. [699] ——, “A learning rule of neural networks via simultaneous pertur- bation and its hardware implementation,” Neural Networks, vol. 8, no. 2, pp. 251–259, 1995. [700] D. Maliuk, H.-G. Stratigopoulos, and Y. Makris, “An analog vlsi multilayer perceptron and its application towards built-in self-test in analog circuits,” in On-Line Testing Symposium (IOLTS), 2010 IEEE 16th International. IEEE, 2010, pp. 71–76. [701] D. Maliuk, H.-G. Stratigopoulos, H. Huang, and Y. Makris, “Analog neural network design for rf built-in self-test,” in Test Conference (ITC), 2010 IEEE International. IEEE, 2010, pp. 1–10. [702] D. Maliuk and Y. Makris, “A dual-mode weight storage analog neural network platform for on-chip applications,” in Circuits and Systems (ISCAS), 2012 IEEE International Symposium on. IEEE, 2012, pp. 2889–2892. [703] ——, “An analog non-volatile neural network platform for prototyping rf bist solutions,” in Proceedings of the conference on Design, Automation & Test in Europe. European Design and Automation Association, 2014, p. 368. [704] ——, “An experimentation platform for on-chip integration of analog neural networks: A pathway to trusted and robust analog/rf ics,” 2014. [705] P. Masa, K. Hoen, and H. Wallinga, “A high-speed analog neural processor,” Micro, IEEE, vol. 14, no. 3, pp. 40–50, 1994. [706] M. Masmoudi, M. Samet, F. Taktak, and A. M. Alimi, “A hardware implementation of neural network for the recognition of printed numerals,” in Microelectronics, 1999. ICM’99. The Eleventh Inter- national Conference on. IEEE, 1999, pp. 113–116. [707] M. Mestari, “An analog neural network implementation in ﬁxed time of adjustable-order statistic ﬁlters and applications,” Neural Networks, IEEE Transactions on, vol. 15, no. 3, pp. 766–785, 2004. [708] J. Michel and Y. Herve, “Vhdl-ams behavioral model of an analog neural networks based on a fully parallel weight perturbation algo- rithm using incremental on-chip learning,” in Industrial Electronics, 2004 IEEE International Symposium on, vol. 1. IEEE, 2004, pp. 211–216. [709] M. Milev and M. Hristov, “Analog implementation of ann with inherent quadratic nonlinearity of the synapses,” Neural Networks, IEEE Transactions on, vol. 14, no. 5, pp. 1187–1200, 2003. 40 [710] S. S. Modi, P. R. Wilson, and A. D. Brown, “Power aware learning for class ab analogue vlsi neural network,” in Circuits and Systems, 2006. ISCAS 2006. Proceedings. 2006 IEEE International Symposium on. IEEE, 2006, pp. 4–pp. [711] A. J. Montalvo, R. S. Gyurcsik, and J. J. Paulos, “Building blocks for a temperature-compensated analog vlsi neural network with on- chip learning,” in Circuits and Systems, 1994. ISCAS’94., 1994 IEEE International Symposium on, vol. 6. IEEE, 1994, pp. 363–366. [712] ——, “An analog vlsi neural network with on-chip perturbation learning,” Solid-State Circuits, IEEE Journal of, vol. 32, no. 4, pp. 535–543, 1997. [713] ——, “Toward a general-purpose analog vlsi neural network with on- chip learning,” Neural Networks, IEEE Transactions on, vol. 8, no. 2, pp. 413–423, 1997. [714] I. P. Morns and S. S. Dlay, “Analog design of a new neural network for optical character recognition,” IEEE transactions on neural networks, vol. 10, no. 4, pp. 951–953, 1999. [715] D. B. Mundie and L. W. Massengill, “A simulation and training technique for analog neural network implementations,” in Neural Net- works, 1994. IEEE World Congress on Computational Intelligence., 1994 IEEE International Conference on, vol. 3. IEEE, 1994, pp. 1975–1980. [716] A. F. Murray, “Analog vlsi and multi-layer perceptions-accuracy, noise and on-chip learning,” Neurocomputing, vol. 4, no. 6, pp. 301–310, 1992. [717] A. Nosratinia, M. Ahmadi, and M. Shridhar, “Implementation issues in a multi-stage feed-forward analog neural network,” in Neural Networks, 1992. IJCNN., International Joint Conference on, vol. 2. IEEE, 1992, pp. 642–647. [718] H.-J. Oh and F. M. Salam, “A modular analog chip for feed-forward networks with on-chip learning,” in Circuits and Systems, 1993., Proceedings of the 36th Midwest Symposium on. IEEE, 1993, pp. 766–769. [719] ——, “Analog cmos implementation of neural network for adaptive signal processing,” in Circuits and Systems, 1994. ISCAS’94., 1994 IEEE International Symposium on, vol. 6. IEEE, 1994, pp. 503–506. [720] C.-H. Pan, H.-Y. Hsieh, and K.-T. Tang, “An analog multilayer perceptron neural network for a portable electronic nose,” Sensors, vol. 13, no. 1, pp. 193–207, 2012. [721] S. Pinjare, “Design and analog vlsi implementation of neural network architecture for signal processing,” European Journal of Scientiﬁc Research, vol. 27, no. 2, pp. 199–216, 2009. [722] O. Richter, R. F. Reinhart, S. Nease, J. Steil, and E. Chicca, “Device mismatch in a neuromorphic system implements random features for regression,” in Biomedical Circuits and Systems Conference (BioCAS), 2015 IEEE. IEEE, 2015, pp. 1–4. [723] F. M. Salam and M.-R. Choi, “An all-mos analog feedforward neural circuit with learning,” in Circuits and Systems, 1990., IEEE International Symposium on. IEEE, 1990, pp. 2508–2511. [724] S. Satyanarayana, Y. Tsividis, and H. Graf, “Analogue neural networks with distributed neurons,” Electronics Letters, vol. 25, no. 5, pp. 302– 304, 1989. [725] R. Shimabukuro, P. Shoemaker, and M. Stewart, “Circuitry for artiﬁ- cial neural networks with non-volatile analog memories,” in Circuits and Systems, 1989., IEEE International Symposium on. IEEE, 1989, pp. 1217–1220. [726] K. Soelberg, R. L. Sigvartsen, T. S. Lande, and Y. Berg, “An analog continuous-time neural network,” Analog Integrated Circuits and Signal Processing, vol. 5, no. 3, pp. 235–246, 1994. [727] L. Song, M. I. Elmasry, and A. Vannelli, “Analog neural network building blocks based on current mode subthreshold operation,” in Circuits and Systems, 1993., ISCAS’93, 1993 IEEE International Symposium on. IEEE, 1993, pp. 2462–2465. [728] L.-Y. Song, A. Vannelli, and M. I. Elmasry, “A compact vlsi imple- mentation of neural networks,” in VLSI Artiﬁcial Neural Networks Engineering. Springer, 1994, pp. 139–156. [729] X. Sun, M. H. Chow, F. H. Leung, D. Xu, Y. Wang, and Y.-S. Lee, “Analogue implementation of a neural network controller for ups inverter applications,” Power Electronics, IEEE Transactions on, vol. 17, no. 3, pp. 305–313, 2002. [730] S. M. Tam, B. Gupta, H. A. Castro, and M. Holler, “Learning on an analog vlsi neural network chip,” in Systems, Man and Cybernetics, 1990. Conference Proceedings., IEEE International Conference on. IEEE, 1990, pp. 701–703. [731] S. Tam, M. Holler, J. Brauch, A. Pine, A. Peterson, S. Anderson, and S. Deiss, “A reconﬁgurable multi-chip analog neural network: recognition and back-propagation training,” in Neural Networks, 1992. IJCNN., International Joint Conference on, vol. 2. IEEE, 1992, pp. 625–630. [732] R. Tawel, “Learning in analog neural network hardware,” Computers & electrical engineering, vol. 19, no. 6, pp. 453–467, 1993. [733] C. S. Thakur, T. J. Hamilton, R. Wang, J. Tapson, and A. van Schaik, “A neuromorphic hardware framework based on population coding,” in Neural Networks (IJCNN), 2015 International Joint Conference on. IEEE, 2015, pp. 1–8. [734] C. Thakur, R. Wang, T. Hamilton, J. Tapson, and A. van Schaik, “A low power trainable neuromorphic integrated circuit that is tolerant to device mismatch,” Circuits and Systems I: Regular Papers, IEEE Transactions on, vol. PP, no. 99, pp. 1–11, 2016. [735] J. Tombs, L. Tarassenko, G. Cairns, and A. Murray, “Cascadability and in-situ learning for vlsi multi-layer networks,” in Artiﬁcial Neural Networks, 1993., Third International Conference on. IET, 1993, pp. 56–60. [736] Y. Tsividis and D. Anastassiou, “Switched-capacitor neural networks,” Electronics Letters, vol. 23, no. 18, pp. 958–959, 1987. [737] M. Valle, D. D. Caviglia, and G. M. Bisio, “Back-propagation learning algorithms for analog vlsi implementation,” in VLSI for Neural Networks and Artiﬁcial Intelligence. Springer, 1994, pp. 35–44. [738] ——, “An experimental analog vlsi neural network with on-chip back-propagation learning,” Analog Integrated Circuits and Signal Processing, vol. 9, no. 3, pp. 231–245, 1996. [739] J. Van der Spiegel, P. Mueller, D. Blackman, P. Chance, C. Donham, R. Etienne-Cummings, and P. Kinget, “An analog neural computer with modular architecture for real-time dynamic computations,” Solid- State Circuits, IEEE Journal of, vol. 27, no. 1, pp. 82–92, 1992. [740] M. Walker, P. Hasler, and L. Akers, “A cmos neural network for pattern association,” IEEE Micro, no. 5, pp. 68–74, 1989. [741] Y. Wang, “Analog cmos implementation of backward error propaga- tion,” in Neural Networks, 1993., IEEE International Conference on. IEEE, 1993, pp. 701–706. [742] K. Wawryn and B. Strzeszewski, “Current mode circuits for pro- grammable wta neural network,” Analog Integrated Circuits and Signal Processing, vol. 27, no. 1, pp. 49–69, 2001. [743] K. Wawryn and A. Mazurek, “Low power, current mode circuits for programmable neural network,” in Circuits and Systems, 2001. ISCAS 2001. The 2001 IEEE International Symposium on, vol. 3. IEEE, 2001, pp. 628–631. [744] D. J. Weller and R. R. Spencer, “A process invariant analog neural network ic with dynamically refreshed weights,” in Circuits and Systems, 1990., Proceedings of the 33rd Midwest Symposium on. IEEE, 1990, pp. 273–276. [745] H. Withagen, “Implementing backpropagation with analog hardware,” in Neural Networks, 1994. IEEE World Congress on Computational Intelligence., 1994 IEEE International Conference on, vol. 4. IEEE, 1994, pp. 2015–2017. [746] S. Wolpert, L. Lee, J. F. Heisler et al., “Circuits for a vlsi-based standalone backpropagation neural network,” in Bioengineering Con- ference, 1992., Proceedings of the 1992 Eighteenth IEEE Annual Northeast. IEEE, 1992, pp. 47–48. [747] T. Yildirim and J. S. Marsland, “A conic section function network synapse and neuron implementation in vlsi hardware,” in Neural Networks, 1996., IEEE International Conference on, vol. 2. IEEE, 1996, pp. 974–979. [748] M. Yildiz, S. Minaei, and I. C. G¨oknar, “A cmos classiﬁer circuit using neural networks with novel architecture,” Neural Networks, IEEE Transactions on, vol. 18, no. 6, pp. 1845–1850, 2007. [749] M. Lee, K. Hwang, and W. Sung, “Fault tolerance analysis of digital feed-forward deep neural networks,” in Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on. IEEE, 2014, pp. 5031–5035. [750] D. Christiani, C. Merkel, and D. Kudithipudi, “Invited: Towards a scalable neuromorphic hardware for classiﬁcation and prediction with stochastic no-prop algorithms,” in Quality Electronic Design (ISQED), 2016 17th International Symposium on. IEEE, 2016, pp. 124–128. [751] F. Distante, M. Sami, R. Stefanelli, and G. S. Gajani, “A conﬁgurable array architecture for wsi implementation of neural nets,” in Com- puters and Communications, 1990. Conference Proceedings., Ninth Annual International Phoenix Conference on. IEEE, 1990, pp. 44– 51. [752] ——, “Area compaction in silicon structures for neural net implemen- tation,” Microprocessing and microprogramming, vol. 28, no. 1, pp. 139–143, 1990. 41 [753] W. Fornaciari and F. Salice, “A low latency digital neural network architecture,” in VLSI for Neural Networks and Artiﬁcial Intelligence. Springer, 1994, pp. 81–91. [754] D. Hammerstrom, “A vlsi architecture for high-performance, low- cost, on-chip learning,” in Neural Networks, 1990., 1990 IJCNN International Joint Conference on. IEEE, 1990, pp. 537–544. [755] ——, “A highly parallel digital architecture for neural network emulation,” in VLSI for artiﬁcial intelligence and neural networks. Springer, 1991, pp. 357–366. [756] T. Islam, S. Ghosh, and H. Saha, “Ann-based signal conditioning and its hardware implementation of a nanostructured porous silicon relative humidity sensor,” Sensors and Actuators B: Chemical, vol. 120, no. 1, pp. 130–141, 2006. [757] Y.-C. Kim and M. Shanblatt, “An implementable digital multilayer neural network (dmnn),” in Neural Networks, 1992. IJCNN., Interna- tional Joint Conference on, vol. 2. IEEE, 1992, pp. 594–600. [758] Y.-C. Kim, M. Shanblatt et al., “A vlsi-based digital multilayer neural network architecture,” in VLSI, 1993.’Design Automation of High Performance VLSI Systems’, Proceedings., Third Great Lakes Symposium on. IEEE, 1993, pp. 27–31. [759] S. Kumar, K. Forward, and M. Palaniswami, “Performance evaluation of a risc neuro-processor for neural networks,” in High Performance Computing, 1996. Proceedings. 3rd International Conference on. IEEE, 1996, pp. 351–356. [760] S. Kung and J. Hwang, “Digital vlsi architectures for neural net- works,” in Circuits and Systems, 1989., IEEE International Sympo- sium on. IEEE, 1989, pp. 445–448. [761] L. Larsson, S. Krol, and K. Lagemann, “Neneb-an application ad- justable single chip neural network processor for mobile real time image processing,” in Neural Networks for Identiﬁcation, Control, Robotics, and Signal/Image Processing, 1996. Proceedings., Interna- tional Workshop on. IEEE, 1996, pp. 154–162. [762] D. Myers and G. Brebner, “The implementation of hardware neural net systems,” in Artiﬁcial Neural Networks, 1989., First IEE Interna- tional Conference on (Conf. Publ. No. 313). IET, 1989, pp. 57–61. [763] S. Pakzad and P. Plaskonos, “Implementation of a digital modular chip for a reconﬁgurable artiﬁcial neural network,” in PARLE’93 Parallel Architectures and Languages Europe. Springer, 1993, pp. 700–703. [764] G. G. Pechanek, S. Vassiliadis, J. G. Delgado-Frias, and G. Tri- antafyllos, “Scalable completely connected digital neural networks,” in Neural Networks, 1994. IEEE World Congress on Computational Intelligence., 1994 IEEE International Conference on, vol. 4. IEEE, 1994, pp. 2078–2083. [765] P. Plaskonos, S. Pakzad, B. Jin, and A. Hurson, “Design of a modular chip for a reconﬁgurable artiﬁcial neural network,” in Developing and Managing Intelligent System Projects, 1993., IEEE International Conference on. IEEE, 1993, pp. 55–62. [766] S. Popescu, “Hardware implementation of fast neural networks using cpld,” in Neural Network Applications in Electrical Engineering, 2000. NEUREL 2000. Proceedings of the 5th Seminar on. IEEE, 2000, pp. 121–124. [767] T. Szab´o, L. Antoni, G. Horv´ath, and B. Feh´er, “A full-parallel digital implementation for pre-trained nns,” in Neural Networks, 2000. IJCNN 2000, Proceedings of the IEEE-INNS-ENNS International Joint Conference on, vol. 2. IEEE, 2000, pp. 49–54. [768] C. Tang and H. Kwan, “Digital implementation of neural networks with quantized neurons,” in Circuits and Systems, 1997. ISCAS’97., Proceedings of 1997 IEEE International Symposium on, vol. 1. IEEE, 1997, pp. 649–652. [769] M. S. Tomlinson Jr and D. J. Walker, “Dnna: A digital neural network architecture,” in International Neural Network Conference. Springer, 1990, pp. 589–592. [770] M. S. Tomlinson Jr, D. J. Walker, and M. A. Sivilotti, “A digital neural network architecture for vlsi,” in Neural Networks, 1990., 1990 IJCNN International Joint Conference on. IEEE, 1990, pp. 545–550. [771] E. Torbey and B. Haroun, “Architectural synthesis for digital neural networks,” in Neural Networks, 1992. IJCNN., International Joint Conference on, vol. 2. IEEE, 1992, pp. 601–606. [772] D. Zhang, G. A. Jullien, W. C. Miller, and E. Swartzlander Jr, “Arithmetic for digital neural networks,” in Computer Arithmetic, 1991. Proceedings., 10th IEEE Symposium on. IEEE, 1991, pp. 58–63. [773] K. Aihara, O. Fujita, and K. Uchimura, “A digital neural network lsi using sparse memory access architecture,” in Microelectronics for Neural Networks, 1996., Proceedings of Fifth International Confer- ence on. IEEE, 1996, pp. 139–148. [774] N. Avellana, A. Strey, R. Holgado, J. A. Fernandes, R. Capillas, and E. Valderrama, “Design of a low-cost and high-speed neurocomputer system,” in Microelectronics for Neural Networks, 1996., Proceedings of Fifth International Conference on. IEEE, 1996, pp. 221–226. [775] J. Ayala, M. L´opez-Vallejo et al., “Design of a pipelined hardware ar- chitecture for real-time neural network computations,” in Circuits and Systems, 2002. MWSCAS-2002. The 2002 45th Midwest Symposium on, vol. 1. IEEE, 2002, pp. I–419. [776] A. Bermak and D. Martinez, “Digital vlsi implementation of a multi- precision neural network classiﬁer,” in Neural Information Processing, 1999. Proceedings. ICONIP’99. 6th International Conference on, vol. 2. IEEE, 1999, pp. 560–565. [777] A. Bermak and A. Bouzerdoum, “Vlsi implementation of a neural network classiﬁer based on the saturating linear activation function,” in Neural Information Processing, 2002. ICONIP’02. Proceedings of the 9th International Conference on, vol. 2. IEEE, 2002, pp. 981– 985. [778] C.-F. Chang and B. Sheu, “Digital vlsi multiprocessor design for neurocomputers,” in Neural Networks, 1992. IJCNN., International Joint Conference on, vol. 2. IEEE, 1992, pp. 1–6. [779] F. Distante, M. Sami, R. Stefanelli, and G. Storti-Gajani, “A compact and fast silicon implementation for layered neural nets,” in VLSI for Artiﬁcial Intelligence and Neural Networks. Springer, 1991, pp. 345– 355. [780] M. Duranton and N. Mauduit, “A general purpose digital architecture for neural network simulations,” in Artiﬁcial Neural Networks, 1989., First IEE International Conference on (Conf. Publ. No. 313). IET, 1989, pp. 62–66. [781] H. Faiedh, Z. Gafsi, K. Torki, and K. Besbes, “Digital hardware implementation of a neural network used for classiﬁcation,” in Mi- croelectronics, 2004. ICM 2004 Proceedings. The 16th International Conference on. IEEE, 2004, pp. 551–554. [782] C. Joseph and A. Gupta, “A novel hardware efﬁcient digital neural network architecture implemented in 130nm technology,” in Computer and Automation Engineering (ICCAE), 2010 The 2nd International Conference on, vol. 3. IEEE, 2010, pp. 82–87. [783] D. Y. Kim, J. M. Kim, H. Jang, J. Jeong, and J. W. Lee, “A neural network accelerator for mobile application processors,” IEEE Transactions on Consumer Electronics, vol. 61, no. 4, pp. 555–563, 2015. [784] D. Orrey, D. Myers, and J. Vincent, “A high performance digital pro- cessor for implementing large artiﬁcial neural networks,” in Custom Integrated Circuits Conference, 1991., Proceedings of the IEEE 1991. IEEE, 1991, pp. 16–3. [785] J. Tuazon, K. Hamidian, and L. Guyette, “A new digital neural network and its application,” in Electrical and Computer Engineering, 1993. Canadian Conference on. IEEE, 1993, pp. 481–485. [786] J. Vincent and D. Myers, “Parameter selection for digital realisations of neural networks,” in Neural Networks: Design Techniques and Tools, IEE Colloquium on. IET, 1991, pp. 7–1. [787] M. Walker and L. Akers, “A neuromorphic approach to adaptive dig- ital circuitry,” in Computers and Communications, 1988. Conference Proceedings., Seventh Annual International Phoenix Conference on. IEEE, 1988, pp. 19–23. [788] C. Alippi and M. E. Nigri, “Hardware requirements to digital vlsi implementation of neural networks,” in Neural Networks, 1991. 1991 IEEE International Joint Conference on. IEEE, 1991, pp. 1873–1878. [789] J.-H. Chung, H. Yoon, and S. R. Maeng, “A systolic array exploiting the inherent parallelisms of artiﬁcial neural networks,” Microprocess- ing and Microprogramming, vol. 33, no. 3, pp. 145–159, 1992. [790] J. Cloutier and P. Y. Simard, “Hardware implementation of the backpropagation without multiplication,” in Microelectronics for Neu- ral Networks and Fuzzy Systems, 1994., Proceedings of the Fourth International Conference on. IEEE, 1994, pp. 46–55. [791] M. Duranton and J. Sirat, “Learning on vlsi: A general purpose digital neurochip,” in Neural Networks, 1989. IJCNN., International Joint Conference on. IEEE, 1986, pp. 613–vol. [792] H. Eguchi, T. Furuta, H. Horiguchi, S. Oteki, and T. Kitaguchi, “Neural network lsi chip with on-chip learning,” in Neural Networks, 1991., IJCNN-91-Seattle International Joint Conference on, vol. 1. IEEE, 1991, pp. 453–456. [793] Y. Kondo, Y. Koshiba, Y. Arima, M. Murasaki, T. Yamada, H. Amishiro, H. Shinohara, and H. Mori, “A 1.2 gﬂops neural network chip exhibiting fast convergence,” in Solid-State Circuits Conference, 1994. Digest of Technical Papers. 41st ISSCC., 1994 IEEE International. IEEE, 1994, pp. 218–219. 42 [794] H. Madokoro and K. Sato, “Hardware implementation of back- propagation neural networks for real-time video image learning and processing,” Journal of Computers, vol. 8, no. 3, pp. 559–566, 2013. [795] D. J. Myers, J. M. Vincent, and D. A. Orrey, “Hannibal: A vlsi building block for neural networks with on-chip backpropagation learning,” Neurocomputing, vol. 5, no. 1, pp. 25–37, 1993. [796] S. Oteki, A. Hashimoto, T. Furuta, S. Motomura, T. Watanabe, D. Stork, and H. Eguchi, “A digital neural network vlsi with on- chip learning using stochastic pulse encoding,” in Neural Networks, 1993. IJCNN’93-Nagoya. Proceedings of 1993 International Joint Conference on, vol. 3. IEEE, 1993, pp. 3039–3045. [797] O. Saito, K. Aihara, O. Fujita, and K. Uchimura, “A 1m synapse self-learning digital neural network chip,” in Solid-State Circuits Con- ference, 1998. Digest of Technical Papers. 1998 IEEE International. IEEE, 1998, pp. 94–95. [798] Y. Sato, K. Shibata, M. Asai, M. Ohki, M. Sugie, T. Sak- aguchi, M. Hashimoto, and Y. Kuwabara, “Development of a high- performance general purpose neuro-computer composed of 512 digital neurons,” in Neural Networks, 1993. IJCNN’93-Nagoya. Proceedings of 1993 International Joint Conference on, vol. 2. IEEE, 1993, pp. 1967–1970. [799] Z. Tang, O. Ishizuka, and H. Matsumoto, “Backpropagation learning in analog t-model neural network hardware,” in Neural Networks, 1993. IJCNN’93-Nagoya. Proceedings of 1993 International Joint Conference on, vol. 1. IEEE, 1993, pp. 899–902. [800] J. Theeten, M. Duranton, N. Mauduit, and J. Sirat, “The lneuro-chip: a digital vlsi with on-chip learning mechanism,” in International Neural Network Conference. Springer, 1990, pp. 593–596. [801] Q. Wang, A. Li, Z. Li, and Y. Wan, “A design and implementation of reconﬁgurable architecture for neural networks based on systolic arrays,” in Advances in Neural Networks-ISNN 2006. Springer, 2006, pp. 1328–1333. [802] J. Wawrzynek, K. Asanovic, and N. Morgan, “The design of a neuro- microprocessor,” IEEE transactions on neural networks/a publication of the IEEE Neural Networks Council, vol. 4, no. 3, pp. 394–399, 1992. [803] S. B. Yun, Y. J. Kim, S. S. Dong, and C. H. Lee, “Hardware implementation of neural network with expansible and reconﬁgurable architecture,” in Neural Information Processing, 2002. ICONIP’02. Proceedings of the 9th International Conference on, vol. 2. IEEE, 2002, pp. 970–975. [804] R. Inigo, A. Bonde, and B. Holcombe, “Self adjusting weights for hardware neural networks,” Electronics Letters, vol. 26, pp. 1630– 1632, 1990. [805] H. Hikawa, “Digital pulse mode neural network with simple synapse multiplier,” in Circuits and Systems, 2001. ISCAS 2001. The 2001 IEEE International Symposium on, vol. 3. IEEE, 2001, pp. 569– 572. [806] ——, “Hardware pulse mode neural network with piecewise linear activation function neurons,” in Circuits and Systems, 2002. ISCAS 2002. IEEE International Symposium on, vol. 2. IEEE, 2002, pp. II–524. [807] Y.-C. Kim and M. A. Shanblatt, “Architecture and statistical model of a pulse-mode digital multilayer neural network,” Neural Networks, IEEE Transactions on, vol. 6, no. 5, pp. 1109–1118, 1995. [808] ——, “Random noise effects in pulse-mode digital multilayer neural networks,” Neural Networks, IEEE Transactions on, vol. 6, no. 1, pp. 220–229, 1995. [809] A. N. Al-Zeftawi, K. M. A. El-Fattah, H. N. Shanan, and T. S. Kamel, “Cmos mixed digital analog reconﬁgurable neural network with gaussian synapses,” in Electrotechnical Conference, 2000. MELECON 2000. 10th Mediterranean, vol. 3. IEEE, 2000, pp. 1198–1201. [810] O. Barkan, W. Smith, and G. Persky, “Design of coupling resistor networks for neural network hardware,” Circuits and Systems, IEEE Transactions on, vol. 37, no. 6, pp. 756–765, 1990. [811] J. Binfet and B. M. Wilamowski, “Microprocessor implementation of fuzzy systems and neural networks,” in Neural Networks, 2001. Proceedings. IJCNN’01. International Joint Conference on, vol. 1. IEEE, 2001, pp. 234–239. [812] J.-C. Bor and C.-Y. Wu, “Pulse-width-modulation feedforward neural network design with on-chip learning,” in Circuits and Systems, 1996., IEEE Asia Paciﬁc Conference on. IEEE, 1996, pp. 369–372. [813] B. E. Boser, E. Sackinger, J. Bromley, Y. Le Cun, and L. D. Jackel, “An analog neural network processor with programmable topology,” Solid-State Circuits, IEEE Journal of, vol. 26, no. 12, pp. 2017–2025, 1991. [814] F. Camboni and M. Valle, “A mixed mode perceptron cell for vlsi neural networks,” in Electronics, Circuits and Systems, 2001. ICECS 2001. The 8th IEEE International Conference on, vol. 1. IEEE, 2001, pp. 377–380. [815] G. Cardarilli, C. D’Alessandro, P. Marinucci, and F. Bordoni, “Vlsi implementation of a modular and programmable neural architecture,” in Microelectronics for Neural Networks and Fuzzy Systems, 1994., Proceedings of the Fourth International Conference on. IEEE, 1994, pp. 218–225. [816] G. Cardarilli, G. Di Stefano, G. Fabrizi, and P. Marinucci, “Analysis and implementation of a vlsi neural network,” in Neural Networks, 1995. Proceedings., IEEE International Conference on, vol. 3. IEEE, 1995, pp. 1482–1486. [817] D. D. Corso, F. Gregoretti, L. Reyneri, and A. Allasia, “A pattern recognition demonstrator based on a silicon neural chip,” in Solid- State Circuits Conference, 1992. ESSCIRC’92. Eighteenth European. IEEE, 1992, pp. 207–212. [818] K. Current and J. Current, “Cmos current-mode circuits for neural networks,” in Circuits and Systems, 1990., IEEE International Sym- posium on. IEEE, 1990, pp. 2971–2974. [819] I. Del Campo, J. Echanobe, G. Bosque, and J. M. Tarela, “Efﬁcient hardware/software implementation of an adaptive neuro-fuzzy sys- tem,” Fuzzy Systems, IEEE Transactions on, vol. 16, no. 3, pp. 761– 778, 2008. [820] H. Djahanshahi, M. Ahmadi, G. Jullien, and W. Miller, “A uniﬁed synapse-neuron building block for hybrid vlsi neural networks,” in Circuits and Systems, 1996. ISCAS’96., Connecting the World., 1996 IEEE International Symposium on, vol. 3. IEEE, 1996, pp. 483–486. [821] ——, “A modular architecture for hybrid vlsi neural networks and its application in a smart photosensor,” in Neural Networks, 1996., IEEE International Conference on, vol. 2. IEEE, 1996, pp. 868–873. [822] H. Djahanshahi, M. Ahmadi, G. A. Jullien, and W. C. Miller, “Design and vlsi implementation of a uniﬁed synapse-neuron architecture,” in VLSI, 1996. Proceedings., Sixth Great Lakes Symposium on. IEEE, 1996, pp. 228–233. [823] H. Djahanshahi, M. Ahmadi, G. Jullien, and W. Miller, “A self- scaling neural hardware structure that reduces the effect of some implementation errors,” in Neural Networks for Signal Processing [1997] VII. Proceedings of the 1997 IEEE Workshop. IEEE, 1997, pp. 588–597. [824] H. Djahanshahi, M. Ahmadi, G. A. Jullien, and W. C. Miller, “Neural network integrated circuits with single-block mixed-signal arrays,” in Signals, Systems &amp; Computers, 1997. Conference Record of the Thirty-First Asilomar Conference on, vol. 2. IEEE, 1997, pp. 1130– 1135. [825] B. Erkmen, R. A. Vural, N. Kahraman, and T. Yildirim, “A mixed mode neural network circuitry for object recognition application,” Circuits, Systems, and Signal Processing, vol. 32, no. 1, pp. 29–46, 2013. [826] S. M. Fakhraie, H. Farshbaf, and K. C. Smith, “Scalable closed- boundary analog neural networks,” Neural Networks, IEEE Trans- actions on, vol. 15, no. 2, pp. 492–504, 2004. [827] J. Fieres, A. Gr¨ubl, S. Philipp, K. Meier, J. Schemmel, and F. Sch¨urmann, “A platform for parallel operation of vlsi neural networks,” in Proc. of the 2004 Brain Inspired Cognitive Systems Conference (BICS2004), 2004. [828] J. Franca et al., “A mixed-mode architecture for implementation of analog neural networks with digital programmability,” in Neural Net- works, 1993. IJCNN’93-Nagoya. Proceedings of 1993 International Joint Conference on, vol. 1. IEEE, 1993, pp. 887–890. [829] H. P. Graf, R. Janow, D. Henderson, and R. Lee, “Reconﬁgurable neural net chip with 32k connections.” in NIPS, 1990, pp. 1032–1038. [830] L. D. Jackel, H. Graf, and R. Howard, “Electronic neural network chips,” Applied optics, vol. 26, no. 23, pp. 5077–5080, 1987. [831] L. Jackel, B. Boser, H. Graf, J. Denker, Y. Le Cun, D. Henderson, O. Matan, R. Howard, and H. Baird, “Vlsi implementations of electronic neural networks: An example in character recognition,” in Systems, Man and Cybernetics, 1990. Conference Proceedings., IEEE International Conference on. IEEE, 1990, pp. 320–322. [832] L. Jackel, B. Boser, J. Denker, H. Graf, Y. Le Cun, I. Guyon, D. Henderson, R. Howard, W. Hubbard, and S. Solla, “Hardware requirements for neural-net optical character recognition,” in Neural Networks, 1990., 1990 IJCNN International Joint Conference on. IEEE, 1990, pp. 855–861. [833] D. Johnson, J. Marsland, and W. Eccleston, “Neural network imple- mentation using a single most per synapse,” Neural Networks, IEEE Transactions on, vol. 6, no. 4, pp. 1008–1011, 1995. 43 [834] V. E. Koosh and R. Goodman, “Vlsi neural network with digital weights and analog multipliers,” in Circuits and Systems, 2001. ISCAS 2001. The 2001 IEEE International Symposium on, vol. 3. IEEE, 2001, pp. 233–236. [835] V. F. Koosh and R. M. Goodman, “Analog vlsi neural network with digital perturbative learning,” Circuits and Systems II: Analog and Digital Signal Processing, IEEE Transactions on, vol. 49, no. 5, pp. 359–368, 2002. [836] J. H. Lee, X. Ma, and K. Likharev, “Cmol crossnets: Possible neu- romorphic nanoelectronic circuits,” Advances in Neural Information Processing Systems, vol. 18, p. 755, 2006. [837] T. Lehmann, E. Bruun, and C. Dietrich, “Mixed analog/digital matrix- vector multiplier for neural network synapses,” Analog Integrated Circuits and Signal Processing, vol. 9, no. 1, pp. 55–63, 1996. [838] K. K. Likharev, “Crossnets: Neuromorphic hybrid cmos/nanoelectronic networks,” Advanced Materials, vol. 3, pp. 322–331, 2011. [839] J. Liu, M. A. Brooke, and K. Hirotsu, “A cmos feedforward neural- network chip with on-chip parallel learning for oscillation cancella- tion,” Neural Networks, IEEE Transactions on, vol. 13, no. 5, pp. 1178–1186, 2002. [840] Y. Liu, L. Zhang, S. Shan, P. Sun, X. Yang, B. Wang, D. Shi, P. Hui, and X. Lin, “Research on compensation for pressure sensor thermal zero drift based on back propagation neural network implemented by hardware circuits,” in Natural Computation (ICNC), 2010 Sixth International Conference on, vol. 2. IEEE, 2010, pp. 796–800. [841] Y. Maeda, A. Nakazawa, and Y. Kanata, “Hardware implementation of a pulse density neural network using simultaneous perturbation learning rule,” Analog Integrated Circuits and Signal Processing, vol. 18, no. 2-3, pp. 153–162, 1999. [842] P. Masa, K. Hoen, and H. Wallinga, “High speed vlsi neural network for high-energy physics,” in Microelectronics for Neural Networks and Fuzzy Systems, 1994., Proceedings of the Fourth International Conference on. IEEE, 1994, pp. 422–428. [843] B. Maundy and E. El-Masry, “Switched-capacitor realisations of arti- ﬁcial neural network learning schemes,” Electronics Letters, vol. 27, no. 1, pp. 85–87, 1991. [844] M. Mirhassani, M. Ahmadi, and W. C. Miller, “A mixed-signal vlsi neural network with on-chip learning,” in Electrical and Computer Engineering, 2003. IEEE CCECE 2003. Canadian Conference on, vol. 1. IEEE, 2003, pp. 591–594. [845] M. Mirhassani, M. Ahmadi, and W. Miller, “A new mixed-signal feed- forward neural network with on-chip learning,” in Neural Networks, 2004. Proceedings. 2004 IEEE International Joint Conference on, vol. 3. IEEE, 2004, pp. 1729–1734. [846] M. Mirhassani, M. Ahmadi, and W. C. Miller, “Design and implemen- tation of novel multi-layer mixed-signal on-chip neural networks,” in Circuits and Systems, 2005. 48th Midwest Symposium on. IEEE, 2005, pp. 413–416. [847] ——, “A feed-forward time-multiplexed neural network with mixed- signal neuron–synapse arrays,” Microelectronic engineering, vol. 84, no. 2, pp. 300–307, 2007. [848] T. Morie, J. Funakoshi, M. Nagata, and A. Iwata, “An analog- digital merged neural circuit using pulse width modulation technique,” Analog Integrated Circuits and Signal Processing, vol. 25, no. 3, pp. 319–328, 2000. [849] A. Nosratinia, M. Ahmadi, M. Shridhar, and G. Jullien, “A hybrid architecture for feed-forward multi-layer neural networks,” in Circuits and Systems, 1992. ISCAS’92. Proceedings., 1992 IEEE International Symposium on, vol. 3. IEEE, 1992, pp. 1541–1544. [850] H. Pan, M. Manic, X. Li, and B. Wilamowski, “Multilevel logic multiplier using vlsi neural network,” in Industrial Technology, 2003 IEEE International Conference on, vol. 1. IEEE, 2003, pp. 327–332. [851] A. Passos Almeida and J. Franca, “A mixed-mode architecture for implementation of analog neural networks with digital programma- bility,” in Neural Networks, 1993. IJCNN’93-Nagoya. Proceedings of 1993 International Joint Conference on, vol. 1. IEEE, 1993, pp. 887–890. [852] E. Sackinger, B. E. Boser, J. Bromley, Y. LeCun, and L. D. Jackel, “Application of the anna neural network chip to high-speed character recognition.” IEEE transactions on neural networks/a publication of the IEEE Neural Networks Council, vol. 3, no. 3, pp. 498–505, 1991. [853] E. S¨ackinger, B. E. Boser, and L. D. Jackel, “A neurocomputer board based on the anna neural network chip,” in Advances in Neural Information Processing Systems, 1992, pp. 773–780. [854] G. Sanchez, T. J. Koickal, T. Sripad, L. C. Gouveia, A. Hamilton, J. Madrenas et al., “Spike-based analog-digital neuromorphic infor- mation processing system for sensor applications,” in Circuits and Systems (ISCAS), 2013 IEEE International Symposium on. IEEE, 2013, pp. 1624–1627. [855] J. Schemmel, S. Hohmann, K. Meier, and F. Sch¨urmann, “A mixed- mode analog neural network using current-steering synapses,” Analog Integrated Circuits and Signal Processing, vol. 38, no. 2-3, pp. 233– 244, 2004. [856] A. Schmid, Y. Leblebici, and D. Mlynek, “A two-stage charge- based analog/digital neuron circuit with adjustable weights,” in Neural Networks, 1999. IJCNN’99. International Joint Conference on, vol. 4. IEEE, 1999, pp. 2357–2362. [857] D. Van den Bout, P. Franzon, J. Paulos, T. Miller, W. Snyder, T. Nagle, and W. Liu, “Scalable vlsi implementations for neural networks,” Journal of VLSI signal processing systems for signal, image and video technology, vol. 1, no. 4, pp. 367–385, 1990. [858] K. Watanabe, L. Wang, H.-W. Cha, and S. Ogawa, “A current-mode approach to cmos neural network implementation,” in Algorithms and Architectures for Parallel Processing, 1997. ICAPP 97., 1997 3rd International Conference on. IEEE, 1997, pp. 625–637. [859] J. Yang, M. Ahmadi, G. A. Jullien, and W. C. Miller, “An in- the-loop training method for vlsi neural networks,” in Circuits and Systems, 1999. ISCAS’99. Proceedings of the 1999 IEEE International Symposium on, vol. 5. IEEE, 1999, pp. 619–622. [860] N. Yazdi, M. Ahmadi, G. A. Jullien, and M. Shridhar, “Pipelined analog multi-layer feedforward neural networks,” in Circuits and Systems, 1993., ISCAS’93, 1993 IEEE International Symposium on. IEEE, 1993, pp. 2768–2771. [861] G. Zatorre, N. Medrano, M. T. Sanz, P. Mart´ınez, S. Celma et al., “Mixed-mode artiﬁcial neuron for cmos integration,” in Electrotechni- cal Conference, 2006. MELECON 2006. IEEE Mediterranean. IEEE, 2006, pp. 381–384. [862] J. B¨urger and C. Teuscher, “Volatile memristive devices as short-term memory in a neuromorphic learning architecture,” in Proceedings of the 2014 IEEE/ACM International Symposium on Nanoscale Archi- tectures. ACM, 2014, pp. 104–109. [863] Z. Dong, S. Duan, X. Hu, L. Wang, and H. Li, “A novel memristive multilayer feedforward small-world neural network with its applica- tions in pid control,” The Scientiﬁc World Journal, vol. 2014, 2014. [864] A. Emelyanov, V. Demin, D. Lapkin, V. Erokhin, S. Battistoni, G. Baldi, S. Iannotta, P. Kashkarov, and M. Kovalchuk, “Pani- based neuromorphic networks???????????? ﬁrst results and close perspectives,” in Memristive Systems (MEMRISYS) 2015 International Conference on. IEEE, 2015, pp. 1–2. [865] A. Emelyanov, D. Lapkin, V. Demin, V. Erokhin, S. Battistoni, G. Baldi, A. Dimonte, A. Korovin, S. Iannotta, P. Kashkarov et al., “First steps towards the realization of a double layer perceptron based on organic memristive devices,” AIP Advances, vol. 6, no. 11, p. 111301, 2016. [866] H. Manem, K. Beckmann, M. Xu, R. Carroll, R. Geer, and N. C. Cady, “An extendable multi-purpose 3d neuromorphic fabric using nanoscale memristors,” in Computational Intelligence for Security and Defense Applications (CISDA), 2015 IEEE Symposium on. IEEE, 2015, pp. 1–8. [867] C. Merkel, D. Kudithipudi, and R. Ptucha, “Heterogeneous cmos/memristor hardware neural networks for real-time target classi- ﬁcation,” in SPIE Sensing Technology+ Applications. International Society for Optics and Photonics, 2014, pp. 911 908–911 908. [868] O. ˇSuch, M. Klimo, and O. ˇSkvarek, “Phoneme discrimination using a pair of neurons built from crs fuzzy logic gates,” in PROCEED- INGS OF THE INTERNATIONAL CONFERENCE ON NUMERICAL ANALYSIS AND APPLIED MATHEMATICS 2014 (ICNAAM-2014), vol. 1648. AIP Publishing, 2015, p. 280010. [869] P. Dong, G. L. Bilbro, and M.-Y. Chow, “Implementation of artiﬁcial neural network for real time applications using ﬁeld programmable analog arrays,” in Neural Networks, 2006. IJCNN’06. International Joint Conference on. IEEE, 2006, pp. 1518–1524. [870] M. Liu, H. Yu, and W. Wang, “Fpaa based on integration of cmos and nanojunction devices for neuromorphic applications,” in Nano- Net. Springer, 2009, pp. 44–48. [871] F. D. Baptista and F. Morgado-Dias, “Automatic general-purpose neural hardware generator,” Neural Computing and Applications, pp. 1–12, 2015. [872] J. Bastos, H. Figueroa, and A. Monti, “Fpga implementation of neural network-based controllers for power electronics applications,” in Ap- plied Power Electronics Conference and Exposition, 2006. APEC’06. Twenty-First Annual IEEE. IEEE, 2006, pp. 6–pp. 44 [873] J. Blake, L. Maguire, T. McGinnity, and L. McDaid, “Using xilinx fpgas to implement neural networks and fuzzy systems,” in Neural and Fuzzy Systems: Design, Hardware and Applications (Digest No: 1997/133), IEE Colloquium on. IET, 1997, pp. 1–1. [874] J. Blake, L. P. Maguire, T. McGinnity, B. Roche, and L. McDaid, “The implementation of fuzzy systems, neural networks and fuzzy neural networks using fpgas,” Information Sciences, vol. 112, no. 1, pp. 151–168, 1998. [875] M. Bohrn, L. Fujcik, and R. Vrba, “Field programmable neural array for feed-forward neural networks,” in Telecommunications and Signal Processing (TSP), 2013 36th International Conference on. IEEE, 2013, pp. 727–731. [876] M. Bonnici, E. J. Gatt, J. Micallef, and I. Grech, “Artiﬁcial neural network optimization for fpga,” in Electronics, Circuits and Systems, 2006. ICECS’06. 13th IEEE International Conference on. IEEE, 2006, pp. 1340–1343. [877] N. M. Botros and M. Abdul-Aziz, “Hardware implementation of an artiﬁcial neural network,” in Neural Networks, 1993., IEEE Interna- tional Conference on. IEEE, 1993, pp. 1252–1257. [878] A. L. Braga, J. Arias-Garcia, C. Llanos, M. Dorn, A. Foltran, and L. S. Coelho, “Hardware implementation of gmdh-type artiﬁcial neural net- works and its use to predict approximate three-dimensional structures of proteins,” in Reconﬁgurable Communication-centric Systems-on- Chip (ReCoSoC), 2012 7th International Workshop on. IEEE, 2012, pp. 1–8. [879] L. Brunelli, E. U. Melcher, A. V. De Brito, and R. Freire, “A novel approach to reduce interconnect complexity in ann hardware implementation,” in Neural Networks, 2005. IJCNN’05. Proceedings. 2005 IEEE International Joint Conference on, vol. 5. IEEE, 2005, pp. 2861–2866. [880] N. Chujo, S. Kuroyanagi, S. Doki, and S. Okuma, “An iterative cal- culation method of the neuron model for hardware implementation,” in Industrial Electronics Society, 2000. IECON 2000. 26th Annual Confjerence of the IEEE, vol. 1. IEEE, 2000, pp. 664–671. [881] B. Deng, M. Zhang, F. Su, J. Wang, X. Wei, and B. Shan, “The implementation of feedforward network on ﬁeld programmable gate array,” in Biomedical Engineering and Informatics (BMEI), 2014 7th International Conference on. IEEE, 2014, pp. 483–487. [882] P. D. Deotale and L. Dole, “Design of fpga based general purpose neural network,” in Information Communication and Embedded Sys- tems (ICICES), 2014 International Conference on. IEEE, 2014, pp. 1–5. [883] A. Dinu and M. Cirstea, “A digital neural network fpga direct hardware implementation algorithm,” in Industrial Electronics, 2007. ISIE 2007. IEEE International Symposium on. IEEE, 2007, pp. 2307– 2312. [884] A. Dinu, M. N. Cirstea, and S. E. Cirstea, “Direct neural-network hardware-implementation algorithm,” Industrial Electronics, IEEE Transactions on, vol. 57, no. 5, pp. 1845–1848, 2010. [885] P. Dondon, J. Carvalho, R. Gardere, P. Lahalle, G. Tsenov, and V. Mladenov, “Implementation of a feed-forward artiﬁcial neural net- work in vhdl on fpga,” in Neural Network Applications in Electrical Engineering (NEUREL), 2014 12th Symposium on. IEEE, 2014, pp. 37–40. [886] Y. Dong, C. Li, Z. Lin, and T. Watanabe, “A hybrid layer-multiplexing and pipeline architecture for efﬁcient fpga-based multilayer neural network,” Nonlinear Theory and Its Applications, IEICE, vol. 2, no. 4, pp. 522–532, 2011. [887] G.-P. Economou, E. Mariatos, N. Economopoulos, D. Lymberopoulos, and C. Goutis, “Fpga implementation of artiﬁcial neural networks: an application on medical expert systems,” in Microelectronics for Neural Networks and Fuzzy Systems, 1994., Proceedings of the Fourth International Conference on. IEEE, 1994, pp. 287–293. [888] P. Ehkan, L. Y. Ann, F. F. Zakaria, and M. N. M. Warip, “Artiﬁcial neural network for character recognition on embedded-based fpga,” in Future Information Technology. Springer, 2014, pp. 281–287. [889] S. Erdogan and A. Wahab, “Design of rm-nc: A reconﬁgurable neu- rocomputer for massively parallel-pipelined computations,” in Neural Networks, 1992. IJCNN., International Joint Conference on, vol. 2. IEEE, 1992, pp. 33–38. [890] P. Ferreira, P. Ribeiro, A. Antunes, and F. Dias, “Artiﬁcial neural networks processor–a hardware implementation using a fpga,” Field Programmable Logic and Application, pp. 1084–1086, 2004. [891] P. Ferreira, P. Ribeiro, A. Antunes, and F. M. Dias, “A high bit resolution fpga implementation of a fnn with a new algorithm for the activation function,” Neurocomputing, vol. 71, no. 1, pp. 71–77, 2007. [892] J. Granado, M. Vega, R. P´erez, J. S`anchez, J. G´omez et al., “Using fpgas to implement artiﬁcial neural networks,” in Electronics, Circuits and Systems, 2006. ICECS’06. 13th IEEE International Conference on. IEEE, 2006, pp. 934–937. [893] S. A. Guccione and M. J. Gonzalez, “A neural network implemen- tation using reconﬁgurable architectures,” in Selected papers from the Oxford 1993 international workshop on ﬁeld programmable logic and applications on More FPGAs, Oxford: Abingdon EE&CS Books, 1994, pp. 443–451. [894] S. Hariprasath and T. Prabakar, “Fpga implementation of multilayer feed forward neural network architecture using vhdl,” in Computing, Communication and Applications (ICCCA), 2012 International Con- ference on. IEEE, 2012, pp. 1–6. [895] H. M. Hasanien, “Fpga implementation of adaptive ann controller for speed regulation of permanent magnet stepper motor drives,” Energy Conversion and Management, vol. 52, no. 2, pp. 1252–1257, 2011. [896] H. Hikawa, “Implementation of simpliﬁed multilayer neural networks with on-chip learning,” in Neural Networks, 1995. Proceedings., IEEE International Conference on, vol. 4. IEEE, 1995, pp. 1633–1637. [897] S. Himavathi, D. Anitha, and A. Muthuramalingam, “Feedforward neural network implementation in fpga using layer multiplexing for effective resource utilization,” Neural Networks, IEEE Transactions on, vol. 18, no. 3, pp. 880–888, 2007. [898] G. R. Hoelzle and F. M. Dias, “Hardware implementation of an artiﬁcial neural network with an embedded microprocessor in a fpga,” in 8th International Conference and Workshop on Ambient Intelligence and Embedded Systems?, Funchal, 2009. [899] M. Hoffman, P. Bauer, B. Hemrnelman, and A. Hasan, “Hardware synthesis of artiﬁcial neural networks using ﬁeld programmable gate arrays and ﬁxed-point numbers,” in Region 5 Conference, 2006 IEEE. IEEE, 2006, pp. 324–328. [900] N. Izeboudjen, A. Farah, S. Titri, and H. Boumeridja, “Digital implementation of artiﬁcial neural networks: from vhdl description to fpga implementation,” in Engineering Applications of Bio-Inspired Artiﬁcial Neural Networks. Springer, 1999, pp. 139–148. [901] R. Joost and R. Salomon, “Time coding output neurons in digital artiﬁcial neural networks,” in Neural Networks (IJCNN), The 2012 International Joint Conference on. IEEE, 2012, pp. 1–8. [902] S. Jung and S. S. Kim, “Hardware implementation of a real-time neural network controller with a dsp and an fpga for nonlinear systems,” Industrial Electronics, IEEE Transactions on, vol. 54, no. 1, pp. 265–271, 2007. [903] S. S. Kim and S. Jung, “Hardware implementation of a real time neural network controller with a dsp and an fpga,” in Robotics and Automation, 2004. Proceedings. ICRA’04. 2004 IEEE International Conference on, vol. 5. IEEE, 2004, pp. 4639–4644. [904] M. Krcma, J. Kastil, and Z. Kotasek, “Mapping trained neural networks to fpnns,” in Design and Diagnostics of Electronic Circuits & Systems (DDECS), 2015 IEEE 18th International Symposium on. IEEE, 2015, pp. 157–160. [905] M. Krips, T. Lammert, and A. Kummert, “Fpga implementation of a neural network for a real-time hand tracking system,” in Electronic Design, Test and Applications, 2002. Proceedings. The First IEEE International Workshop on. IEEE, 2002, pp. 313–317. [906] C.-h. Kung, M. J. Devaney, C.-M. Kung, C.-m. Huang, Y.-j. Wang, and C.-T. Kuo, “The vlsi implementation of an artiﬁcial neural network scheme embedded in an automated inspection quality management system,” in Instrumentation and Measurement Technology Confer- ence, 2002. IMTC/2002. Proceedings of the 19th IEEE, vol. 1. IEEE, 2002, pp. 239–244. [907] A. Laudani, G. M. Lozito, F. R. Fulginei, and A. Salvini, “An efﬁcient architecture for ﬂoating point based miso neural neworks on fpga,” in Computer Modelling and Simulation (UKSim), 2014 UKSim-AMSS 16th International Conference on. IEEE, 2014, pp. 12–17. [908] U. Lotriˇc and P. Buli´c, “Applicability of approximate multipliers in hardware neural networks,” Neurocomputing, vol. 96, pp. 57–65, 2012. [909] G.-M. Lozito, A. Laudani, F. R. Fulginei, and A. Salvini, “Fpga implementations of feed forward neural network by using ﬂoating point hardware accelerators,” Advances in Electrical and Electronic Engineering, vol. 12, no. 1, p. 30, 2014. [910] H. H. Makwana, D. J. Shah, and P. P. Gandhi, “Fpga implementa- tion of artiﬁcial neural network,” International Journal of Emerging Technology and Advanced Engineering, vol. 3, no. 1, 2013. [911] N. P. Mand, F. Robino, and J. Oberg, “Artiﬁcial neural network emulation on noc based multi-core fpga platform,” in NORCHIP, 2012. IEEE, 2012, pp. 1–4. 45 [912] K. Mohamad, M. F. O. Mahmud, F. H. Adnan, and W. F. H. Abdullah, “Design of single neuron on fpga,” in Humanities, Science and Engineering Research (SHUSER), 2012 IEEE Symposium on. IEEE, 2012, pp. 133–136. [913] E. Z. Mohammed and H. K. Ali, “Hardware implementation of artiﬁcial neural network using ﬁeld programmable gate array,” Inter- national Journal of Computer Theory and Engineering, vol. 5, no. 5, 2013. [914] A. Muthuramalingam, S. Himavathi, and E. Srinivasan, “Neural network implementation using fpga: issues and application,” Inter- national journal of information technology, vol. 4, no. 2, pp. 86–92, 2008. [915] B. Noory and V. Groza, “A reconﬁgurable approach to hardware implementation of neural networks,” in Electrical and Computer Engineering, 2003. IEEE CCECE 2003. Canadian Conference on, vol. 3. IEEE, 2003, pp. 1861–1864. [916] S¸ . ONIGA and A. BUCHMAN, “A new method for hardware im- plementation of artiﬁcial neural network used in smart sensors,” in The 10th International Symposium for Design and Technology of Electronic Packages Conference Proceedings, Bucures¸ti. Citeseer, 2004, pp. 23–26. [917] S. Oniga, A. Tisan, D. Mic, A. Buchman, and A. Vida-Ratiu, “Hand postures recognition system using artiﬁcial neural networks implemented in fpga,” in Electronics Technology, 30th International Spring Seminar on. IEEE, 2007, pp. 507–512. [918] ——, “Optimizing fpga implementation of feed-forward neural net- works,” in Proceedings of the 11th International Conference on Optimization of Electrical and Electronic Equipment OPTIM, vol. 2008, 2008, pp. 22–23. [919] S. Oniga, A. Tisan, D. Mic, C. Lung, I. Orha, A. Buchman, and A. Vida-Ratiu, “Fpga implementation of feed-forward neural networks for smart devices development,” in Signals, Circuits and Systems, 2009. ISSCS 2009. International Symposium on. IEEE, 2009, pp. 1–4. [920] T. Orlowska-Kowalska and M. Kaminski, “Fpga implementation of the multilayer neural network for the speed estimation of the two- mass drive system,” Industrial Informatics, IEEE Transactions on, vol. 7, no. 3, pp. 436–445, 2011. [921] A. P´erez-Uribe and E. Sanchez, “Fpga implementation of an adaptable-size neural network,” in Artiﬁcial Neural Networks?ICANN 96. Springer, 1996, pp. 383–388. [922] Y. Qi, B. Zhang, T. M. Taha, H. Chen, and R. Hasan, “Fpga design of a multicore neuromorphic processing system,” in Aerospace and Electronics Conference, NAECON 2014-IEEE National. IEEE, 2014, pp. 255–258. [923] R. Raeisi and A. Kabir, “Implementation of artiﬁcial neural network on fpga,” in American Society for Engineering Education. Citeseer, 2006. [924] H. F. Restrepo, R. Hoffmann, A. Perez-Uribe, C. Teuscher, and E. Sanchez, “A networked fpga-based hardware implementation of a neural network application,” in Field-Programmable Custom Comput- ing Machines, 2000 IEEE Symposium on. IEEE, 2000, pp. 337–338. [925] S. Sahin, Y. Becerikli, and S. Yazici, “Neural network implemen- tation in hardware using fpgas,” in Neural Information Processing. Springer, 2006, pp. 1105–1112. [926] V. Salapura, M. Gschwind, and O. Maischberger, “A fast fpga im- plementation of a general purpose neuron,” in Field-Programmable Logic Architectures, Synthesis and Applications. Springer, 1994, pp. 175–182. [927] B. Salem, A. Karim, S. B. Othman, and S. B. Saoud, “Design and implementation of a neural command rule on a fpga circuit,” in Electronics, Circuits and Systems, 2005. ICECS 2005. 12th IEEE International Conference on. IEEE, 2005, pp. 1–4. [928] A. Savran and S. ¨Unsal, “Hardware implementation of a feed forward neural network using fpgas,” in The third International Conference on Electrical and Electronics Engineering (ELECO 2003), 2003, pp. 3–7. [929] S. Shaari, H. Mekki, N. Khorissi et al., “Fpga-based artiﬁcial neural network for prediction of solar radiation data from sunshine duration and air temperature,” in Computational Technologies in Electrical and Electronics Engineering, 2008. SIBIRCON 2008. IEEE Region 8 International Conference on. IEEE, 2008, pp. 118–123. [930] S. K. Shah and D. D. Vishwakarma, “Fpga implementation of ann for reactive routing protocols in manet,” in Communication, Networks and Satellite (ComNetSat), 2012 IEEE International Conference on. IEEE, 2012, pp. 11–14. [931] S. Shreejith, B. Anshuman, and S. A. Fahmy, “Accelerated artiﬁcial neural networks on fpga for fault detection in automotive systems,” in 2016 Design, Automation & Test in Europe Conference & Exhibition (DATE). IEEE, 2016, pp. 37–42. [932] A. M. Soares, J. O. Pinto, B. K. Bose, L. C. Leite, L. E. da Silva, and M. E. Romero, “Field programmable gate array (fpga) based neural network implementation of stator ﬂux oriented vector control of induction motor drive,” in Industrial Technology, 2006. ICIT 2006. IEEE International Conference on. IEEE, 2006, pp. 31–34. [933] D. Sonowal and M. Bhuyan, “Fpga implementation of neural network for linearization of thermistor characteristics,” in 2012 International Conference on Devices, Circuits and Systems (ICDCS), 2012. [934] T. WANG and L. WANG, “A modularization hardware implemen- tation approach for artiﬁcial neural network,” in 2nd International Conference on Electrical, Computer Engineering and Electronics, 2015. [935] J. Wang, S. Yang, B. Deng, X. Wei, and H. Yu, “Multi-fpga imple- mentation of feedforward network and its performance analysis,” in Control Conference (CCC), 2015 34th Chinese. IEEE, 2015, pp. 3457–3461. [936] D. Wang, L. Deng, P. Tang, C. Ma, and J. Pei, “Fpga-based neuro- morphic computing system with a scalable routing network,” in 2015 15th Non-Volatile Memory Technology Symposium (NVMTS). IEEE, 2015, pp. 1–4. [937] E. Won, “A hardware implementation of artiﬁcial neural networks using ﬁeld programmable gate arrays,” Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment, vol. 581, no. 3, pp. 816–820, 2007. [938] A. Youssef, K. Mohammed, and A. Nasar, “A reconﬁgurable, generic and programmable feed forward neural network implementation in fpga,” in Computer Modelling and Simulation (UKSim), 2012 UKSim 14th International Conference on. IEEE, 2012, pp. 9–13. [939] D. Zhang, H. Li, and S. Y. Foo, “A simpliﬁed fpga implementation of neural network algorithms integrated with stochastic theory for power electronics applications,” in Industrial Electronics Society, 2005. IECON 2005. 31st Annual Conference of IEEE. IEEE, 2005, pp. 6–pp. [940] D. Zhang and H. Li, “A low cost digital implementation of feed- forward neural networks applied to a variable-speed wind turbine system,” in Power Electronics Specialists Conference, 2006. PESC’06. 37th IEEE. IEEE, 2006, pp. 1–6. [941] J. B. Ahn, “Computation of backpropagation learning algorithm using neuron machine architecture,” in Computational Intelligence, Mod- elling and Simulation (CIMSim), 2013 Fifth International Conference on. IEEE, 2013, pp. 23–28. [942] R. J. Aliaga, R. Gadea, R. J. Colom, J. Cerd´a, N. Ferrando, and V. Her- rero, “A mixed hardware-software approach to ﬂexible artiﬁcial neural network training on fpga,” in Systems, Architectures, Modeling, and Simulation, 2009. SAMOS’09. International Symposium on. IEEE, 2009, pp. 1–8. [943] G. Alizadeh, J. Frounchi, M. Baradaran Nia, M. Zariﬁ, and S. As- garifar, “An fpga implementation of an artiﬁcial neural network for prediction of cetane number,” in Computer and Communication Engineering, 2008. ICCCE 2008. International Conference on. IEEE, 2008, pp. 605–608. [944] R. G. Biradar, A. Chatterjee, P. Mishra, and K. George, “Fpga imple- mentation of a multilayer artiﬁcial neural network using system-on- chip design methodology,” in Cognitive Computing and Information Processing (CCIP), 2015 International Conference on. IEEE, 2015, pp. 1–6. [945] M. A. C¸ avus¸lu, C. Karakuzu, S. S¸ ahin, and M. Yakut, “Neural network training based on fpga with ﬂoating point number format and it?s performance,” Neural Computing and Applications, vol. 20, no. 2, pp. 195–202, 2011. [946] P. O. Domingos, F. M. Silva, and H. C. Neto, “An efﬁcient and scal- able architecture for neural networks with backpropagation learning,” in Field Programmable Logic and Applications, 2005. International Conference on. IEEE, 2005, pp. 89–94. [947] J. G. Eldredge and B. L. Hutchings, “Density enhancement of a neural network using fpgas and run-time reconﬁguration,” in FPGAs for Custom Computing Machines, 1994. Proceedings. IEEE Workshop on. IEEE, 1994, pp. 180–188. [948] ——, “Rrann: a hardware implementation of the backpropagation algorithm using reconﬁgurable fpgas,” in Neural Networks, 1994. IEEE World Congress on Computational Intelligence., 1994 IEEE International Conference on, vol. 4. IEEE, 1994, pp. 2097–2102. [949] R. Gadea, J. Cerd´a, F. Ballester, and A. Mochol´ı, “Artiﬁcial neural network implementation on a single fpga of a pipelined on-line 46 backpropagation,” in Proceedings of the 13th international symposium on System synthesis. IEEE Computer Society, 2000, pp. 225–230. [950] B. Girau and A. Tisserand, “On-line arithmetic-based reprogrammable hardware implementation of multilayer perceptron back-propagation,” in microneuro. IEEE, 1996, p. 168. [951] B. Girau, “On-chip learning of fpga-inspired neural nets,” in Neural Networks, 2001. Proceedings. IJCNN’01. International Joint Confer- ence on, vol. 1. IEEE, 2001, pp. 222–227. [952] R. G. Giron´es, R. C. Palero, J. C. Boluda, and A. S. Cort´es, “Fpga implementation of a pipelined on-line backpropagation,” Journal of VLSI signal processing systems for signal, image and video technol- ogy, vol. 40, no. 2, pp. 189–213, 2005. [953] H. Hikawa, “Frequency-based multilayer neural network with on-chip learning and enhanced neuron characteristics,” Neural Networks, IEEE Transactions on, vol. 10, no. 3, pp. 545–553, 1999. [954] N. Izeboudjen, A. Farah, H. Bessalah, A. Bouridene, and N. Chikhi, “Towards a platform for fpga implementation of the mlp based back propagation algorithm,” in Computational and Ambient Intelligence. Springer, 2007, pp. 497–505. [955] F. Moreno, J. Alarc´on, R. Salvador, and T. Riesgo, “Reconﬁgurable hardware architecture of a shape recognition system based on special- ized tiny neural networks with online training,” Industrial Electronics, IEEE Transactions on, vol. 56, no. 8, pp. 3253–3263, 2009. [956] M. Moussa, S. Areibi, and K. Nichols, “On the arithmetic precision for implementing back-propagation networks on fpga: a case study,” in FPGA Implementations of Neural Networks. Springer, 2006, pp. 37–61. [957] K. R. Nichols, M. A. Moussa, and S. M. Areibi, “Feasibility of ﬂoating-point arithmetic in fpga based artiﬁcial neural networks,” in In CAINE. Citeseer, 2002. [958] F. Ortega-Zamorano, J. M. Jerez, D. Urda Munoz, R. M. Luque- Baena, and L. Franco, “Efﬁcient implementation of the backpropa- gation algorithm in fpgas and microcontrollers,” 2015. [959] V. Pandya, S. Areibi, and M. Moussa, “A handel-c implementation of the back-propagation algorithm on ﬁeld programmable gate arrays,” in Reconﬁgurable Computing and FPGAs, 2005. ReConFig 2005. International Conference on. IEEE, 2005, pp. 8–pp. [960] S. Pinjare and A. Kumar, “Implementation of neural network back propagation training algorithm on fpga,” International Journal of Computer Applications, vol. 52, no. 6, pp. 1–7, 2012. [961] Z. Ruan, J. Han, and Y. Han, “Bp neural network implementation on real-time reconﬁgurable fpga system for a soft-sensing process,” in Neural Networks and Brain, 2005. ICNN&B’05. International Conference on, vol. 2. IEEE, 2005, pp. 959–963. [962] T. Sangeetha and C. Meenal, “Digital implementation of artiﬁcial neu- ral network for function approximation and pressure control applica- tions,” IOSR Journal of Electronics and Communication Engineering (IOSR-JECE), vol. 5, no. 5, pp. 34–3, 2013. [963] A. W. Savich, M. Moussa, and S. Areibi, “The impact of arithmetic representation on implementing mlp-bp on fpgas: A study,” Neural Networks, IEEE Transactions on, vol. 18, no. 1, pp. 240–252, 2007. [964] L. Shoushan, C. Yan, X. Wenshang, and Z. Tongjun, “A single layer architecture to fpga implementation of bp artiﬁcial neural network,” in Informatics in Control, Automation and Robotics (CAR), 2010 2nd International Asia Conference on, vol. 2. IEEE, 2010, pp. 258–264. [965] Y. Sun and A. C. Cheng, “Machine learning on-a-chip: A high- performance low-power reusable neuron architecture for artiﬁcial neural networks in ecg classiﬁcations,” Computers in biology and medicine, vol. 42, no. 7, pp. 751–757, 2012. [966] G. Acosta and M. Tosini, “A ﬁrmware digital neural network for cli- mate prediction applications,” in Intelligent Control, 2001.(ISIC’01). Proceedings of the 2001 IEEE International Symposium on. IEEE, 2001, pp. 127–131. [967] Y. Ago, Y. Ito, and K. Nakano, “An fpga implementation for neural networks with the fdfm processor core approach,” International Jour- nal of Parallel, Emergent and Distributed Systems, vol. 28, no. 4, pp. 308–320, 2013. [968] R. Aliaga, R. Gadea, R. Colom, J. M. Monzo, C. Lerche, J. D. Martinez, A. Sebasti´a, and F. Mateo, “Multiprocessor soc implemen- tation of neural network training on fpga,” in Advances in Electronics and Micro-electronics, 2008. ENICS’08. International Conference on. IEEE, 2008, pp. 149–154. [969] L. Y. Ann, P. Ehkan, and M. Mashor, “Possibility of hybrid multilay- ered perceptron neural network realisation on fpga and its challenges,” in Advanced Computer and Communication Engineering Technology. Springer, 2016, pp. 1051–1061. [970] M. Bahoura and C.-W. Park, “Fpga-implementation of an adaptive neural network for rf power ampliﬁer modeling,” in New Circuits and Systems Conference (NEWCAS), 2011 IEEE 9th International. IEEE, 2011, pp. 29–32. [971] ——, “Fpga-implementation of high-speed mlp neural network.” in ICECS, 2011, pp. 426–429. [972] F. Benrekia, M. Attari, A. Bermak, and K. Belhout, “Fpga implemen- tation of a neural network classiﬁer for gas sensor array applications,” in Systems, Signals and Devices, 2009. SSD’09. 6th International Multi-Conference on. IEEE, 2009, pp. 1–6. [973] J.-L. Beuchat, J.-O. Haenni, and E. Sanchez, “Hardware reconﬁg- urable neural networks,” in Parallel and Distributed Processing. Springer, 1998, pp. 91–98. [974] J.-L. Beuchat and E. Sanchez, “A reconﬁgurable neuroprocessor with on-chip pruning,” in ICANN 98. Springer, 1998, pp. 1159–1164. [975] ——, “Using on-line arithmetic and reconﬁguration for neuroproces- sor implementation,” in International Work-Conference on Artiﬁcial Neural Networks. Springer, 1999, pp. 129–138. [976] N. M. Botros and M. Abdul-Aziz, “Hardware implementation of an artiﬁcial neural network using ﬁeld programmable gate arrays (fpga’s),” IEEE Transactions on Industrial Electronics, vol. 41, no. 6, pp. 67–665, 1994. [977] A. L. Braga, C. H. Llanos, D. G¨ohringer, J. Obie, J. Becker, and M. H¨ubner, “Performance, accuracy, power consumption and resource utilization analysis for hardware/software realized artiﬁcial neural networks,” in Bio-Inspired Computing: Theories and Applications (BIC-TA), 2010 IEEE Fifth International Conference on. IEEE, 2010, pp. 1629–1636. [978] A. Canas, E. M. Ortigosa, E. Ros, and P. M. Ortigosa, “Fpga implementation of a fully and partially connected mlp,” in FPGA Implementations of Neural Networks. Springer, 2006, pp. 271–296. [979] M. B. Carvalho, A. M. Amaral, L. E. da Silva Ramos, C. A. P. da Silva Martins, and P. Ekel, “Artiﬁcial neural network engine: Parallel and parameterized architecture implemented in fpga,” in Pattern Recognition and Machine Intelligence. Springer, 2005, pp. 294–299. [980] N. Chalhoub, F. Muller, and M. Auguin, “Fpga-based generic neural network architecture,” in Industrial Embedded Systems, 2006. IES’06. International Symposium on. IEEE, 2006, pp. 1–4. [981] R. M. da Silva, N. Nedjah, and L. de Macedo Mourelle, “Reconﬁg- urable mac-based architecture for parallel hardware implementation on fpgas of artiﬁcial neural networks using fractional ﬁxed point rep- resentation,” in Artiﬁcial Neural Networks–ICANN 2009. Springer, 2009, pp. 475–484. [982] B. Denby, P. Garda, B. Granado, C. Kiesling, J.-C. Pr´evotet, and A. Wassatsch, “Fast triggering in high-energy physics experiments using hardware neural networks,” Neural Networks, IEEE Transac- tions on, vol. 14, no. 5, pp. 1010–1027, 2003. [983] A. P. d. A. Ferreira and E. N. d. S. Barros, “A high performance full pipelined arquitecture of mlp neural networks in fpga,” in Elec- tronics, Circuits, and Systems (ICECS), 2010 17th IEEE International Conference on. IEEE, 2010, pp. 742–745. [984] D. Ferrer, R. Gonz´alez, R. Fleitas, J. P. Acle, and R. Canetti, “Neurofpga-implementing artiﬁcial neural networks on programmable logic devices,” in Design, Automation and Test in Europe Conference and Exhibition, 2004. Proceedings, vol. 3. IEEE, 2004, pp. 218–223. [985] L. Gatet, F. Bony, and H. Tap-Beteille, “Digital nn implementa- tions in a fpga for distance measurement and surface classiﬁcation,” in Instrumentation and Measurement Technology Conference, 2009. I2MTC’09. IEEE. IEEE, 2009, pp. 842–845. [986] A. Gomperts, A. Ukil, and F. Zurﬂuh, “Implementation of neural net- work on parameterized fpga.” in AAAI Spring Symposium: Embedded Reasoning, 2010. [987] ——, “Development and implementation of parameterized fpga-based general purpose neural networks for online applications,” Industrial Informatics, IEEE Transactions on, vol. 7, no. 1, pp. 78–89, 2011. [988] M. Gorgo´n and M. Wrzesi´nski, “Neural network implementation in reprogrammable fpga devices–an example for mlp,” in Artiﬁcial Intelligence and Soft Computing–ICAISC 2006. Springer, 2006, pp. 19–28. [989] T. Horita, I. Takanami, M. Akiba, M. Terauchi, and T. Kanno, “An fpga-based multiple-weight-and-neuron-fault tolerant digital multi- layer perceptron (full version),” in Transactions on Computational Science XXV. Springer, 2015, pp. 148–171. [990] Z. Jin and A. C. Cheng, “A self-healing autonomous neural network hardware for trustworthy biomedical systems,” in Field- 47 Programmable Technology (FPT), 2011 International Conference on. IEEE, 2011, pp. 1–8. [991] F. A. Khan, M. Uppal, W.-C. Song, M.-J. Kang, and A. M. Mirza, “Fpga implementation of a neural network for character recognition,” in Advances in Neural Networks-ISNN 2006. Springer, 2006, pp. 1357–1365. [992] Y.-C. Kim, D.-K. Kang, and T.-W. Lee, “Risc-based coprocessor with a dedicated vlsi neural network,” in Electronics, Circuits and Systems, 1998 IEEE International Conference on, vol. 3. IEEE, 1998, pp. 281–283. [993] D. Kyoung and K. Jung, “Fully-pipelining hardware implementation of neural network for text-based images retrieval,” in Advances in Neural Networks-ISNN 2006. Springer, 2006, pp. 1350–1356. [994] C. Latino, M. Moreno-Armend´ariz, M. Hagan et al., “Realizing gen- eral mlp networks with minimal fpga resources,” in Neural Networks, 2009. IJCNN 2009. International Joint Conference on. IEEE, 2009, pp. 1722–1729. [995] Y. Lee and S.-B. Ko, “An fpga-based face detector using neural network and a scalable ﬂoating point unit,” in Proceedings of the 5th WSEAS International Conference on Circuits, Systems, Electronics, Control & Signal Processing. World Scientiﬁc and Engineering Academy and Society (WSEAS), 2006, pp. 315–320. [996] M. A. A. Le´on, A. R. Castro, and R. R. L. Ascencio, “An artiﬁcial neural network on a ﬁeld programmable gate array as a virtual sensor,” in Design of Mixed-Mode Integrated Circuits and Applications, 1999. Third International Workshop on. IEEE, 1999, pp. 114–117. [997] Z. Lin, Y. Dong, Y. Li, and T. Watanabe, “A hybrid architecture for efﬁcient fpga-based implementation of multilayer neural network,” in Circuits and Systems (APCCAS), 2010 IEEE Asia Paciﬁc Conference on. IEEE, 2010, pp. 616–619. [998] U. Lotriˇc and P. Buli´c, “Logarithmic multiplier in hardware imple- mentation of neural networks,” in Adaptive and Natural Computing Algorithms. Springer, 2011, pp. 158–168. [999] N. Nedjah, R. M. da Silva, L. de Macedo Mourelle, and M. V. C. da Silva, “Reconﬁgurable mac-based architecture for parallel hard- ware implementation on fpgas of artiﬁcial neural networks,” in Artiﬁcial Neural Networks-ICANN 2008. Springer, 2008, pp. 169– 178. [1000] N. Nedjah, R. M. da Silva, L. Mourelle, and M. V. C. da Silva, “Dynamic mac-based architecture of artiﬁcial neural networks suitable for hardware implementation on fpgas,” Neurocomputing, vol. 72, no. 10, pp. 2171–2179, 2009. [1001] N. Nedjah, R. M. da Silva, and L. de Macedo Mourelle, “Compact yet efﬁcient hardware implementation of artiﬁcial neural networks with customized topology,” Expert Systems with Applications, vol. 39, no. 10, pp. 9191–9206, 2012. [1002] N. Nedjah and L. de Macedo Mourelle, “A reconﬁgurable hardware for artiﬁcial neural networks,” in Hardware for Soft Computing and Soft Computing for Hardware. Springer, 2014, pp. 59–69. [1003] E. M. Ortigosa, A. Ca˜nas, E. Ros, and R. R. Carrillo, “Fpga implemen- tation of a perceptron-like neural network for embedded applications,” in Artiﬁcial Neural Nets Problem Solving Methods. Springer, 2003, pp. 1–8. [1004] E. M. Ortigosa, P. M. Ortigosa, A. Ca˜nas, E. Ros, R. Ag´ıs, and J. Ortega, “Fpga implementation of multi-layer perceptrons for speech recognition,” in Field Programmable Logic and Application. Springer, 2003, pp. 1048–1052. [1005] E. M. Ortigosa, A. Ca˜nas, E. Ros, P. M. Ortigosa, S. Mota, and J. D´ıaz, “Hardware description of multi-layer perceptrons with different ab- straction levels,” Microprocessors and Microsystems, vol. 30, no. 7, pp. 435–444, 2006. [1006] E. M. Ortigosa, A. Ca˜nas, R. Rodr´ıguez, J. D´ıaz, and S. Mota, “To- wards an optimal implementation of mlp in fpga,” in Reconﬁgurable Computing: Architectures and Applications. Springer, 2006, pp. 46– 51. [1007] A. T. ¨OZDEM˙IR and K. DANIS¸ MAN, “Fully parallel ann-based arrhythmia classiﬁer on a single-chip fpga: Fpaac,” Turkish Journal of Electrical Engineering and Computer Science, vol. 19, no. 4, pp. 667–687, 2011. [1008] E. Pasero and M. Perri, “Hw-sw codesign of a ﬂexible neural controller through a fpga-based neural network programmed in vhdl,” in Neural Networks, 2004. Proceedings. 2004 IEEE International Joint Conference on, vol. 4. IEEE, 2004, pp. 3161–3165. [1009] J. C. Patra, H. Y. Lee, P. K. Meher, and E. L. Ang, “Field pro- grammable gate array implementation of a neural network-based intelligent sensor system,” in Control, Automation, Robotics and Vision, 2006. ICARCV’06. 9th International Conference on. IEEE, 2006, pp. 1–5. [1010] A. Perez-Garcia, G. Tornez-Xavier, L. Flores-Nava, F. Gomez- Castaneda, and J. Moreno-Cadenas, “Multilayer perceptron network with integrated training algorithm in fpga,” in Electrical Engineering, Computing Science and Automatic Control (CCE), 2014 11th Inter- national Conference on. IEEE, 2014, pp. 1–6. [1011] W. Qinruo, Y. Bo, X. Yun, and L. Bingru, “The hardware structure design of perceptron with fpga implementation,” in Systems, Man and Cybernetics, 2003. IEEE International Conference on, vol. 1. IEEE, 2003, pp. 762–767. [1012] S. Rani and P. Kanagasabapathy, “Multilayer perceptron neural network architecture using vhdl with combinational logic sigmoid function,” in Signal Processing, Communications and Networking, 2007. ICSCN’07. International Conference on. IEEE, 2007, pp. 404– 409. [1013] R. Rezvani, M. Katiraee, A. H. Jamalian, S. Mehrabi, and A. Vezvaei, “A new method for hardware design of multi-layer perceptron neural networks with online training,” in Cognitive Informatics & Cognitive Computing (ICCI* CC), 2012 IEEE 11th International Conference on. IEEE, 2012, pp. 527–534. [1014] J. Skodzik, V. Altmann, B. Wagner, P. Danielis, and D. Timmermann, “A highly integrable fpga-based runtime-conﬁgurable multilayer per- ceptron,” in Advanced Information Networking and Applications (AINA), 2013 IEEE 27th International Conference on. IEEE, 2013, pp. 429–436. [1015] M. M. Syiam, H. Klash, I. Mahmoud, and S. Haggag, “Hardware implementation of neural network on fpga for accidents diagnosis of the multi-purpose research reactor of egypt,” in Microelectronics, 2003. ICM 2003. Proceedings of the 15th International Conference on. IEEE, 2003, pp. 326–329. [1016] Y. Taright and M. Hubin, “Fpga implementation of a multilayer per- ceptron neural network using vhdl,” in Signal Processing Proceedings, 1998. ICSP’98. 1998 Fourth International Conference on, vol. 2. IEEE, 1998, pp. 1311–1314. [1017] S. Tatikonda and P. Agarwal, “Field programmable gate array (fpga) based neural network implementation of motion control and fault diagnosis of induction motor drive,” in Industrial Technology, 2008. ICIT 2008. IEEE International Conference on. IEEE, 2008, pp. 1–6. [1018] S. Vitabile, V. Conti, F. Gennaro, and F. Sorbello, “Efﬁcient mlp digital implementation on fpga,” in Digital System Design, 2005. Proceedings. 8th Euromicro Conference on. IEEE, 2005, pp. 218– 222. [1019] P. ˇSkoda, T. Lipi´c, ´A. Srp, B. M. Rogina, K. Skala, and F. Vajda, “Implementation framework for artiﬁcial neural networks on fpga,” in MIPRO, 2011 Proceedings of the 34th International Convention. IEEE, 2011, pp. 274–278. [1020] D. F. Wolf, R. A. Romero, and E. Marques, “Using embedded processors in hardware models of artiﬁcial neural networks,” in V Simposio Brasileiro de automac¸ ˜ao inteligente, Brasil, 2001. [1021] D. F. Wolf, G. Faria, R. A. Romero, E. Marques, M. A. Teixeira, A. A. Ribeiro, L. C. Fernandes, J. M. Scatena, and R. Mezencio, “A pipeline hardware implementation for an artiﬁcial neural network,” in Congresso da Sociedade Brasileira de Computac˜ao–SBC, Encontro Nacional de Inteligˆencia Artiﬁcial–ENIA, 2001, pp. 1528–1536. [1022] I. G. Yu, Y. M. Lee, S. W. Yeo, and C. H. Lee, “Design on supervised/unsupervised learning reconﬁgurable digital neural net- work structure,” in PRICAI 2006: Trends in Artiﬁcial Intelligence. Springer, 2006, pp. 1201–1205. [1023] J. Zhu, G. J. Milne, and B. Gunther, “Towards an fpga based reconﬁg- urable computing environment for neural network implementations,” 1999. [1024] A. Basu, S. Shuo, H. Zhou, M. H. Lim, and G.-B. Huang, “Silicon spiking neurons for hardware implementation of extreme learning machines,” Neurocomputing, vol. 102, pp. 125–134, 2013. [1025] C. Merkel and D. Kudithipudi, “Neuromemristive extreme learning machines for pattern classiﬁcation,” in VLSI (ISVLSI), 2014 IEEE Computer Society Annual Symposium on. IEEE, 2014, pp. 77–82. [1026] ——, “A current-mode cmos/memristor hybrid implementation of an extreme learning machine,” in Proceedings of the 24th edition of the great lakes symposium on VLSI. ACM, 2014, pp. 241–242. [1027] M. Suri and V. Parmar, “Exploiting intrinsic variability of ﬁlamen- tary resistive memory for extreme learning machine architectures,” Nanotechnology, IEEE Transactions on, vol. 14, no. 6, pp. 963–968, 2015. [1028] E. Yao, S. Hussain, A. Basu, and G.-B. Huang, “Computation using mismatch: Neuromorphic extreme learning machines,” in Biomedical 48 Circuits and Systems Conference (BioCAS), 2013 IEEE. IEEE, 2013, pp. 294–297. [1029] S. Decherchi, P. Gastaldo, A. Leoncini, and R. Zunino, “Efﬁcient digital implementation of extreme learning machines for classiﬁca- tion,” Circuits and Systems II: Express Briefs, IEEE Transactions on, vol. 59, no. 8, pp. 496–500, 2012. [1030] E. Gatt, J. Micallef, and E. Chilton, “An analog vlsi time-delay neural network implementation for phoneme recognition,” Cellular Neural Networks and their Applications (CNNA 2000), p. 315, 2000. [1031] J. Van der Spiegel, C. Donham, R. Etienne-Cummings, S. Fernando, P. Mueller, and D. Blackman, “Large scale analog neural computer with programmable architecture and programmable time constants for temporal pattern analysis,” in Neural Networks, 1994. IEEE World Congress on Computational Intelligence., 1994 IEEE International Conference on, vol. 3. IEEE, 1994, pp. 1830–1835. [1032] M. Bahoura and C.-W. Park, “Fpga-implementation of dynamic time delay neural network for power ampliﬁer behavioral modeling,” Analog Integrated Circuits and Signal Processing, vol. 73, no. 3, pp. 819–828, 2012. [1033] M. Bahoura, “Fpga implementation of high-speed neural network for power ampliﬁer behavioral modeling,” Analog Integrated Circuits and Signal Processing, vol. 79, no. 3, pp. 507–527, 2014. [1034] R. S. N. Ntoun´e, M. Bahoura, and C.-W. Park, “Fpga-implementation of pipelined neural network for power ampliﬁer modeling,” in New Circuits and Systems Conference (NEWCAS), 2012 IEEE 10th Inter- national. IEEE, 2012, pp. 109–112. [1035] R. Woodburn, A. Astaras, R. Dalzell, A. F. Murray, and D. K. Mc- Neill, “Computing with uncertainty in probabilistic neural networks on silicon,” in Proceedings of Symposium on neural computation, 2000. [1036] A. Serb, J. Bill, A. Khiat, R. Berdan, R. Legenstein, and T. Pro- dromakis, “Unsupervised learning in probabilistic neural networks with multi-state metal-oxide memristive synapses,” Nature commu- nications, vol. 7, p. 12611, 2016. [1037] N. Aibe, M. Yasunaga, I. Yoshihara, and J. H. Kim, “A probabilistic neural network hardware system using a learning-parameter parallel architecture,” in Neural Networks, 2002. IJCNN’02. Proceedings of the 2002 International Joint Conference on, vol. 3. IEEE, 2002, pp. 2270–2275. [1038] N. Aibe, R. Mizuno, M. Nakamura, M. Yasunaga, and I. Yoshihara, “Performance evaluation system for probabilistic neural network hardware,” Artiﬁcial Life and Robotics, vol. 8, no. 2, pp. 208–213, 2004. [1039] N. Bu, T. Hamamoto, T. Tsuji, and O. Fukuda, “Fpga implementation of a probabilistic neural network for a bioelectric human interface,” in Circuits and Systems, 2004. MWSCAS’04. The 2004 47th Midwest Symposium on, vol. 3. IEEE, 2004, pp. iii–29. [1040] M. Figueiredo, C. Gloster et al., “Implementation of a probabilistic neural network for multi-spectral image classiﬁcation on an fpga based custom computing machine,” in Neural Networks, 1998. Pro- ceedings. Vth Brazilian Symposium on. IEEE, 1998, pp. 174–179. [1041] G. Minchin and A. Zaknich, “A design for fpga implementation of the probabilistic neural network,” in Neural Information Processing, 1999. Proceedings. ICONIP’99. 6th International Conference on, vol. 2. IEEE, 1999, pp. 556–559. [1042] F. Zhou, J. Liu, Y. Yu, X. Tian, H. Liu, Y. Hao, S. Zhang, W. Chen, J. Dai, and X. Zheng, “Field-programmable gate array implementation of a probabilistic neural network for motor cortical decoding in rats,” Journal of neuroscience methods, vol. 185, no. 2, pp. 299–306, 2010. [1043] X. Zhu and Y. Chen, “Improved fpga implementation of probabilistic neural network for neural decoding,” in Apperceiving Computing and Intelligence Analysis (ICACIA), 2010 International Conference on. IEEE, 2010, pp. 198–202. [1044] R. Dogaru, A. Murgan, S. Ortmann, and M. Glesner, “A modiﬁed rbf neural network for efﬁcient current-mode vlsi implementation,” in Microelectronics for Neural Networks, 1996., Proceedings of Fifth International Conference on. IEEE, 1996, pp. 265–270. [1045] J. Sitte, L. Zhang, and U. Rueckert, “Characterization of analog local cluster neural network hardware for control,” Neural Networks, IEEE Transactions on, vol. 18, no. 4, pp. 1242–1253, 2007. [1046] M. Verleysen, P. Thissen, J.-L. Voz, and J. Madrenas, “An analog processor architecture for a neural network classiﬁer,” Micro, IEEE, vol. 14, no. 3, pp. 16–28, 1994. [1047] P. Maffezzoni and P. Gubian, “Vlsi design of radial functions hardware generator for neural computations,” in Microelectronics for Neural Networks and Fuzzy Systems, 1994., Proceedings of the Fourth International Conference on. IEEE, 1994, pp. 252–259. [1048] M. Suri, V. Parmar, A. Singla, R. Malviya, and S. Nair, “Neu- romorphic hardware accelerated adaptive authentication system,” in Computational Intelligence, 2015 IEEE Symposium Series on. IEEE, 2015, pp. 1206–1213. [1049] H. Zhuang, K.-S. Low, and W.-Y. Yau, “A pulsed neural network with on-chip learning and its practical applications,” Industrial Electronics, IEEE Transactions on, vol. 54, no. 1, pp. 34–42, 2007. [1050] S. Halgamuge, W. Poechmueller, C. Grimm, and M. Glesner, “Fuzzy interpretable dynamically developing neural networks with fpga based implementation,” in Microelectronics for Neural Networks and Fuzzy Systems, 1994., Proceedings of the Fourth International Conference on. IEEE, 1994, pp. 226–234. [1051] J.-S. Kim and S. Jung, “Hardware implementation of a neural network controller on fpga for a humanoid robot arm,” in Advanced Intelligent Mechatronics, 2008. AIM 2008. IEEE/ASME International Conference on. IEEE, 2008, pp. 1164–1169. [1052] J. Kim and S. Jung, “Implementation of the rbf neural chip with the back-propagation algorithm for on-line learning,” Applied Soft Computing, vol. 29, pp. 233–244, 2015. [1053] M. Porrmann, U. Witkowski, H. Kalte, and U. R¨uckert, “Imple- mentation of artiﬁcial neural networks on a reconﬁgurable hardware accelerator,” in euromicro-pdp. IEEE, 2002, p. 0243. [1054] S. Chakradhar, M. Sankaradas, V. Jakkula, and S. Cadambi, “A dynamically conﬁgurable coprocessor for convolutional neural net- works,” in ACM SIGARCH Computer Architecture News, vol. 38, no. 3. ACM, 2010, pp. 247–257. [1055] T. Chen, S. Zhang, S. Liu, Z. Du, T. Luo, Y. Gao, J. Liu, D. Wang, C. Wu, N. Sun et al., “A small-footprint accelerator for large-scale neural networks,” ACM Transactions on Computer Systems (TOCS), vol. 33, no. 2, p. 6, 2015. [1056] Y.-H. Chen, T. Krishna, J. S. Emer, and V. Sze, “Eyeriss: An energy- efﬁcient reconﬁgurable accelerator for deep convolutional neural networks,” IEEE Journal of Solid-State Circuits, 2016. [1057] F. Conti and L. Benini, “A ultra-low-energy convolution engine for fast brain-inspired vision in multicore clusters,” in Design, Automation & Test in Europe Conference & Exhibition (DATE), 2015. IEEE, 2015, pp. 683–688. [1058] D. Kim, J. Kung, S. Chai, S. Yalamanchili, and S. Mukhopadhyay, “Neurocube: A programmable digital neuromorphic architecture with high-density 3d memory,” in Computer Architecture (ISCA), 2016 ACM/IEEE 43rd Annual International Symposium on. IEEE, 2016, pp. 380–392. [1059] O. Nomura, T. Morie, K. Korekado, M. Matsugu, and A. Iwata, “A convolutional neural network vlsi architecture using thresholding and weight decomposition,” in Knowledge-Based Intelligent Information and Engineering Systems. Springer, 2004, pp. 995–1001. [1060] O. Nomura, T. Morie, M. Matsugu, and A. Iwata, “A convolutional neural network vlsi architecture using sorting model for reducing multiply-and-accumulation operations,” in Advances in Natural Com- putation. Springer, 2005, pp. 1006–1014. [1061] M. Kang, S. K. Gonugondla, M.-S. Keel, and N. R. Shanbhag, “An energy-efﬁcient memory-based high-throughput vlsi architecture for convolutional networks,” in Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on. IEEE, 2015, pp. 1037–1041. [1062] K. Korekado, T. Morie, O. Nomura, H. Ando, T. Nakano, M. Matsugu, and A. Iwata, “A convolutional neural network vlsi for image recogni- tion using merged/mixed analog-digital architecture,” in Knowledge- Based Intelligent Information and Engineering Systems. Springer, 2003, pp. 169–176. [1063] D. Garbin, O. Bichler, E. Vianello, Q. Rafhay, C. Gamrat, L. Perniola, G. Ghibaudo, and B. DeSalvo, “Variability-tolerant convolutional neural network for pattern recognition applications based on oxram synapses,” in Electron Devices Meeting (IEDM), 2014 IEEE Interna- tional. IEEE, 2014, pp. 28–4. [1064] D. Garbin, E. Vianello, O. Bichler, Q. Rafhay, C. Gamrat, G. Ghibaudo, B. DeSalvo, and L. Perniola, “Hfo?-based oxram devices as synapses for convolutional neural networks,” 2015. [1065] D. Garbin, E. Vianello, O. Bichler, M. Azzaz, Q. Rafhay, P. Candelier, C. Gamrat, G. Ghibaudo, B. DeSalvo, and L. Perniola, “On the impact of oxram-based synapses variability on convolutional neural networks performance,” in Nanoscale Architectures (NANOARCH), 2015 IEEE/ACM International Symposium on. IEEE, 2015, pp. 193– 198. [1066] J. Chung and T. Shin, “Simplifying deep neural networks for neu- romorphic architectures,” in Design Automation Conference (DAC), 2016 53nd ACM/EDAC/IEEE. IEEE, 2016, pp. 1–6. 49 [1067] C. Farabet, C. Poulet, J. Y. Han, and Y. LeCun, “Cnp: An fpga- based processor for convolutional networks,” in Field Programmable Logic and Applications, 2009. FPL 2009. International Conference on. IEEE, 2009, pp. 32–37. [1068] C. Farabet, Y. LeCun, K. Kavukcuoglu, E. Culurciello, B. Martini, P. Akselrod, and S. Talay, “Large-scale fpga-based convolutional networks,” Machine Learning on Very Large Data Sets, vol. 1, 2011. [1069] N. Li, S. Takaki, Y. Tomiokay, and H. Kitazawa, “A multistage dataﬂow implementation of a deep convolutional neural network based on fpga for high-speed object recognition,” in 2016 IEEE Southwest Symposium on Image Analysis and Interpretation (SSIAI). IEEE, 2016, pp. 165–168. [1070] M. Motamedi, P. Gysel, V. Akella, and S. Ghiasi, “Design space exploration of fpga-based deep convolutional neural networks,” in 2016 21st Asia and South Paciﬁc Design Automation Conference (ASP-DAC). IEEE, 2016, pp. 575–580. [1071] Y. Qiao, J. Shen, T. Xiao, Q. Yang, M. Wen, and C. Zhang, “Fpga- accelerated deep convolutional neural networks for high throughput and energy efﬁciency,” 2016. [1072] J. Qiu, J. Wang, S. Yao, K. Guo, B. Li, E. Zhou, J. Yu, T. Tang, N. Xu, S. Song et al., “Going deeper with embedded fpga plat- form for convolutional neural network,” in Proceedings of the 2016 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays. ACM, 2016, pp. 26–35. [1073] T. Shin, Y. Kang, S. Yang, S. Kim, and J. Chung, “Live demonstration: Real-time image classiﬁcation on a neuromorphic computing system with zero off-chip memory access,” in Circuits and Systems (ISCAS), 2016 IEEE International Symposium on. IEEE, 2016, pp. 449–449. [1074] N. Suda, V. Chandra, G. Dasika, A. Mohanty, Y. Ma, S. Vrud- hula, J.-s. Seo, and Y. Cao, “Throughput-optimized opencl-based fpga accelerator for large-scale convolutional neural networks,” in Proceedings of the 2016 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays. ACM, 2016, pp. 16–25. [1075] C. Zhang, P. Li, G. Sun, Y. Guan, B. Xiao, and J. Cong, “Opti- mizing fpga-based accelerator design for deep convolutional neural networks,” in Proceedings of the 2015 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays. ACM, 2015, pp. 161–170. [1076] S. K. Boddhu, J. C. Gallagher, and S. Vigraham, “A reconﬁgurable analog neural network for evolvable hardware applications: Intrinsic evolution and extrinsic veriﬁcation,” in Evolutionary Computation, 2006. CEC 2006. IEEE Congress on. IEEE, 2006, pp. 3145–3152. [1077] M. Brownlow, L. Tarassenko, and A. Murray, “Results from pulse- stream vlsi neural network devices,” in VLSI for Artiﬁcial Intelligence and Neural Networks. Springer, 1991, pp. 215–224. [1078] G. Cauwenberghs, “A learning analog neural network chip with continuous-time recurrent dynamics,” Advances in Neural Information Processing Systems, pp. 858–858, 1994. [1079] ——, “An analog vlsi recurrent neural network learning a continuous- time trajectory,” Neural Networks, IEEE Transactions on, vol. 7, no. 2, pp. 346–361, 1996. [1080] ——, “Adaptation, learning and storage in analog vlsi,” in ASIC Conference and Exhibit, 1996. Proceedings., Ninth Annual IEEE International. IEEE, 1996, pp. 273–278. [1081] W. A. Fisher, R. J. Fujimoto, and R. C. Smithson, “A programmable analog neural network processor,” Neural Networks, IEEE Transac- tions on, vol. 2, no. 2, pp. 222–229, 1991. [1082] J. C. Gallagher and J. M. Fiore, “Continuous time recurrent neural networks: a paradigm for evolvable analog controller circuits,” in The Proceedings of the National Aerospace and Electronics Conference. Citeseer, 2000, pp. 299–304. [1083] J. C. Gallagher, “A neuromorphic paradigm for extrinsically evolved hybrid analog/digital device controllers: initial explorations,” in Evolv- able Hardware, 2001. Proceedings. The Third NASA/DoD Workshop on. IEEE, 2001, pp. 48–55. [1084] J. C. Gallagher, S. K. Boddhu, and S. Vigraham, “A reconﬁgurable continuous time recurrent neural network for evolvable hardware applications,” in Evolutionary Computation, 2005. The 2005 IEEE Congress on, vol. 3. IEEE, 2005, pp. 2461–2468. [1085] J. C. Gallagher, K. S. Deshpande, and M. Wolff, “An adaptive neu- romorphic chip for augmentative control of air breathing jet turbine engines,” in Evolutionary Computation, 2008. CEC 2008.(IEEE World Congress on Computational Intelligence). IEEE Congress on. IEEE, 2008, pp. 2644–2650. [1086] P. Hasler and L. Akers, “Implementation of analog neural networks,” in Computers and Communications, 1991. Conference Proceedings., Tenth Annual International Phoenix Conference on. IEEE, 1991, pp. 32–38. [1087] G. Kothapalli, “An analogue recurrent neural network for trajectory learning and other industrial applications,” in Industrial Informatics, 2005. INDIN’05. 2005 3rd IEEE International Conference on. IEEE, 2005, pp. 462–467. [1088] J. A. Lansner and T. Lehmann, “An analog cmos chip set for neural networks with arbitrary topologies,” Neural Networks, IEEE Transactions on, vol. 4, no. 3, pp. 441–444, 1993. [1089] T. Morie and Y. Amemiya, “An all-analog expandable neural network lsi with on-chip backpropagation learning,” Solid-State Circuits, IEEE Journal of, vol. 29, no. 9, pp. 1086–1093, 1994. [1090] F. Salam, N. Khachab, M. Ismail, and Y. Wang, “An analog mos implementation of the synaptic weights for feedback neural nets,” in Circuits and Systems, 1989., IEEE International Symposium on. IEEE, 1989, pp. 1223–1226. [1091] F. Salam and Y. Wang, “A learning algorithm for feedback neural network chips,” in Circuits and Systems, 1991., IEEE International Sympoisum on. IEEE, 1991, pp. 1377–1379. [1092] J. Schemmel, K. Meier, and F. Sch¨urmann, “A vlsi implementation of an analog neural network suited for genetic algorithms,” in Evolvable Systems: From Biology to Hardware. Springer, 2001, pp. 50–61. [1093] A. THAKOOR, S. EBERHARDT, and T. Daud, “Electronic neural network for dynamic resource allocation,” in AIAA Computing in Aerospace... Conference: A Collection of Technical Papers, vol. 8. American Institute of Aeronautics and Astronautics, 1991, p. 339. [1094] Y. Ota and B. M. Wilamowski, “Cmos implementation of a pulse- coded neural network with a current controlled oscillator,” in IEEE International Symposium on Circuits and Systems, 1996, pp. III–410. [1095] B. Girau, “Digital hardware implementation of 2d compatible neural networks,” in Neural Networks, 2000. IJCNN 2000, Proceedings of the IEEE-INNS-ENNS International Joint Conference on, vol. 3. IEEE, 2000, pp. 506–511. [1096] V. Gupta, K. Khare, and R. Singh, “Fpga design and implementation issues of artiﬁcial neural network based pid controllers,” in Advances in Recent Technologies in Communication and Computing, 2009. ARTCom’09. International Conference on. IEEE, 2009, pp. 860– 862. [1097] S. Li, C. Wu, H. Li, B. Li, Y. Wang, and Q. Qiu, “Fpga acceler- ation of recurrent neural network based language model,” in Field- Programmable Custom Computing Machines (FCCM), 2015 IEEE 23rd Annual International Symposium on. IEEE, 2015, pp. 111– 118. [1098] C.-J. Lin and C.-Y. Lee, “Fpga implementation of a recurrent neural fuzzy network with on-chip learning for prediction and identiﬁca- tion applications,” Journal of Information Science and Engineering, vol. 25, no. 2, pp. 575–589, 2009. [1099] Y. Maeda and M. Wakamura, “Simultaneous perturbation learning rule for recurrent neural networks and its fpga implementation,” Neural Networks, IEEE Transactions on, vol. 16, no. 6, pp. 1664–1672, 2005. [1100] A. Polepalli, N. Soures, and D. Kudithipudi, “Digital neuromorphic design of a liquid state machine for real-time processing,” in Reboot- ing Computing (ICRC), IEEE International Conference on. IEEE, 2016, pp. 1–8. [1101] A. Polepalli and D. Kudithipudi, “Reconﬁgurable digital design of a liquid state machine for spatio-temporal data,” in Proceedings of the 3rd ACM International Conference on Nanoscale Computing and Communication. ACM, 2016, p. 15. [1102] P. Petre and J. Cruz-Albrecht, “Neuromorphic mixed-signal circuitry for asynchronous pulse processing,” in Rebooting Computing (ICRC), IEEE International Conference on. IEEE, 2016, pp. 1–4. [1103] D. Kudithipudi, Q. Saleh, C. Merkel, J. Thesing, and B. Wysocki, “Design and analysis of a neuromemristive reservoir computing ar- chitecture for biosignal processing,” Frontiers in Neuroscience, vol. 9, p. 502, 2015. [1104] B. Schrauwen, M. D?Haene, D. Verstraeten, and J. Van Campenhout, “Compact hardware liquid state machines on fpga for real-time speech recognition,” Neural networks, vol. 21, no. 2, pp. 511–523, 2008. [1105] Q. Wang, Y. Li, and P. Li, “Liquid state machine based pattern recognition on fpga with ﬁring-activity dependent power gating and approximate computing,” in Circuits and Systems (ISCAS), 2016 IEEE International Symposium on. IEEE, 2016, pp. 361–364. [1106] Y. Yi, Y. Liao, B. Wang, X. Fu, F. Shen, H. Hou, and L. Liu, “Fpga based spike-time dependent encoder and reservoir design in neuro- morphic computing processors,” Microprocessors and Microsystems, 2016. 50 [1107] P. Hylander, J. Meader, and E. Frie, “Vlsi implementation of pulse coded winner take all networks,” in Circuits and Systems, 1993., Proceedings of the 36th Midwest Symposium on. IEEE, 1993, pp. 758–761. [1108] E. Neftci, E. Chicca, M. Cook, G. Indiveri, and R. Douglas, “State- dependent sensory processing in networks of vlsi spiking neurons,” in Circuits and Systems (ISCAS), Proceedings of 2010 IEEE Interna- tional Symposium on. IEEE, 2010, pp. 2789–2792. [1109] E. Neftci and G. Indiveri, “A device mismatch compensation method for vlsi neural networks,” in Biomedical Circuits and Systems Con- ference (BioCAS), 2010 IEEE. IEEE, 2010, pp. 262–265. [1110] T. Kamio, H. Adachi, H. Ninomiya, and H. Asai, “A design method of dwt analog neuro chip for vlsi implementation,” in Instrumentation and Measurement Technology Conference, 1997. IMTC/97. Proceed- ings. Sensing, Processing, Networking., IEEE, vol. 2. IEEE, 1997, pp. 1210–1214. [1111] N. I. Khachab and M. Ismail, “A new continuous-time mos implemen- tation of feedback neural networks,” in Circuits and Systems, 1989., Proceedings of the 32nd Midwest Symposium on. IEEE, 1989, pp. 221–224. [1112] B. W. Lee, J.-C. Lee, and B. J. Sheu, “Vlsi image processor using analog programmable synapses and neurons,” in Neural Networks, 1990., 1990 IJCNN International Joint Conference on. IEEE, 1990, pp. 575–580. [1113] B. Linares-Barranco, E. Sanchez-Sinencio, A. Rodriguez-Vazquez, and J. L. Huertas, “A modular t-mode design approach for analog neural network hardware implementations,” Solid-State Circuits, IEEE Journal of, vol. 27, no. 5, pp. 701–713, 1992. [1114] B. Linares-Barranco, E. Sanchez-Sinencio, A. Rodriguez-Vazquez, and J. Huertas, “Modular analog continuous-time vlsi neural networks with on-chip hebbian learning and analog storage,” in Circuits and Systems, 1992. ISCAS’92. Proceedings., 1992 IEEE International Symposium on, vol. 3. IEEE, 1992, pp. 1533–1536. [1115] J. J. Paulos and P. W. Hollis, “Neural networks using analog multipli- ers,” in Circuits and Systems, 1988., IEEE International Symposium on. IEEE, 1988, pp. 499–502. [1116] M. Verleysen, B. Sirletti, A. M. Vandemeulebroecke, and P. G. Jespers, “Neural networks for high-storage content-addressable mem- ory: Vlsi circuit and learning algorithm,” Solid-State Circuits, IEEE Journal of, vol. 24, no. 3, pp. 562–569, 1989. [1117] H.-K. Yang, E. El-Masry et al., “A cmos current-mode pwm technique for analog neural network implementations,” in Circuits and Systems, 1994. ISCAS’94., 1994 IEEE International Symposium on, vol. 6. IEEE, 1994, pp. 355–358. [1118] P. Alla, G. Dreyfus, J. Gascuel, A. Johannet, L. Personnaz, J. Roman, and M. Weinfeld, Silicon integration of learning algorithms and other auto-adaptive properties in a digital feedback neural network. Springer, 1991. [1119] K. V. Asari and C. Eswaran, “Systolic array implementation of arti- ﬁcial neural networks,” Microprocessors and Microsystems, vol. 18, no. 8, pp. 481–488, 1994. [1120] F. Blayo and P. Hurat, “A reconﬁgurable wsi neural network,” in Wafer Scale Integration, 1989. Proceedings.,[1st] International Conference on. IEEE, 1989, pp. 141–150. [1121] W. Fornaciari and F. Salice, “An automatic vlsi implementation of hopﬁeld anns,” in Circuits and Systems, 1994., Proceedings of the 37th Midwest Symposium on, vol. 1. IEEE, 1994, pp. 499–502. [1122] A. Johannet, L. Personnaz, G. Dreyfus, J.-D. Gascuel, and M. We- infeld, “Speciﬁcation and implementation of a digital hopﬁeld-type associative memory with on-chip training,” Neural Networks, IEEE Transactions on, vol. 3, no. 4, pp. 529–539, 1992. [1123] C. Lehmann, M. Viredaz, and F. Blayo, “A generic systolic array building block for neural networks with on-chip learning,” Neural Networks, IEEE Transactions on, vol. 4, no. 3, pp. 400–407, 1993. [1124] A. Masaki, Y. Hirai, and M. Yamada, “Neural networks in cmos: a case study,” Circuits and Devices Magazine, IEEE, vol. 6, no. 4, pp. 12–17, 1990. [1125] D. E. Van Den Bout and T. K. Miller III, “A digital architecture employing stochasticism for the simulation of hopﬁeld neural nets,” Circuits and Systems, IEEE Transactions on, vol. 36, no. 5, pp. 732– 738, 1989. [1126] M. Weinfeld, “A fully digital integrated cmos hopﬁeld network including the learning algorithm,” in VLSI for Artiﬁcial Intelligence. Springer, 1989, pp. 169–178. [1127] W. Wike, D. Van den Bout, and T. Miller III, “The vlsi implementation of stonn,” in Neural Networks, 1990., 1990 IJCNN International Joint Conference on. IEEE, 1990, pp. 593–598. [1128] M. Yasunaga, N. Masuda, M. Yagyu, M. Asai, K. Shibata, M. Ooyama, M. Yamada, T. Sakaguchi, and M. Hashimoto, “A self- learning digital neural network using wafer-scale lsi,” Solid-State Circuits, IEEE Journal of, vol. 28, no. 2, pp. 106–114, 1993. [1129] J. Tomberg, T. Ritoniemi, K. Kaski, and H. Tenhunen, “Fully digital neural network implementation based on pulse density modulation,” in Custom Integrated Circuits Conference, 1989., Proceedings of the IEEE 1989. IEEE, 1989, pp. 12–7. [1130] J. E. Tomberg and K. K. Kaski, “Pulse-density modulation technique in vlsi implementations of neural network algorithms,” Solid-State Circuits, IEEE Journal of, vol. 25, no. 5, pp. 1277–1286, 1990. [1131] R. Aibara, Y. Mitsui, and T. Ae, “A cmos chip design of binary neural network with delayed synapses,” in Circuits and Systems, 1991., IEEE International Sympoisum on. IEEE, 1991, pp. 1307–1310. [1132] L. Chen, M. Wedlake, G. Deliyannides, and H. Kwok, “Hybrid archi- tecture for analogue neural network and its circuit implementation,” in Circuits, Devices and Systems, IEE Proceedings-, vol. 143, no. 2. IET, 1996, pp. 123–128. [1133] J. E. Hansen, J. Skelton, and D. Allstot, “A time-multiplexed switched- capacitor circuit for neural network applications,” in Circuits and Systems, 1989., IEEE International Symposium on. IEEE, 1989, pp. 2177–2180. [1134] P. W. Hollis and J. J. Paulos, “Artiﬁcial neural networks using mos analog multipliers,” Solid-State Circuits, IEEE Journal of, vol. 25, no. 3, pp. 849–855, 1990. [1135] B. W. Lee and B. J. Sheu, “Hardware annealing in electronic neural networks,” Circuits and Systems, IEEE Transactions on, vol. 38, no. 1, pp. 134–137, 1991. [1136] B. Maundy and E. El-Masry, “Pulse arithmetic in switched capacitor neural networks,” in Circuits and Systems, 1990., Proceedings of the 33rd Midwest Symposium on. IEEE, 1990, pp. 285–288. [1137] A. Moopenn, T. Duong, and A. Thakoor, “Digital-analog hybrid synapse chips for electronic neural networks,” in Advances in Neural Information Processing Systems, 1990, pp. 769–776. [1138] D. Tank and J. J. Hopﬁeld, “Simple’neural’optimization networks: An a/d converter, signal decision circuit, and a linear programming circuit,” Circuits and Systems, IEEE Transactions on, vol. 33, no. 5, pp. 533–541, 1986. [1139] D. Abramson, K. Smith, P. Logothetis, and D. Duke, “Fpga based implementation of a hopﬁeld neural network for solving constraint satisfaction problems,” in Euromicro Conference, 1998. Proceedings. 24th, vol. 2. IEEE, 1998, pp. 688–693. [1140] K. K. Likharev, “Neuromorphic cmol circuits,” in Nanotechnology, 2003. IEEE-NANO 2003. 2003 Third IEEE Conference on, vol. 1. IEEE, 2003, pp. 339–342. [1141] M. Atencia, H. Boumeridja, G. Joya, F. Garc´ıa-Lagos, and F. San- doval, “Fpga implementation of a systems identiﬁcation module based upon hopﬁeld networks,” Neurocomputing, vol. 70, no. 16, pp. 2828– 2835, 2007. [1142] H. Boumeridja, M. Atencia, G. Joya, and F. Sandoval, “Fpga imple- mentation of hopﬁeld networks for systems identiﬁcation,” in Inter- national Work-Conference on Artiﬁcial Neural Networks. Springer, 2005, pp. 582–589. [1143] V. De Florio, G. Deconinck, and R. Belmans, “A massively par- allel architecture for hopﬁeld-type neural network computers,” in International Conference on Massively Parallel Computing Systems (MPCS?02), 2002. [1144] M. Gschwind, V. Salapura, and O. Maischberger, “A generic building block for hopﬁeld neural networks with on-chip learning,” in IEEE International Symposium on Circuits and Systems, Atlanta, GA. Cite- seer, 1996. [1145] S. M. Saif, H. M. Abbas, and S. M. Nassar, “An fpga implementation of a competitive hopﬁeld neural network for use in histogram equal- ization,” in Neural Networks, 2006. IJCNN’06. International Joint Conference on. IEEE, 2006, pp. 2815–2822. [1146] M. Stepanova, F. Lin, and V. C.-L. Lin, “A hopﬁeld neural classiﬁer and its fpga implementation for identiﬁcation of symmetrically struc- tured dna motifs,” The Journal of VLSI Signal Processing Systems for Signal, Image, and Video Technology, vol. 48, no. 3, pp. 239–254, 2007. [1147] A. Varma et al., “A novel digital neural network for the travel- ling salesman problem,” in Neural Information Processing, 2002. ICONIP’02. Proceedings of the 9th International Conference on, vol. 3. IEEE, 2002, pp. 1320–1324. [1148] M. Wakamura and Y. Maeda, “Fpga implementation of hopﬁeld neural network via simultaneous perturbation rule,” in SICE 2003 Annual Conference (IEEE Cat. No. 03TH8734), 2003. 51 [1149] S. Duan, Z. Dong, X. Hu, L. Wang, and H. Li, “Small-world hopﬁeld neural networks with weight salience priority and memristor synapses for digit recognition,” Neural Computing and Applications, pp. 1–8, 2015. [1150] X. Guo, F. Merrikh-Bayat, L. Gao, B. D. Hoskins, F. Alibart, B. Linares-Barranco, L. Theogarajan, C. Teuscher, and D. B. Strukov, “Modeling and experimental demonstration of a hopﬁeld network analog-to-digital converter with hybrid cmos/memristor circuits,” Frontiers in neuroscience, vol. 9, 2015. [1151] S. Hu, Y. Liu, Z. Liu, T. Chen, J. Wang, Q. Yu, L. Deng, Y. Yin, and S. Hosaka, “Associative memory realized by a reconﬁgurable memristive hopﬁeld neural network,” Nature communications, vol. 6, 2015. [1152] B. Liu, Y. Chen, B. Wysocki, and T. Huang, “The circuit realization of a neuromorphic computing system with memristor-based synapse design,” in Neural Information Processing. Springer, 2012, pp. 357– 365. [1153] ——, “Reconﬁgurable neuromorphic computing system with memristor-based synapse design,” Neural Processing Letters, vol. 41, no. 2, pp. 159–167, 2013. [1154] ——, “Reconﬁgurable neuromorphic computing system with memristor-based synapse design,” Neural Processing Letters, vol. 41, no. 2, pp. 159–167, 2015. [1155] J. A. Clemente, W. Mansour, R. Ayoubi, F. Serrano, H. Mecha, H. Ziade, W. El Falou, and R. Velazco, “Hardware implementation of a fault-tolerant hopﬁeld neural network on fpgas,” Neurocomputing, vol. 171, pp. 1606–1609, 2016. [1156] H. Harmanani, J. Hannouche, and N. Khoury, “A neural networks algorithm for the minimum colouring problem using fpgas,” Inter- national Journal of Modelling and Simulation, vol. 30, no. 4, pp. 506–513, 2010. [1157] W. Mansour, R. Ayoubi, H. Ziade, R. Velazco, and W. El Falou, “An optimal implementation on fpga of a hopﬁeld neural network,” Advances in Artiﬁcial Neural Systems, vol. 2011, p. 7, 2011. [1158] M. A. d. A. d. Sousa, E. L. Horta, S. T. Kofuji, and E. Del-Moral- Hernandez, “Architecture analysis of an fpga-based hopﬁeld neural network,” Advances in Artiﬁcial Neural Systems, vol. 2014, p. 15, 2014. [1159] A. Srinivasulu, “Digital very-large-scale integration (vlsi) hopﬁeld neural network implementation on ﬁeld programmable gate arrays (fpga) for solving constraint satisfaction problems,” Journal of Engi- neering and Technology Research, vol. 4, no. 1, pp. 11–21, 2012. [1160] A. G. Andreou and K. A. Boahen, “Synthetic neural circuits using current-domain signal representations,” Neural Computation, vol. 1, no. 4, pp. 489–501, 1989. [1161] A. G. Andreou, K. A. Boahen, P. O. Pouliquen, A. Pavasovic, R. E. Jenkins, and K. Strohbehn, “Current-mode subthreshold mos circuits for analog vlsi neural systems,” IEEE transactions on neural networks/a publication of the IEEE Neural Networks Council, vol. 2, no. 2, pp. 205–213, 1990. [1162] K. A. Boahen, A. G. Andreou, P. O. Pouliquen, and A. Pavasovic, “Architectures for associative memories using current-mode analog mos circuits,” in Proceedings of the decennial Caltech conference on VLSI on Advanced research in VLSI. MIT Press, 1989, pp. 175–193. [1163] K. A. Boahen, P. O. Pouliquen, A. G. Andreou, and R. E. Jenkins, “A heteroassociative memory using current-mode mos analog vlsi circuits,” Circuits and Systems, IEEE Transactions on, vol. 36, no. 5, pp. 747–755, 1989. [1164] R. E. Howard, D. B. Schwartz, J. S. Denker, R. W. Epworth, H. P. Graf, W. E. Hubbard, L. D. Jackel, B. L. Straughn, and D. Tennant, “An associative memory based on an electronic neural network architecture,” Electron Devices, IEEE Transactions on, vol. 34, no. 7, pp. 1553–1556, 1987. [1165] T. Kaulmann, M. Ferber, U. Witkowski, and U. R¨uckert, “Analog vlsi implementation of adaptive synapses in pulsed neural networks,” in Computational Intelligence and Bioinspired Systems. Springer, 2005, pp. 455–462. [1166] B. Linares-Barranco, E. S´anchez-Sinencio, A. Rodriguez-Vazquez, and J. L. Huertas, “A cmos analog adaptive bam with on-chip learning and weight refreshing,” Neural Networks, IEEE Transactions on, vol. 4, no. 3, pp. 445–455, 1993. [1167] B. J. Maundy and E. I. El-Masry, “Feedforward associative mem- ory switched-capacitor artiﬁcial neural networks,” Analog Integrated Circuits and Signal Processing, vol. 1, no. 4, pp. 321–338, 1991. [1168] C. McCarley and P. Szabo, “Analog vlsi for implementation of a ?hyperassociative memory? neural network,” in Systems, Man and Cybernetics, 1995. Intelligent Systems for the 21st Century., IEEE International Conference on, vol. 3. IEEE, 1995, pp. 2076–2080. [1169] K. Saeki, T. Morita, and Y. Sekine, “Associative memory using pulse- type hardware neural network with stdp synapses,” in Intelligent Systems Design and Applications (ISDA), 2011 11th International Conference on. IEEE, 2011, pp. 947–951. [1170] S. R. Hasan and N. K. Siong, “A vlsi bam neural network chip for pattern recognition applications,” in Neural Networks, 1995. Proceedings., IEEE International Conference on, vol. 1. IEEE, 1995, pp. 164–168. [1171] H. Graf and P. De Vegvar, “A cmos associative memory chip based on neural networks,” in Solid-State Circuits Conference. Digest of Technical Papers. 1987 IEEE International, vol. 30. IEEE, 1987, pp. 304–305. [1172] H. P. Graf, L. D. Jackel, and W. E. Hubbard, “Vlsi implementation of a neural network model,” Computer, vol. 21, no. 3, pp. 41–49, 1988. [1173] A. Heittmann and U. R¨uckert, “Mixed mode vlsi implementation of a neural associative memory,” Analog Integrated Circuits and Signal Processing, vol. 30, no. 2, pp. 159–172, 2002. [1174] U. Ruckert and K. Goser, “Vlsi architectures for associative net- works,” in Circuits and Systems, 1988., IEEE International Sympo- sium on. IEEE, 1988, pp. 755–758. [1175] U. Ruckert, “An associative memory with neural architecture and its vlsi implementation,” in System Sciences, 1991. Proceedings of the Twenty-Fourth Annual Hawaii International Conference on, vol. 1. IEEE, 1991, pp. 212–218. [1176] C.-Y. Wu and J.-F. Lan, “Cmos current-mode neural associative memory design with on-chip learning,” Neural Networks, IEEE Trans- actions on, vol. 7, no. 1, pp. 167–181, 1996. [1177] Y. V. Pershin and M. Di Ventra, “Experimental demonstration of asso- ciative memory with memristive neural networks,” Neural Networks, vol. 23, no. 7, pp. 881–886, 2010. [1178] L. Wang, H. Li, S. Duan, T. Huang, and H. Wang, “Pavlov associative memory in a memristive neural network and its circuit implementa- tion,” Neurocomputing, 2015. [1179] D. Hammerstrom, C. Gao, S. Zhu, and M. Butts, “Fpga implementa- tion of very large associative memories–scaling issues,” 2003. [1180] B. J. Leiner, V. Q. Lorena, T. M. Cesar, and M. V. Lorenzo, “Hardware architecture for fpga implementation of a neural network and its application in images processing,” in Electronics, Robotics and Automotive Mechanics Conference, 2008. CERMA’08. IEEE, 2008, pp. 405–410. [1181] J. Li, Y. Katori, and T. Kohno, “Hebbian learning in fpga silicon neuronal network,” in The 1st IEEE/IIAE International Conference on Intelligent Systems and Image Processing 2013 (ICISIP2013), 2013. [1182] D. Reay, T. Green, and B. Williams, “Field programmable gate array implementation of a neural network accelerator,” in Hardware Implementation of Neural Networks and Fuzzy Logic, IEE Colloquium on. IET, 1994, pp. 2–1. [1183] E. Neftci, “Stochastic neuromorphic learning machines for weakly labeled data,” in Computer Design (ICCD), 2016 IEEE 34th Interna- tional Conference on. IEEE, 2016, pp. 670–673. [1184] E. O. Neftci, B. U. Pedroni, S. Joshi, M. Al-Shedivat, and G. Cauwen- berghs, “Stochastic synapses enable efﬁcient brain-inspired learning machines,” Frontiers in Neuroscience, vol. 10, 2016. [1185] A. Torralba, F. Colodro, E. Ibanez, and L. G. Franquelo, “Two digital circuits for a fully parallel stochastic neural network,” Neural Networks, IEEE Transactions on, vol. 6, no. 5, pp. 1264–1268, 1995. [1186] W.-C. Fang, B. J. Sheu, and J.-C. Lee, “Real-time computing of op- tical ﬂow using adaptive vlsi neuroprocessors,” in Computer Design: VLSI in Computers and Processors, 1990. ICCD’90. Proceedings, 1990 IEEE International Conference on. IEEE, 1990, pp. 122–125. [1187] S. L. Bade and B. L. Hutchings, “Fpga-based stochastic neural networks-implementation,” in FPGAs for Custom Computing Ma- chines, 1994. Proceedings. IEEE Workshop on. IEEE, 1994, pp. 189–198. [1188] H. Li, Y. Hayakawa, S. Sato, and K. Nakajima, “A new digital architecture of inverse function delayed neuron with the stochastic logic,” in Circuits and Systems, 2004. MWSCAS’04. The 2004 47th Midwest Symposium on, vol. 2. IEEE, 2004, pp. II–393. [1189] N. Nedjah and L. de Macedo Mourelle, “Stochastic reconﬁgurable hardware for neural networks,” in Digital System Design, 2003. Proceedings. Euromicro Symposium on. IEEE, 2003, pp. 438–442. [1190] ——, “Fpga-based hardware architecture for neural networks: binary radix vs. stochastic,” in null. IEEE, 2003, p. 111. 52 [1191] ——, “Reconﬁgurable hardware for neural networks: binary versus stochastic,” Neural Computing and Applications, vol. 16, no. 3, pp. 249–255, 2007. [1192] M. van Daalen, P. Jeavons, and J. Shawe-Taylor, “A stochastic neural architecture that exploits dynamically reconﬁgurable fpgas,” in FPGAs for Custom Computing Machines, 1993. Proceedings. IEEE Workshop on. IEEE, 1993, pp. 202–211. [1193] A. Jayakumar and J. Alspector, “A cascadable neural network chip set with on-chip learning using noise and gain annealing,” in Custom Integrated Circuits Conference, 1992., Proceedings of the IEEE 1992. IEEE, 1992, pp. 19–5. [1194] H. Pujol, J. Klein, E. Belhaire, and P. Garda, “Ra: An analog neuro- computer for the synchronous boltzmann machine,” in Microelectron- ics for Neural Networks and Fuzzy Systems, 1994., Proceedings of the Fourth International Conference on. IEEE, 1994, pp. 449–455. [1195] C. Schneider and H. Card, “Analog vlsi models of mean ﬁeld networks,” in VLSI for Artiﬁcial Intelligence and Neural Networks. Springer, 1991, pp. 185–194. [1196] C. R. Schneider and H. C. Card, “Analog cmos deterministic boltz- mann circuits,” Solid-State Circuits, IEEE Journal of, vol. 28, no. 8, pp. 907–914, 1993. [1197] Y. Arima, K. Mashiko, K. Okada, T. Yamada, A. Maeda, H. Kondoh, and S. Kayano, “A self-learning neural network chip with 125 neurons and 10 k self-organization synapses,” Solid-State Circuits, IEEE Journal of, vol. 26, no. 4, pp. 607–611, 1991. [1198] Y. Arima, K. Mashiko, K. Okada, T. Yamada, A. Maeda, H. Notani, H. Kondoh, and S. Kayano, “A 336-neuron, 28 k-synapse, self- learning neural network chip with branch-neuron-unit architecture,” Solid-State Circuits, IEEE Journal of, vol. 26, no. 11, pp. 1637–1644, 1991. [1199] S. Sato, M. Yumine, T. Yama, J. Murota, K. Nakajima, and Y. Sawada, “Lsi implementation of pulse-output neural network with programmable synapse,” in Neural Networks, 1992. IJCNN., International Joint Conference on, vol. 1. IEEE, 1992, pp. 172– 177. [1200] M. N. Bojnordi and E. Ipek, “Memristive boltzmann machine: A hard- ware accelerator for combinatorial optimization and deep learning,” in 2016 IEEE International Symposium on High Performance Computer Architecture (HPCA). IEEE, 2016, pp. 1–13. [1201] H. Chen, P. Fleury, and A. F. Murray, “Minimising contrastive divergence in noisy, mixed-mode vlsi neurons,” in Advances in Neural Information Processing Systems, 2003, p. None. [1202] C. Lu, C. Hong, and H. Chen, “A scalable and programmable architecture for the continuous restricted boltzmann machine in vlsi,” in Circuits and Systems, 2007. ISCAS 2007. IEEE International Symposium on. IEEE, 2007, pp. 1297–1300. [1203] C.-C. Lu and H. Chen, “Current-mode computation with noise in a scalable and programmable probabilistic neural vlsi system,” in Artiﬁcial Neural Networks–ICANN 2009. Springer, 2009, pp. 401– 409. [1204] P. Knag, C. Liu, and Z. Zhang, “A 1.40 mm 2 141mw 898gops sparse neuromorphic processor in 40nm cmos,” in VLSI Circuits (VLSI- Circuits), 2016 IEEE Symposium on. IEEE, 2016, pp. 1–2. [1205] B. U. Pedroni, S. Das, J. V. Arthur, P. A. Merolla, B. L. Jackson, D. S. Modha, K. Kreutz-Delgado, and G. Cauwenberghs, “Mapping generative models onto a network of digital spiking neurons,” IEEE transactions on biomedical circuits and systems, vol. 10, no. 4, pp. 837–854, 2016. [1206] M. Raﬁque, B. Lee, and M. Jeon, “Hybrid neuromorphic system for automatic speech recognition,” Electronics Letters, vol. 52, no. 17, pp. 1428–1430, 2016. [1207] A. M. Sheri, A. Raﬁque, W. Pedrycz, and M. Jeon, “Contrastive divergence for memristor-based restricted boltzmann machine,” Engi- neering Applications of Artiﬁcial Intelligence, vol. 37, pp. 336–342, 2015. [1208] M. Suri, V. Parmar, A. Kumar, D. Querlioz, and F. Alibart, “Neu- romorphic hybrid rram-cmos rbm architecture,” in 2015 15th Non- Volatile Memory Technology Symposium (NVMTS). IEEE, 2015, pp. 1–6. [1209] S. K. Kim, L. C. McAfee, P. L. McMahon, and K. Olukotun, “A highly scalable restricted boltzmann machine fpga implementation,” in Field Programmable Logic and Applications, 2009. FPL 2009. International Conference on. IEEE, 2009, pp. 367–372. [1210] L.-W. Kim, S. Asaad, and R. Linsker, “A fully pipelined fpga architecture of a factored restricted boltzmann machine artiﬁcial neural network,” ACM Transactions on Reconﬁgurable Technology and Systems (TRETS), vol. 7, no. 1, p. 5, 2014. [1211] D. Le Ly and P. Chow, “High-performance reconﬁgurable hardware architecture for restricted boltzmann machines,” Neural Networks, IEEE Transactions on, vol. 21, no. 11, pp. 1780–1792, 2010. [1212] S. Das, B. U. Pedroni, P. Merolla, J. Arthur, A. S. Cassidy, B. L. Jackson, D. Modha, G. Cauwenberghs, and K. Kreutz-Delgado, “Gibbs sampling with low-power spiking digital neurons,” in Circuits and Systems (ISCAS), 2015 IEEE International Symposium on. IEEE, 2015, pp. 2704–2707. [1213] B. Ahn, “Computation of deep belief networks using special-purpose hardware architecture,” in Neural Networks (IJCNN), 2014 Interna- tional Joint Conference on. IEEE, 2014, pp. 141–148. [1214] K. Sanni, G. Garreau, J. L. Molin, and A. G. Andreou, “Fpga implementation of a deep belief network architecture for character recognition using stochastic computation,” in Information Sciences and Systems (CISS), 2015 49th Annual Conference on. IEEE, 2015, pp. 1–5. [1215] H. C. Card, C. R. Schneider, and R. S. Schneider, “Learning capacitive weights in analog cmos neural networks,” Journal of VLSI signal processing systems for signal, image and video technology, vol. 8, no. 3, pp. 209–225, 1994. [1216] G. Cauwenberghs, C. F. Neugebauer, and A. Yariv, “An adaptive cmos matrix-vector multiplier for large scale analog hardware neural network applications,” in Neural Networks, 1991., IJCNN-91-Seattle International Joint Conference on, vol. 1. IEEE, 1991, pp. 507–511. [1217] C. Schneider and H. Card, “Cmos implementation of analog hebbian synaptic learning circuits,” in Neural Networks, 1991., IJCNN-91- Seattle International Joint Conference on, vol. 1. IEEE, 1991, pp. 437–442. [1218] ——, “Analog cmos synaptic learning circuits adapted from inverte- brate biology,” Circuits and Systems, IEEE Transactions on, vol. 38, no. 12, pp. 1430–1438, 1991. [1219] ——, “Analogue cmos hebbian synapses,” Electronics letters, vol. 27, no. 9, pp. 785–786, 1991. [1220] T. Shima, T. Kimura, Y. Kamatani, T. Itakura, Y. Fujita, and T. Iida, “Neuro chips with on-chip backprop and/or hebbian learning,” in Solid-State Circuits Conference, 1992. Digest of Technical Papers. 39th ISSCC, 1992 IEEE International. IEEE, 1992, pp. 138–139. [1221] P. Del Giudice, S. Fusi, D. Badoni, V. Dante, and D. J. Amit, “Learning attractors in an asynchronous, stochastic electronic neural network,” Network: Computation in Neural Systems, vol. 9, no. 2, pp. 183–205, 1998. [1222] K. Likharev, A. Mayr, I. Muckra, and ¨O. T¨urel, “Crossnets: High- performance neuromorphic architectures for cmol circuits,” Annals of the New York Academy of Sciences, vol. 1006, no. 1, pp. 146–163, 2003. [1223] R. U. Zaman and D. C. Wunsch, “An adaptive vlsi neural network chip,” in Neural Networks, 1994. IEEE World Congress on Computa- tional Intelligence., 1994 IEEE International Conference on, vol. 4. IEEE, 1994, pp. 2018–2021. [1224] A. Aggarwal and B. Hamilton, “Training artiﬁcial neural networks with memristive synapses: Hspice-matlab co-simulation,” in Neural Network Applications in Electrical Engineering (NEUREL), 2012 11th Symposium on. IEEE, 2012, pp. 101–106. [1225] A. Ascoli, F. Corinto, M. Gilli, and R. Tetzlaff, “Memristor for neuromorphic applications: Models and circuit implementations,” in Memristors and Memristive Systems. Springer, 2014, pp. 379–403. [1226] K. D. Cantley, A. Subramaniam, H. J. Stiegler, R. A. Chapman, and E. M. Vogel, “Hebbian learning in spiking neural networks with nanocrystalline silicon tfts and memristive synapses,” Nanotechnol- ogy, IEEE Transactions on, vol. 10, no. 5, pp. 1066–1073, 2011. [1227] ——, “Neural learning circuits utilizing nano-crystalline silicon tran- sistors and memristors,” Neural Networks and Learning Systems, IEEE Transactions on, vol. 23, no. 4, pp. 565–573, 2012. [1228] R. Kubendran, “Electromagnetic and laplace domain analysis of memristance and associative learning using memristive synapses modeled in spice,” in Devices, Circuits and Systems (ICDCS), 2012 International Conference on. IEEE, 2012, pp. 622–626. [1229] X. Wang, C. Li, T. Huang, and S. Duan, “A weakly connected mem- ristive neural network for associative memory,” Neural Processing Letters, vol. 40, no. 3, pp. 275–288, 2014. [1230] S. Wen, Z. Zeng, and T. Huang, “Associative learning of integrate- and-ﬁre neurons with memristor-based synapses,” Neural processing letters, vol. 38, no. 1, pp. 69–80, 2013. [1231] M. Ziegler and H. Kohlstedt, “Mimic synaptic behavior with a single ﬂoating gate transistor: A memﬂash synapse,” Journal of Applied Physics, vol. 114, no. 19, p. 194506, 2013. 53 [1232] R. D´lugosz, T. Tala´ska, W. Pedrycz, and R. Wojtyna, “Realization of the conscience mechanism in cmos implementation of winner- takes-all self-organizing neural networks,” Neural Networks, IEEE Transactions on, vol. 21, no. 6, pp. 961–971, 2010. [1233] J. R. Mann and S. Gilbert, “An analog self-organizing neural network chip,” in Advances in neural information processing systems, 1989, pp. 739–747. [1234] B. J. Maundy and E. El-Masry, “A self-organizing switched-capacitor neural network,” Circuits and Systems, IEEE Transactions on, vol. 38, no. 12, pp. 1556–1563, 1991. [1235] B. Sheu, J. Choi, and C.-F. Chang, “An analog neural network processor for self-organizing mapping,” in Solid-State Circuits Con- ference, 1992. Digest of Technical Papers. 39th ISSCC, 1992 IEEE International. IEEE, 1992, pp. 136–137. [1236] R. Cambio and D. C. Hendry, “Low-power digital neuron for som implementations,” Electronics Letters, vol. 39, no. 5, pp. 448–450, 2003. [1237] R. Długosz, M. Kolasa, W. Pedrycz, and M. Szulc, “Parallel pro- grammable asynchronous neighborhood mechanism for kohonen som implemented in cmos technology,” Neural Networks, IEEE Transac- tions on, vol. 22, no. 12, pp. 2091–2104, 2011. [1238] C. Lehmann and F. Blayo, “A vlsi implementation of a generic systolic synaptic building block for neural networks,” in VLSI for Artiﬁcial Intelligence and Neural Networks. Springer, 1991, pp. 325–334. [1239] L. Marchese, “The internet search engines based on artiﬁcial neural systems implemented in hardware would enable a powerful and ﬂex- ible context-based research of professional and scientiﬁc documents,” International Journal of Web Engineering, vol. 4, no. 1, pp. 1–14, 2015. [1240] N. Mauduit, M. Duranton, J. Gobert, and J.-A. Sirat, “Lneuro 1.0: A piece of hardware lego for building neural network systems,” Neural Networks, IEEE Transactions on, vol. 3, no. 3, pp. 414–422, 1992. [1241] A. Rajah and M. K. Hani, “Asic design of a kohonen neural network microchip,” in Semiconductor Electronics, 2004. ICSE 2004. IEEE International Conference on. IEEE, 2004, pp. 4–pp. [1242] L. Rodriguez, B. Miramond, and B. Granado, “Toward a sparse self- organizing map for neuromorphic architectures,” ACM Journal on Emerging Technologies in Computing Systems (JETC), vol. 11, no. 4, p. 33, 2015. [1243] W.-C. Fang, B. J. Sheu, O.-C. Chen, and J. Choi, “A vlsi neural pro- cessor for image data compression using self-organization networks,” Neural Networks, IEEE Transactions on, vol. 3, no. 3, pp. 506–518, 1992. [1244] J. Mann, R. Lippmann, B. Berger, and J. Raffel, “A self-organizing neural net chip,” in Custom Integrated Circuits Conference, 1988., Proceedings of the IEEE 1988. IEEE, 1988, pp. 10–3. [1245] K. Ben Khalifa, B. Girau, F. Alexandre, and M. Bedoui, “Parallel fpga implementation of self-organizing maps,” in Microelectronics, 2004. ICM 2004 Proceedings. The 16th International Conference on. IEEE, 2004, pp. 709–712. [1246] S. Brassai, “Fpga based hardware implementation of a self-organizing map,” in Intelligent Engineering Systems (INES), 2014 18th Interna- tional Conference on. IEEE, 2014, pp. 101–104. [1247] M. Porrmann, U. Witkowski, H. Kalte, and U. R¨uckert, “Dynami- cally reconﬁgurable hardware?a new perspective for neural network implementations,” in Field-Programmable Logic and Applications: Reconﬁgurable Computing Is Going Mainstream. Springer, 2002, pp. 1048–1057. [1248] H. Tamukoh and M. Sekine, “A dynamically reconﬁgurable platform for self-organizing neural network hardware,” in Neural Information Processing. Models and Applications. Springer, 2010, pp. 439–446. [1249] L. O. Chua and L. Yang, “Cellular neural networks: Applications,” IEEE Transactions on circuits and systems, vol. 35, no. 10, pp. 1273– 1290, 1988. [1250] Z. Wang, Y. Ma, F. Cheng, and L. Yang, “Review of pulse-coupled neural networks,” Image and Vision Computing, vol. 28, no. 1, pp. 5–13, 2010. [1251] J. Cruz and L. Chua, “A cnn chip for connected component detection,” IEEE transactions on Circuits and Systems, vol. 38, no. 7, pp. 812– 817, 1991. [1252] ——, “A 16× 16 cellular neural network universal chip: The ﬁrst complete single-chip dynamic computer array with distributed mem- ory and with gray-scale input-output,” Analog Integrated Circuits and Signal Processing, vol. 15, no. 3, pp. 227–237, 1998. [1253] G. Dalla Betta, S. Grafﬁ, Z. M. Kovacs, and G. Masetti, “Cmos imple- mentation of an analogically programmable cellular neural network,” Circuits and Systems II: Analog and Digital Signal Processing, IEEE Transactions on, vol. 40, no. 3, pp. 206–215, 1993. [1254] K. Halonen, V. Porra, T. Roska, and L. Chua, “Programmable analog vlsi cnn chip with local digital logic,” in Circuits and Systems, 1991., IEEE International Sympoisum on. IEEE, 1991, pp. 1291–1294. [1255] H. Harrer, J. A. Nossek, and R. Stelzl, “An analog implementation of discrete-time cellular neural networks,” Neural Networks, IEEE Transactions on, vol. 3, no. 3, pp. 466–476, 1992. [1256] P. Kinget and M. Steyaert, “Analogue cmos vlsi implementation of cellular neural networks with continuously programmable templates,” in Circuits and Systems, 1994. ISCAS’94., 1994 IEEE International Symposium on, vol. 6. IEEE, 1994, pp. 367–370. [1257] P. Kinget and M. S. Steyaert, “A programmable analog cellular neural network cmos chip for high speed image processing,” IEEE Journal of Solid-State Circuits, vol. 30, no. 3, pp. 235–243, 1995. [1258] A. Rodriguez-Vazquez, S. Espejo, R. Dominguez-Castron, J. L. Huertas, and E. Sanchez-Sinencio, “Current-mode techniques for the implementation of continuous-and discrete-time cellular neural networks,” IEEE Transactions on Circuits and Systems II: Analog and Digital Signal Processing, vol. 40, no. 3, pp. 132–146, 1993. [1259] L. Yang, L. Chua, and K. Krieg, “Vlsi implementation of cellular neural networks,” in Circuits and Systems, 1990., IEEE International Symposium on. IEEE, 1990, pp. 2425–2427. [1260] M. Salerno, F. Sargeni, and V. Bonaiuto, “A dedicated multi-chip programmable system for cellular neural networks,” Analog Integrated Circuits and Signal Processing, vol. 18, no. 2-3, pp. 277–288, 1999. [1261] P. Arena, L. Fortuna, M. Frasca, L. Patan´e, and M. Pollino, “An autonomous mini-hexapod robot controlled through a cnn-based cpg vlsi chip,” in Cellular Neural Networks and Their Applications, 2006. CNNA’06. 10th International Workshop on. IEEE, 2006, pp. 1–6. [1262] A. Ascoli, R. Tetzlaff, L. Chua, J. Strachan, and R. Williams, “Fading memory effects in a memristor for cellular nanoscale network appli- cations,” in 2016 Design, Automation & Test in Europe Conference & Exhibition (DATE). IEEE, 2016, pp. 421–425. [1263] F. Corinto, A. Ascoli, Y.-S. Kim, and K.-S. Min, “Cellular non- linear networks with memristor synapses,” in Memristor Networks. Springer, 2014, pp. 267–291. [1264] S. Duan, X. Hu, Z. Dong, L. Wang, and P. Mazumder, “Memristor- based cellular nonlinear/neural network: Design, analysis, and appli- cations,” 2014. [1265] Z. Guo, J. Wang, and Z. Yan, “Attractivity analysis of memristor-based cellular neural networks with time-varying delays,” Neural Networks and Learning Systems, IEEE Transactions on, vol. 25, no. 4, pp. 704– 717, 2014. [1266] S. Qin, J. Wang, and X. Xue, “Convergence and attractivity of memristor-based cellular neural networks with time delays,” Neural Networks, vol. 63, pp. 223–233, 2015. [1267] J.-J. Mart´ınez, F. J. Toledo, and J. Ferr´andez, “New emulated discrete model of cnn architecture for fpga and dsp applications,” in Inter- national Work-Conference on Artiﬁcial Neural Networks. Springer, 2003, pp. 33–40. [1268] ´A. R´ak, B. G. So´os, and G. Cserey, “Stochastic bitstream-based cnn and its implementation on fpga,” International Journal of Circuit Theory and Applications, vol. 37, no. 4, pp. 587–612, 2009. [1269] J. Chen and T. Shibata, “A hardware-implementation-friendly pulse- coupled neural network algorithm for analog image-feature-generation circuits,” Japanese journal of applied physics, vol. 46, no. 4S, p. 2271, 2007. [1270] ——, “A neuron-mos-based vlsi implementation of pulse-coupled neural networks for image feature generation,” Circuits and Systems I: Regular Papers, IEEE Transactions on, vol. 57, no. 6, pp. 1143–1153, 2010. [1271] Y. Ota and B. M. Wilamowski, “Analog implementation of pulse- coupled neural networks,” Neural Networks, IEEE Transactions on, vol. 10, no. 3, pp. 539–544, 1999. [1272] Y. Xiong, W.-H. Han, K. Zhao, Y.-B. Zhang, and F.-H. Yang, “An analog cmos pulse coupled neural network for image segmentation,” in Solid-State and Integrated Circuit Technology (ICSICT), 2010 10th IEEE International Conference on. IEEE, 2010, pp. 1883–1885. [1273] D. Matolin, S. Getzlaff et al., “An analog vlsi pulsed neural network implementation for image segmentation,” in null. IEEE, 2004, pp. 51–55. [1274] D. Matolin, J. Schreiter, R. SCHUFFNY, A. Heittmann, and U. Ra- macher, “Simulation and implementation of an analog vlsi pulse- coupled neural network for image segmentation,” in MWSCAS: Mid- west symposium on circuits and systems, 2004. 54 [1275] H. Torikai, H. Hamanaka, and T. Saito, “Novel digital spiking neuron and its pulse-coupled network: spike position coding and multiplex communication,” in Neural Networks, 2005. IJCNN’05. Proceedings. 2005 IEEE International Joint Conference on, vol. 5. IEEE, 2005, pp. 3249–3254. [1276] M. Ehrlich, C. Mayr, H. Eisenreich, S. Henker, A. Srowig, A. Gr¨ubl, J. Schemmel, and R. Sch¨uffny, “Wafer-scale vlsi implementations of pulse coupled neural networks,” in Proceedings of the International Conference on Sensors, Circuits and Instrumentation Systems, 2007. [1277] J. Vega-Pineda, M. Chac´on-Murgu´ıa, R. Camarillo-Cisneros et al., “Synthesis of pulsed-coupled neural networks in fpgas for real- time image segmentation,” in Neural Networks, 2006. IJCNN’06. International Joint Conference on. IEEE, 2006, pp. 4051–4055. [1278] J. Secco, M. Farina, D. Demarchi, and F. Corinto, “Memristor cellular automata through belief propagation inspired algorithm,” in 2015 International SoC Design Conference (ISOCC). IEEE, 2015, pp. 211–212. [1279] H. De Garis and M. Korkin, “The cam-brain machine (cbm): an fpga- based hardware tool that evolves a 1000 neuron-net circuit module in seconds and updates a 75 million neuron artiﬁcial brain for real-time robot control,” Neurocomputing, vol. 42, no. 1, pp. 35–68, 2002. [1280] K. Isobe and H. Torikai, “A novel hardware-efﬁcient asynchronous cellular automaton model of spike-timing dependent synaptic plastic- ity,” 2015. [1281] T. Matsubara, H. Torikai, and T. Hishiki, “A generalized rotate-and- ﬁre digital spiking neuron model and its on-fpga learning,” Circuits and Systems II: Express Briefs, IEEE Transactions on, vol. 58, no. 10, pp. 677–681, 2011. [1282] T. Matsubara and H. Torikai, “Asynchronous cellular automaton-based neuron: theoretical analysis and on-fpga learning,” Neural Networks and Learning Systems, IEEE Transactions on, vol. 24, no. 5, pp. 736– 748, 2013. [1283] Y.-H. Kuo, C.-I. Kao, and J.-J. Chen, “A fuzzy neural network model and its hardware implementation,” Fuzzy Systems, IEEE Transactions on, vol. 1, no. 3, pp. 171–183, 1993. [1284] T. Ibrayev, A. P. James, C. Merkel, and D. Kudithipudi, “A design of htm spatial pooler for face recognition using memristor-cmos hybrid circuits,” in 2016 IEEE International Symposium on Circuits and Systems, IEEE, May, 2016. [1285] J. Hawkins and S. Blakeslee, On intelligence. Macmillan, 2007. [1286] S. Haykin, “Neural networks: A comprehensive foundation,” 2004. [1287] A. Damak, M. Krid, D. S. Masmoudi, and N. Derbel, “Fpga im- plementation of programmable pulse mode neural network with on chip learning,” in Design and Test of Integrated Systems in Nanoscale Technology, 2006. DTIS 2006. International Conference on. IEEE, 2006, pp. 159–164. [1288] R. Hasan and T. M. Taha, “Enabling back propagation training of memristor crossbar neuromorphic processors,” in Neural Networks (IJCNN), 2014 International Joint Conference on. IEEE, 2014, pp. 21–28. [1289] H. Hikawa, “A new digital pulse-mode neuron with adjustable acti- vation function,” Neural Networks, IEEE Transactions on, vol. 14, no. 1, pp. 236–242, 2003. [1290] P. W. Hollis and J. J. Paulos, “A neural network learning algorithm tai- lored for vlsi implementation,” Neural Networks, IEEE Transactions on, vol. 5, no. 5, pp. 784–791, 1994. [1291] H. Ishii, T. Shibata, H. Kosaka, and T. Ohmi, “Hardware- backpropagation learning of neuron mos neural networks,” in Electron Devices Meeting, 1992. IEDM’92. Technical Digest., International. IEEE, 1992, pp. 435–438. [1292] S. Kondo, T. Shibata, and T. Ohmi, “Superior generalization capability of hardware-learing algorithm developed for self-learning neuron-mos neural networks,” Japanese journal of applied physics, vol. 34, no. 2S, p. 1066, 1995. [1293] M. Krid, A. Dammak, and D. S. Masmoudi, “Fpga implementation of programmable pulse mode neural network with on chip learning for signature application,” in Electronics, Circuits and Systems, 2006. ICECS’06. 13th IEEE International Conference on. IEEE, 2006, pp. 942–945. [1294] M. Prezioso, I. Kataeva, F. Merrikh-Bayat, B. Hoskins, G. Adam, T. Sota, K. Likharev, and D. Strukov, “Modeling and implemen- tation of ﬁring-rate neuromorphic-network classiﬁers with bilayer pt/al2o3/tio2¿¿¿ x/pt memristors,” in 2015 IEEE International Elec- tron Devices Meeting (IEDM). IEEE, 2015, pp. 17–4. [1295] D. Soudry, D. Di Castro, A. Gal, A. Kolodny, and S. Kvatinsky, “Memristor-based multilayer neural networks with online gradient descent training,” 2015. [1296] E. Zamanidoost, M. Klachko, D. Strukov, and I. Kataeva, “Low area overhead in-situ training approach for memristor-based classiﬁer,” in Nanoscale Architectures (NANOARCH), 2015 IEEE/ACM Interna- tional Symposium on. IEEE, 2015, pp. 139–142. [1297] D. Negrov, I. Karandashev, V. Shakirov, Y. Matveyev, W. Dunin- Barkowski, and A. Zenkevich, “An approximate backpropagation learning rule for memristor based neural networks using synaptic plasticity,” Neurocomputing, 2016. [1298] D. Neil, M. Pfeiffer, and S.-C. Liu, “Learning to be efﬁcient: Al- gorithms for training low-latency, low-compute deep spiking neural networks,” in Proceedings of the 31st Annual ACM Symposium on Applied Computing. ACM, 2016, pp. 293–298. [1299] M. Ueda, Y. Nishitani, Y. Kaneko, and A. Omote, “Back-propagation operation for analog neural network hardware with synapse compo- nents having hysteresis characteristics,” PLoS ONE, vol. 9, no. 11, p. e112659, 11 2014. [1300] S. Hussain, R. Gopalakrishnan, A. Basu, and S.-C. Liu, “Morpholog- ical learning: Increased memory capacity of neuromorphic systems with binary synapses exploiting aer based reconﬁguration,” in Neural Networks (IJCNN), The 2013 International Joint Conference on. IEEE, 2013, pp. 1–7. [1301] B. Li, Y. Wang, Y. Wang, Y. Chen, and H. Yang, “Training itself: Mixed-signal training acceleration for memristor-based neural net- work,” in Design Automation Conference (ASP-DAC), 2014 19th Asia and South Paciﬁc. IEEE, 2014, pp. 361–366. [1302] M. V. Nair and P. Dudek, “Gradient-descent-based learning in memris- tive crossbar arrays,” in Neural Networks (IJCNN), 2015 International Joint Conference on. IEEE, 2015, pp. 1–7. [1303] E. Rosenthal, S. Greshnikov, D. Soudry, and S. Kvatinsky, “A fully analog memristor-based neural network with online gradient training,” in Circuits and Systems (ISCAS), 2016 IEEE International Symposium on. IEEE, 2016, pp. 1394–1397. [1304] Y. Maeda and T. Tada, “Fpga implementation of a pulse density neural network with learning ability using simultaneous perturbation,” Neural Networks, IEEE Transactions on, vol. 14, no. 3, pp. 688–695, 2003. [1305] J. Fieres, J. Schemmel, and K. Meier, “Training convolutional net- works of threshold neurons suited for low-power hardware imple- mentation,” in Neural Networks, 2006. IJCNN’06. International Joint Conference on. IEEE, 2006, pp. 21–28. [1306] J. Fieres, K. Meier, and J. Schemmel, “A convolutional neural network tolerant of synaptic faults for low-power analog hardware,” in Artiﬁcial Neural Networks in Pattern Recognition. Springer, 2006, pp. 122–132. [1307] C. H. Bennett, S. La Barbera, A. F. Vincent, J.-O. Klein, F. Alibart, and D. Querlioz, “Exploiting the short-term to long-term plasticity transition in memristive nanodevice learning architectures,” in Neural Networks (IJCNN), 2016 International Joint Conference on. IEEE, 2016, pp. 947–954. [1308] D. Chabi, W. Zhao, D. Querlioz, and J.-O. Klein, “Robust neural logic block (nlb) based on memristor crossbar array,” in Nanoscale Architectures (NANOARCH), 2011 IEEE/ACM International Sympo- sium on. IEEE, 2011, pp. 137–143. [1309] D. Chabi, D. Querlioz, W. Zhao, and J.-O. Klein, “Robust learning approach for neuro-inspired nanoscale crossbar architecture,” ACM Journal on Emerging Technologies in Computing Systems (JETC), vol. 10, no. 1, p. 5, 2014. [1310] D. Chabi, Z. Wang, C. Bennett, J.-O. Klein, and W. Zhao, “Ultra high density memristor neural crossbar for on-chip supervised learning,” 2015. [1311] D. Chabi, W. Zhao, D. Querlioz, and J.-O. Klein, “On-chip universal supervised learning methods for neuro-inspired block of memristive nanodevices,” ACM Journal on Emerging Technologies in Computing Systems (JETC), vol. 11, no. 4, p. 34, 2015. [1312] M. Hu, H. Li, Y. Chen, Q. Wu, G. S. Rose, and R. W. Linderman, “Memristor crossbar-based neuromorphic computing system: A case study,” Neural Networks and Learning Systems, IEEE Transactions on, vol. 25, no. 10, pp. 1864–1878, 2014. [1313] M. A. Lewis, “Toward biomorphic control using custom avlsi chips,” in In Proceedings of IEEE International Conference on Robotics and Automation. Citeseer, 2000. [1314] Y. Liu, H. Huang, and T. Huang, “Associate learning law in a memristive neural network,” in Advanced Computational Intelligence (ICACI), 2013 Sixth International Conference on. IEEE, 2013, pp. 212–217. [1315] V. Petridis and K. Paraschidis, “On the properties of the feedforward method: A simple training law for on-chip learning,” Neural Networks, IEEE Transactions on, vol. 6, no. 6, pp. 1536–1541, 1995. 55 [1316] Q. Qiu, Z. Li, K. Ahmed, H. Li, and M. Hu, “Neuromorphic acceleration for context aware text image recognition,” in Signal Processing Systems (SiPS), 2014 IEEE Workshop on. IEEE, 2014, pp. 1–6. [1317] Q. Qiu, Z. Li, K. Ahmed, W. Liu, S. F. Habib, H. H. Li, and M. Hu, “A neuromorphic architecture for context aware text image recognition,” Journal of Signal Processing Systems, pp. 1–15, 2015. [1318] A. Schmid, Y. Leblebici, and D. Mlynek, “Mixed analogue–digital artiﬁcial-neural-network architecture with on-chip learning,” IEE Proceedings-Circuits, Devices and Systems, vol. 146, no. 6, pp. 345– 349, 1999. [1319] T. M. Taha, R. Hasan, and C. Yakopcic, “Memristor crossbar based multicore neuromorphic processors,” in System-on-Chip Conference (SOCC), 2014 27th IEEE International. IEEE, 2014, pp. 383–389. [1320] R. Woodburn, H. M. Reekie, and A. F. Murray, “Pulse-stream circuits for on-chip learning in analogue vlsi neural networks,” in Circuits and Systems, 1994. ISCAS’94., 1994 IEEE International Symposium on, vol. 4. IEEE, 1994, pp. 103–106. [1321] L. Buhry, S. Sa¨ıghi, W. Ben Salem, and S. Renaud, “Adjusting neuron models in neuromimetic ics using the differential evolution algorithm,” in Neural Engineering, 2009. NER’09. 4th International IEEE/EMBS Conference on. IEEE, 2009, pp. 681–684. [1322] L. Buhry, S. Saighi, A. Giremus, E. Grivel, and S. Renaud, “Auto- mated tuning of analog neuromimetic integrated circuits,” in Biomed- ical Circuits and Systems Conference, 2009. BioCAS 2009. IEEE. IEEE, 2009, pp. 13–16. [1323] L. Buhry, M. Pace, and S. Sa¨ıghi, “Global parameter estimation of an hodgkin–huxley formalism using membrane voltage recordings: Application to neuro-mimetic analog integrated circuits,” Neurocom- puting, vol. 81, pp. 75–85, 2012. [1324] F. Grassia, L. Buhry, T. L´evi, J. Tomas, A. Destexhe, and S. Sa¨ıghi, “Tunable neuromimetic integrated system for emulating cortical neu- ron models,” Frontiers in neuroscience, vol. 5, 2011. [1325] L. Buhry, S. Sa¨ıghi, A. Giremus, E. Grivel, and S. Renaud, “Param- eter estimation of the hodgkin-huxley model using metaheuristics: application to neuromimetic analog integrated circuits,” in Biomedical Circuits and Systems Conference, 2008. BioCAS 2008. IEEE. IEEE, 2008, pp. 173–176. [1326] K. D. Carlson, N. Dutt, J. M. Nageswaran, and J. L. Krichmar, “Design space exploration and parameter tuning for neuromorphic ap- plications,” in Proceedings of the Ninth IEEE/ACM/IFIP International Conference on Hardware/Software Codesign and System Synthesis. IEEE Press, 2013, p. 20. [1327] K. D. Carlson, M. Beyeler, N. Dutt, and J. L. Krichmar, “Gpgpu accelerated simulation and parameter tuning for neuromorphic appli- cations,” in Design Automation Conference (ASP-DAC), 2014 19th Asia and South Paciﬁc. IEEE, 2014, pp. 570–577. [1328] K. D. Carlson, J. M. Nageswaran, N. Dutt, and J. L. Krichmar, “An efﬁcient automated parameter tuning framework for spiking neural networks,” Neuromorphic Engineering Systems and Applications, p. 168, 2015. [1329] G. Howard, E. Gale, L. Bull, B. de Lacy Costello, and A. Adamatzky, “Towards evolving spiking networks with memristive synapses,” in Artiﬁcial Life (ALIFE), 2011 IEEE Symposium on. IEEE, 2011, pp. 14–21. [1330] ——, “Evolution of plastic learning in spiking networks via memris- tive connections,” Evolutionary Computation, IEEE Transactions on, vol. 16, no. 5, pp. 711–729, 2012. [1331] G. Howard, L. Bull, B. de Lacy Costello, E. Gale, and A. Adamatzky, “Evolving spiking networks with variable resistive memories,” Evo- lutionary computation, vol. 22, no. 1, pp. 79–103, 2014. [1332] G. D. Howard, L. Bull, B. D. L. Costello, E. Gale, and A. Adamatzky, “Evolving memristive neural networks,” in Memristor Networks. Springer, 2014, pp. 293–322. [1333] D. Howard, L. Bull, and B. de Lacy Costello, “Evolving unipolar memristor spiking neural networks,” in Artiﬁcial Life and Computa- tional Intelligence. Springer, 2015, pp. 258–272. [1334] J. Maher, B. McGinley, P. Rocke, and F. Morgan, “Intrinsic hardware evolution of neural networks in reconﬁgurable analogue and digital devices,” in Field-Programmable Custom Computing Machines, 2006. FCCM’06. 14th Annual IEEE Symposium on. IEEE, 2006, pp. 321– 322. [1335] G. Orchard, A. Russell, K. Mazurek, F. Tenore, and R. Etienne- Cummings, “Conﬁguring silicon neural networks using genetic al- gorithms,” in Circuits and Systems, 2008. ISCAS 2008. IEEE Inter- national Symposium on. IEEE, 2008, pp. 1048–1051. [1336] C. D. Schuman, J. S. Plank, A. Disney, and J. Reynolds, “An evo- lutionary optimization framework for neural networks and neuromor- phic architectures,” in Neural Networks (IJCNN), 2016 International Joint Conference on. IEEE, 2016, pp. 145–154. [1337] C. D. Schuman, A. Disney, S. P. Singh, G. Bruer, J. P. Mitchell, A. Klibisz, and J. S. Plank, “Parallel evolutionary optimization for neuromorphic network training,” in Proceedings of the Workshop on Machine Learning in High Performance Computing Environments. IEEE Press, 2016, pp. 36–46. [1338] D. Braendler and T. Hendtlass, “The suitability of particle swarm op- timisation for training neural hardware,” in Developments in Applied Artiﬁcial Intelligence. Springer, 2002, pp. 190–199. [1339] S. Cawley, F. Morgan, B. McGinley, S. Pande, L. McDaid, and J. Harkin, “The impact of neural model resolution on hardware spiking neural network behaviour,” 2010. [1340] S. Cawley, F. Morgan, B. McGinley, S. Pande, L. McDaid, S. Carrillo, and J. Harkin, “Hardware spiking neural network prototyping and application,” Genetic Programming and Evolvable Machines, vol. 12, no. 3, pp. 257–280, 2011. [1341] J. Fe, R. J. Aliaga, and R. Gadea, “Experimental platform for accelerate the training of anns with genetic algorithm and embedded system on fpga,” in Natural and Artiﬁcial Computation in Engineering and Medical Applications. Springer, 2013, pp. 413–420. [1342] K.-S. Low, V. Krishnan, H. Zhuang, and W.-Y. Yau, “On-chip genetic algorithm optimized pulse based rbf neural network for unsupervised clustering problem,” in Advances in Natural Computation. Springer, 2006, pp. 851–860. [1343] S. Merchant, G. D. Peterson, S. K. Park, and S. G. Kong, “Fpga implementation of evolvable block-based neural networks,” in Evolu- tionary Computation, 2006. CEC 2006. IEEE Congress on. IEEE, 2006, pp. 3129–3136. [1344] S. Merchant, G. D. Peterson, and S. G. Kong, “Intrinsic embedded hardware evolution of block-based neural networks.” in ERSA. Cite- seer, 2006, pp. 211–214. [1345] S. G. Merchant and G. D. Peterson, “An evolvable artiﬁcial neural network platform for dynamic environments,” in Circuits and Systems, 2008. MWSCAS 2008. 51st Midwest Symposium on. IEEE, 2008, pp. 77–80. [1346] ——, “Evolvable block-based neural network design for applications in dynamic environments,” VLSI Design, vol. 2010, p. 4, 2010. [1347] F. Morgan, S. Cawley, B. McGinley, S. Pande, L. J. McDaid, B. Glackin, J. Maher, and J. Harkin, “Exploring the evolution of noc-based spiking neural networks on fpgas,” in Field-Programmable Technology, 2009. FPT 2009. International Conference on. IEEE, 2009, pp. 300–303. [1348] V. P. Nambiar, M. Khalil-Hani, R. Sahnoun, and M. Marsono, “Hardware implementation of evolvable block-based neural networks utilizing a cost efﬁcient sigmoid-like activation function,” Neurocom- puting, vol. 140, pp. 228–241, 2014. [1349] M. NirmalaDevi, N. Mohankumar, and S. Arumugam, “Modeling and analysis of neuro–genetic hybrid system on fpga,” Elektronika ir Elektrotechnika, vol. 96, no. 8, pp. 43–46, 2015. [1350] A. Upegui and E. Sanchez, “Evolving hardware with self- reconﬁgurable connectivity in xilinx fpgas,” in Adaptive Hardware and Systems, 2006. AHS 2006. First NASA/ESA Conference on. IEEE, 2006, pp. 153–162. [1351] A. Bezborah, “A hardware architecture for training of artiﬁcial neural networks using particle swarm optimization,” in Intelligent Systems, Modelling and Simulation (ISMS), 2012 Third International Confer- ence on. IEEE, 2012, pp. 67–70. [1352] M. A. Cavuslu, C. Karakuzu, and F. Karakaya, “Neural identiﬁcation of dynamic systems on fpga with improved pso learning,” Applied Soft Computing, vol. 12, no. 9, pp. 2707–2718, 2012. [1353] A. Farmahini-Farahani, S. M. Fakhraie, and S. Safari, “Scalable archi- tecture for on-chip neural network training using swarm intelligence,” in Proceedings of the conference on Design, automation and test in Europe. ACM, 2008, pp. 1340–1345. [1354] C.-J. Lin and H.-M. Tsai, “Fpga implementation of a wavelet neural network with particle swarm optimization learning,” Mathematical and Computer Modelling, vol. 47, no. 9, pp. 982–996, 2008. [1355] P. Hasler and L. Akers, “Circuit implementation of a trainable neural network using the generalized hebbian algorithm with supervised techniques,” in Neural Networks, 1992. IJCNN., International Joint Conference on, vol. 1. IEEE, 1992, pp. 160–165. [1356] X. Hu, S. Duan, and L. Wang, “A novel chaotic neural network using memristive synapse with applications in associative memory,” 56 in Abstract and Applied Analysis, vol. 2012. Hindawi Publishing Corporation, 2012. [1357] T. C. Jackson, R. Shi, A. A. Sharma, J. A. Bain, J. A. Weldon, and L. Pileggi, “Implementing delay insensitive oscillatory neural networks using cmos and emerging technology,” Analog Integrated Circuits and Signal Processing, vol. 89, no. 3, pp. 619–629, 2016. [1358] F. Merrikh-Bayat and S. B. Shouraki, “The neuro-fuzzy computing system with the capacity of implementation on a memristor cross- bar and optimization-free hardware training,” Fuzzy Systems, IEEE Transactions on, vol. 22, no. 5, pp. 1272–1287, 2014. [1359] G. Rachmuth and C.-S. Poon, “Design of a neuromorphic hebbian synapse using analog vlsi,” in Neural Engineering, 2003. Conference Proceedings. First International IEEE EMBS Conference on. IEEE, 2003, pp. 221–224. [1360] M. Rossmann, A. B¨uhlmeier, G. Manteuffel, and K. Goser, “short-and long-term dynamics in a stochastic pulse stream neuron implemented in fpga,” in Artiﬁcial Neural Networks?ICANN’97. Springer, 1997, pp. 1241–1246. [1361] M. Schafer and G. Hartmann, “A ﬂexible hardware architecture for online hebbian learning in the sender-oriented pcnn-neurocomputer spike 128 k,” in Microelectronics for Neural, Fuzzy and Bio-Inspired Systems, 1999. MicroNeuro’99. Proceedings of the Seventh Interna- tional Conference on. IEEE, 1999, pp. 316–323. [1362] M. Soltiz, C. Merkel, D. Kudithipudi, and G. S. Rose, “Rram-based adaptive neural logic block for implementing non-linearly separable functions in a single layer,” in Nanoscale Architectures (NANOARCH), 2012 IEEE/ACM International Symposium on. IEEE, 2012, pp. 218– 225. [1363] J. Tomberg and K. Kaski, “An effective training method for fully dig- ital pulse-density modulated neural network architecture,” in Circuits and Systems, 1991., IEEE International Sympoisum on. IEEE, 1991, pp. 1497–1500. [1364] S. Acciarito, A. Cristini, L. Di Nunzio, G. M. Khanal, and G. Susi, “An a vlsi driving circuit for memristor-based stdp,” in Ph. D. Research in Microelectronics and Electronics (PRIME), 2016 12th Conference on. IEEE, 2016, pp. 1–4. [1365] A. Aﬁﬁ, A. Ayatollahi, and F. Raissi, “Stdp implementation using memristive nanodevice in cmos-nano neuromorphic networks,” IEICE Electronics Express, vol. 6, no. 3, pp. 148–153, 2009. [1366] ——, “Implementation of biologically plausible spiking neural net- work models on the memristor crossbar-based cmos/nano circuits,” in Circuit Theory and Design, 2009. ECCTD 2009. European Confer- ence on. IEEE, 2009, pp. 563–566. [1367] J. V. Arthur and K. Boahen, “Recurrently connected silicon neurons with active dendrites for one-shot learning,” in Neural Networks, 2004. Proceedings. 2004 IEEE International Joint Conference on, vol. 3. IEEE, 2004, pp. 1699–1704. [1368] M. R. Azghadi, B. Linares-Barranco, D. Abbott, and P. H. Leong, “A hybrid cmos-memristor neuromorphic synapse,” IEEE Transactions on Biomedical Circuits and Systems, 2016. [1369] Y. Babacan and F. Kac¸ar, “Memristor emulator with spike-timing- dependent-plasticity,” AEU-International Journal of Electronics and Communications, vol. 73, pp. 16–22, 2017. [1370] R. Berdan, E. Vasilaki, A. Khiat, G. Indiveri, A. Serb, and T. Pro- dromakis, “Emulating short-term synaptic dynamics with memristive devices,” Scientiﬁc reports, vol. 6, 2016. [1371] J. Bill and R. Legenstein, “A compound memristive synapse model for statistical learning through stdp in spiking neural networks,” Frontiers in neuroscience, vol. 8, 2014. [1372] S. Brink, S. Nease, P. Hasler, S. Ramakrishnan, R. Wunderlich, A. Basu, and B. Degnan, “A learning-enabled neuron array ic based upon transistor channel models of biological phenomena,” Biomedical Circuits and Systems, IEEE Transactions on, vol. 7, no. 1, pp. 71–81, 2013. [1373] W. Cai and R. Tetzlaff, “Advanced memristive model of synapses with adaptive thresholds,” in Cellular Nanoscale Networks and Their Applications (CNNA), 2012 13th International Workshop on. IEEE, 2012, pp. 1–6. [1374] W. Cai, F. Ellinger, and R. Tetzlaff, “Neuronal synapse as a memristor: Modeling pair-and triplet-based stdp rule,” Biomedical Circuits and Systems, IEEE Transactions on, vol. 9, no. 1, pp. 87–95, 2015. [1375] K. D. Cantley, A. Subramaniam, H. J. Stiegler, R. Chapman, E. M. Vogel et al., “Spike timing-dependent synaptic plasticity using mem- ristors and nano-crystalline silicon tft memories,” in Nanotechnology (IEEE-NANO), 2011 11th IEEE Conference on. IEEE, 2011, pp. 421–425. [1376] W. Chan and J. Lohn, “Spike timing dependent plasticity with memristive synapse in neuromorphic systems,” in Neural Networks (IJCNN), The 2012 International Joint Conference on. IEEE, 2012, pp. 1–6. [1377] L. Chen, C. Li, T. Huang, X. He, H. Li, and Y. Chen, “Stdp learning rule based on memristor with stdp property,” in Neural Networks (IJCNN), 2014 International Joint Conference on. IEEE, 2014, pp. 1–6. [1378] P.-Y. Chen, B. Lin, I. Wang, T.-H. Hou, J. Ye, S. Vrudhula, J.-s. Seo, Y. Cao, S. Yu et al., “Mitigating effects of non-ideal synaptic device characteristics for on-chip learning,” in Computer-Aided Design (IC- CAD), 2015 IEEE/ACM International Conference on. IEEE, 2015, pp. 194–199. [1379] P.-Y. Chen, L. Gao, and S. Yu, “Design of resistive synaptic array for implementing on-chip sparse learning,” IEEE Transactions on Multi- Scale Computing Systems, vol. 2, no. 4, pp. 257–264, 2016. [1380] M. Chu, B. Kim, S. Park, H. Hwang, M. Jeon, B. H. Lee, and B.-G. Lee, “Neuromorphic hardware system for visual pattern recognition with memristor array and cmos neuron,” Industrial Electronics, IEEE Transactions on, vol. 62, no. 4, pp. 2410–2419, 2015. [1381] E. Covi, S. Brivio, A. Serb, T. Prodromakis, M. Fanciulli, and S. Spiga, “Analog memristive synapse in spiking networks imple- menting unsupervised learning,” Frontiers in neuroscience, vol. 10, 2016. [1382] J. M. Cruz-Albrecht, T. Derosier, and N. Srinivasa, “A scalable neural chip with synaptic electronics using cmos integrated memristors,” Nanotechnology, vol. 24, no. 38, p. 384011, 2013. [1383] Y. Dai, C. Li, and H. Wang, “Expanded hp memristor model and simulation in stdp learning,” Neural Computing and Applications, vol. 24, no. 1, pp. 51–57, 2014. [1384] B. DeSalvo, E. Vianello, O. Thomas, F. Clermidy, O. Bichler, C. Gam- rat, and L. Perniola, “Emerging resistive memories for low power embedded applications and neuromorphic systems,” in Circuits and Systems (ISCAS), 2015 IEEE International Symposium on. IEEE, 2015, pp. 3088–3091. [1385] N. Du, M. Kiani, C. G. Mayr, T. You, D. B¨urger, I. Skorupa, O. G. Schmidt, and H. Schmidt, “Single pairing spike-timing dependent plasticity in bifeo3 memristors with a time window of 25 ms to 125 µs,” Frontiers in neuroscience, vol. 9, 2015. [1386] I. Ebong and P. Mazumder, “Memristor based stdp learning network for position detection,” in Microelectronics (ICM), 2010 International Conference on. IEEE, 2010, pp. 292–295. [1387] I. Ebong, D. Deshpande, Y. Yilmaz, and P. Mazumder, “Multi- purpose neuro-architecture with memristors,” in Nanotechnology (IEEE-NANO), 2011 11th IEEE Conference on. IEEE, 2011, pp. 431–435. [1388] I. E. Ebong and P. Mazumder, “Cmos and memristor-based neural network design for position detection,” Proceedings of the IEEE, vol. 100, no. 6, pp. 2050–2060, 2012. [1389] E. Gale, B. D. L. Costello, and A. Adamatzky, “Observation and characterization of memristor current spikes and their application to neuromorphic computation,” in NUMERICAL ANALYSIS AND APPLIED MATHEMATICS ICNAAM 2012: International Conference of Numerical Analysis and Applied Mathematics, vol. 1479, no. 1. AIP Publishing, 2012, pp. 1898–1901. [1390] C. Gamrat, O. Bichler, and D. Roclin, “Memristive based device arrays combined with spike based coding can enable efﬁcient im- plementations of embedded neuromorphic circuits,” in 2015 IEEE International Electron Devices Meeting (IEDM). IEEE, 2015, pp. 4–5. [1391] W. He, K. Huang, N. Ning, K. Ramanathan, G. Li, Y. Jiang, J. Sze, L. Shi, R. Zhao, and J. Pei, “Enabling an integrated rate-temporal learning scheme on memristor,” Scientiﬁc reports, vol. 4, p. 4755, 2014. [1392] S. Hu, H. Wu, Y. Liu, T. Chen, Z. Liu, Q. Yu, Y. Yin, and S. Hosaka, “Design of an electronic synapse with spike time dependent plasticity based on resistive memory device,” Journal of Applied Physics, vol. 113, no. 11, p. 114502, 2013. [1393] M. Hu, Y. Chen, J. J. Yang, Y. Wang, and H. Li, “A memristor-based dynamic synapse for spiking neural networks,” IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 2016. [1394] F. L. M. Huayaney, S. Nease, and E. Chicca, “Learning in silicon beyond stdp: A neuromorphic implementation of multi-factor synap- tic plasticity with calcium-based dynamics,” IEEE Transactions on Circuits and Systems I: Regular Papers, vol. 63, no. 12, pp. 2189– 2199, 2016. 57 [1395] F. L. M. Huayaney and E. Chicca, “A vlsi implementation of a calcium-based plasticity learning model,” in Circuits and Systems (ISCAS), 2016 IEEE International Symposium on. IEEE, 2016, pp. 373–376. [1396] G. Indiveri and S. Fusi, “Spike-based learning in vlsi networks of integrate-and-ﬁre neurons,” in Circuits and Systems, 2007. ISCAS 2007. IEEE International Symposium on. IEEE, 2007, pp. 3371– 3374. [1397] S. H. Jo, T. Chang, I. Ebong, B. B. Bhadviya, P. Mazumder, and W. Lu, “Nanoscale memristor device as synapse in neuromorphic systems,” Nano letters, vol. 10, no. 4, pp. 1297–1301, 2010. [1398] Y. Kaneko, Y. Nishitani, M. Ueda, and A. Tsujimura, “Neural network based on a three-terminal ferroelectric memristor to enable on-chip pattern recognition,” in VLSI Technology (VLSIT), 2013 Symposium on. IEEE, 2013, pp. T238–T239. [1399] Y. Kaneko, Y. Nishitani, and M. Ueda, “Ferroelectric artiﬁcial synapses for recognition of a multishaded image,” Electron Devices, IEEE Transactions on, vol. 61, no. 8, pp. 2827–2833, 2014. [1400] O. Kavehei, S. Al-Sarawi, K.-R. Cho, N. Iannella, S.-J. Kim, K. Eshraghian, and D. Abbott, “Memristor-based synaptic networks and logical operations using in-situ computing,” in Intelligent Sensors, Sensor Networks and Information Processing (ISSNIP), 2011 Seventh International Conference on. IEEE, 2011, pp. 137–142. [1401] Y. Kim, Y. Zhang, and P. Li, “A digital neuromorphic vlsi architecture with memristor crossbar synaptic array for machine learning,” in SOC Conference (SOCC), 2012 IEEE International. IEEE, 2012, pp. 328– 333. [1402] ——, “A reconﬁgurable digital neuromorphic processor with mem- ristive synaptic crossbar for cognitive computing,” ACM Journal on Emerging Technologies in Computing Systems (JETC), vol. 11, no. 4, p. 38, 2015. [1403] G. Lecerf, J. Tomas, and S. Sa¨ıghi, “Excitatory and inhibitory mem- ristive synapses for spiking neural networks,” in Circuits and Systems (ISCAS), 2013 IEEE International Symposium on. IEEE, 2013, pp. 1616–1619. [1404] Y. Li, L. Xu, Y.-P. Zhong, Y.-X. Zhou, S.-J. Zhong, Y.-Z. Hu, L. O. Chua, and X.-S. Miao, “Associative learning with temporal contigu- ity in a memristive circuit for large-scale neuromorphic networks,” Advanced Electronic Materials, vol. 1, no. 8, 2015. [1405] Q. Li, A. Serb, T. Prodromakis, and H. Xu, “A memristor spice model accounting for synaptic activity dependence,” PloS one, vol. 10, no. 3, p. e0120506, 2015. [1406] G. Li, L. Deng, D. Wang, W. Wang, F. Zeng, Z. Zhang, H. Li, S. Song, J. Pei, and L. Shi, “Hierarchical chunking of sequential memory on neuromorphic architecture with reduced synaptic plasticity,” Frontiers in Computational Neuroscience, vol. 10, 2016. [1407] B. Linares-Barranco and T. Serrano-Gotarredona, “Exploiting mem- ristance in adaptive asynchronous spiking neuromorphic nanotechnol- ogy systems,” in Nanotechnology, 2009. IEEE-NANO 2009. 9th IEEE Conference on. IEEE, 2009, pp. 601–604. [1408] D. Mahalanabis, M. Sivaraj, W. Chen, S. Shah, H. Barnaby, M. Koz- icki, J. B. Christen, and S. Vrudhula, “Demonstration of spike timing dependent plasticity in cbram devices with silicon neurons,” in Circuits and Systems (ISCAS), 2016 IEEE International Symposium on. IEEE, 2016, pp. 2314–2317. [1409] S. Mandal, B. Long, and R. Jha, “Study of synaptic behavior in doped transition metal oxide-based reconﬁgurable devices,” Electron Devices, IEEE Transactions on, vol. 60, no. 12, pp. 4219–4225, 2013. [1410] P. Mazumder, D. Hu, I. Ebong, X. Zhang, Z. Xu, and S. Ferrari, “Digital implementation of a virtual insect trained by spike-timing dependent plasticity,” Integration, the VLSI Journal, vol. 54, pp. 109– 117, 2016. [1411] Y. Meng, K. Zhou, J. J. Monzon, and C.-S. Poon, “Iono-neuromorphic implementation of spike-timing-dependent synaptic plasticity,” in En- gineering in Medicine and Biology Society, EMBC, 2011 Annual International Conference of the IEEE. IEEE, 2011, pp. 7274–7277. [1412] K. Moon, S. Park, D. Lee, J. Woo, E. Cha, S. Lee, and H. Hwang, “Resistive-switching analogue memory device for neuromorphic ap- plication,” in Silicon Nanoelectronics Workshop (SNW), 2014 IEEE. IEEE, 2014, pp. 1–2. [1413] H. Mostafa, A. Khiat, A. Serb, C. G. Mayr, G. Indiveri, and T. Pro- dromakis, “Implementation of a spike-based perceptron learning rule using tio2- x memristors,” Frontiers in Neuroscience, vol. 9, p. 357, 2015. [1414] H. Mostafa, C. Mayr, and G. Indiveri, “Beyond spike-timing depen- dent plasticity in memristor crossbar arrays,” in Circuits and Systems (ISCAS), 2016 IEEE International Symposium on. IEEE, 2016, pp. 926–929. [1415] R. Naous, M. Al-Shedivat, E. Neftci, G. Cauwenberghs, and K. N. Salama, “Stochastic synaptic plasticity with memristor crossbar ar- rays,” in Circuits and Systems (ISCAS), 2016 IEEE International Symposium on. IEEE, 2016, pp. 2078–2081. [1416] A. Pantazi, S. Wo´zniak, T. Tuma, and E. Eleftheriou, “All-memristive neuromorphic computing with level-tuned neurons,” Nanotechnology, vol. 27, no. 35, p. 355205, 2016. [1417] S. Park, H. Kim, M. Choo, J. Noh, A. Sheri, S. Jung, K. Seo, J. Park, S. Kim, W. Lee et al., “Rram-based synapse for neuromorphic sys- tem with pattern recognition function,” in Electron Devices Meeting (IEDM), 2012, pp. 10–2. [1418] S. Park, A. Sheri, J. Kim, J. Noh, J. Jang, M. Jeon, B. Lee, B. Lee, B. Lee, and H.-j. Hwang, “Neuromorphic speech systems using advanced reram-based synapse,” IEDM Tech Dig, vol. 25, pp. 1–25, 2013. [1419] J. Park, M.-W. Kwon, H. Kim, and B.-G. Park, “Neuromorphic system based on cmos inverters and si-based synaptic device,” Journal of Nanoscience and Nanotechnology, vol. 16, no. 5, pp. 4709–4712, 2016. [1420] M. Payvand, J. Rofeh, A. Sodhi, and L. Theogarajan, “A cmos- memristive self-learning neural network for pattern classiﬁca- tion applications,” in Nanoscale Architectures (NANOARCH), 2014 IEEE/ACM International Symposium on. IEEE, 2014, pp. 92–97. [1421] M. Payvand and L. Theogarajan, “Exploiting local connectivity of cmol architecture for highly parallel orientation selective neuro- morphic chips,” in Nanoscale Architectures (NANOARCH), 2015 IEEE/ACM International Symposium on. IEEE, 2015, pp. 187–192. [1422] J. A. P´erez-Carrasco, C. Zamarre˜no-Ramos, T. Serrano-Gotarredona, and B. Linares-Barranco, “On neuromorphic spiking architectures for asynchronous stdp memristive systems,” in Circuits and Systems (ISCAS), Proceedings of 2010 IEEE International Symposium on. IEEE, 2010, pp. 1659–1662. [1423] M. Prezioso, F. Merrikh-Bayat, B. Chakrabarti, and D. Strukov, “Rram-based hardware implementations of artiﬁcial neural networks: progress update and challenges ahead,” in SPIE OPTO. International Society for Optics and Photonics, 2016, pp. 974 918–974 918. [1424] M. Prezioso, Y. Zhong, D. Gavrilov, F. Merrikh-Bayat, B. Hoskins, G. Adam, K. Likharev, and D. Strukov, “Spiking neuromorphic networks with metal-oxide memristors,” in Circuits and Systems (ISCAS), 2016 IEEE International Symposium on. IEEE, 2016, pp. 177–180. [1425] M. Prezioso, F. M. Bayat, B. Hoskins, K. Likharev, and D. Strukov, “Self-adaptive spike-time-dependent plasticity of metal-oxide mem- ristors,” Scientiﬁc reports, vol. 6, 2016. [1426] M. Prezioso, “Experimental analog implementation of neural networks on integrated metal-oxide memristive crossbar arrays,” in Nanotech- nology (IEEE-NANO), 2016 IEEE 16th International Conference on. IEEE, 2016, pp. 276–279. [1427] D. Querlioz, W. Zhao, P. Dollfus, J. Klein, O. Bichler, and C. Gam- rat, “Bioinspired networks with nanoscale memristive devices that combine the unsupervised and supervised learning approaches,” in Nanoscale Architectures (NANOARCH), 2012 IEEE/ACM Interna- tional Symposium on. IEEE, 2012, pp. 203–210. [1428] Q. Ren, Q. Long, Z. Zhang, and J. Zhao, “Information transfer characteristic in memristic neuromorphic network,” in Advances in Neural Networks–ISNN 2013. Springer, 2013, pp. 1–8. [1429] S. Sa¨ıghi, J. Tomas, Y. Bornat, B. Belhadj, O. Malot, and S. Renaud, “Real-time multi-board architecture for analog spiking neural net- works,” in Circuits and Systems (ISCAS), Proceedings of 2010 IEEE International Symposium on. IEEE, 2010, pp. 1939–1942. [1430] K. Seo, I. Kim, S. Jung, M. Jo, S. Park, J. Park, J. Shin, K. P. Biju, J. Kong, K. Lee et al., “Analog memory and spike-timing-dependent plasticity characteristics of a nanoscale titanium oxide bilayer resistive switching device,” Nanotechnology, vol. 22, no. 25, p. 254023, 2011. [1431] T. Serrano-Gotarredona and B. Linares-Barranco, “Design of adaptive nano/cmos neural architectures,” in Electronics, Circuits and Systems (ICECS), 2012 19th IEEE International Conference on. IEEE, 2012, pp. 949–952. [1432] T. Serrano-Gotarredona, T. Masquelier, and B. Linares-Barranco, “Spike-timing-dependent-plasticity with memristors,” in Memristor Networks. Springer, 2014, pp. 211–247. [1433] M. Shahsavari, P. Falez, and P. Boulet, “Combining a volatile and nonvolatile memristor in artiﬁcial synapse to improve learning in spiking neural networks,” in Nanoscale Architectures (NANOARCH), 58 2016 IEEE/ACM International Symposium on. IEEE, 2016, pp. 67– 72. [1434] S. Sheik, M. Coath, G. Indiveri, S. L. Denham, T. Wennekers, and E. Chicca, “Emergent auditory feature tuning in a real-time neuromorphic vlsi system,” Frontiers in neuroscience, vol. 6, 2012. [1435] A. M. Sheri, H. Hwang, M. Jeon, and B.-g. Lee, “Neuromorphic character recognition system with two pcmo memristors as a synapse,” Industrial Electronics, IEEE Transactions on, vol. 61, no. 6, pp. 2933– 2941, 2014. [1436] A. Singha, B. Muralidharan, and B. Rajendran, “Analog memristive time dependent learning using discrete nanoscale rram devices,” in Neural Networks (IJCNN), 2014 International Joint Conference on. IEEE, 2014, pp. 2248–2255. [1437] G. S. Snider, “Spike-timing-dependent learning in memristive nanode- vices,” in Nanoscale Architectures, 2008. NANOARCH 2008. IEEE International Symposium on. IEEE, 2008, pp. 85–92. [1438] A. Subramaniam, K. D. Cantley, G. Bersuker, D. Gilmer, and E. M. Vogel, “Spike-timing-dependent plasticity using biologically realistic action potentials and low-temperature materials,” Nanotechnology, IEEE Transactions on, vol. 12, no. 3, pp. 450–459, 2013. [1439] M. Suri, D. Querlioz, O. Bichler, G. Palma, E. Vianello, D. Vuillaume, C. Gamrat, and B. DeSalvo, “Bio-inspired stochastic computing using binary cbram synapses,” Electron Devices, IEEE Transactions on, vol. 60, no. 7, pp. 2402–2409, 2013. [1440] Z. Wang, W. Zhao, W. Kang, Y. Zhang, J.-O. Klein, and C. Chap- pert, “Ferroelectric tunnel memristor-based neuromorphic network with 1t1r crossbar architecture,” in Neural Networks (IJCNN), 2014 International Joint Conference on. IEEE, 2014, pp. 29–34. [1441] Z. Wang, S. Ambrogio, S. Balatti, and D. Ielmini, “A 2-transistor/1- resistor artiﬁcial synapse capable of communication and stochastic learning in neuromorphic systems,” Frontiers in neuroscience, vol. 8, 2014. [1442] T. Werner, E. Vianello, O. Bichler, D. Garbin, D. Cattaert, B. Yvert, B. De Salvo, and L. Perniola, “Spiking neural networks based on oxram synapses for real-time unsupervised spike sorting,” Frontiers in Neuroscience, vol. 10, 2016. [1443] T. Werner, D. Garbin, E. Vianello, O. Bichler, D. Cattaert, B. Yvert, B. De Salvo, and L. Perniola, “Real-time decoding of brain activity by embedded spiking neural networks using oxram synapses,” in Circuits and Systems (ISCAS), 2016 IEEE International Symposium on. IEEE, 2016, pp. 2318–2321. [1444] J. H. Wijekoon and P. Dudek, “Vlsi circuits implementing com- putational models of neocortical circuits,” Journal of neuroscience methods, vol. 210, no. 1, pp. 93–109, 2012. [1445] S. Wo´zniak, T. Tuma, A. Pantazi, and E. Eleftheriou, “Learning spatio-temporal patterns in the presence of input noise using phase- change memristors,” in Circuits and Systems (ISCAS), 2016 IEEE International Symposium on. IEEE, 2016, pp. 365–368. [1446] X. Wu, V. Saxena, and K. Zhu, “Homogeneous spiking neuromorphic system for real-world pattern recognition,” 2015. [1447] ——, “A cmos spiking neuron for dense memristor-synapse connec- tivity for brain-inspired computing,” in Neural Networks (IJCNN), 2015 International Joint Conference on. IEEE, 2015, pp. 1–6. [1448] X. Wu, V. Saxena, K. Zhu, and S. Balagopal, “A cmos spiking neuron for brain-inspired neural networks with resistive synapses and in situ learning,” IEEE Transactions on Circuits and Systems II: Express Briefs, vol. 62, no. 11, pp. 1088–1092, 2015. [1449] X. Yang, W. Chen, and F. Z. Wang, “A supervised spiking time dependant plasticity network based on memristors,” in Computational Intelligence and Informatics (CINTI), 2013 IEEE 14th International Symposium on. IEEE, 2013, pp. 447–451. [1450] Y. Zhang, Z. Zeng, and S. Wen, “Implementation of memristive neural networks with spike-rate-dependent plasticity synapses,” in Neural Networks (IJCNN), 2014 International Joint Conference on. IEEE, 2014, pp. 2226–2233. [1451] L. Zheng, S. Shin, and S.-M. S. Kang, “Memristor-based synapses and neurons for neuromorphic computing,” in Circuits and Systems (ISCAS), 2015 IEEE International Symposium on. IEEE, 2015, pp. 1150–1153. [1452] Q. Zou, Y. Bornat, J. Tomas, S. Renaud, and A. Destexhe, “Real- time simulations of networks of hodgkin–huxley neurons using analog circuits,” Neurocomputing, vol. 69, no. 10, pp. 1137–1140, 2006. [1453] Q. Zou, Y. Bornat, S. Sa¨Ighi, J. Tomas, S. Renaud, and A. Destexhe, “Analog-digital simulations of full conductance-based networks of spiking neurons with spike timing dependent plasticity,” Network: computation in neural systems, vol. 17, no. 3, pp. 211–233, 2006. [1454] J. Sj¨ostr¨om and W. Gerstner, “Spike-timing dependent plasticity,” Spike-timing dependent plasticity, vol. 35, 2010. [1455] Y. Kanazawa, T. Asai, and Y. Amemiya, “A hardware depressing synapse and its application to contrast-invariant pattern recognition,” in SICE 2003 Annual Conference, vol. 2. IEEE, 2003, pp. 1558– 1563. [1456] J. Kang, B. Gao, P. Huang, L. Liu, X. Liu, H. Yu, S. Yu, and H.-S. P. Wong, “Rram based synaptic devices for neuromorphic visual systems,” in Digital Signal Processing (DSP), 2015 IEEE International Conference on. IEEE, 2015, pp. 1219–1222. [1457] S. R. Schultz and M. A. Jabri, “A silicon basis for synaptic plasticity,” Neural Processing Letters, vol. 2, no. 6, pp. 23–27, 1995. [1458] N. Izeboudjen, C. Larbes, and A. Farah, “A new classiﬁcation approach for neural networks hardware: from standards chips to embedded systems on chip,” Artiﬁcial Intelligence Review, vol. 41, no. 4, pp. 491–534, 2014. [1459] G. Indiveri, “Computation in neuromorphic analog vlsi systems,” in Neural Nets WIRN Vietri-01. Springer, 2002, pp. 3–20. [1460] A. Jim´enez-Fern´andez, E. Cerezuela-Escudero, L. Mir´o-Amarante, M. J. Dom´ınguez-Morales, F. de As´ıs G´omez-Rodr´ıguez, A. Linares- Barranco, and G. Jim´enez-Moreno, “A binaural neuromorphic au- ditory sensor for fpga: a spike signal processing approach,” IEEE Transactions on Neural Networks and Learning Systems, 2016. [1461] S. Jones, R. Meddis, S. C. Lim, and A. R. Temple, “Toward a digital neuromorphic pitch extraction system,” Neural Networks, IEEE Transactions on, vol. 11, no. 4, pp. 978–987, 2000. [1462] C. S. Thakur, R. M. Wang, S. Afshar, T. J. Hamilton, J. C. Tapson, S. A. Shamma, and A. van Schaik, “Sound stream segregation: a neuromorphic approach to solve the ?cocktail party problem? in real- time,” Frontiers in neuroscience, vol. 9, 2015. [1463] F. Trujillo-Romero and S. Caballero-Morales, “Speaker identiﬁcation using neural networks on an fpga,” in Electronics, Robotics and Au- tomotive Mechanics Conference (CERMA), 2012 IEEE Ninth. IEEE, 2012, pp. 197–202. [1464] J. Bailey, R. Wilcock, P. R. Wilson, and J. E. Chad, “Behavioral simu- lation and synthesis of biological neuron systems using synthesizable vhdl,” Neurocomputing, vol. 74, no. 14, pp. 2392–2406, 2011. [1465] B. Belhadj, J. Tomas, O. Malot, G. N. Kaoua, Y. Bornat, and S. Renaud, “Fpga-based architecture for real-time synaptic plasticity computation,” in Electronics, Circuits and Systems, 2008. ICECS 2008. 15th IEEE International Conference on. IEEE, 2008, pp. 93–96. [1466] M. Berzish, C. Eliasmith, and B. Tripp, “Real-time fpga simulation of surrogate models of large spiking networks,” in International Conference on Artiﬁcial Neural Networks. Springer, 2016, pp. 349– 356. [1467] M. Beuler, A. Tchaptchet, W. Bonath, S. Postnova, and H. A. Braun, “Real-time simulations of synchronization in a conductance-based neuronal network with a digital fpga hardware-core,” in Artiﬁcial Neural Networks and Machine Learning–ICANN 2012. Springer, 2012, pp. 97–104. [1468] S. Y. Bonabi, H. Asgharian, R. Bakhtiari, S. Safari, and M. N. Ahmadabadi, “Fpga implementation of a cortical network based on the hodgkin-huxley neuron model,” in Neural Information Processing. Springer, 2012, pp. 243–250. [1469] S. Y. Bonabi, H. Asgharian, S. Safari, and M. N. Ahmadabadi, “Fpga implementation of a biological neural network based on the hodgkin- huxley neuron model,” Frontiers in neuroscience, vol. 8, 2014. [1470] A. S. Cassidy, J. Georgiou, and A. G. Andreou, “Design of silicon brains in the nano-cmos era: Spiking neurons, learning synapses and neural architecture optimization,” Neural Networks, vol. 45, pp. 4–26, 2013. [1471] E. D’Angelo, G. Danese, G. Florimbi, F. Leporati, A. Majani, S. Masoli, S. Solinas, and E. Torti, “The human brain project: High performance computing for brain cells hw/sw simulation and understanding,” in Digital System Design (DSD), 2015 Euromicro Conference on. IEEE, 2015, pp. 740–747. [1472] B. Glackin, L. Maguire, T. McGinnity, A. Belatreche, and Q. Wu, “Implementation of a biologically realistic spiking neuron model on fpga hardware,” in Proceedings of the 8th Joint Conference on Information Sciences, vols, 2005, pp. 1–3. [1473] E. Graas, E. Brown, and R. H. Lee, “An fpga-based approach to high- speed simulation of conductance-based neuron models,” Neuroinfor- matics, vol. 2, no. 4, pp. 417–435, 2004. [1474] B. Leung, Y. Pan, C. Schroeder, S. O. Memik, G. Memik, and M. J. Hartmann, “Towards an ?early neural circuit simulator?: A fpga 59 implementation of processing in the rat whisker system,” in Field Pro- grammable Logic and Applications, 2008. FPL 2008. International Conference on. IEEE, 2008, pp. 191–196. [1475] P. Machado, J. Wade, and T. M. McGinnity, “Si elegans: Fpga hardware emulation of c. elegans nematode nervous system,” in Nature and Biologically Inspired Computing (NaBIC), 2014 Sixth World Congress on. IEEE, 2014, pp. 65–71. [1476] A. Majani, M. C. Lorena, G. Danese, and F. Leporati, “A hardware accelerator for real time simulation of complex neuronal models,” in Digital System Design (DSD), 2012 15th Euromicro Conference on. IEEE, 2012, pp. 931–937. [1477] T. S. Mak, G. Rachmuth, K. P. Lam, and C.-S. Poon, “Field programmable gate array implementation of neuronal ion channel dynamics,” in Neural Engineering, 2005. Conference Proceedings. 2nd International IEEE EMBS Conference on. IEEE, 2005, pp. 144–148. [1478] T. S. Mak, G. Rachmuth, K.-P. Lam, and C.-S. Poon, “A component- based fpga design framework for neuronal ion channel dynamics simulations,” Neural Systems and Rehabilitation Engineering, IEEE Transactions on, vol. 14, no. 4, pp. 410–418, 2006. [1479] J. C. Moctezuma, J. P. McGeehan, and J. L. Nunez-Yanez, “Numeri- cally efﬁcient and biophysically accurate neuroprocessing platform,” in Reconﬁgurable Computing and FPGAs (ReConFig), 2013 Interna- tional Conference on. IEEE, 2013, pp. 1–6. [1480] ——, “Biologically compatible neural networks with reconﬁgurable hardware,” Microprocessors and Microsystems, vol. 39, no. 8, pp. 693–703, 2015. [1481] M. Mokhtar, D. M. Halliday, and A. M. Tyrrell, “Hippocampus- inspired spiking neural network on fpga,” in Evolvable Systems: From Biology to Hardware. Springer, 2008, pp. 362–371. [1482] S. Nazari, K. Faez, E. Karami, and M. Amiri, “A digital neurmorphic circuit for a simpliﬁed model of astrocyte dynamics,” Neuroscience letters, vol. 582, pp. 21–26, 2014. [1483] S. Nazari, K. Faez, M. Amiri, and E. Karami, “A digital implementa- tion of neuron–astrocyte interaction for neuromorphic applications,” Neural Networks, vol. 66, pp. 79–90, 2015. [1484] S. Nazari, M. Amiri, K. Faez, and M. Amiri, “Multiplier-less digital implementation of neuron–astrocyte signalling on fpga,” Neurocom- puting, 2015. [1485] S. Nazari, K. Faez, M. Amiri, and E. Karami, “A novel digital implementation of neuron–astrocyte interactions,” Journal of Com- putational Electronics, vol. 14, no. 1, pp. 227–239, 2015. [1486] M. Nouri, G. R. Karimi, A. Ahmadi, and D. Abbott, “Digital multi- plierless implementation of the biological ﬁtzhugh–nagumo model,” Neurocomputing, 2015. [1487] P. Pourhaj, D. H.-Y. Teng, K. Wahid, and S.-B. Ko, “A novel scalable parallel architecture for biological neural simulations,” in Circuits and Systems (ISCAS), Proceedings of 2010 IEEE International Symposium on. IEEE, 2010, pp. 3152–3155. [1488] P. Pourhaj and D. H. Teng, “Fpga based pipelined architecture for action potential simulation in biological neural systems,” in Electrical and Computer Engineering (CCECE), 2010 23rd Canadian Confer- ence on. IEEE, 2010, pp. 1–4. [1489] B. Prieto, J. de Lope, and D. Maravall, “Reconﬁgurable hardware implementation of neural networks for humanoid locomotion,” in Artiﬁcial Intelligence and Knowledge Engineering Applications: A Bioinspired Approach. Springer, 2005, pp. 395–404. [1490] M. Rossmann, B. Hesse, K. Goser, A. Buhlmeier, and G. Manteuffel, “Implementation of a biologically inspired neuron-model in fpga,” in Microelectronics for Neural Networks, 1996., Proceedings of Fifth International Conference on. IEEE, 1996, pp. 322–329. [1491] H. Shayani, P. J. Bentley, and A. M. Tyrrell, “Hardware implemen- tation of a bio-plausible neuron model for evolution and growth of spiking neural networks on fpga,” in Adaptive Hardware and Systems, 2008. AHS’08. NASA/ESA Conference on. IEEE, 2008, pp. 236–243. [1492] B. E. Shi, E. K. Tsang, S. Y. Lam, and Y. Meng, “Expandable hardware for computing cortical feature maps,” in Circuits and Systems, 2006. ISCAS 2006. Proceedings. 2006 IEEE International Symposium on. IEEE, 2006, pp. 4–pp. [1493] G. Smaragdos, S. Isaza, M. F. van Eijk, I. Sourdis, and C. Strydis, “Fpga-based biophysically-meaningful modeling of olivocerebellar neurons,” in Proceedings of the 2014 ACM/SIGDA international symposium on Field-programmable gate arrays. ACM, 2014, pp. 89–98. [1494] D. B. Thomas and W. Luk, “Fpga accelerated simulation of biologi- cally plausible spiking neural networks,” in Field Programmable Cus- tom Computing Machines, 2009. FCCM’09. 17th IEEE Symposium on. IEEE, 2009, pp. 45–52. [1495] R. Weinstein and R. Lee, “Design of high performance physiologically-complex motoneuron models in fpgas,” in Neural Engineering, 2005. Conference Proceedings. 2nd International IEEE EMBS Conference on. IEEE, 2005, pp. 526–528. [1496] R. K. Weinstein and R. H. Lee, “Architectures for high-performance fpga implementations of neural models,” Journal of Neural Engineer- ing, vol. 3, no. 1, p. 21, 2006. [1497] R. K. Weinstein, M. S. Reid, and R. H. Lee, “Methodology and design ﬂow for assisted neural-model implementations in fpgas,” Neural Systems and Rehabilitation Engineering, IEEE Transactions on, vol. 15, no. 1, pp. 83–93, 2007. [1498] S. Yang, J. Wang, S. Li, B. Deng, X. Wei, H. Yu, and H. Li, “Cost- efﬁcient fpga implementation of basal ganglia and their parkinsonian analysis,” Neural Networks, vol. 71, pp. 62–75, 2015. [1499] Y. Zhang, J. Nuˇnez-Yaˇnez, J. McGeehan, E. Regan, and S. Kelly, “A biophysically accurate ﬂoating point somatic neuroprocessor,” in Field Programmable Logic and Applications, 2009. FPL 2009. International Conference on. IEEE, 2009, pp. 26–31. [1500] B. van Liempd, D. Herrera, and M. Figueroa, “An fpga-based accel- erator for analog vlsi artiﬁcial neural network emulation,” in Digital System Design: Architectures, Methods and Tools (DSD), 2010 13th Euromicro Conference on. IEEE, 2010, pp. 771–778. [1501] I. Vourkas, A. Abusleme, V. Ntinas, G. C. Sirakoulis, and A. Rubio, “A digital memristor emulator for fpga-based artiﬁcial neural networks,” in Veriﬁcation and Security Workshop (IVSW), IEEE International. IEEE, 2016, pp. 1–4. [1502] A. G. Andreou, T. Figliolia, K. Sanni, T. S. Murray, G. Tognetti, D. R. Mendat, J. L. Molin, M. Villemur, P. O. Pouliquen, P. Julian et al., “Bio-inspired system architecture for energy efﬁcient, bigdata computing with application to wide area motion imagery,” in Circuits & Systems (LASCAS), 2016 IEEE 7th Latin American Symposium on. IEEE, 2016, pp. 1–6. [1503] M. Azhar and K. R. Dimond, “Design of an fpga based adaptive neural controller for intelligent robot navigation,” in Digital System Design, 2002. Proceedings. Euromicro Symposium on. IEEE, 2002, pp. 283–290. [1504] D. Braendler, T. Hendtlass, and P. O. Donoghue, “Deterministic bit- stream digital neurons,” Neural Networks, IEEE Transactions on, vol. 13, no. 6, pp. 1514–1525, 2002. [1505] E. J. Brauer, J. J. Abbas, B. Callaway, J. Colvin, and J. Farris, “Hard- ware implementation of a neural network pattern shaper algorithm,” in Neural Networks, 1999. IJCNN’99. International Joint Conference on, vol. 4. IEEE, 1999, pp. 2315–2318. [1506] J. Byungik Ahn, “Neuron machine: Parallel and pipelined digital neurocomputing architecture,” in Computational Intelligence and Cy- bernetics (CyberneticsCom), 2012 IEEE International Conference on. IEEE, 2012, pp. 143–147. [1507] M. Krcma, Z. Kotasek, and J. Kastil, “Fault tolerant ﬁeld pro- grammable neural networks,” in Nordic Circuits and Systems Confer- ence (NORCAS): NORCHIP & International Symposium on System- on-Chip (SoC), 2015. IEEE, 2015, pp. 1–4. [1508] S. Li, X. Liu, M. Mao, H. H. Li, Y. Chen, B. Li, and Y. Wang, “Heterogeneous systems with reconﬁgurable neuromorphic computing accelerators,” in Circuits and Systems (ISCAS), 2016 IEEE Interna- tional Symposium on. IEEE, 2016, pp. 125–128. [1509] J. M. Moreno, J. Cabestany, E. Cant´o, J. Faura, and J. M. Insenser, “The role of dynamic reconﬁguration for implementing artiﬁcial neural networks models in programmable hardware,” in Engineering Applications of Bio-Inspired Artiﬁcial Neural Networks. Springer, 1999, pp. 85–94. [1510] M. Xiaobin, J. Lianwen, S. Dongsheng, and E. Junxun, “A mixed parallel neural networks computing unit implemented in fpga,” in Neural Networks and Signal Processing, 2003. Proceedings of the 2003 International Conference on, vol. 1. IEEE, 2003, pp. 324–327. [1511] J. L´azaro, J. Arias, A. Astarloa, U. Bidarte, and A. Zuloaga, “Hard- ware architecture for a general regression neural network coproces- sor,” Neurocomputing, vol. 71, no. 1, pp. 78–87, 2007. [1512] S. N. Truong, K. Van Pham, W. Yang, K.-S. Min, Y. Abbas, and C. J. Kang, “Live demonstration: Memristor synaptic array with fpga-implemented neurons for neuromorphic pattern recognition,” in Circuits and Systems (APCCAS), 2016 IEEE Asia Paciﬁc Conference on. IEEE, 2016, pp. 742–743. [1513] S. N. Truong, K. Van Pham, W. Yang, J. Song, H.-S. Mo, and K.- S. Min, “Fpga-based training and recalling system for memristor 60 synapses,” in Electronics, Information, and Communications (ICEIC), 2016 International Conference on. IEEE, 2016, pp. 1–4. [1514] A. Tisan, M. Cirstea, S. Oniga, and A. Buchman, “Artiﬁcial olfaction system with hardware on-chip learning neural networks,” in Opti- mization of Electrical and Electronic Equipment (OPTIM), 2010 12th International Conference on. IEEE, 2010, pp. 884–889. [1515] A. Tisan and J. Chin, “An end user platform for implementing artiﬁcial neutron networks on fpga,” in Industrial Informatics (INDIN), 2015 IEEE 13th International Conference on. IEEE, 2015, pp. 856–859. [1516] M. Krid, A. Damak, and D. S. Masmoudi, “Hardware implementation of a pulse mode neural network-based edge detection system,” AEU- International Journal of Electronics and Communications, vol. 63, no. 10, pp. 810–820, 2009. [1517] P. Lysaght, J. Stockwood, J. Law, and D. Girma, “Artiﬁcial neural net- work implementation on a ﬁne-grained fpga,” in Field-Programmable Logic Architectures, Synthesis and Applications. Springer, 1994, pp. 421–431. [1518] M. Martincigh and A. Abramo, “A new architecture for digital stochastic pulse-mode neurons based on the voting circuit,” Neural Networks, IEEE Transactions on, vol. 16, no. 6, pp. 1685–1693, 2005. [1519] J. Rodrigues de Oliveira Neto, J. P. C. Cajueiro, and J. Ranhel, “Neural encoding and spike generation for spiking neural networks implemented in fpga,” in Electronics, Communications and Computers (CONIELECOMP), 2015 International Conference on. IEEE, 2015, pp. 55–61. [1520] M. Pearson, M. Nibouche, I. Gilhespy, K. Gurney, C. Melhuish, B. Mitchinson, and A. G. Pipe, “A hardware based implementation of a tactile sensory system for neuromorphic signal processing appli- cations,” in Acoustics, Speech and Signal Processing, 2006. ICASSP 2006 Proceedings. 2006 IEEE International Conference on, vol. 4. IEEE, 2006, pp. IV–IV. [1521] A. Al Maashri, M. DeBole, C.-L. Yu, V. Narayanan, and C. Chakrabarti, “A hardware architecture for accelerating neuromor- phic vision algorithms,” in Signal Processing Systems (SiPS), 2011 IEEE Workshop on. IEEE, 2011, pp. 355–360. [1522] A. Al Maashri, M. Debole, M. Cotter, N. Chandramoorthy, Y. Xiao, V. Narayanan, and C. Chakrabarti, “Accelerating neuromorphic vision algorithms for recognition,” in Design Automation Conference (DAC), 2012 49th ACM/EDAC/IEEE. IEEE, 2012, pp. 579–584. [1523] M. DeBole, A. A. Maashri, M. Cotter, C.-L. Yu, C. Chakrabarti, and V. Narayanan, “A framework for accelerating neuromorphic-vision algorithms on fpgas,” in Computer-Aided Design (ICCAD), 2011 IEEE/ACM International Conference on. IEEE, 2011, pp. 810–813. [1524] S. Kestur, M. S. Park, J. Sabarad, D. Dantara, V. Narayanan, Y. Chen, and D. Khosla, “Emulating mammalian vision on reconﬁgurable hard- ware,” in Field-Programmable Custom Computing Machines (FCCM), 2012 IEEE 20th Annual International Symposium on. IEEE, 2012, pp. 141–148. [1525] M. S. Prieto and A. R. Allen, “A hybrid system for embedded machine vision using fpgas and neural networks,” Machine Vision and Applications, vol. 20, no. 6, pp. 379–394, 2009. [1526] J. Liu and C. Wang, “A survey of neuromorphic engineering– biological nervous systems realized on silicon,” in Testing and Di- agnosis, 2009. ICTD 2009. IEEE Circuits and Systems International Conference on. IEEE, 2009, pp. 1–4. [1527] K. Iwata, Y. Okane, Y. Ishihara, K. Sugita, S. Ono, M. Abe, S. Chiba, M. Takato, K. Saito, and F. Uchikoba, “Insect-type microrobot with mounted bare chip ic of artiﬁcial neural networks,” in 2016 IEEE 29th International Conference on Micro Electro Mechanical Systems (MEMS). IEEE, 2016, pp. 1204–1207. [1528] T. Nanami and T. Kohno, “Simple cortical and thalamic neuron models for digital arithmetic circuit implementation,” Frontiers in Neuroscience, vol. 10, p. 181, 2016. [1529] J. E. Smith, “Efﬁcient digital neurons for large scale cortical archi- tectures,” in Computer Architecture (ISCA), 2014 ACM/IEEE 41st International Symposium on. IEEE, 2014, pp. 229–240. [1530] Y. Zhang, J. P. Mcgeehan, E. M. Regan, S. Kelly, and J. L. Nunez- Yanez, “Biophysically accurate foating point neuroprocessors for reconﬁgurable logic,” Computers, IEEE Transactions on, vol. 62, no. 3, pp. 599–608, 2013. [1531] P. J. Murtagh and A. C. Tsoi, “A reconﬁgurable bit-serial vlsi systolic array neuro-chip,” Journal of Parallel and Distributed Computing, vol. 44, no. 1, pp. 53–70, 1997. [1532] J. Ouali and G. Saucier, “Fast generation of neuro-asics,” in Interna- tional Neural Network Conference. Springer, 1990, pp. 563–567. [1533] U. Ramacher, “Synapse-x: a general-purpose neurocomputer archi- tecture,” in Neural Networks, 1991. 1991 IEEE International Joint Conference on. IEEE, 1991, pp. 2168–2176. [1534] U. Ramacher, J. Beichter, and N. Bruls, “Architecture of a general- purpose neural signal processor,” in Neural Networks, 1991., IJCNN- 91-Seattle International Joint Conference on, vol. 1. IEEE, 1991, pp. 443–446. [1535] U. Ramacher, “Synapse?a neurocomputer that synthesizes neural algorithms on a parallel systolic engine,” Journal of Parallel and Distributed Computing, vol. 14, no. 3, pp. 306–318, 1992. [1536] M. Yasunaga, N. Masuda, M. Yagyu, M. Asai, M. Yamada, and A. Masaki, “Design, fabrication and evaluation of a 5-inch wafer scale neural network lsi composed on 576 digital neurons,” in Neural Networks, 1990., 1990 IJCNN International Joint Conference on. IEEE, 1990, pp. 527–535. [1537] M. Yasunaga, N. Masuda, M. Yagyu, M. Asai, K. Shibata, M. Ooyama, M. Yamada, T. Sakaguchi, and M. Hashimoto, “A self- learning neural network composed of 1152 digital neurons in wafer- scale lsis,” in Neural Networks, 1991. 1991 IEEE International Joint Conference on. IEEE, 1991, pp. 1844–1849. [1538] R. Eickhoff, T. Kaulmann, and U. Ruckert, “Sirens: A simple re- conﬁgurable neural hardware structure for artiﬁcial neural network implementations,” in Neural Networks, 2006. IJCNN’06. International Joint Conference on. IEEE, 2006, pp. 2830–2837. [1539] P. Richert, L. Spaanenburg, M. Kespert, J. Nijhuis, M. Schwarz, and A. Siggelkow, “Asics for prototyping of pulse-density modulated neural networks,” in VLSI design of neural networks. Springer, 1991, pp. 125–151. [1540] T. Schoenauer, N. Mehrtash, and H. Klar, “Architecture of a neu- roprocessor chip for pulse-coded neural networks,” in International Conference on Computational Intelligence and Neuroscience (IC- CIN’98),(JCIS’98). Citeseer, 1998, pp. 17–20. [1541] D. L. Hung and J. Wang, “Digital hardware realization of a recurrent neural network for solving the assignment problem,” Neurocomputing, vol. 51, pp. 447–461, 2003. [1542] J. Vlontzos and S. Kung, “Digital neural network architecture and implementation,” in VLSI Design of Neural Networks. Springer, 1991, pp. 205–227. [1543] Y. Wang and F. Salam, “Experiments using cmos neural network chips as pattern/character recognizers,” in Circuits and Systems, 1991., IEEE International Sympoisum on. IEEE, 1991, pp. 1196–1199. [1544] J. Beichter, N. Bruels, E. Sicheneder, U. Ramacher, and H. Klar, “De- sign of a general-purpose neural signal processor,” Neurocomputing, vol. 5, no. 1, pp. 17–23, 1993. [1545] Z. Butler, A. Murray, and A. Smith, “Vlsi bit-serial neural networks,” in VLSI for Artiﬁcial Intelligence. Springer, 1989, pp. 201–208. [1546] T. Cornu and P. Ienne, “Performance of digital neuro-computers,” in Microelectronics for Neural Networks and Fuzzy Systems, 1994., Proceedings of the Fourth International Conference on. IEEE, 1994, pp. 87–93. [1547] C. Y. Fu, B. Law, G. Chapline, and D. Swenson, “A novel technology for fabricating customizable vlsi artiﬁcial neural network chips,” in Neural Networks, 1992. IJCNN., International Joint Conference on, vol. 2. IEEE, 1992, pp. 631–636. [1548] J.-D. Gascuel, M. Weinfeld, and S. Chakroun, “A digital cmos fully connected neural network with in-circuit learning capability and automatic identiﬁcation of spurious attractors.” in Neural Networks, 1991., IJCNN-91-Seattle International Joint Conference on, vol. 2. IEEE, 1991, p. 923. [1549] M. Grifﬁn, G. Tahara, K. Knorpp, R. Pinkham, and B. Riley, “An 11-million transistor neural network execution engine,” in Solid-State Circuits Conference, 1991. Digest of Technical Papers. 38th ISSCC., 1991 IEEE International. IEEE, 1991, pp. 180–313. [1550] Y. Hirai, K. Kamada, M. Yamada, and M. Ooyama, “A digital neuro- chip with unlimited connectability for large scale neural networks,” in Neural Networks, 1989. IJCNN., International Joint Conference on. IEEE, 1989, pp. 163–169. [1551] J.-S. Ker, Y.-H. Kuo, R.-C. Wen, and B.-D. Liu, “Hardware imple- mentation of cmac neural network with reduced storage requirement,” Neural Networks, IEEE Transactions on, vol. 8, no. 6, pp. 1545–1556, 1997. [1552] S. Lucas and B. Damper, “Syntactic neural networks in vlsi,” in VLSI for Artiﬁcial Intelligence and Neural Networks. Springer, 1991, pp. 305–314. [1553] R. Mason, W. Robertson, and D. Pincock, “An hierarchical vlsi neural network architecture,” Solid-State Circuits, IEEE Journal of, vol. 27, no. 1, pp. 106–108, 1992. 61 [1554] H. Nakahira, S. Sakiyama, M. Maruyama, K. Hasegawa, T. Kouda, S. Maruno, Y. Shimeki, T. Satonaka, and Y. Nagano, “A digital neuroprocessor using quantizer neurons,” in VLSI Circuits, 1993. Digest of Technical Papers. 1993 Symposium on. IEEE, 1993, pp. 35–36. [1555] J. Ouali, G. Saucier, and J. Trilhe, “Fast design of digital dedicated neuro chips,” in VLSI Design of Neural Networks. Springer, 1991, pp. 187–203. [1556] K. Uchimura, O. Saito, and Y. Amemiya, “A high-speed digital neural network chip with low-power chain-reaction architecture,” Solid-State Circuits, IEEE Journal of, vol. 27, no. 12, pp. 1862–1867, 1992. [1557] T. Watanabe, K. Kimura, M. Aoki, T. Sakata, and K. Ito, “A single 1.5- v digital chip for a 10 6 synapse neural network,” Neural Networks, IEEE Transactions on, vol. 4, no. 3, pp. 387–393, 1993. [1558] B. A. White and M. I. Elmasry, “The digi-neocognitron: a digital neocognitron neural network model for vlsi,” Neural Networks, IEEE Transactions on, vol. 3, no. 1, pp. 73–85, 1992. [1559] M. Yasunaga, N. Masuda, M. Asai, M. Yamada, A. Masaki, and Y. Hi- rai, “A wafer scale integration neural network utilizing completely digital circuits,” in Neural Networks, 1989. IJCNN., International Joint Conference on. IEEE, 1989, pp. 213–217. [1560] D. Zhang and M. I. Elmasry, “Vlsi compressor design with applica- tions to digital neural networks,” Very Large Scale Integration (VLSI) Systems, IEEE Transactions on, vol. 5, no. 2, pp. 230–233, 1997. [1561] M. A. Lewis, R. Etienne-Cummings, M. J. Hartmann, Z. R. Xu, and A. H. Cohen, “An in silico central pattern generator: silicon oscil- lator, coupling, entrainment, and physical computation,” Biological Cybernetics, vol. 88, no. 2, pp. 137–151, 2003. [1562] F. Akopyan, J. Sawada, A. Cassidy, R. Alvarez-Icaza, J. Arthur, P. Merolla, N. Imam, Y. Nakamura, P. Datta, G.-J. Nam et al., “Truenorth: Design and tool ﬂow of a 65mw 1 million neuron programmable neurosynaptic chip.” [1563] J. V. Arthur, P. A. Merolla, F. Akopyan, R. Alvarez, A. Cassidy, S. Chandra, S. K. Esser, N. Imam, W. Risk, D. B. D. Rubin et al., “Building block of a programmable neuromorphic substrate: A digital neurosynaptic core,” in Neural Networks (IJCNN), The 2012 International Joint Conference on. IEEE, 2012, pp. 1–8. [1564] A. S. Cassidy, P. Merolla, J. V. Arthur, S. K. Esser, B. L. Jackson, R. Alvarez-Icaza, P. Datta, J. Sawada, T. M. Wong, V. Feldman et al., “Cognitive computing building block: A versatile and efﬁcient digital neuron model for neurosynaptic cores.” in IJCNN, 2013, pp. 1–10. [1565] A. S. Cassidy, R. Alvarez-Icaza, F. Akopyan, J. Sawada, J. V. Arthur, P. A. Merolla, P. Datta, M. G. Tallada, B. Taba, A. Andreopoulos et al., “Real-time scalable cortical computing at 46 giga-synaptic ops/watt with,” in Proceedings of the international conference for high performance computing, networking, storage and analysis. IEEE Press, 2014, pp. 27–38. [1566] N. Imam, F. Akopyan, J. Arthur, P. Merolla, R. Manohar, and D. S. Modha, “A digital neurosynaptic core using event-driven qdi circuits,” in Asynchronous Circuits and Systems (ASYNC), 2012 18th IEEE International Symposium on. IEEE, 2012, pp. 25–32. [1567] P. Merolla, J. Arthur, F. Akopyan, N. Imam, R. Manohar, and D. S. Modha, “A digital neurosynaptic core using embedded crossbar memory with 45pj per spike in 45nm,” in Custom Integrated Circuits Conference (CICC), 2011 IEEE. IEEE, 2011, pp. 1–4. [1568] P. A. Merolla, J. V. Arthur, R. Alvarez-Icaza, A. S. Cassidy, J. Sawada, F. Akopyan, B. L. Jackson, N. Imam, C. Guo, Y. Nakamura et al., “A million spiking-neuron integrated circuit with a scalable communica- tion network and interface,” Science, vol. 345, no. 6197, pp. 668–673, 2014. [1569] J.-s. Seo, B. Brezzo, Y. Liu, B. D. Parker, S. K. Esser, R. K. Montoye, B. Rajendran, J. A. Tierno, L. Chang, D. S. Modha et al., “A 45nm cmos neuromorphic chip with a scalable architecture for learning in networks of spiking neurons,” in Custom Integrated Circuits Conference (CICC), 2011 IEEE. IEEE, 2011, pp. 1–4. [1570] R. Ara´ujo, N. Waniek, and J. Conradt, “Development of a dynamically extendable spinnaker chip computing module,” in Artiﬁcial Neural Networks and Machine Learning–ICANN 2014. Springer, 2014, pp. 821–828. [1571] P. U. Diehl and M. Cook, “Efﬁcient implementation of stdp rules on spinnaker neuromorphic hardware,” in Neural Networks (IJCNN), 2014 International Joint Conference on. IEEE, 2014, pp. 4288–4295. [1572] S. Furber, S. Temple, and A. Brown, “High-performance computing for systems of spiking neurons,” in AISB?06 workshop on GC5: Architecture of Brain and Mind, vol. 2, 2006, pp. 29–36. [1573] S. Furber and A. Brown, “Biologically-inspired massively-parallel architectures-computing beyond a million processors,” in Application of Concurrency to System Design, 2009. ACSD’09. Ninth Interna- tional Conference on. IEEE, 2009, pp. 3–12. [1574] S. Furber, “To build a brain,” Spectrum, IEEE, vol. 49, no. 8, pp. 44–49, 2012. [1575] S. B. Furber, D. R. Lester, L. A. Plana, J. D. Garside, E. Painkras, S. Temple, and A. D. Brown, “Overview of the spinnaker system architecture,” Computers, IEEE Transactions on, vol. 62, no. 12, pp. 2454–2467, 2013. [1576] S. B. Furber, F. Galluppi, S. Temple, L. Plana et al., “The spinnaker project,” Proceedings of the IEEE, vol. 102, no. 5, pp. 652–665, 2014. [1577] X. Jin, S. B. Furber, and J. V. Woods, “Efﬁcient modelling of spiking neural networks on a scalable chip multiprocessor,” in Neural Net- works, 2008. IJCNN 2008.(IEEE World Congress on Computational Intelligence). IEEE International Joint Conference on. IEEE, 2008, pp. 2812–2819. [1578] X. Jin, M. Lujan, L. A. Plana, S. Davies, S. Temple, and S. Furber, “Modeling spiking neural networks on spinnaker,” Computing in Science & Engineering, vol. 12, no. 5, pp. 91–97, 2010. [1579] J. C. Knight, P. J. Tully, B. A. Kaplan, A. Lansner, and S. B. Furber, “Large-scale simulations of plastic neural networks on neuromorphic hardware,” Frontiers in Neuroanatomy, vol. 10, 2016. [1580] J. C. Knight and S. B. Furber, “Synapse-centric mapping of cortical models to the spinnaker neuromorphic architecture,” Frontiers in Neuroscience, vol. 10, 2016. [1581] X. Lagorce, E. Stromatias, F. Galluppi, L. A. Plana, S.-C. Liu, S. B. Furber, and R. B. Benosman, “Breaking the millisecond barrier on spinnaker: implementing asynchronous event-based plastic models with microsecond resolution,” Frontiers in neuroscience, vol. 9, 2015. [1582] J. Navaridas, S. Furber, J. Garside, X. Jin, M. Khan, D. Lester, M. Luj´an, J. Miguel-Alonso, E. Painkras, C. Patterson et al., “Spin- naker: fault tolerance in a power-and area-constrained large-scale neuromimetic architecture,” Parallel Computing, vol. 39, no. 11, pp. 693–708, 2013. [1583] E. Painkras, L. A. Plana, J. Garside, S. Temple, S. Davidson, J. Pepper, D. Clark, C. Patterson, and S. Furber, “Spinnaker: a multi- core system-on-chip for massively-parallel neural net simulation,” in Custom Integrated Circuits Conference (CICC), 2012 IEEE. IEEE, 2012, pp. 1–4. [1584] E. Painkras, L. Plana, J. Garside, S. Temple, F. Galluppi, C. Patterson, D. R. Lester, A. D. Brown, S. B. Furber et al., “Spinnaker: A 1-w 18-core system-on-chip for massively-parallel neural network simulation,” Solid-State Circuits, IEEE Journal of, vol. 48, no. 8, pp. 1943–1953, 2013. [1585] L. A. Plana, D. Clark, S. Davidson, S. Furber, J. Garside, E. Painkras, J. Pepper, S. Temple, and J. Bainbridge, “Spinnaker: Design and implementation of a gals multicore system-on-chip,” ACM Journal on Emerging Technologies in Computing Systems (JETC), vol. 7, no. 4, p. 17, 2011. [1586] A. Rast, X. Jin, M. Khan, and S. Furber, “The deferred event model for hardware-oriented spiking neural networks,” in Advances in Neuro- Information Processing. Springer, 2009, pp. 1057–1064. [1587] A. D. Rast, X. Jin, F. Galluppi, L. A. Plana, C. Patterson, and S. Furber, “Scalable event-driven native parallel processing: the spinnaker neuromimetic system,” in Proceedings of the 7th ACM international conference on Computing frontiers. ACM, 2010, pp. 21–30. [1588] T. Sharp, C. Patterson, and S. Furber, “Distributed conﬁguration of massively-parallel simulation on spinnaker neuromorphic hardware,” in Neural Networks (IJCNN), The 2011 International Joint Conference on. IEEE, 2011, pp. 1099–1105. [1589] T. Sharp, L. A. Plana, F. Galluppi, and S. Furber, “Event-driven simulation of arbitrary spiking neural networks on spinnaker,” in Neural Information Processing. Springer, 2011, pp. 424–430. [1590] T. Sharp, F. Galluppi, A. Rast, and S. Furber, “Power-efﬁcient simulation of detailed cortical microcircuits on spinnaker,” Journal of neuroscience methods, vol. 210, no. 1, pp. 110–118, 2012. [1591] T. Sharp and S. Furber, “Correctness and performance of the spinnaker architecture,” in Neural Networks (IJCNN), The 2013 International Joint Conference on. IEEE, 2013, pp. 1–8. [1592] E. Stromatias, F. Galluppi, C. Patterson, and S. Furber, “Power analysis of large-scale, real-time neural networks on spinnaker,” in Neural Networks (IJCNN), The 2013 International Joint Conference on. IEEE, 2013, pp. 1–8. [1593] E. Stromatias, D. Neil, F. Galluppi, M. Pfeiffer, S.-C. Liu, and S. Furber, “Scalable energy-efﬁcient, low-latency implementations of trained spiking deep belief networks on spinnaker,” in Neural 62 Networks (IJCNN), 2015 International Joint Conference on. IEEE, 2015, pp. 1–8. [1594] S. Furber, “Large-scale neuromorphic computing systems,” Journal of Neural Engineering, vol. 13, no. 5, p. 051001, 2016. [1595] S. Nease, S. George, P. Hasler, S. Koziol, and S. Brink, “Modeling and implementation of voltage-mode cmos dendrites on a reconﬁgurable analog platform,” IEEE transactions on biomedical circuits and systems, vol. 6, no. 1, pp. 76–84, 2012. [1596] L. Alvado, J. Tomas, R.-L. Masson, V. Douence et al., “Design of an analogue asic using subthreshold cmos transistors to model biological neurons,” in Custom Integrated Circuits, 2001, IEEE Conference on. IEEE, 2001, pp. 97–100. [1597] A. G. Andreou, “Synthetic neural systems using current-mode cir- cuits,” in Circuits and Systems, 1990., IEEE International Symposium on. IEEE, 1990, pp. 2428–2432. [1598] ——, “On physical models of neural computation and their analog vlsi implementation,” in Physics and Computation, 1994. PhysComp’94, Proceedings., Workshop on. IEEE, 1994, pp. 255–264. [1599] M. R. Azghadi, S. Al-Sarawi, D. Abbott, and N. Iannella, “Pairing frequency experiments in visual cortex reproduced in a neuromorphic stdp circuit,” in Electronics, Circuits, and Systems (ICECS), 2013 IEEE 20th International Conference on. IEEE, 2013, pp. 229–232. [1600] C. Bartolozzi, S. Mitra, and G. Indiveri, “An ultra low power current- mode ﬁlter for neuromorphic systems and biomedical signal process- ing,” in Biomedical Circuits and Systems Conference, 2006. BioCAS 2006. IEEE. IEEE, 2006, pp. 130–133. [1601] M. Cheely and T. Horiuchi, “A vlsi model of range-tuned neurons in the bat echolocation system,” in Circuits and Systems, 2003. ISCAS’03. Proceedings of the 2003 International Symposium on, vol. 4. IEEE, 2003, pp. IV–872. [1602] C.-H. Chen, H.-C. Wu, and H. Chen, “A conductance-based neuronal network in vlsi for studying the cpr circuit of the crayﬁsh,” in Circuits and Systems (ISCAS), 2012 IEEE International Symposium on. IEEE, 2012, pp. 2051–2054. [1603] G. S. Cymbalyuk, G. N. Patel, R. L. Calabrese, S. P. DeWeerth, and A. H. Cohen, “Modeling alternation to synchrony with inhibitory cou- pling: A neuromorphic vlsi approach,” Neural computation, vol. 12, no. 10, pp. 2259–2278, 2000. [1604] J. Georgiou, E. M. Drakakis, C. Toumazou, and P. Premanoj, “An analogue micropower log-domain silicon circuit for the hodgkin and huxley nerve axon,” in Circuits and Systems, 1999. ISCAS’99. Proceedings of the 1999 IEEE International Symposium on, vol. 2. IEEE, 1999, pp. 286–289. [1605] V. S. Ghaderi, D. Song, J.-M. C. Bouteiller, J. Choma, and T. W. Berger, “A programmable analog subthreshold biomimetic model for bi-directional communication with the brain,” in Engineering in Medicine and Biology Society (EMBC), 2013 35th Annual Interna- tional Conference of the IEEE. IEEE, 2013, pp. 787–790. [1606] J. Holleman and I. Arel, “Low-power analog deep learning archi- tecture for image processing,” in Govern. Microcircuit Appl. Critical Technol. Conf.(GOMAC Tech), St. Louis, MO, USA, 2015. [1607] G. Indiveri and G. Bisio, “Analog subthreshold vlsi implementation of a neuromorphic model of the visual cortex for pre-attentive vision,” in Microelectronics for Neural Networks and Fuzzy Systems, 1994., Proceedings of the Fourth International Conference on. IEEE, 1994, pp. 439–448. [1608] G. Indiveri, “Neuromorphic analog vlsi sensor for visual tracking: Circuits and application examples,” Circuits and Systems II: Analog and Digital Signal Processing, IEEE Transactions on, vol. 46, no. 11, pp. 1337–1347, 1999. [1609] Y. J. Lee, J. Lee, K. K. Kim, Y.-B. Kim, and J. Ayers, “Low power cmos electronic central pattern generator design for a biomimetic underwater robot,” Neurocomputing, vol. 71, no. 1, pp. 284–296, 2007. [1610] S.-C. Liu, “A neuromorphic avlsi model of global motion processing in the ﬂy,” Circuits and Systems II: Analog and Digital Signal Processing, IEEE Transactions on, vol. 47, no. 12, pp. 1458–1467, 2000. [1611] J. Lu, S. Young, I. Arel, and J. Holleman, “A 1 tops/w analog deep machine-learning engine with ﬂoating-gate storage in 0.13 µm cmos,” Solid-State Circuits, IEEE Journal of, vol. 50, no. 1, pp. 270–281, 2015. [1612] M. A. C. Maher, S. P. Deweerth, M. A. Mahowald, and C. A. Mead, “Implementing neural architectures using analog vlsi circuits,” Circuits and Systems, IEEE Transactions on, vol. 36, no. 5, pp. 643– 652, 1989. [1613] N. K. Mandloi, G. Indiveri, and C. Bartolozzi, “Compact analog temporal edge detector circuit with programmable adaptive threshold for neuromorphic vision sensors,” Circuits and Systems I: Regular Papers, IEEE Transactions on, vol. 61, no. 11, pp. 3094–3104, 2014. [1614] T. Morie, K. Uchimura, and Y. Amemiya, “Analog lsi implementation of self-learning neural networks,” Computers & Electrical Engineer- ing, vol. 25, no. 5, pp. 339–355, 1999. [1615] T. G. Morris, T. K. Horiuchi, and S. P. DeWeerth, “Object-based selection within an analog vlsi visual attention system,” Circuits and Systems II: Analog and Digital Signal Processing, IEEE Transactions on, vol. 45, no. 12, pp. 1564–1572, 1998. [1616] K. Nakada, T. Asai, T. Hirose, and Y. Amemiya, “Analog cmos implementation of a neuromorphic oscillator with current-mode low- pass ﬁlters,” in Circuits and Systems, 2005. ISCAS 2005. IEEE International Symposium on. IEEE, 2005, pp. 1923–1926. [1617] K. I. Papadimitriou, S.-C. Liu, G. Indiveri, and E. M. Drakakis, “Neuromorphic log-domain silicon synapse circuits obey bernoulli dynamics: a unifying tutorial analysis,” Frontiers in neuroscience, vol. 8, 2014. [1618] T. Yu and G. Cauwenberghs, “Analog vlsi biophysical neurons and synapses with programmable membrane channel kinetics,” Biomedical Circuits and Systems, IEEE Transactions on, vol. 4, no. 3, pp. 139– 148, 2010. [1619] ——, “Log-domain time-multiplexed realization of dynamical conductance-based synapses,” in Circuits and Systems (ISCAS), Pro- ceedings of 2010 IEEE International Symposium on. IEEE, 2010, pp. 2558–2561. [1620] O. Rossetto, C. Jutten, J. Herault, and I. Kreuzer, “Analog vlsi synaptic matrices as building blocks for neural networks,” Micro, IEEE, vol. 9, no. 6, pp. 56–63, 1989. [1621] L. Alvado, J. Tomas, S. Saıghi, S. Renaud, T. Bal, A. Destexhe, and G. Le Masson, “Hardware computation of conductance-based neuron models,” Neurocomputing, vol. 58, pp. 109–115, 2004. [1622] A. Basu, S. Ramakrishnan, C. Petre, S. Koziol, S. Brink, and P. E. Hasler, “Neural dynamics in reconﬁgurable silicon,” Biomedical Cir- cuits and Systems, IEEE Transactions on, vol. 4, no. 5, pp. 311–319, 2010. [1623] H. Chen, S. Sa¨ıghi, L. Buhry, and S. Renaud, “Real-time simulation of biologically realistic stochastic neurons in vlsi,” Neural Networks, IEEE Transactions on, vol. 21, no. 9, pp. 1511–1517, 2010. [1624] V. Douence, A. Laﬂaqui`ere, S. Le Masson, T. Bal, and G. Le Masson, “Analog electronic system for simulating biological neurons,” in Engineering Applications of Bio-Inspired Artiﬁcial Neural Networks. Springer, 1999, pp. 188–197. [1625] R. Jung, E. J. Brauer, and J. J. Abbas, “Real-time interaction between a neuromorphic electronic circuit and the spinal cord,” Neural Systems and Rehabilitation Engineering, IEEE Transactions on, vol. 9, no. 3, pp. 319–326, 2001. [1626] A. Laﬂaquiere, S. Le Masson, D. Dupeyron, and G. Le Masson, “Ana- log circuits emulating biological neurons in real-time experiments,” in Engineering in Medicine and Biology Society, 1997. Proceedings of the 19th Annual International Conference of the IEEE, vol. 5. IEEE, 1997, pp. 2035–2038. [1627] A. Laﬂaquiere, S. L. Masson, J. P. Dom, and G. L. Masson, “Accurate analog vlsi model of calcium-dependent bursting neurons,” in Neural Networks,1997., International Conference on, vol. 2, Jun 1997, pp. 882–887 vol.2. [1628] B. Linares-Barranco, E. S´anchez-Sinencio, A. Rodr´ıguez-V´azquez, and J. L. Huertas, “Cmos analog neural network systems based on oscillatory neurons,” in Silicon Implementation of Pulse Coded Neural Networks. Springer, 1994, pp. 199–247. [1629] Q. Ma, Y.-G. Li, M. R. Haider, and Y. Massoud, “A low-power neuromorphic bandpass ﬁlter for biosignal processing,” in Wireless and Microwave Technology Conference (WAMICON), 2013 IEEE 14th Annual. IEEE, 2013, pp. 1–3. [1630] C. Mayr, J. Partzsch, M. Noack, S. Hanzsche, S. Scholze, S. Hoppner, G. Ellguth, and R. Schuffny, “A biological-realtime neuromorphic system in 28 nm cmos using low-leakage switched capacitor circuits,” 2014. [1631] S. Millner, A. Hartel, J. Schemmel, and K. Meier, “Towards bi- ologically realistic multi-compartment neuron model emulation in analog vlsi,” in European Symposium on Artiﬁcial Neural Networks, Computational Intelligence and Machine Learnin (ESANN), 2012. [1632] G. Passetti, F. Corradi, M. Raglianti, D. Zambrano, C. Laschi, and G. Indiveri, “Implementation of a neuromorphic vestibular sensor with analog vlsi neurons,” in Biomedical Circuits and Systems Conference (BioCAS), 2013 IEEE. IEEE, 2013, pp. 174–177. 63 [1633] R. Pinto, P. Varona, A. Volkovskii, A. Sz¨ucs, H. D. Abarbanel, and M. I. Rabinovich, “Synchronous behavior of two coupled electronic neurons,” Physical Review E, vol. 62, no. 2, p. 2644, 2000. [1634] G. Rachmuth and C.-S. Poon, “Transistor analogs of emergent iono- neuronal dynamics,” HFSP journal, vol. 2, no. 3, pp. 156–166, 2008. [1635] S. Renaud-Le Masson, G. Le Masson, L. Alvado, S. Saıghi, and J. Tomas, “A neural simulation system based on biologically realistic electronic neurons,” Information Sciences, vol. 161, no. 1, pp. 57–69, 2004. [1636] G. Rovere, Q. Ning, C. Bartolozzi, and G. Indiveri, “Ultra low leakage synaptic scaling circuits for implementing homeostatic plasticity in neuromorphic architectures,” in Circuits and Systems (ISCAS), 2014 IEEE International Symposium on. IEEE, 2014, pp. 2073–2076. [1637] S. Ryckebusch, J. M. Bower, and C. Mead, “Modeling small oscillat- ing biological networks in analog vlsi,” 1989. [1638] S. Saighi, J. Tomas, Y. Bornat, and S. Renaud, “A conductance-based silicon neuron with dynamically tunable model parameters,” in Neural Engineering, 2005. Conference Proceedings. 2nd International IEEE EMBS Conference on. IEEE, 2005, pp. 285–288. [1639] S. Sa¨ıghi, J. Tomas, Y. Bornat, and S. Renaud, “A neuromimetic integrated circuit for interactive real-time simulation,” in Artiﬁcial Intelligence and Knowledge Engineering Applications: A Bioinspired Approach. Springer, 2005, pp. 338–346. [1640] S. Sa¨ıghi, Y. Bornat, J. Tomas, and S. Renaud, “Neuromimetic ics and system for parameters extraction in biological neuron models,” in Circuits and Systems, 2006. ISCAS 2006. Proceedings. 2006 IEEE International Symposium on. IEEE, 2006, pp. 5–pp. [1641] S. Sa¨ıghi, Y. Bornat, J. Tomas, G. Le Masson, and S. Renaud, “A library of analog operators based on the hodgkin-huxley formalism for the design of tunable, real-time, silicon neurons,” Biomedical Circuits and Systems, IEEE Transactions on, vol. 5, no. 1, pp. 3–19, 2011. [1642] S. Santurkar and B. Rajendran, “C. elegans chemotaxis inspired neuromorphic circuit for contour tracking and obstacle avoidance,” in Neural Networks (IJCNN), 2015 International Joint Conference on. IEEE, 2015, pp. 1–8. [1643] R. Serrano-Gotarredona, T. Serrano-Gotarredona, A. Acosta-Jim´enez, C. Serrano-Gotarredona, J. A. Perez-Carrasco, B. Linares-Barranco, A. Linares-Barranco, G. Jim´enez-Moreno, and A. Civit-Ballcels, “On real-time aer 2-d convolutions hardware for neuromorphic spike-based cortical processing,” Neural Networks, IEEE Transactions on, vol. 19, no. 7, pp. 1196–1219, 2008. [1644] S. Sharma, P. Gupta, and C. Markan, “Adaptive neuromorphic circuit for stereoscopic disparity using ocular dominance map,” Neuroscience journal, vol. 2016, 2016. [1645] R. Z. Shi and T. K. Horiuchi, “A neuromorphic vlsi model of bat interaural level difference processing for azimuthal echolocation,” Circuits and Systems I: Regular Papers, IEEE Transactions on, vol. 54, no. 1, pp. 74–88, 2007. [1646] M. F. Simoni and S. P. DeWeerth, “Two-dimensional variation of bursting properties in a silicon-neuron half-center oscillator,” Neu- ral Systems and Rehabilitation Engineering, IEEE Transactions on, vol. 14, no. 3, pp. 281–289, 2006. [1647] S. Still, K. Hepp, and R. J. Douglas, “Neuromorphic walking gait control,” Neural Networks, IEEE Transactions on, vol. 17, no. 2, pp. 496–508, 2006. [1648] J. Tomas, Y. Bornat, S. Saighi, T. Levi, and S. Renaud, “Design of a modular and mixed neuromimetic asic,” in Electronics, Circuits and Systems, 2006. ICECS’06. 13th IEEE International Conference on. IEEE, 2006, pp. 946–949. [1649] A. Van Schaik and S. Shamma, “A neuromorphic sound localizer for a smart mems system,” Analog Integrated Circuits and Signal Processing, vol. 39, no. 3, pp. 267–273, 2004. [1650] E. A. Vittoz, “Analog vlsi implementation of neural networks,” in Circuits and Systems, 1990., IEEE International Symposium on. IEEE, 1990, pp. 2524–2527. [1651] Y. Wang and S.-C. Liu, “Active processing of spatio-temporal input patterns in silicon dendrites,” Biomedical Circuits and Systems, IEEE Transactions on, vol. 7, no. 3, pp. 307–318, 2013. [1652] T. Yu and G. Cauwenberghs, “Analog vlsi neuromorphic network with programmable membrane channel kinetics,” in Circuits and Systems, 2009. ISCAS 2009. IEEE International Symposium on. IEEE, 2009, pp. 349–352. [1653] ——, “Biophysical synaptic dynamics in an analog vlsi network of hodgkin-huxley neurons,” in Engineering in Medicine and Biology Society, 2009. EMBC 2009. Annual International Conference of the IEEE. IEEE, 2009, pp. 3335–3338. [1654] T. Yu, T. J. Sejnowski, and G. Cauwenberghs, “Biophysical neural spiking, bursting, and excitability dynamics in reconﬁgurable analog vlsi,” Biomedical Circuits and Systems, IEEE Transactions on, vol. 5, no. 5, pp. 420–429, 2011. [1655] T. Yu, J. Park, S. Joshi, C. Maier, and G. Cauwenberghs, “Event- driven neural integration and synchronicity in analog vlsi,” in En- gineering in Medicine and Biology Society (EMBC), 2012 Annual International Conference of the IEEE. IEEE, 2012, pp. 775–778. [1656] X. Zhou, Y. Guo, A. C. Parker, C.-C. Hsu, and J. Choma, “Biomimetic non-linear cmos adder for neuromorphic circuits,” in Neural Engi- neering (NER), 2013 6th International IEEE/EMBS Conference on. IEEE, 2013, pp. 876–879. [1657] P. Martins Engel and R. F. Molz, “A new proposal for implementa- tion of competitive neural networks in analog hardware,” in Neural Networks, 1998. Proceedings. Vth Brazilian Symposium on. IEEE, 1998, pp. 186–191. [1658] J. Holleman, I. Arel, S. Young, and J. Lu, “Analog inference circuits for deep learning,” in Biomedical Circuits and Systems Conference (BioCAS), 2015 IEEE. IEEE, 2015, pp. 1–4. [1659] H. A. Castro, S. M. Tam, and M. A. Holler, “Implementation and performance of an analog nonvolatile neural network,” Analog Integrated Circuits and Signal Processing, vol. 4, no. 2, pp. 97–113, 1993. [1660] R. Damle, V. Rao, and F. Kern, “Robust control of smart structures using neural network hardware,” Smart materials and structures, vol. 6, no. 3, p. 301, 1997. [1661] R. Etienne-Cummings, C. Donham, J. Van der Spiegel, and P. Mueller, “Spatiotemporal computation with a general purpose analog neural computer: Real-time visual motion estimation,” in Neural Networks, 1994. IEEE World Congress on Computational Intelligence., 1994 IEEE International Conference on, vol. 3. IEEE, 1994, pp. 1836– 1841. [1662] M. Holler, S. Tam, H. Castro, and R. Benson, “An electrically trainable artiﬁcial neural network (etann) with 10240 ’ﬂoating gate’ synapses,” in Neural Networks, 1989. IJCNN., International Joint Conference on. IEEE, 1989, pp. 191–196. [1663] P. Mueller, J. Van der Spiegel, D. Blackman, T. Chiu, T. Clare, J. Dao, C. Donham, T. Hsieh, and M. Loinaz, “A general purpose analog neural computer,” in Neural Networks, 1989. IJCNN., International Joint Conference on. IEEE, 1989, pp. 177–182. [1664] ——, “A general purpose analog neural computer,” in Neural Net- works, 1989. IJCNN., International Joint Conference on. IEEE, 1989, pp. 177–182. [1665] S. Churcher, D. J. Baxter, A. Hamilton, A. F. Murray, and H. M. Reekie, “Generic analog neural computation-the epsilon chip,” in Advances in Neural Information Processing Systems, 1993, pp. 773– 780. [1666] M. Al-Nsour and H. S. Abdel-Aty-Zohdy, “Analog computational cir- cuits for neural network implementations,” in Electronics, Circuits and Systems, 1999. Proceedings of ICECS’99. The 6th IEEE International Conference on, vol. 1. IEEE, 1999, pp. 299–302. [1667] A. G. Andreou and T. G. Edwards, “Analog vlsi neuromorphic processing: case study of a multiple-target-tracking system,” in Neural Networks, 1994. IEEE World Congress on Computational Intelli- gence., 1994 IEEE International Conference on, vol. 3. IEEE, 1994, pp. 1903–1906. [1668] D. Badoni, S. Bertazzoni, S. Buglioni, G. Salina, D. J. Amit, and S. Fusi, “Electronic implementation of an analog attractor neural network with stochastic learning,” NETWORK, 1994. [1669] G. Bo, D. Caviglia, and M. Valle, “A current-mode two-quadrant multiplier for analogue array-based neural systems,” International journal of electronics, vol. 87, no. 4, pp. 407–411, 2000. [1670] M. Brownlow, L. Tarassenko, and A. Murray, “Analogue computation using vlsi neural network devices,” Electronics Letters, vol. 26, no. 16, pp. 1297–1299, 1990. [1671] A. B¨uhlmeier, G. Manteuffel, M. Rossmann, and K. Goser, “Robot learning in analog neural hardware,” in Artiﬁcial Neural Net- works?ICANN 96. Springer, 1996, pp. 311–316. [1672] A. Cichocki, J. Ramirez-Angulo, and R. Unbehauen, “Architectures for analog vlsi implementation of neural networks for solving linear equations with inequality constraints,” in Circuits and Systems, 1992. ISCAS’92. Proceedings., 1992 IEEE International Symposium on, vol. 3. IEEE, 1992, pp. 1529–1532. [1673] M. Conti, G. Guaitini, and C. Turchetti, “An analog cmos neural net- work with on-chip learning and multilevel weight storage,” Artiﬁcial Neural Networks?ICANN 96, pp. 761–766, 1996. 64 [1674] P. Daniell, W. Waller, and D. Bisset, “An implementation of fully analogue sum-of-product neural models in vlsi,” in Artiﬁcial Neural Networks, 1989., First IEE International Conference on (Conf. Publ. No. 313). IET, 1989, pp. 52–56. [1675] R. Dlugosz, T. Talaska, and W. Pedrycz, “Current-mode analog adaptive mechanism for ultra-low-power neural networks,” Circuits and Systems II: Express Briefs, IEEE Transactions on, vol. 58, no. 1, pp. 31–35, 2011. [1676] J. Ghosh, P. Lacour, and S. Jackson, “Ota-based neural network architectures with on-chip tuning of synapses,” Circuits and Systems II: Analog and Digital Signal Processing, IEEE Transactions on, vol. 41, no. 1, pp. 49–58, 1994. [1677] A. Hamilton, A. F. Murray, and L. Tarassenko, “Programmable analog pulse-ﬁring neural networks,” in Advances in neural information processing systems, 1989, pp. 671–677. [1678] T. J. Hamilton and J. Tapson, “A neuromorphic cross-correlation chip,” in 2011 IEEE International Symposium of Circuits and Systems (ISCAS). [1679] A. Heittmann, U. Ramacher, D. Matolin, J. Schreiter, and R. Sch¨uffny, “An analog vlsi pulsed neural network for image segmentation using adaptive connection weights,” in Artiﬁcial Neural Networks?ICANN 2002. Springer, 2002, pp. 1293–1298. [1680] C. S. Ho, J. J. LlOU, M. GEORGIOPOULOS, G. L. HEILEMAN, and C. CHRISTODOULOU, “Analogue circuit design and implementation of an adaptive resonance theory (art) neural network architecture,” International Journal of Electronics, vol. 76, no. 2, pp. 271–291, 1994. [1681] P. Houselander and J. Taylor, “Implementation of artiﬁcial neural networks using current mode analogue circuit techniques,” in Current Mode Analogue Circuits, IEE Colloquium on. IET, 1989, pp. 5–1. [1682] G. Jackson, A. Hamilton, and A. F. Murray, “The epsilon processor card: A framework for analog neural computation,” in Microelectron- ics for Neural Networks and Fuzzy Systems, 1994., Proceedings of the Fourth International Conference on. IEEE, 1994, pp. 280–286. [1683] M. Mirhassani, M. Ahmadi, G. Jullien et al., “Robust analog neural network based on continuous valued number system,” in Circuits and Systems, 2008. ISCAS 2008. IEEE International Symposium on. IEEE, 2008, pp. 1384–1387. [1684] G. Moon, M. E. Zaghloul, and R. W. Newcomb, “Vlsi implementation of synaptic weighting and summing in pulse coded neural-type cells.” IEEE transactions on neural networks/a publication of the IEEE Neural Networks Council, vol. 3, no. 3, pp. 394–403, 1991. [1685] T. Morris and S. DeWeerth, “Analog vlsi excitatory feedback circuits for attentional shifts and tracking,” in Neuromorphic systems engi- neering. Springer, 1998, pp. 151–174. [1686] S. Mosin, “An approach to construction the neuromorphic classiﬁer for analog fault testing and diagnosis,” in Embedded Computing (MECO), 2015 4th Mediterranean Conference on. IEEE, 2015, pp. 258–261. [1687] ——, “A technique of analog circuits testing and diagnosis based on neuromorphic classiﬁer,” in Advances in Signal Processing and Intelligent Recognition Systems. Springer, 2016, pp. 381–393. [1688] P. Mueller, J. Van der Spiegel, V. Agami, D. Blackman, P. Chance, C. Donham, R. Etienne, J. Flinn, J. Kim, M. Massa et al., “Design and performance of a prototype analog neural computer,” Neurocom- puting, vol. 4, no. 6, pp. 311–324, 1992. [1689] A. Murray, A. Smith, and L. Tarassenko, “Fully-programmable ana- logue vlsi devices for the implementation of neural networks,” in VLSI for artiﬁcial intelligence. Springer, 1989, pp. 236–244. [1690] K. Nakada, T. Asai, and H. Hayashi, “A silicon resonate-and-ﬁre neuron based on the volterra system,” in Int. symp. on nonlinear theory and its applications. Citeseer, 2005, pp. 82–85. [1691] K. Nakada, J. Igarashi, A. Tetsuya, and H. Hayashi, “Noise effects on performance of signal detection in an analog vlsi resonate-and ﬁre neuron,” in Circuits and Systems, 2006. ISCAS 2006. Proceedings. 2006 IEEE International Symposium on. IEEE, 2006, pp. 5183– 5186. [1692] M. Onorato, M. Valle, D. Caviglia, and G. Bisio, “Non-linear circuit effects on analog vlsi neural network implementations,” in Microelec- tronics for Neural Networks and Fuzzy Systems, 1994., Proceedings of the Fourth International Conference on. IEEE, 1994, pp. 430–438. [1693] F. Pelayo, A. Prieto, B. Pino, and P. Martin-Smith, “Analog vlsi implementation of a neural network with competitive learning,” in Cellular Neural Networks and their Applications, 1990. CNNA-90 Proceedings., 1990 IEEE International Workshop on. IEEE, 1990, pp. 197–205. [1694] F. J. Pelayo, E. Ros, P. Martin-Smith, F. Fern´andez, and A. Prieto, “A vlsi approach to the implementation of additive and shunting neural networks,” in From Natural to Artiﬁcial Neural Computation. Springer, 1995, pp. 728–735. [1695] L. Reyneri, H. Withagen, J. Hegt, and M. Chiaberge, “A comparison between analog and pulse stream vlsi hardware for neural networks and fuzzy systems,” in Microelectronics for Neural Networks and Fuzzy Systems, 1994., Proceedings of the Fourth International Con- ference on. IEEE, 1994, pp. 77–86. [1696] A. Rodr´ıguez-V´azquez, A. Rueda, J. L. Huertas, and R. Dominguez- Castro, “Switched-capacitor neural networks for linear programming,” Electronics Letters, vol. 24, no. 8, pp. 496–498, 1988. [1697] A. Rodriguez-Vazquez, R. Dominguez-Castro, A. Rueda, J. L. Huer- tas, and E. Sanchez-Sinencio, “Nonlinear switched capacitorneu- ral’networks for optimization problems,” Circuits and Systems, IEEE Transactions on, vol. 37, no. 3, pp. 384–398, 1990. [1698] S. D. Roy, A. Chaudhary et al., “Compact analogue neural network: a new paradigm for neural based combinatorial optimisation,” in Circuits, Devices and Systems, IEE Proceedings-, vol. 146, no. 3. IET, 1999, pp. 111–116. [1699] F. Salam and Y. Wang, “Neural circuits for programmable analog mos vlsi implementation,” in Circuits and Systems, 1989., Proceedings of the 32nd Midwest Symposium on. IEEE, 1989, pp. 240–243. [1700] N. Saxena and J. J. Clark, “A four-quadrant cmos analog multiplier for analog neural networks,” Solid-State Circuits, IEEE Journal of, vol. 29, no. 6, pp. 746–749, 1994. [1701] D. B. Schwartz, R. E. Howard, and W. E. Hubbard, “A programmable analog neural network chip,” Solid-State Circuits, IEEE Journal of, vol. 24, no. 2, pp. 313–319, 1989. [1702] T. Serrano-Gotarredona and B. Linares-Barranco, “A real-time clus- tering microchip neural engine,” Very Large Scale Integration (VLSI) Systems, IEEE Transactions on, vol. 4, no. 2, pp. 195–209, 1996. [1703] T. Shibata and T. Ohmi, “A self-learning neural-network lsi using neuron mosfets,” in VLSI Technology, 1992. Digest of Technical Papers. 1992 Symposium on. IEEE, 1992, pp. 84–85. [1704] A. B. Torralba and J. Herault, “An efﬁcient neuromorphic analog network for motion estimation,” Circuits and Systems I: Fundamental Theory and Applications, IEEE Transactions on, vol. 46, no. 2, pp. 269–280, 1999. [1705] Y. Tsividis and S. Satyanarayana, “Analogue circuits for variable- synapse electronic neural networks,” Electronics Letters, vol. 23, no. 24, pp. 1313–1314, 1987. [1706] G. Wilson, “Cmos subthreshold-mode i/v converter for analogue neural network applications,” Electronics Letters, vol. 32, no. 11, pp. 990–991, 1996. [1707] R. Wojtyna and T. Talaska, “Transresistance cmos neuron for adaptive neural networks implemented in hardware,” TECHNICAL SCIENCES, vol. 54, no. 4, 2006. [1708] R. Wojtyna, “A concept of current-mode long-term analog memory for neural-network learning on silicon,” in Signal Processing Algorithms, Architectures, Arrangements, and Applications (SPA), 2008. IEEE, 2008, pp. 121–126. [1709] H. Yanai and Y. Sawada, “Integrator neurons for analog neural networks,” Circuits and Systems, IEEE Transactions on, vol. 37, no. 6, pp. 854–856, 1990. [1710] A. G. Andreou, R. C. Meitzler, K. Strohbehn, and K. Boahen, “Analog vlsi neuromorphic image acquisition and pre-processing systems,” Neural Networks, vol. 8, no. 7, pp. 1323–1347, 1995. [1711] X. Arreguit and E. Vittoz, “Perception systems implemented in analog vlsi for real-time applications,” in From Perception to Action Conference, 1994., Proceedings. IEEE, 1994, pp. 170–180. [1712] J. Cosp, J. Madrenas, and J. Cabestany, “A vlsi implementation of a neuromorphic network for scene segmentation,” in Microelectronics for Neural, Fuzzy and Bio-Inspired Systems, 1999. MicroNeuro’99. Proceedings of the Seventh International Conference on. IEEE, 1999, pp. 403–408. [1713] J. Cosp and J. Madrenas, “Scene segmentation using neuromorphic os- cillatory networks,” Neural Networks, IEEE Transactions on, vol. 14, no. 5, pp. 1278–1296, 2003. [1714] H. P. Graf, C. R. Nohl, and J. Ben, “Image recognition with an analog neural net chip,” Machine vision and applications, vol. 8, no. 2, pp. 131–140, 1995. [1715] T. K. Horiuchi and C. Koch, “Analog vlsi-based modeling of the primate oculomotor system,” Neural Computation, vol. 11, no. 1, pp. 243–265, 1999. [1716] G. Indiveri and P. Verschure, “Autonomous vehicle guidance us- ing analog vlsi neuromorphic sensors,” in Artiﬁcial Neural Net- works?ICANN’97. Springer, 1997, pp. 811–816. 65 [1717] G. Indiveri, A. M. Whatley, and J. Kramer, “A reconﬁgurable neuro- morphic vlsi multi-chip system applied to visual motion computation,” in Microelectronics for Neural, Fuzzy and Bio-Inspired Systems, 1999. MicroNeuro’99. Proceedings of the Seventh International Conference on. IEEE, 1999, pp. 37–44. [1718] S.-C. Liu, J. Kramer, G. Indiveri, T. Delbru `Eck, T. Burg, and R. Dou- glas, “Orientation-selective avlsi spiking neurons,” Neural Networks, vol. 14, no. 6, pp. 629–643, 2001. [1719] C. Markan and P. Gupta, “Neuromorphic building blocks for adaptable cortical feature maps,” in Very Large Scale Integration, 2007. VLSI- SoC 2007. IFIP International Conference on. IEEE, 2007, pp. 7–12. [1720] R. Nawrocki, S. E. Shaheen, R. M. Voyles et al., “A neuromorphic ar- chitecture from single transistor neurons with organic bistable devices for weights,” in Neural Networks (IJCNN), The 2011 International Joint Conference on. IEEE, 2011, pp. 450–456. [1721] R. H. Hahnloser, R. Sarpeshkar, M. A. Mahowald, R. J. Douglas, and H. S. Seung, “Digital selection and analogue ampliﬁcation coexist in a cortex-inspired silicon circuit,” Nature, vol. 405, no. 6789, pp. 947–951, 2000. [1722] S. Han, “Biologically plausible vlsi neural network implementation with asynchronous neuron and spike-based synapse,” in Neural Net- works, 2005. IJCNN’05. Proceedings. 2005 IEEE International Joint Conference on, vol. 5. IEEE, 2005, pp. 3244–3248. [1723] I. S. Han, “Biologically inspired hardware implementation of neural networks with programmable conductance,” in Neural Networks, 2007. IJCNN 2007. International Joint Conference on. IEEE, 2007, pp. 2336–2340. [1724] B. Kaplan, D. Bruderle, J. Schemmel, and K. Meier, “High- conductance states on a neuromorphic hardware system,” in Neural Networks, 2009. IJCNN 2009. International Joint Conference on. IEEE, 2009, pp. 1524–1530. [1725] P. Merolla and K. A. Boahen, “A recurrent model of orientation maps with simple and complex cells,” Departmental Papers (BE), p. 26, 2003. [1726] P. A. Merolla and K. Boahen, “Dynamic computation in a recurrent network of heterogeneous silicon neurons,” in Circuits and Systems, 2006. ISCAS 2006. Proceedings. 2006 IEEE International Symposium on. IEEE, 2006, pp. 4–pp. [1727] E. Ozalevli and C. M. Higgins, “Reconﬁgurable biologically inspired visual motion systems using modular neuromorphic vlsi chips,” Cir- cuits and Systems I: Regular Papers, IEEE Transactions on, vol. 52, no. 1, pp. 79–92, 2005. [1728] H. Pan, M. Manic, X. Li, and B. Wilamowski, “Multilevel logic multiplier using vlsi neural network,” in Industrial Technology, 2003 IEEE International Conference on, vol. 1. IEEE, 2003, pp. 327–332. [1729] G. N. Patel, E. A. Brown, and S. P. DeWeerth, “A neuromorphic vlsi system for modeling the neural control of axial locomotion,” in Advances in neural information processing systems, 2000, pp. 724– 730. [1730] J.-H. Shin, “Novel impulse neural circuits for pulsed neural network,” in Neural Networks, 1993. IJCNN’93-Nagoya. Proceedings of 1993 International Joint Conference on, vol. 3. IEEE, 1993, pp. 3017– 3022. [1731] F. Tenore, R. Etienne-Cummings, and M. A. Lewis, “Entrainment of silicon central pattern generators for legged locomotory control,” in Advances in Neural Information Processing Systems, 2003, p. None. [1732] ——, “A programmable array of silicon neurons for the control of legged locomotion,” in Circuits and Systems, 2004. ISCAS’04. Proceedings of the 2004 International Symposium on, vol. 5. IEEE, 2004, pp. V–349. [1733] F. Tenore, R. J. Vogelstein, R. Etienne-Cummings, G. Cauwenberghs, M. A. Lewis, and P. Hasler, “A spiking silicon central pattern generator with ﬂoating gate synapses [robot control applications],” in Circuits and Systems, 2005. ISCAS 2005. IEEE International Symposium on. IEEE, 2005, pp. 4106–4109. [1734] F. Tenore, R. J. Vogelstein, R. Etienne-Cummings, G. Cauwenberghs, and P. Hasler, “A ﬂoating-gate programmable array of silicon neurons for central pattern generating networks,” in Circuits and Systems, 2006. ISCAS 2006. Proceedings. 2006 IEEE International Symposium on. IEEE, 2006, pp. 4–pp. [1735] J. Tomberg, T. Ritoniemi, H. Tenhunen, and K. Kaski, “Vlsi im- plementation of pulse density modulated neural network structure,” in Circuits and Systems, 1989., IEEE International Symposium on. IEEE, 1989, pp. 2104–2107. [1736] J. Tomberg and K. Kaski, “Feasibility of synchronous pulse-density modulation arithmetic in integrated circuit implementations of arti- ﬁcial neural networks,” in Circuits and Systems, 1992. ISCAS’92. Proceedings., 1992 IEEE International Symposium on, vol. 5. IEEE, 1992, pp. 2232–2235. [1737] M. A. Viredaz, C. Lehmann, F. Blayo, and P. Ienne, “Mantra: A multi- model neural-network computer,” in VLSI for Neural Networks and Artiﬁcial Intelligence. Springer, 1994, pp. 93–102. [1738] R. J. Vogelstein, U. Mallik, E. Culurciello, G. Cauwenberghs, and R. Etienne-Cummings, “A multichip neuromorphic system for spike- based visual information processing,” Neural computation, vol. 19, no. 9, pp. 2281–2300, 2007. [1739] R. J. Vogelstein, F. Tenore, L. Guevremont, R. Etienne-Cummings, and V. K. Mushahwar, “A silicon central pattern generator controls locomotion in vivo,” Biomedical Circuits and Systems, IEEE Trans- actions on, vol. 2, no. 3, pp. 212–222, 2008. [1740] W. Waller, D. Bisset, and P. Daniell, “An analogue neuron suitable for a data frame architecture,” in VLSI for Artiﬁcial Intelligence and Neural Networks. Springer, 1991, pp. 195–204. [1741] Y. Wang and S.-C. Liu, “A two-dimensional conﬁgurable active silicon dendritic neuron array,” Circuits and Systems I: Regular Papers, IEEE Transactions on, vol. 58, no. 9, pp. 2159–2171, 2011. [1742] B. M. Wilamowski, Y. Ota, and M. L. Padgett, “Cmos realization of a pulse-stream artiﬁcial neural network using a current controlled oscillator,” in World Congr. Neural Networks, 1996, pp. 1317–1320. [1743] R. Yentis Jr and M. Zaghloul, “Vlsi implementation of locally connected neural network for solving partial differential equations,” Circuits and Systems I: Fundamental Theory and Applications, IEEE Transactions on, vol. 43, no. 8, pp. 687–690, 1996. [1744] T. Zahn, R. Izak, and K. Trott, “Mixed analog-digital neurochip for acoustical attention,” Graduiertenkolleg ?Informatik und Technik?, TU Ilmenau, 1996. [1745] N. Zhang, D. C. Wunsch et al., “A switched-resistor approach to hardware implementation of neural networks,” in Fuzzy Systems, 2005. FUZZ’05. The 14th IEEE International Conference on. IEEE, 2005, pp. 336–340. [1746] D. D. Corso and L. Reyneri, “Mixing analog and digital techniques for silicon neural networks,” in Circuits and Systems, 1990., IEEE International Symposium on. IEEE, 1990, pp. 2446–2449. [1747] D. Del Corso and L. Reyneri, “Mixing analog and digital techniques for silicon neural networks,” in Circuits and Systems, 1990., IEEE International Symposium on. IEEE, 1990, pp. 2446–2449. [1748] D. Del Corso, F. Gregoretti, and L. Reyneri, “Use of pulse rate and width modulations in a mixed analog/digital cell for artiﬁcial neural systems,” in Neurocomputing. Springer, 1990, pp. 157–160. [1749] J. G. Elias, “Artiﬁcial dendritic trees,” Neural Computation, vol. 5, no. 4, pp. 648–664, 1993. [1750] ——, “Silicon dendritic trees,” in Silicon Implementation of Pulse Coded Neural Networks. Springer, 1994, pp. 39–63. [1751] J. G. Elias and D. P. Northmore, “Switched-capacitor neuromorphs with wide-range variable dynamics,” Neural Networks, IEEE Trans- actions on, vol. 6, no. 6, pp. 1542–1548, 1995. [1752] ——, “Vlsi neuromorphs: Building blocks for neural circuits,” in The Neurobiology of Computation. Springer, 1995, pp. 385–390. [1753] Y. Horio, K. Aihara, and O. Yamamoto, “Neuron-synapse ic chip- set for large-scale chaotic neural networks,” Neural Networks, IEEE Transactions on, vol. 14, no. 5, pp. 1393–1404, 2003. [1754] B. W. Lee and B. J. Sheu, “General-purpose neural chips with electrically programmable synapses and gain-adjustable neurons,” IEEE journal of solid-state circuits, vol. 27, no. 9, pp. 1299–1302, 1992. [1755] J. Lenero-Bardallo, T. Serrano-Gotarredona, B. Linares-Barranco et al., “Compact calibration circuit for large neuromorphic arrays,” in Circuits and Systems, 2008. ISCAS 2008. IEEE International Symposium on. IEEE, 2008, pp. 1776–1779. [1756] A. F. Murray and A. V. Smith, “A novel computational and signalling method for vlsi neural networks,” in Solid-state Circuits Conference, 1987. ESSCIRC’87. 13th European. IEEE, 1987, pp. 19–22. [1757] C. Neugebauer and A. Yariv, “A parallel analog ccd/cmos neural net- work ic,” in Neural Networks, 1991., IJCNN-91-Seattle International Joint Conference on, vol. 1. IEEE, 1991, pp. 447–451. [1758] J. I. Raffel, J. R. Mann, R. Berger, A. M. Soares, and S. L. Gilbert, “A generic architecture for wafer-scale neuromorphic systems,” Lincoln Laboratory Journal;(USA), vol. 2, 1989. [1759] S. Satyanarayana, Y. Tsividis, and H. P. Graf, “A reconﬁgurable vlsi neural network,” Solid-State Circuits, IEEE Journal of, vol. 27, no. 1, pp. 67–81, 1992. [1760] F. N. Sibai and S. D. Kulkarni, “A time-multiplexed reconﬁgurable neuroprocessor,” IEEE Micro, vol. 17, no. 1, pp. 58–65, 1997. 66 [1761] G. Charles, C. Gordon, and W. E. Alexander, “An implementation of a biological neural model using analog-digital integrated circuits,” in Behavioral Modeling and Simulation Workshop, 2008. BMAS 2008. IEEE International. IEEE, 2008, pp. 78–83. [1762] T. Y. W. Choi, P. A. Merolla, J. V. Arthur, K. A. Boahen, and B. E. Shi, “Neuromorphic implementation of orientation hypercolumns,” Circuits and Systems I: Regular Papers, IEEE Transactions on, vol. 52, no. 6, pp. 1049–1060, 2005. [1763] M. DeYong, R. L. Findley, and C. Fields, “The design, fabrication, and test of a new vlsi hybrid analog-digital neural processing element,” Neural Networks, IEEE Transactions on, vol. 3, no. 3, pp. 363–374, 1992. [1764] M. DeYong and C. Fields, “Applications of hybrid analog-digital neural networks in signal processing,” in Proc. IEEE Int. Symp. on Circuits and Systems, 1992, pp. 2212–2215. [1765] M. Deyong, T. C. Eskridge, and C. Fields, “Temporal signal process- ing with high-speed hybrid analog-digital neural networks,” in Analog VLSI Neural Networks. Springer, 1993, pp. 105–126. [1766] M. DeYong and C. Fields, “Silicon neurons for phase and frequency detection and pattern generation,” in Silicon Implementation of Pulse Coded Neural Networks. Springer, 1994, pp. 65–77. [1767] R. J. Douglas, M. A. Mahowald, and K. A. Martin, “Hybrid analog- digital architectures for neuromorphic systems,” in Neural Networks, 1994. IEEE World Congress on Computational Intelligence., 1994 IEEE International Conference on, vol. 3. IEEE, 1994, pp. 1848– 1853. [1768] C. M. Higgins and C. Koch, “Multi-chip neuromorphic motion processing,” in Advanced Research in VLSI, 1999. Proceedings. 20th Anniversary Conference on. IEEE, 1999, pp. 309–323. [1769] ——, “A modular multi-chip neuromorphic architecture for real-time visual motion processing,” Analog Integrated Circuits and Signal Processing, vol. 24, no. 3, pp. 195–211, 2000. [1770] M. Hock, A. Hartel, J. Schemmel, and K. Meier, “An analog dynamic memory array for neuromorphic hardware,” in Circuit Theory and Design (ECCTD), 2013 European Conference on. IEEE, 2013, pp. 1–4. [1771] M. L. Mumford, D. K. Andes, and L. R. Kern, “The mod 2 neurocomputer system design,” Neural Networks, IEEE Transactions on, vol. 3, no. 3, pp. 423–433, 1992. [1772] E. O. Neftci, B. Toth, G. Indiveri, and H. D. Abarbanel, “Dynamic state and parameter estimation applied to neuromorphic systems,” Neural Computation, vol. 24, no. 6, p. 1, 2011. [1773] ——, “Dynamic state and parameter estimation applied to neuromor- phic systems,” Neural computation, vol. 24, no. 7, pp. 1669–1694, 2012. [1774] B. E. Shi and E. K. Tsang, “A neuromorphic multi-chip model of a disparity selective complex cell,” in Advances in Neural Information Processing Systems, 2003, p. None. [1775] Z. Yang, K. Cameron, W. Lewinger, B. Webb, and A. Murray, “Neuromorphic control of stepping pattern generation: A dynamic model with analog circuit implementation,” Neural Networks and Learning Systems, IEEE Transactions on, vol. 23, no. 3, pp. 373– 384, 2012. [1776] C. Bartolozzi, G. Indiveri, B. Sch¨olkopf, J. Platt, and T. Hofmann, “A selective attention multi-chip system with dynamic synapses and spiking neurons,” Advances in Neural Information Processing Systems, vol. 19, p. 113, 2007. [1777] L. Alvado, S. Sa¨ıghi, J. Tomas, and S. Renaud, “An exponential- decay synapse integrated circuit for bio-inspired neural networks.” in Computational Methods in Neural Modeling. Springer, 2003, pp. 670–677. [1778] V. Douence, S. Renaud-Le Masson, S. Sa¨ıghi, and G. Le Masson, “A ﬁeld-programmable conductance array ic for biological neurons modeling,” in Bio-Inspired Applications of Connectionism. Springer, 2001, pp. 31–38. [1779] B. W. Lee and B. Sheu, “A compact and general-purpose neural chip with electrically programmable synapses,” in Custom Integrated Circuits Conference, 1990., Proceedings of the IEEE 1990. IEEE, 1990, pp. 26–6. [1780] J. Leero-Bardallo, T. Serrano-Gotarredona, and B. Linares-Barranco, “A calibration technique for very low current and compact tunable neuromorphic cells: application to 5-bit 20-na dacs,” Circuits and Systems II: Express Briefs, IEEE Transactions on, vol. 55, no. 6, pp. 522–526, 2008. [1781] J. Beerhold, M. Jansen, and R. Eckmiller, “Pulse-processing neural net hardware with selectable topology and adaptive weights and delays,” in Neural Networks, 1990., 1990 IJCNN International Joint Conference on. IEEE, 1990, pp. 569–574. [1782] B. V. Benjamin, P. Gao, E. McQuinn, S. Choudhary, A. R. Chan- drasekaran, J. Bussat, R. Alvarez-Icaza, J. V. Arthur, P. A. Merolla, and K. Boahen, “Neurogrid: A mixed-analog-digital multichip system for large-scale neural simulations,” Proceedings of the IEEE, vol. 102, no. 5, pp. 699–716, 2014. [1783] K. Boahen, “Neurogrid: emulating a million neurons in the cortex,” in Conf. Proc. IEEE Eng. Med. Biol. Soc, 2006, p. 6702. [1784] S. Menon, S. Fok, A. Neckar, O. Khatib, and K. Boahen, “Controlling articulated robots in task-space with spiking silicon neurons,” in Biomedical Robotics and Biomechatronics (2014 5th IEEE RAS & EMBS International Conference on. IEEE, 2014, pp. 181–186. [1785] P. Merolla, J. Arthur, R. Alvarez, J.-M. Bussat, and K. Boahen, “A multicast tree router for multichip neuromorphic systems,” Circuits and Systems I: Regular Papers, IEEE Transactions on, vol. 61, no. 3, pp. 820–833, 2014. [1786] T. Pfeil, T. C. Potjans, S. Schrader, W. Potjans, J. Schemmel, M. Diesmann, and K. Meier, “Is a 4-bit synaptic weight resolution enough?–constraints on enabling spike-timing dependent plasticity in neuromorphic hardware,” Frontiers in neuroscience, vol. 6, 2012. [1787] L. Chua, “Memristor-the missing circuit element,” IEEE Transactions on circuit theory, vol. 18, no. 5, pp. 507–519, 1971. [1788] D. B. Strukov, G. S. Snider, D. R. Stewart, and R. S. Williams, “The missing memristor found,” nature, vol. 453, no. 7191, pp. 80–83, 2008. [1789] B. Linares-Barranco and T. Serrano-Gotarredona, “Memristance can explain spike-time-dependent-plasticity in neural synapses,” Nature precedings, vol. 1, p. 2009, 2009. [1790] L. Deng, D. Wang, Z. Zhang, P. Tang, G. Li, and J. Pei, “Energy consumption analysis for various memristive networks under different learning strategies,” Physics Letters A, vol. 380, no. 7, pp. 903–909, 2016. [1791] C.-C. Hsieh, A. Roy, Y.-F. Chang, D. Shahrjerdi, and S. K. Banerjee, “A sub-1-volt analog metal oxide memristive-based synaptic device with large conductance change for energy-efﬁcient spike-based com- puting systems,” Applied Physics Letters, vol. 109, no. 22, p. 223501, 2016. [1792] B. Liu, M. Hu, H. Li, Y. Chen, and C. J. Xue, “Bio-inspired ultra lower-power neuromorphic computing engine for embedded systems,” in Proceedings of the Ninth IEEE/ACM/IFIP International Conference on Hardware/Software Codesign and System Synthesis. IEEE Press, 2013, p. 23. [1793] B. Rajendran, Y. Liu, J.-s. Seo, K. Gopalakrishnan, L. Chang, D. J. Friedman, and M. B. Ritter, “Speciﬁcations of nanoscale devices and circuits for neuromorphic computational systems,” Electron Devices, IEEE Transactions on, vol. 60, no. 1, pp. 246–253, 2013. [1794] T. M. Taha, R. Hasan, C. Yakopcic, and M. R. McLean, “Exploring the design space of specialized multicore neural processors,” in Neural Networks (IJCNN), The 2013 International Joint Conference on. IEEE, 2013, pp. 1–8. [1795] ——, “Low power neuromorphic architectures to enable pervasive deployment of intrusion detection systems,” in Cybersecurity Systems for Human Cognition Augmentation. Springer, 2014, pp. 151–168. [1796] T. Tang, R. Luo, B. Li, H. Li, Y. Wang, and H. Yang, “Energy efﬁcient spiking neural network design with rram devices,” in Integrated Circuits (ISIC), 2014 14th International Symposium on. IEEE, 2014, pp. 268–271. [1797] Y. Wang, T. Tang, L. Xia, B. Li, P. Gu, H. Yang, H. Li, and Y. Xie, “Energy efﬁcient rram spiking neural network for real time classiﬁcation,” in Proceedings of the 25th edition on Great Lakes Symposium on VLSI. ACM, 2015, pp. 189–194. [1798] L. Chen, C. Li, T. Huang, and X. Wang, “Quick noise-tolerant learning in a multi-layer memristive neural network,” Neurocomputing, vol. 129, pp. 122–126, 2014. [1799] B. Gao, P. Huang, B. Chen, L. Liu, X. Liu, and J. Kang, “Origin and suppressing methodology of intrinsic variations in metal-oxide rram based synaptic devices,” in Solid-State and Integrated Circuit Technology (ICSICT), 2014 12th IEEE International Conference on. IEEE, 2014, pp. 1–3. [1800] D. Querlioz, O. Bichler, and C. Gamrat, “Simulation of a memristor- based spiking neural network immune to device variations,” in Neural Networks (IJCNN), The 2011 International Joint Conference on. IEEE, 2011, pp. 1775–1781. [1801] D. Querlioz, O. Bichler, P. Dollfus, and C. Gamrat, “Immunity to device variations in a spiking neural network with memristive 67 nanodevices,” Nanotechnology, IEEE Transactions on, vol. 12, no. 3, pp. 288–295, 2013. [1802] X. Zhu, “Impact of imprecise programming of memristor on building hardware neural network,” in Electrical and Control Engineering (ICECE), 2011 International Conference on. IEEE, 2011, pp. 4527– 4529. [1803] G. Indiveri, R. Legenstein, G. Deligeorgis, T. Prodromakis et al., “Integration of nanoscale memristor synapses in neuromorphic com- puting architectures,” Nanotechnology, vol. 24, no. 38, p. 384010, 2013. [1804] S. Mandal, A. El-Amin, K. Alexander, B. Rajendran, and R. Jha, “Novel synaptic memory device for neuromorphic computing,” Sci- entiﬁc reports, vol. 4, 2014. [1805] H. Wang, H. Li, and R. E. Pino, “Memristor-based synapse design and training scheme for neuromorphic computing architecture,” in Neural Networks (IJCNN), The 2012 International Joint Conference on. IEEE, 2012, pp. 1–5. [1806] X. Zhu, C. Du, Y. Jeong, and W. D. Lu, “Emulation of synaptic metaplasticity in memristors,” Nanoscale, vol. 9, no. 1, pp. 45–51, 2017. [1807] X. Shi, S. Duan, L. Wang, T. Huang, and C. Li, “A novel mem- ristive electronic synapse-based hermite chaotic neural network with application in cryptography,” Neurocomputing, vol. 166, pp. 487–495, 2015. [1808] T. C. Jackson, A. A. Sharma, J. A. Bain, J. A. Weldon, and L. Pileggi, “An rram-based oscillatory neural network,” in Circuits & Systems (LASCAS), 2015 IEEE 6th Latin American Symposium on. IEEE, 2015, pp. 1–4. [1809] ——, “Oscillatory neural networks based on tmo nano-oscillators and multi-level rram cells,” IEEE journal on Emerging and Selected Topics in Circuits and Systems, vol. 5, no. 2, pp. 230–241, 2015. [1810] A. Sharma, T. Jackson, M. Schulaker, C. Kuo, C. Augustine, J. Bain, H.-S. Wong, S. Mitra, L. Pileggi, and J. Weldon, “High performance, integrated 1t1r oxide-based oscillator: Stack engineering for low- power operation in neural network applications,” in VLSI Technology (VLSI Technology), 2015 Symposium on. IEEE, 2015, pp. T186– T187. [1811] W. Cai and R. Tetzlaff, “Synapse as a memristor,” in Memristor Networks. Springer, 2014, pp. 113–128. [1812] T. Chang, S.-H. Jo, K.-H. Kim, P. Sheridan, S. Gaba, and W. Lu, “Synaptic behaviors and modeling of a metal oxide memristive device,” Applied physics A, vol. 102, no. 4, pp. 857–863, 2011. [1813] L. Chua, “Memristor, hodgkin–huxley, and edge of chaos,” Nanotech- nology, vol. 24, no. 38, p. 383001, 2013. [1814] F. Corinto, A. Ascoli, and S.-M. Kang, “Memristor-based neural circuits,” in Circuits and Systems (ISCAS), 2013 IEEE International Symposium on. IEEE, 2013, pp. 417–420. [1815] F. Corinto, M. Gilli, A. Ascoli, and R. Tetzlaff, “Complex dynamics in neuromorphic memristor circuits,” in Circuit Theory and Design (ECCTD), 2013 European Conference on. IEEE, 2013, pp. 1–4. [1816] S. Gaba, P. Sheridan, J. Zhou, S. Choi, and W. Lu, “Stochastic memristive devices for computing and neuromorphic applications,” Nanoscale, vol. 5, no. 13, pp. 5872–5878, 2013. [1817] M. Hu, Y. Wang, W. Wen, Y. Wang, and H. Li, “Leveraging stochastic memristor devices in neuromorphic hardware systems,” IEEE Journal on Emerging and Selected Topics in Circuits and Systems, vol. PP, no. 99, pp. 1–12, 2016. [1818] R. Naous, M. AlShedivat, E. Neftci, G. Cauwenberghs, and K. N. Salama, “Memristor-based neural networks: Synaptic versus neuronal stochasticity,” AIP Advances, vol. 6, no. 11, p. 111304, 2016. [1819] P. Lorenzi, V. Sucre, G. Romano, R. Rao, and F. Irrera, “Memristor based neuromorphic circuit for visual pattern recognition,” in Memris- tive Systems (MEMRISYS) 2015 International Conference on. IEEE, 2015, pp. 1–2. [1820] A. Wu, Z. Zeng, and J. Xiao, “Dynamic evolution evoked by external inputs in memristor-based wavelet neural networks with different memductance functions,” Advances in Difference Equations, vol. 2013, no. 1, pp. 1–14, 2013. [1821] G. C. Adam, B. D. Hoskins, M. Prezioso, F. Merrikh-Bayat, B. Chakrabarti, and D. B. Strukov, “3-d memristor crossbars for analog and neuromorphic computing applications,” IEEE Transactions on Electron Devices, vol. 64, no. 1, pp. 312–318, 2017. [1822] S. Agarwal, T.-T. Quach, O. Parekh, A. H. Hsia, E. P. DeBenedictis, C. D. James, M. J. Marinella, and J. B. Aimone, “Energy scaling advantages of resistive memory crossbar based computation and its application to sparse coding,” Frontiers in neuroscience, vol. 9, 2015. [1823] M. Bavandpour, H. Soleimani, B. Linares-Barranco, D. Abbott, and L. O. Chua, “Generalized reconﬁgurable memristive dynamical sys- tem (mds) for neuromorphic applications,” Frontiers in neuroscience, vol. 9, 2015. [1824] D. Chabi, Z. Wang, W. Zhao, and J.-O. Klein, “On-chip supervised learning rule for ultra high density neural crossbar using memristor for synapse and neuron,” in Proceedings of the 2014 IEEE/ACM International Symposium on Nanoscale Architectures. ACM, 2014, pp. 7–12. [1825] Q. Chen, Q. Qiu, H. Li, and Q. Wu, “A neuromorphic architecture for anomaly detection in autonomous large-area trafﬁc monitoring,” in Computer-Aided Design (ICCAD), 2013 IEEE/ACM International Conference on. IEEE, 2013, pp. 202–205. [1826] P.-Y. Chen and S. Yu, “Partition sram and rram based synaptic arrays for neuro-inspired computing,” in Circuits and Systems (ISCAS), 2016 IEEE International Symposium on. IEEE, 2016, pp. 2310–2313. [1827] P. Chi, S. Li, C. Xu, T. Zhang, J. Zhao, Y. Liu, Y. Wang, and Y. Xie, “Prime: A novel processing-in-memory architecture for neural network computation in reram-based main memory,” in Proceedings of the 43rd International Symposium on Computer Architecture. IEEE Press, 2016, pp. 27–39. [1828] H. Choi, H. Jung, J. Lee, J. Yoon, J. Park, D.-j. Seong, W. Lee, M. Hasan, G.-Y. Jung, and H. Hwang, “An electrically modiﬁ- able synapse array of resistive switching memory,” Nanotechnology, vol. 20, no. 34, p. 345201, 2009. [1829] R. Hasan, C. Yakopcic, and T. M. Taha, “Ex-situ training of dense memristor crossbar for neuromorphic applications,” in Nanoscale Ar- chitectures (NANOARCH), 2015 IEEE/ACM International Symposium on. IEEE, 2015, pp. 75–81. [1830] M. Hu, H. Li, Q. Wu, and G. S. Rose, “Hardware realization of bsb recall function using memristor crossbar arrays,” in Proceedings of the 49th Annual Design Automation Conference. ACM, 2012, pp. 498–503. [1831] M. Hu, J. P. Strachan, Z. Li, E. M. Grafals, N. Davila, C. Graves, S. Lam, N. Ge, R. S. Williams, and J. Yang, “Dot-product engine for neuromorphic computing: Programming 1t1m crossbar to accelerate matrix-vector multiplication,” in Proceedings of DAC, vol. 53, 2016. [1832] H. Jiang, W. Zhu, F. Luo, K. Bai, C. Liu, X. Zhang, J. J. Yang, Q. Xia, Y. Chen, and Q. Wu, “Cyclical sensing integrate-and-ﬁre circuit for memristor array based neuromorphic computing,” in Circuits and Systems (ISCAS), 2016 IEEE International Symposium on. IEEE, 2016, pp. 930–933. [1833] I. Kataeva, F. Merrikh-Bayat, E. Zamanidoost, and D. Strukov, “Ef- ﬁcient training algorithms for neural networks based on memristive crossbar circuits,” in Neural Networks (IJCNN), 2015 International Joint Conference on. IEEE, 2015, pp. 1–8. [1834] K.-H. Kim, S. Gaba, D. Wheeler, J. M. Cruz-Albrecht, T. Hussain, N. Srinivasa, and W. Lu, “A functional hybrid memristor crossbar- array/cmos system for data storage and neuromorphic applications,” Nano letters, vol. 12, no. 1, pp. 389–395, 2011. [1835] H. Li, X. Liu, M. Mao, Y. Chen, Q. Wu, and M. Bamell, “Neuro- morphic hardware acceleration enabled by emerging technologies,” in Integrated Circuits (ISIC), 2014 14th International Symposium on. IEEE, 2014, pp. 124–127. [1836] Z. Li, C. Liu, Y. Wang, B. Yan, C. Yang, J. Yang, and H. Li, “An overview on memristor crossabr based neuromorphic circuit and architecture,” in Very Large Scale Integration (VLSI-SoC), 2015 IFIP/IEEE International Conference on. IEEE, 2015, pp. 52–56. [1837] H. Li, B. Liu, X. Liu, M. Mao, Y. Chen, Q. Wu, and Q. Qiu, “The applications of memristor devices in next-generation cortical processor designs,” in Circuits and Systems (ISCAS), 2015 IEEE International Symposium on. IEEE, 2015, pp. 17–20. [1838] H. H. Li, C. Liu, B. Yan, C. Yang, L. Song, Z. Li, Y. Chen, W. Zhu, Q. Wu, and H. Jiang, “Spiking-based matrix computation by lever- aging memristor crossbar array,” in Computational Intelligence for Security and Defense Applications (CISDA), 2015 IEEE Symposium on. IEEE, 2015, pp. 1–4. [1839] B. Liu, M. Hu, H. Li, Z.-H. Mao, Y. Chen, T. Huang, and W. Zhang, “Digital-assisted noise-eliminating training for memristor crossbar- based analog neuromorphic computing engine,” in Design Automation Conference (DAC), 2013 50th ACM/EDAC/IEEE. IEEE, 2013, pp. 1–6. [1840] X. Liu, M. Mao, H. Li, Y. Chen, H. Jiang, J. J. Yang, Q. Wu, and M. Barnell, “A heterogeneous computing system with memristor- based neuromorphic accelerators,” in High Performance Extreme Computing Conference (HPEC), 2014 IEEE. IEEE, 2014, pp. 1– 6. 68 [1841] B. Liu, H. Li, Y. Chen, X. Li, T. Huang, Q. Wu, and M. Barnell, “Reduction and ir-drop compensations techniques for reliable neuro- morphic computing systems,” in Computer-Aided Design (ICCAD), 2014 IEEE/ACM International Conference on. IEEE, 2014, pp. 63– 70. [1842] X. Liu, M. Mao, B. Liu, H. Li, Y. Chen, B. Li, Y. Wang, H. Jiang, M. Barnell, Q. Wu et al., “Reno: a high-efﬁcient reconﬁgurable neuromorphic computing accelerator design,” in Design Automation Conference (DAC), 2015 52nd ACM/EDAC/IEEE. IEEE, 2015, pp. 1–6. [1843] C. Liu, B. Yan, C. Yang, L. Song, Z. Li, B. Liu, Y. Chen, H. Li, Q. Wu, and H. Jiang, “A spiking neuromorphic design with resistive crossbar,” in Proceedings of the 52nd Annual Design Automation Conference. ACM, 2015, p. 14. [1844] B. Liu, W. Wen, Y. Chen, X. Li, C.-R. Wu, and T.-Y. Ho, “Eda challenges for memristor-crossbar based neuromorphic computing,” in Proceedings of the 25th edition on Great Lakes Symposium on VLSI. ACM, 2015, pp. 185–188. [1845] X. Liu, M. Mao, B. Liu, B. Li, Y. Wang, H. Jiang, M. Barnell, Q. Wu, J. Yang, H. Li et al., “Harmonica: A framework of heterogeneous computing systems with memristor-based neuromorphic computing accelerators,” IEEE Transactions on Circuits and Systems I: Regular Papers, vol. 63, no. 5, pp. 617–628, 2016. [1846] C. Liu, Y. Chen, and H. Li, “Neural processor design enabled by memristor technology,” in Rebooting Computing (ICRC), IEEE International Conference on. IEEE, 2016, pp. 1–4. [1847] Y. Long, E. M. Jung, J. Kung, and S. Mukhopadhyay, “Reram crossbar based recurrent neural network for human activity detection,” in Neural Networks (IJCNN), 2016 International Joint Conference on. IEEE, 2016, pp. 939–946. [1848] C. Merkel and D. Kudithipudi, “Unsupervised learning in neuromem- ristive systems,” in 2015 National Aerospace and Electronics Confer- ence (NAECON). IEEE, 2015, pp. 336–338. [1849] S. Park, J. Noh, M.-l. Choo, A. M. Sheri, M. Chang, Y.-B. Kim, C. J. Kim, M. Jeon, B.-G. Lee, B. H. Lee et al., “Nanoscale rram- based synaptic electronics: toward a neuromorphic computing device,” Nanotechnology, vol. 24, no. 38, p. 384009, 2013. [1850] S. Park, M. Chu, J. Kim, J. Noh, M. Jeon, B. H. Lee, H. Hwang, B. Lee, and B.-g. Lee, “Electronic system with memristive synapses for pattern recognition,” Scientiﬁc reports, vol. 5, 2015. [1851] M. Prezioso, F. Merrikh-Bayat, B. Hoskins, G. Adam, K. K. Likharev, and D. B. Strukov, “Training and operation of an integrated neuro- morphic network based on metal-oxide memristors,” Nature, vol. 521, no. 7550, pp. 61–64, 2015. [1852] F. Rothganger, B. R. Evans, J. B. Aimone, and E. P. DeBenedictis, “Training neural hardware with noisy components,” in Neural Net- works (IJCNN), 2015 International Joint Conference on. IEEE, 2015, pp. 1–8. [1853] A. Sengupta and K. Roy, “Spin-transfer torque magnetic neuron for low power neuromorphic computing,” in Neural Networks (IJCNN), 2015 International Joint Conference on. IEEE, 2015, pp. 1–7. [1854] A. Shaﬁee, A. Nag, N. Muralimanohar, R. Balasubramonian, J. Stra- chan, M. Hu, R. S. Williams, and V. Srikumar, “Isaac: A convolutional neural network accelerator with in-situ analog arithmetic in crossbars.” ISCA, 2016. [1855] J. A. Starzyk et al., “Comparison of two memristor based neural network learning schemes for crossbar architecture,” in Advances in Computational Intelligence. Springer, 2013, pp. 492–499. [1856] J. Starzyk et al., “Memristor crossbar architecture for synchronous neural networks,” Circuits and Systems I: Regular Papers, IEEE Transactions on, vol. 61, no. 8, pp. 2390–2401, 2014. [1857] M. S. Tarkov, “Mapping neural network computations onto memristor crossbar,” in Control and Communications (SIBCON), 2015 Interna- tional Siberian Conference on. IEEE, 2015, pp. 1–4. [1858] M. Tarkov, “Mapping weight matrix of a neural network?s layer onto memristor crossbar,” Optical Memory and Neural Networks, vol. 24, no. 2, pp. 109–115, 2015. [1859] S. N. Truong, S.-J. Ham, and K.-S. Min, “Neuromorphic crossbar circuit with nanoscale ﬁlamentary-switching binary memristors for speech recognition,” Nanoscale research letters, vol. 9, no. 1, pp. 1– 9, 2014. [1860] S. N. Truong and K.-S. Min, “New memristor-based crossbar array architecture with 50-% area reduction and 48-% power saving for matrix-vector multiplication of analog neuromorphic computing,” Journal of semiconductor technology and science, vol. 14, no. 3, pp. 356–363, 2014. [1861] S. N. Truong, K. Van Pham, W. Yang, and K.-S. Min, “Sequen- tial memristor crossbar for neuromorphic pattern recognition,” IEEE Transactions on Nanotechnology, vol. 15, no. 6, pp. 922–930, 2016. [1862] S. Wang, W. Wang, C. Yakopcic, E. Shin, G. Subramanyam, and T. Taha, “Reconﬁgurable neuromorphic crossbars based on titanium oxide memristors,” Electronics Letters, vol. 52, no. 20, pp. 1673–1675, 2016. [1863] D. Wheeler, K. Kim, S. Gaba, E. Wang, S. Kim, I. Valles, J. Li, Y. Royter, J. Cruz-Albrecht, T. Hussain et al., “Cmos-integrated memristors for neuromorphic architectures,” in Semiconductor Device Research Symposium (ISDRS), 2011 International. IEEE, 2011, pp. 1–2. [1864] D. Wheeler, I. Alvarado-Rodriguez, K. Elliott, J. Kally, J. Hermiz, H. Hunt, T. Hussain, and N. Srinivasa, “Fabrication and characteriza- tion of tungsten-oxide-based memristors for neuromorphic circuits,” in Cellular Nanoscale Networks and their Applications (CNNA), 2014 14th International Workshop on. IEEE, 2014, pp. 1–2. [1865] L. Xia, T. Tang, W. Huangfu, M. Cheng, X. Yin, B. Li, Y. Wang, and H. Yang, “Switched by input: Power efﬁcient structure for rram-based convolutional neural network,” in Proceedings of the 53rd Annual Design Automation Conference. ACM, 2016, p. 125. [1866] G. Xie, G. Liu, and S. Zhang, “Expression of emotion using a system combined artiﬁcial neural network and memristor-based crossbar array,” in Control Conference (CCC), 2016 35th Chinese. IEEE, 2016, pp. 9837–9841. [1867] L. Xu, C. Li, and L. Chen, “Analog memristor based neuromorphic crossbar circuit for image recognition,” in Intelligent Control and In- formation Processing (ICICIP), 2015 Sixth International Conference on. IEEE, 2015, pp. 155–160. [1868] C. Yakopcic and T. M. Taha, “Energy efﬁcient perceptron pattern recognition using segmented memristor crossbar arrays,” in Neural Networks (IJCNN), The 2013 International Joint Conference on. IEEE, 2013, pp. 1–8. [1869] C. Yakopcic, R. Hasan, and T. M. Taha, “Tolerance to defective memristors in a neuromorphic learning circuit,” in Aerospace and Electronics Conference, NAECON 2014-IEEE National. IEEE, 2014, pp. 243–249. [1870] C. Yakopcic, R. Hasan, T. M. Taha, M. R. McLean, and D. Palmer, “Efﬁcacy of memristive crossbars for neuromorphic processors,” in Neural Networks (IJCNN), 2014 International Joint Conference on. IEEE, 2014, pp. 15–20. [1871] C. Yakopcic, R. Hasan, T. M. Taha, M. McLean, and D. Palmer, “Memristor-based neuron circuit and method for applying learning algorithm in spice?” Electronics Letters, vol. 50, no. 7, pp. 492–494, 2014. [1872] C. Yakopcic, T. Taha, and M. McLean, “Method for ex-situ training in memristor-based neuromorphic circuit using robust weight program- ming method,” Electronics Letters, vol. 51, no. 12, pp. 899–900, 2015. [1873] C. Yakopcic, R. Hasan, and T. M. Taha, “Memristor based neuro- morphic circuit for ex-situ training of multi-layer neural network algorithms,” in Neural Networks (IJCNN), 2015 International Joint Conference on. IEEE, 2015, pp. 1–7. [1874] C. Yakopcic, T. M. Taha, G. Subramanyam, and R. E. Pino, “Impact of memristor switching noise in a neuromorphic crossbar,” in 2015 National Aerospace and Electronics Conference (NAECON). IEEE, 2015, pp. 320–326. [1875] C. Yakopcic, R. Hasan, T. M. Taha, and D. Palmer, “Spice analysis of dense memristor crossbars for low power neuromorphic processor designs,” in 2015 National Aerospace and Electronics Conference (NAECON). IEEE, 2015, pp. 305–311. [1876] C. Yakopcic and T. M. Taha, “Ex-situ programming in a neuromorphic memristor based crossbar circuit,” in 2015 National Aerospace and Electronics Conference (NAECON). IEEE, 2015, pp. 300–304. [1877] C. Yakopcic, M. Z. Alom, and T. M. Taha, “Memristor crossbar deep network implementation based on a convolutional neural network,” in Neural Networks (IJCNN), 2016 International Joint Conference on. IEEE, 2016, pp. 963–970. [1878] B. Yan, A. M. Mahmoud, J. J. Yang, Q. Wu, Y. Chen, and H. H. Li, “A neuromorphic asic design using one-selector-one-memristor crossbar,” in Circuits and Systems (ISCAS), 2016 IEEE International Symposium on. IEEE, 2016, pp. 1390–1393. [1879] P. Yao, H. Wu, B. Gao, G. Zhang, and H. Qian, “The effect of variation on neuromorphic network based on 1t1r memristor array,” in 2015 15th Non-Volatile Memory Technology Symposium (NVMTS). IEEE, 2015, pp. 1–3. 69 [1880] K. Yogendra, D. Fan, and K. Roy, “Coupled spin torque nano oscillators for low power neural computation,” Magnetics, IEEE Transactions on, vol. 51, no. 10, pp. 1–9, 2015. [1881] S. Yu, B. Gao, Z. Fang, H. Yu, J. Kang, and H.-S. Wong, “A neuromorphic visual system using rram synaptic devices with sub- pj energy and tolerance to variability: experimental characterization and large-scale modeling,” in Electron Devices Meeting (IEDM), 2012 IEEE International. IEEE, 2012, pp. 10–4. [1882] S. Yu, P.-Y. Chen, Y. Cao, L. Xia, Y. Wang, and H. Wu, “Scaling-up resistive synaptic arrays for neuro-inspired architecture: Challenges and prospect,” in 2015 IEEE International Electron Devices Meeting (IEDM). IEEE, 2015, pp. 17–3. [1883] S. P. Adhikari, C. Yang, H. Kim, and L. O. Chua, “Memristor bridge synapse-based neural network and its learning,” Neural Networks and Learning Systems, IEEE Transactions on, vol. 23, no. 9, pp. 1426– 1435, 2012. [1884] S. P. Adhikari, H. Kim, R. K. Budhathoki, C. Yang, and J.-M. Kim, “Learning with memristor bridge synapse-based neural networks,” in Cellular Nanoscale Networks and their Applications (CNNA), 2014 14th International Workshop on. IEEE, 2014, pp. 1–2. [1885] S. P. Adhikari, H. Kim, R. K. Budhathoki, C. Yang, and L. O. Chua, “A circuit-based learning architecture for multilayer neural networks with memristor bridge synapses,” Circuits and Systems I: Regular Papers, IEEE Transactions on, vol. 62, no. 1, pp. 215–223, 2015. [1886] H. Kim, M. P. Sah, C. Yang, T. Roska, and L. O. Chua, “Memristor bridge synapses,” Proceedings of the IEEE, vol. 100, no. 6, pp. 2061– 2070, 2012. [1887] ——, “Memristor bridge-based artiﬁcial neural weighting circuit,” in Memristor Networks. Springer, 2014, pp. 249–265. [1888] M. P. Sah, C. Yang, H. Kim, and L. Chua, “A voltage mode memristor bridge synaptic circuit with memristor emulators,” Sensors, vol. 12, no. 3, pp. 3587–3604, 2012. [1889] M. P. Sah, C. Yang, H. Kim, and L. O. Chua, “Memristor circuit for artiﬁcial synaptic weighting of pulse inputs,” in Circuits and Systems (ISCAS), 2012 IEEE International Symposium on. IEEE, 2012, pp. 1604–1607. [1890] M. P. Sah, C. Yang, R. K. Budhathoki, and H. Kim, “Features of memristor emulator-based artiﬁcial neural synapses,” in Circuits and Systems (ISCAS), 2013 IEEE International Symposium on. IEEE, 2013, pp. 421–424. [1891] M. S. Tarkov, “Hopﬁeld network with interneuronal connections based on memristor bridges,” in International Symposium on Neural Networks. Springer, 2016, pp. 196–203. [1892] ——, “Oscillatory neural associative memories with synapses based on memristor bridges,” Optical Memory and Neural Networks, vol. 25, no. 4, pp. 219–227, 2016. [1893] L. Wang, X. Wang, S. Duan, and H. Li, “A spintronic memristor bridge synapse circuit and the application in memrisitive cellular automata,” Neurocomputing, vol. 167, pp. 346–351, 2015. [1894] L. Chen, C. Li, T. Huang, Y. Chen, S. Wen, and J. Qi, “A synapse memristor model with forgetting effect,” Physics Letters A, vol. 377, no. 45, pp. 3260–3265, 2013. [1895] P. Zhang, C. Li, T. Huang, L. Chen, and Y. Chen, “Forgetting memris- tor based neuromorphic system for pattern training and recognition,” Neurocomputing, vol. 222, pp. 47–53, 2017. [1896] Z. Chen, B. Gao, Z. Zhou, P. Huang, H. Li, W. Ma, D. Zhu, L. Liu, X. Liu, J. Kang et al., “Optimized learning scheme for grayscale image recognition in a rram based analog neuromorphic system,” in 2015 IEEE International Electron Devices Meeting (IEDM). IEEE, 2015, pp. 17–7. [1897] S. Danilin and S. Shchanikov, “Neural network control over operation accuracy of memristor-based hardware,” in 2015 International Con- ference on Mechanical Engineering, Automation and Control Systems (MEACS). IEEE, 2015, pp. 1–5. [1898] S. Danilin, S. Shchanikov, and A. Galushkin, “The research of memristor-based neural network components operation accuracy in control and communication systems,” in Control and Communications (SIBCON), 2015 International Siberian Conference on. IEEE, 2015, pp. 1–6. [1899] L. Deng, G. Li, N. Deng, D. Wang, Z. Zhang, W. He, H. Li, J. Pei, and L. Shi, “Complex learning in bio-plausible memristive networks,” Scientiﬁc reports, vol. 5, 2015. [1900] D. Querlioz, P. Dollfus, O. Bichler, and C. Gamrat, “Learning with memristive devices: How should we model their behavior?” in Nanoscale Architectures (NANOARCH), 2011 IEEE/ACM Interna- tional Symposium on. IEEE, 2011, pp. 150–156. [1901] G. S. Rose, R. Pino, and Q. Wu, “A low-power memristive neuro- morphic circuit utilizing a global/local training mechanism,” in Neural networks (IJCNN), the 2011 international joint conference on. IEEE, 2011, pp. 2080–2086. [1902] M. Soltiz, D. Kudithipudi, C. Merkel, G. S. Rose, and R. E. Pino, “Memristor-based neural logic blocks for nonlinearly separable func- tions,” Computers, IEEE Transactions on, vol. 62, no. 8, pp. 1597– 1606, 2013. [1903] W. Woods, J. R. Burger, and C. Teuscher, “On the inﬂuence of synaptic weight states in a locally competitive algorithm for mem- ristive hardware,” in Nanoscale Architectures (NANOARCH), 2014 IEEE/ACM International Symposium on. IEEE, 2014, pp. 19–24. [1904] W. Woods, J. Burger, and C. Teuscher, “Synaptic weight states in a lo- cally competitive algorithm for neuromorphic memristive hardware,” 2015. [1905] C.-R. Wu, W. Wen, T.-Y. Ho, and Y. Chen, “Thermal optimization for memristor-based hybrid neuromorphic computing systems,” in 2016 21st Asia and South Paciﬁc Design Automation Conference (ASP- DAC). IEEE, 2016, pp. 274–279. [1906] M. Al-Shedivat, R. Naous, E. Neftci, G. Cauwenberghs, and K. N. Salama, “Inherently stochastic spiking neurons for probabilistic neural computation,” in Neural Engineering (NER), 2015 7th International IEEE/EMBS Conference on. IEEE, 2015, pp. 356–359. [1907] M. Al-Shedivat, R. Naous, G. Cauwenberghs, and K. N. Salama, “Memristors empower spiking neurons with stochasticity,” 2015. [1908] Y. Babacan, F. Kac¸ar, and K. G¨urkan, “A spiking and bursting neuron circuit based on memristor,” Neurocomputing, 2016. [1909] V. Demin, V. Erokhin, A. Emelyanov, S. Battistoni, G. Baldi, S. Iannotta, P. Kashkarov, and M. Kovalchuk, “Hardware elemen- tary perceptron based on polyaniline memristive devices,” Organic Electronics, vol. 25, pp. 16–20, 2015. [1910] A. Mehonic and A. J. Kenyon, “Emulating the electrical activity of the neuron using a silicon oxide rram cell,” Frontiers in neuroscience, vol. 10, 2016. [1911] J. Shamsi, A. Amirsoleimani, S. Mirzakuchaki, and M. Ahmadi, “Modular neuron comprises of memristor-based synapse,” Neural Computing and Applications, pp. 1–11, 2015. [1912] L. Wang, M. Duan, and S. Duan, “Memristive perceptron for combi- national logic classiﬁcation,” Mathematical Problems in Engineering, vol. 2013, 2013. [1913] A. Galushkin, “Neural networks realizations using memristors,” in Engineering and Telecommunication (EnT), 2014 International Con- ference on. IEEE, 2014, pp. 77–81. [1914] M. S. Feali and A. Ahmadi, “Transient response characteristic of memristor circuits and biological-like current spikes,” Neural Com- puting and Applications, pp. 1–11, 2016. [1915] E. Gale, B. de Lacy Costello, and A. Adamatzky, “Emergent spiking in non-ideal memristor networks,” Microelectronics Journal, vol. 45, no. 11, pp. 1401–1415, 2014. [1916] ——, “Spiking in memristor networks,” in Memristor Networks. Springer, 2014, pp. 365–387. [1917] M. S. Feali and A. Ahmadi, “Realistic hodgkin–huxley axons using stochastic behavior of memristors,” Neural Processing Letters, pp. 1–14, 2016. [1918] M. D. Pickett, G. Medeiros-Ribeiro, and R. S. Williams, “A scalable neuristor built with mott memristors,” Nature materials, vol. 12, no. 2, pp. 114–117, 2013. [1919] F. M. Bayat, S. B. Shouraki, and I. E. P. Afrakoti, “Bottleneck of using a single memristive device as a synapse,” Neurocomputing, vol. 115, pp. 166–168, 2013. [1920] F. M. Bayat and S. B. Shouraki, “Nonlinear behavior of memristive devices during tuning process and its impact on stdp learning rule in memristive neural networks,” Neural Computing and Applications, vol. 26, no. 1, pp. 67–75, 2015. [1921] A. Serb, R. Berdan, A. Khiat, S. Li, E. Vasilaki, C. Papavassiliou, and T. Prodromakis, “Memristors as synapse emulators in the context of event-based computation,” in Circuits and Systems (ISCAS), 2014 IEEE International Symposium on. IEEE, 2014, pp. 2085–2088. [1922] S. Gi, I. Yeo, M. Chu, S. Kim, and B. Lee, “Fundamental issues of implementing hardware neural networks using memristor,” in 2015 International SoC Design Conference (ISOCC). IEEE, 2015, pp. 215–216. [1923] R. E. Pino, H. Li, Y. Chen, M. Hu, and B. Liu, “Statistical memristor modeling and case study in neuromorphic computing,” in Design Automation Conference (DAC), 2012 49th ACM/EDAC/IEEE. IEEE, 2012, pp. 585–590. 70 [1924] G. Bao and Z. Zeng, “Multistability of periodic delayed recurrent neu- ral network with memristors,” Neural Computing and Applications, vol. 23, no. 7-8, pp. 1963–1967, 2013. [1925] Z. Cai, L. Huang, M. Zhu, and D. Wang, “Finite-time stabilization control of memristor-based neural networks,” Nonlinear Analysis: Hybrid Systems, vol. 20, pp. 37–54, 2016. [1926] A. Chandrasekar, R. Rakkiyappan, and X. Li, “Effects of bounded and unbounded leakage time-varying delays in memristor-based recurrent neural networks with different memductance functions,” Neurocom- puting, 2016. [1927] J. Chen, Z. Zeng, and P. Jiang, “Global mittag-lefﬂer stability and synchronization of memristor-based fractional-order neural networks,” Neural Networks, vol. 51, pp. 1–8, 2014. [1928] ——, “On the periodic dynamics of memristor-based neural networks with time-varying delays,” Information Sciences, vol. 279, pp. 358– 373, 2014. [1929] ——, “Global exponential almost periodicity of a delayed memristor- based neural networks,” Neural Networks, vol. 60, pp. 33–43, 2014. [1930] L. Chen, R. Wu, J. Cao, and J.-B. Liu, “Stability and synchronization of memristor-based fractional-order delayed neural networks,” Neural Networks, vol. 71, pp. 37–44, 2015. [1931] S. Duan, H. Wang, L. Wang, T. Huang, and C. Li, “Impulsive effects and stability analysis on memristive neural networks with variable delays,” 2016. [1932] Z. Guo, J. Wang, and Z. Yan, “Global exponential dissipativity and stabilization of memristor-based recurrent neural networks with time- varying delays,” Neural Networks, vol. 48, pp. 158–172, 2013. [1933] J. Hu and J. Wang, “Global uniform asymptotic stability of memristor- based recurrent neural networks with time delays,” in Neural Networks (IJCNN), The 2010 International Joint Conference on. IEEE, 2010, pp. 1–8. [1934] P. Jiang, Z. Zeng, and J. Chen, “Almost periodic solutions for a memristor-based neural networks with leakage, time-varying and distributed delays,” Neural Networks, vol. 68, pp. 34–45, 2015. [1935] J. Li, M. Hu, and L. Guo, “Exponential stability of stochastic memristor-based recurrent neural networks with time-varying delays,” Neurocomputing, vol. 138, pp. 92–98, 2014. [1936] Q. Li, S. Tang, H. Zeng, and T. Zhou, “On hyperchaos in a small memristive neural network,” Nonlinear Dynamics, vol. 78, no. 2, pp. 1087–1099, 2014. [1937] N. Li, J. Cao, and M. Zhou, “Anti-synchronization control for memristor-based recurrent neural networks,” in Advances in Neural Networks–ISNN 2014. Springer, 2014, pp. 27–34. [1938] R. Li and J. Cao, “Stability analysis of reaction-diffusion uncertain memristive neural networks with time-varying delays and leakage term,” Applied Mathematics and Computation, vol. 278, pp. 54–69, 2016. [1939] K. Mathiyalagan, R. Anbuvithya, R. Sakthivel, J. H. Park, and P. Prakash, “Reliable stabilization for memristor-based recurrent neu- ral networks with time-varying delays,” Neurocomputing, vol. 153, pp. 140–147, 2015. [1940] Z. Meng and Z. Xiang, “Stability analysis of stochastic memristor- based recurrent neural networks with mixed time-varying delays,” Neural Computing and Applications, pp. 1–13, 2016. [1941] J. Qi, C. Li, and T. Huang, “Stability of delayed memristive neu- ral networks with time-varying impulses,” Cognitive neurodynamics, vol. 8, no. 5, pp. 429–436, 2014. [1942] R. Rakkiyappan, G. Velmurugan, F. A. Rihan, and S. Lakshmanan, “Stability analysis of memristor-based complex-valued recurrent neu- ral networks with time delays,” Complexity, 2014. [1943] R. Rakkiyappan, G. Velmurugan, and J. Cao, “Finite-time stability analysis of fractional-order complex-valued memristor-based neural networks with time delays,” Nonlinear Dynamics, vol. 78, no. 4, pp. 2823–2836, 2014. [1944] Z. Vasilkoski, H. Ames, B. Chandler, A. Gorchetchnikov, J. L´eveill´e, G. Livitz, E. Mingolla, and M. Versace, “Review of stability properties of neural plasticity rules for implementation on memristive neuromor- phic hardware,” in Neural Networks (IJCNN), The 2011 International Joint Conference on. IEEE, 2011, pp. 2563–2569. [1945] X. Wang, C. Li, T. Huang, and S. Duan, “Global exponential stability of a class of memristive neural networks with time-varying delays,” Neural Computing and Applications, vol. 24, no. 7-8, pp. 1707–1715, 2014. [1946] X. Wang, C. Li, and T. Huang, “Delay-dependent robust stability and stabilization of uncertain memristive delay neural networks,” Neurocomputing, vol. 140, pp. 155–161, 2014. [1947] W. Wang, L. Li, H. Peng, J. Xiao, and Y. Yang, “Synchronization con- trol of memristor-based recurrent neural networks with perturbations,” Neural Networks, vol. 53, pp. 8–14, 2014. [1948] X. Wang, C. Li, T. Huang, and S. Duan, “Global exponential stability of a class of memristive neural networks with time-varying delays,” Neural Computing and Applications, vol. 24, no. 7-8, pp. 1707–1715, 2014. [1949] L. Wang and Y. Shen, “Finite-time stabilizability and instabilizability of delayed memristive neural networks with nonlinear discontinuous controller,” 2015. [1950] F. Wang, Y. Yang, X. Xu, and L. Li, “Global asymptotic stability of impulsive fractional-order bam neural networks with time delay,” Neural Computing and Applications, pp. 1–8, 2015. [1951] Z. Wang, S. Ding, Z. Huang, and H. Zhang, “Exponential stability and stabilization of delayed memristive neural networks based on quadratic convex combination method,” 2015. [1952] H. Wang, S. Duan, T. Huang, L. Wang, and C. Li, “Exponential sta- bility of complex-valued memristive recurrent neural networks,” IEEE Transactions on Neural Networks and Learning Systems, vol. PP, no. 99, pp. 1–6, 2016. [1953] H. Wang, S. Duan, T. Huang, C. Li, and L. Wang, “Novel stability criteria for impulsive memristive neural networks with time-varying delays,” Circuits, Systems, and Signal Processing, pp. 1–22, 2016. [1954] S. Wen, Z. Zeng, and T. Huang, “Exponential stability analysis of memristor-based recurrent neural networks with time-varying delays,” Neurocomputing, vol. 97, pp. 233–240, 2012. [1955] S. Wen and Z. Zeng, “Dynamics analysis of a class of memristor- based recurrent networks with time-varying delays in the presence of strong external stimuli,” Neural processing letters, vol. 35, no. 1, pp. 47–59, 2012. [1956] S. Wen, Z. Zeng, and T. Huang, “Dynamic behaviors of memristor- based delayed recurrent networks,” Neural Computing and Applica- tions, vol. 23, no. 3-4, pp. 815–821, 2013. [1957] S. Wen, Z. Zeng, T. Huang, and X. Yu, “Noise cancellation of memristive neural networks,” Neural Networks, vol. 60, pp. 74–83, 2014. [1958] S. Wen, T. Huang, Z. Zeng, Y. Chen, and P. Li, “Circuit design and exponential stabilization of memristive neural networks,” Neural Networks, vol. 63, pp. 48–56, 2015. [1959] A. Wu, J. Zhang, and Z. Zeng, “Dynamic behaviors of a class of memristor-based hopﬁeld networks,” Physics Letters A, vol. 375, no. 15, pp. 1661–1665, 2011. [1960] A. Wu and Z. Zeng, “Dynamic behaviors of memristor-based recurrent neural networks with time-varying delays,” Neural Networks, vol. 36, pp. 1–10, 2012. [1961] ——, “Exponential stabilization of memristive neural networks with time delays,” Neural Networks and Learning Systems, IEEE Transac- tions on, vol. 23, no. 12, pp. 1919–1929, 2012. [1962] ——, “Dynamic behaviors of hybrid lotka-volterra recurrent neu- ral networks with memristor characteristics,” in Neural Networks (IJCNN), The 2012 International Joint Conference on. IEEE, 2012, pp. 1–8. [1963] H. Wu and L. Zhang, “Almost periodic solution for memristive neural networks with time-varying delays,” Journal of Applied Mathematics, vol. 2013, 2013. [1964] A. Wu and Z. Zeng, “Lagrange stability of memristive neural networks with discrete and distributed delays,” Neural Networks and Learning Systems, IEEE Transactions on, vol. 25, no. 4, pp. 690–703, 2014. [1965] A. Wu, Z. Zeng, and C. Fu, “Dynamic analysis of memristive neural system with unbounded time-varying delays,” Journal of the Franklin Institute, vol. 351, no. 5, pp. 3032–3041, 2014. [1966] A. Wu and Z. Zeng, “An improved criterion for stability and at- tractability of memristive neural networks with time-varying delays,” Neurocomputing, vol. 145, pp. 316–323, 2014. [1967] ——, “New global exponential stability results for a memristive neural system with time-varying delays,” Neurocomputing, vol. 144, pp. 553– 559, 2014. [1968] A. Wu and Z. Jin-E, “Multistability of memristive neural networks with time-varying delays,” Complexity, 2014. [1969] A. Wu and Z. Zeng, “Input-to-state stability of memristive neural system with time delays,” Circuits, Systems, and Signal Processing, vol. 33, no. 3, pp. 681–698, 2014. [1970] ——, “Improved conditions for global exponential stability of a general class of memristive neural networks,” Communications in Nonlinear Science and Numerical Simulation, vol. 20, no. 3, pp. 975– 985, 2015. 71 [1971] ——, “Algebraical criteria of stability for delayed memristive neural networks,” Advances in Difference Equations, vol. 2015, no. 1, pp. 1–12, 2015. [1972] ——, “Global mittag–lefﬂer stabilization of fractional-order mem- ristive neural networks,” IEEE transactions on neural networks and learning systems, vol. 28, no. 1, pp. 206–217, 2017. [1973] Y. Xin, Y. Li, Z. Cheng, and X. Huang, “Global exponential stability for switched memristive neural networks with time-varying delays,” Neural Networks, vol. 80, pp. 34–42, 2016. [1974] C. Yang, K. Zhong, S. Zhu, and Y. Shen, “Algebraic conditions for synchronization stability of memristive neural networks,” in Control Conference (CCC), 2014 33rd Chinese. IEEE, 2014, pp. 5055–5058. [1975] D. Yang, G. Qiu, and C. Li, “Global exponential stability of mem- ristive neural networks with impulse time window and time-varying delays,” Neurocomputing, 2015. [1976] S. Yang, C. Li, and T. Huang, “Exponential stabilization and syn- chronization for fuzzy model of memristive neural networks by periodically intermittent control,” Neural Networks, vol. 75, pp. 162– 172, 2016. [1977] G. Zhang, Y. Shen, and J. Sun, “Global exponential stability of a class of memristor-based recurrent neural networks with time-varying delays,” Neurocomputing, vol. 97, pp. 149–154, 2012. [1978] G. Zhang, Y. Shen, Q. Yin, and J. Sun, “Global exponential periodicity and stability of a class of memristor-based recurrent neural networks with multiple delays,” Information Sciences, vol. 232, pp. 386–396, 2013. [1979] G. Zhang and Y. Shen, “Exponential stabilization of memristor-based chaotic neural networks with time-varying delays via intermittent control,” 2014. [1980] G. Zhang, Y. Shen, and C. Xu, “Global exponential stability in a lagrange sense for memristive recurrent neural networks with time- varying delays,” Neurocomputing, vol. 149, pp. 1330–1336, 2015. [1981] W. Zhang, C. Li, T. Huang, and J. Huang, “Stability and synchroniza- tion of memristor-based coupling neural networks with time-varying delays via intermittent control,” Neurocomputing, vol. 173, pp. 1066– 1072, 2016. [1982] K. Zhong, Q. Yang, and S. Zhu, “New algebraic conditions for iss of memristive neural networks with variable delays,” Neural Computing and Applications, pp. 1–9, 2016. [1983] A. Abdurahman, H. Jiang, and Z. Teng, “Finite-time synchronization for memristor-based neural networks with time-varying delays,” Neu- ral Networks, 2015. [1984] A. Abdurahman, H. Jiang, and K. Rahman, “Function projective synchronization of memristor-based cohen–grossberg neural networks with time-varying delays,” Cognitive Neurodynamics, pp. 1–11, 2015. [1985] R. Anbuvithya, K. Mathiyalagan, R. Sakthivel, and P. Prakash, “Non- fragile synchronization of memristive bam networks with random feedback gain ﬂuctuations,” Communications in Nonlinear Science and Numerical Simulation, vol. 29, no. 1, pp. 427–440, 2015. [1986] A. Ascoli, R. Tetzlaff, V. Lanza, F. Corinto, and M. Gilli, “Memristor plasticity enables emergence of synchronization in neuromorphic networks,” in Circuits and Systems (ISCAS), 2014 IEEE International Symposium on. IEEE, 2014, pp. 2261–2264. [1987] A. Ascoli, V. Lanza, F. Corinto, and R. Tetzlaff, “Synchronization conditions in simple memristor neural networks,” Journal of the Franklin Institute, vol. 352, no. 8, pp. 3196–3220, 2015. [1988] H.-B. Bao and J.-D. Cao, “Projective synchronization of fractional- order memristor-based neural networks,” Neural Networks, vol. 63, pp. 1–9, 2015. [1989] H. Bao, J. H. Park, and J. Cao, “Adaptive synchronization of fractional-order memristor-based neural networks with time delay,” Nonlinear Dynamics, pp. 1–12, 2015. [1990] ——, “Matrix measure strategies for exponential synchronization and anti-synchronization of memristor-based neural networks with time- varying delays,” Applied Mathematics and Computation, vol. 270, pp. 543–556, 2015. [1991] H. Bao and J. H. Park, “Finite-time synchronization of memristor- based neural networks,” in Control and Decision Conference (CCDC), 2015 27th Chinese. IEEE, 2015, pp. 1732–1735. [1992] H. Bao, J. H. Park, and J. Cao, “Exponential synchronization of cou- pled stochastic memristor-based neural networks with time-varying probabilistic delay coupling and impulsive delay,” Neural Networks and Learning Systems, IEEE Transactions on, vol. 27, no. 1, pp. 190– 201, 2016. [1993] A. Chandrasekar, R. Rakkiyappan, J. Cao, and S. Lakshmanan, “Synchronization of memristor-based recurrent neural networks with two delay components based on second-order reciprocally convex approach,” Neural Networks, vol. 57, pp. 79–93, 2014. [1994] F. Corinto, A. Ascoli, V. Lanza, and M. Gilli, “Memristor synaptic dynamics’ inﬂuence on synchronous behavior of two hindmarsh-rose neurons,” in Neural Networks (IJCNN), The 2011 International Joint Conference on. IEEE, 2011, pp. 2403–2408. [1995] S. Ding and Z. Wang, “Stochastic exponential synchronization control of memristive neural networks with multiple time-varying delays,” Neurocomputing, vol. 162, pp. 16–25, 2015. [1996] ——, “Lag quasi-synchronization for memristive neural networks with switching jumps mismatch,” Neural Computing and Applica- tions, pp. 1–12, 2016. [1997] S. Ding, Z. Wang, Z. Huang, and H. Zhang, “Novel switching jumps dependent exponential synchronization criteria for memristor-based neural networks,” Neural Processing Letters, pp. 1–14, 2016. [1998] Z. Guo, S. Yang, and J. Wang, “Global exponential synchronization of multiple memristive neural networks with time delay via nonlinear coupling,” 2014. [1999] Z. Guo, J. Wang, and Z. Yan, “Global exponential synchronization of two memristor-based recurrent neural networks with time delays via static or dynamic coupling,” Systems, Man, and Cybernetics: Systems, IEEE Transactions on, vol. 45, no. 2, pp. 235–249, 2015. [2000] Z. Guo, S. Yang, and J. Wang, “Global synchronization of stochasti- cally disturbed memristive neurodynamics via discontinuous control laws,” IEEE/CAA Journal of Automatica Sinica, vol. 3, no. 2, pp. 121–131, 2016. [2001] X. Han, H. Wu, and B. Fang, “Adaptive exponential synchronization of memristive neural networks with mixed time-varying delays,” Neurocomputing, 2016. [2002] A. Heittmann and T. G. Noll, “Sensitivity of neuromorphic circuits using nanoelectronic resistive switches to pulse synchronization,” in Proceedings of the 21st edition of the great lakes symposium on Great lakes symposium on VLSI. ACM, 2011, pp. 375–378. [2003] M. Jiang, S. Wang, J. Mei, and Y. Shen, “Finite-time synchronization control of a class of memristor-based recurrent neural networks,” Neural Networks, vol. 63, pp. 133–140, 2015. [2004] M. Jiang, J. Mei, and J. Hu, “New results on exponential synchroniza- tion of memristor-based chaotic neural networks,” Neurocomputing, vol. 156, pp. 60–67, 2015. [2005] N. Li and J. Cao, “New synchronization criteria for memristor-based networks: Adaptive control and feedback control schemes,” Neural Networks, vol. 61, pp. 1–9, 2015. [2006] H. Liu, Z. Wang, B. Shen, and X. Kan, “Synchronization for discrete- time memristive recurrent neural networks with time-delays,” in Control Conference (CCC), 2015 34th Chinese. IEEE, 2015, pp. 3478–3483. [2007] M. Liu, H. Jiang, and C. Hu, “Finite-time synchronization of memristor-based cohen–grossberg neural networks with time-varying delays,” Neurocomputing, vol. 194, pp. 1–9, 2016. [2008] K. Mathiyalagan, J. H. Park, and R. Sakthivel, “Synchronization for delayed memristive bam neural networks using impulsive control with random nonlinearities,” Applied Mathematics and Computation, vol. 259, pp. 967–979, 2015. [2009] K. Mathiyalagan, R. Anbuvithya, R. Sakthivel, J. H. Park, and P. Prakash, “Non-fragile h? synchronization of memristor-based neural networks using passivity theory,” Neural Networks, vol. 74, pp. 85– 100, 2016. [2010] R. Sakthivel, R. Anbuvithya, K. Mathiyalagan, Y.-K. Ma, and P. Prakash, “Reliable anti-synchronization conditions for bam memris- tive neural networks with different memductance functions,” Applied Mathematics and Computation, vol. 275, pp. 213–228, 2016. [2011] Y. Shi and P. Zhu, “Synchronization of memristive competitive neural networks with different time scales,” Neural Computing and Applications, vol. 25, no. 5, pp. 1163–1168, 2014. [2012] Y. Song and S. Wen, “Synchronization control of stochastic memristor-based neural networks with mixed delays,” Neurocomput- ing, vol. 156, pp. 121–128, 2015. [2013] G. Velmurugan and R. Rakkiyappan, “Hybrid projective synchroniza- tion of fractional-order memristor-based neural networks with time delays,” Nonlinear Dynamics, pp. 1–14, 2015. [2014] G. Velmurugan, R. Rakkiyappan, and J. Cao, “Finite-time synchro- nization of fractional-order memristor-based neural networks with time delays,” Neural Networks, vol. 73, pp. 36–46, 2016. [2015] C. K. Volos, I. Kyprianidis, and I. Stouboulos, “The memristor as an electric synapse-synchronization phenomena,” in Digital Signal Processing (DSP), 2011 17th International Conference on. IEEE, 2011, pp. 1–6. 72 [2016] C. K. Volos, I. Kyprianidis, I. Stouboulos, E. Tlelo-Cuautle, and S. Vaidyanathan, “Memristor: A new concept in synchronization of coupled neuromorphic circuits,” Journal of Engineering Science and Technology Review, vol. 8, no. 2, pp. 157–173, 2015. [2017] Y. Wan and J. Cao, “Periodicity and synchronization of coupled memristive neural networks with supremums,” Neurocomputing, vol. 159, pp. 137–143, 2015. [2018] G. Wang and Y. Shen, “Exponential synchronization of coupled memristive neural networks with time delays,” Neural Computing and Applications, vol. 24, no. 6, pp. 1421–1430, 2014. [2019] L. Wang, Y. Shen, Q. Yin, and G. Zhang, “Adaptive synchronization of memristor-based neural networks with time-varying delays,” 2014. [2020] W. Wang, L. Li, H. Peng, J. Kurths, J. Xiao, and Y. Yang, “Finite- time anti-synchronization control of memristive neural networks with stochastic perturbations,” Neural Processing Letters, pp. 1–15, 2014. [2021] L. Wang and Y. Shen, “Design of controller on synchronization of memristor-based neural networks with time-varying delays,” Neuro- computing, vol. 147, pp. 372–379, 2015. [2022] X. Wang, C. Li, T. Huang, and L. Chen, “Dual-stage impulsive control for synchronization of memristive chaotic neural networks with discrete and continuously distributed delays,” Neurocomputing, vol. 149, pp. 621–628, 2015. [2023] W. Wang, L. Li, H. Peng, J. Kurths, J. Xiao, and Y. Yang, “Anti- synchronization control of memristive neural networks with multiple proportional delays,” Neural Processing Letters, pp. 1–15, 2015. [2024] ——, “Anti-synchronization control of memristive neural networks with multiple proportional delays,” Neural Processing Letters, vol. 43, no. 1, pp. 269–283, 2016. [2025] ——, “Finite-time anti-synchronization control of memristive neural networks with stochastic perturbations,” Neural Processing Letters, vol. 43, no. 1, pp. 49–63, 2016. [2026] S. Wen, G. Bao, Z. Zeng, Y. Chen, and T. Huang, “Global exponential synchronization of memristor-based recurrent neural networks with time-varying delays,” Neural Networks, vol. 48, pp. 195–203, 2013. [2027] S. Wen, Z. Zeng, T. Huang, and Y. Zhang, “Exponential adaptive lag synchronization of memristive neural networks via fuzzy method and applications in pseudorandom number generators,” Fuzzy Systems, IEEE Transactions on, vol. 22, no. 6, pp. 1704–1713, 2014. [2028] A. Wu, Z. Zeng, X. Zhu, and J. Zhang, “Exponential synchronization of memristor-based recurrent neural networks with time delays,” Neurocomputing, vol. 74, no. 17, pp. 3043–3050, 2011. [2029] A. Wu, S. Wen, and Z. Zeng, “Synchronization control of a class of memristor-based recurrent neural networks,” Information Sciences, vol. 183, no. 1, pp. 106–116, 2012. [2030] A. Wu and Z. Zeng, “Anti-synchronization control of a class of memristive recurrent neural networks,” Communications in Nonlinear Science and Numerical Simulation, vol. 18, no. 2, pp. 373–385, 2013. [2031] H. Wu, L. Zhang, S. Ding, X. Guo, and L. Wang, “Complete periodic synchronization of memristor-based neural networks with time-varying delays,” Discrete Dynamics in Nature and Society, vol. 2013, 2013. [2032] H. Wu, R. Li, X. Zhang, and R. Yao, “Adaptive ﬁnite-time complete periodic synchronization of memristive neural networks with time delays,” Neural Processing Letters, pp. 1–21, 2014. [2033] H. Wu, R. Li, R. Yao, and X. Zhang, “Weak, modiﬁed and function projective synchronization of chaotic memristive neural networks with time delays,” Neurocomputing, vol. 149, pp. 667–676, 2015. [2034] H. Wu, X. Zhang, R. Li, and R. Yao, “Adaptive anti-synchronization and h? anti-synchronization for memristive neural networks with mixed time delays and reaction–diffusion terms,” Neurocomputing, vol. 168, pp. 726–740, 2015. [2035] Z. Yan, S. Bi, and X. Xue, “Global exponential anti-synchronization of coupled memristive chaotic neural networks with time-varying delays,” in Advances in Neural Networks–ISNN 2015. Springer, 2015, pp. 192–201. [2036] X. Yang, J. Cao, and W. Yu, “Exponential synchronization of memris- tive cohen–grossberg neural networks with mixed delays,” Cognitive neurodynamics, vol. 8, no. 3, pp. 239–249, 2014. [2037] S. Yang, Z. Guo, and J. Wang, “Robust synchronization of multiple memristive neural networks with uncertain parameters via nonlinear coupling,” Systems, Man, and Cybernetics: Systems, IEEE Transac- tions on, vol. 45, no. 7, pp. 1077–1086, 2015. [2038] X. Yang, J. Cao, and J. Qiu, “pth moment exponential stochastic syn- chronization of coupled memristor-based neural networks with mixed delays via delayed impulsive control,” Neural Networks, vol. 65, pp. 80–91, 2015. [2039] X. Yang, J. Cao, and J. Liang, “Exponential synchronization of memristive neural networks with delays: Interval matrix method,” 2016. [2040] G. Zhang, Y. Shen, and L. Wang, “Global anti-synchronization of a class of chaotic memristive neural networks with time-varying delays,” Neural networks, vol. 46, pp. 1–8, 2013. [2041] G. Zhang and Y. Shen, “New algebraic criteria for synchronization stability of chaotic memristive neural networks with time-varying delays,” Neural Networks and Learning Systems, IEEE Transactions on, vol. 24, no. 10, pp. 1701–1707, 2013. [2042] ——, “Exponential synchronization of delayed memristor-based chaotic neural networks via periodically intermittent control,” Neural Networks, vol. 55, pp. 1–10, 2014. [2043] G. Zhang, J. Hu, and Y. Shen, “Exponential lag synchronization for delayed memristive recurrent neural networks,” Neurocomputing, vol. 154, pp. 86–93, 2015. [2044] ——, “New results on synchronization control of delayed memristive neural networks,” Nonlinear Dynamics, pp. 1–12, 2015. [2045] W. Zhang, C. Li, T. Huang, and X. He, “Synchronization of memristor-based coupling recurrent neural networks with time- varying delays and impulses,” 2015. [2046] H. Zhao, L. Li, H. Peng, J. Kurths, J. Xiao, and Y. Yang, “Anti- synchronization for stochastic memristor-based neural networks with non-modeled dynamics via adaptive control approach,” The European Physical Journal B, vol. 88, no. 5, pp. 1–10, 2015. [2047] M. S. Ali, R. Saravanakumar, and J. Cao, “New passivity criteria for memristor-based neutral-type stochastic bam neural networks with mixed time-varying delays,” Neurocomputing, 2015. [2048] R. Anbuvithya, K. Mathiyalagan, R. Sakthivel, and P. Prakash, “Passivity of memristor-based bam neural networks with different memductance and uncertain delays,” Cognitive Neurodynamics, pp. 1–13, 2016. [2049] L. Duan and L. Huang, “Periodicity and dissipativity for memristor- based mixed time-varying delayed neural networks via differential inclusions,” Neural Networks, vol. 57, pp. 12–22, 2014. [2050] Z. Guo, J. Wang, and Z. Yan, “Passivity and passiﬁcation of memristor-based recurrent neural networks with time-varying delays,” Neural Networks and Learning Systems, IEEE Transactions on, vol. 25, no. 11, pp. 2099–2109, 2014. [2051] X. Li, R. Rakkiyappan, and G. Velmurugan, “Dissipativity analysis of memristor-based complex-valued neural networks with time-varying delays,” Information Sciences, vol. 294, pp. 645–665, 2015. [2052] R. Li, J. Cao, and Z. Tu, “Passivity analysis of memristive neural networks with probabilistic time-varying delays,” Neurocomputing, vol. 191, pp. 249–262, 2016. [2053] Z. Meng and Z. Xiang, “Passivity analysis of memristor-based recur- rent neural networks with mixed time-varying delays,” Neurocomput- ing, 2015. [2054] R. Rakkiyappan, A. Chandrasekar, and J. Cao, “Passivity and passi- ﬁcation of memristor-based recurrent neural networks with additive time-varying delays,” IEEE Trans. Neural Netw. Learn. Syst, vol. 10, 2014. [2055] R. Rakkiyappan, A. Chandrasekar, S. Laksmanan, and J. H. Park, “State estimation of memristor-based recurrent neural networks with time-varying delays based on passivity theory,” Complexity, vol. 19, no. 4, pp. 32–43, 2014. [2056] R. Rakkiyappan, G. Velmurugan, X. Li, and D. O?Regan, “Global dissipativity of memristor-based complex-valued neural networks with time-varying delays,” Neural Computing and Applications, pp. 1–21, 2015. [2057] G. Velmurugan, R. Rakkiyappan, and S. Lakshmanan, “Passivity analysis of memristor-based complex-valued neural networks with time-varying delays,” Neural Processing Letters, pp. 1–24, 2014. [2058] L. Wang and Y. Shen, “New results on passivity analysis of memristor- based neural networks with time-varying delays,” Neurocomputing, vol. 144, pp. 208–214, 2014. [2059] S. Wen, Z. Zeng, T. Huang, and Y. Chen, “Passivity analysis of memristor-based recurrent neural networks with time-varying delays,” Journal of the Franklin Institute, vol. 350, no. 8, pp. 2354–2370, 2013. [2060] A. Wu and Z. Zeng, “Passivity analysis of memristive neural networks with different memductance functions,” Communications in Nonlinear Science and Numerical Simulation, vol. 19, no. 1, pp. 274–285, 2014. [2061] ——, “Exponential passivity of memristive neural networks with time delays,” Neural Networks, vol. 49, pp. 11–18, 2014. [2062] J. Xiao, S. Zhong, and Y. Li, “Improved passivity criteria for mem- ristive neural networks with interval multiple time-varying delays,” Neurocomputing, 2015. 73 [2063] G. Zhang, Y. Shen, Q. Yin, and J. Sun, “Passivity analysis for memristor-based recurrent neural networks with discrete and dis- tributed delays,” Neural Networks, vol. 61, pp. 49–58, 2015. [2064] G. W. Burr, R. M. Shelby, A. Sebastian, S. Kim, S. Kim, S. Sidler, K. Virwani, M. Ishii, P. Narayanan, A. Fumarola et al., “Neuromor- phic computing using non-volatile memory,” Advances in Physics: X, vol. 2, no. 1, pp. 89–124, 2017. [2065] O. Bichler, D. Roclin, C. Gamrat, and D. Querlioz, “Design ex- ploration methodology for memristor-based spiking neuromorphic architectures with the xnet event-driven simulator,” in Nanoscale Ar- chitectures (NANOARCH), 2013 IEEE/ACM International Symposium on. IEEE, 2013, pp. 7–12. [2066] F. Clermidy, R. Heliot, A. Valentian, C. Gamrat, O. Bichler, M. Du- ranton, B. Blehadj, and O. Temam, “Advanced technologies for brain- inspired computing,” in Design Automation Conference (ASP-DAC), 2014 19th Asia and South Paciﬁc. IEEE, 2014, pp. 563–569. [2067] B. DeSalvo, E. Vianello, D. Garbin, O. Bichler, and L. Perniola, “From memory in our brain to emerging resistive memories in neuromorphic systems,” in Memory Workshop (IMW), 2015 IEEE International. IEEE, 2015, pp. 1–4. [2068] D. Roclin, O. Bichler, C. Gamrat, and J.-O. Klein, “Sneak paths ef- fects in cbram memristive devices arrays for spiking neural networks,” in Proceedings of the 2014 IEEE/ACM International Symposium on Nanoscale Architectures. ACM, 2014, pp. 13–18. [2069] S. Yu and H.-S. Wong, “Modeling the switching dynamics of programmable-metallization-cell (pmc) memory and its application as synapse device for a neuromorphic computation system,” in Electron Devices Meeting (IEDM), 2010 IEEE International. IEEE, 2010, pp. 22–1. [2070] J. W. Jang, B. Attarimashalkoubeh, A. Prakash, H. Hwang, and Y. H. Jeong, “Scalable neuron circuit using conductive-bridge ram for pattern reconstructions,” IEEE Transactions on Electron Devices, vol. PP, no. 99, pp. 1–4, 2016. [2071] M. Aono and T. Hasegawa, “The atomic switch,” Proceedings of the IEEE, vol. 98, no. 12, pp. 2228–2236, 2010. [2072] A. V. Avizienis, H. O. Sillin, C. Martin-Olmos, H. H. Shieh, M. Aono, A. Z. Stieg, and J. K. Gimzewski, “Neuromorphic atomic switch networks,” PloS one, vol. 7, no. 8, p. e42772, 2012. [2073] T. Hasegawa, T. Ohno, K. Terabe, T. Tsuruoka, T. Nakayama, J. K. Gimzewski, and M. Aono, “Learning abilities achieved by a single solid-state atomic switch,” Advanced materials, vol. 22, no. 16, pp. 1831–1834, 2010. [2074] A. Nayak, T. Ohno, T. Tsuruoka, K. Terabe, T. Hasegawa, J. K. Gimzewski, and M. Aono, “Controlling the synaptic plasticity of a cu2s gap-type atomic switch,” Advanced Functional Materials, vol. 22, no. 17, pp. 3606–3613, 2012. [2075] H. O. Sillin, R. Aguilera, H.-H. Shieh, A. V. Avizienis, M. Aono, A. Z. Stieg, and J. K. Gimzewski, “A theoretical and experimental study of neuromorphic atomic switch networks for reservoir computing,” Nanotechnology, vol. 24, no. 38, p. 384004, 2013. [2076] A. Z. Stieg, A. V. Avizienis, H. O. Sillin, R. Aguilera, H.-H. Shieh, C. Martin-Olmos, E. J. Sandouk, M. Aono, and J. K. Gimzewski, “Self-organization and emergence of dynamical structures in neuro- morphic atomic switch networks,” in Memristor Networks. Springer, 2014, pp. 173–209. [2077] T. Tsuruoka, T. Hasegawa, K. Terabe, and M. Aono, “Conductance quantization and synaptic behavior in a ta2o5-based atomic switch,” Nanotechnology, vol. 23, no. 43, p. 435705, 2012. [2078] T. Tsuruoka, T. Hasegawa, and M. Aono, “Synaptic plasticity and memristive behavior operated by atomic switches,” in Cellular Nanoscale Networks and their Applications (CNNA), 2014 14th International Workshop on. IEEE, 2014, pp. 1–2. [2079] R. Yang, K. Terabe, Y. Yao, T. Tsuruoka, T. Hasegawa, J. K. Gimzewski, and M. Aono, “Synaptic plasticity and memory functions achieved in a wo3- x-based nanoionics device by using the principle of atomic switch operation,” Nanotechnology, vol. 24, no. 38, p. 384003, 2013. [2080] S. Ambrogio, N. Ciocchini, M. Laudato, V. Milo, A. Pirovano, P. Fantini, and D. Ielmini, “Unsupervised learning by spike timing dependent plasticity in phase change memory (pcm) synapses,” Fron- tiers in Neuroscience, vol. 10, 2016. [2081] S. B. Eryilmaz, D. Kuzum, R. Jeyasingh, S. Kim, M. BrightSky, C. Lam, and H.-S. P. Wong, “Brain-like associative learning using a nanoscale non-volatile phase change synaptic device array,” Frontiers in neuroscience, vol. 8, 2014. [2082] D. Garbin, M. Suri, O. Bichler, D. Querlioz, C. Gamrat, and B. De- Salvo, “Probabilistic neuromorphic system using binary phase-change memory (pcm) synapses: Detailed power consumption analysis,” in Nanotechnology (IEEE-NANO), 2013 13th IEEE Conference on. IEEE, 2013, pp. 91–94. [2083] B. L. Jackson, B. Rajendran, G. S. Corrado, M. Breitwisch, G. W. Burr, R. Cheek, K. Gopalakrishnan, S. Raoux, C. T. Rettner, A. Padilla et al., “Nanoscale electronic synapses using phase change devices,” ACM Journal on Emerging Technologies in Computing Systems (JETC), vol. 9, no. 2, p. 12, 2013. [2084] D.-H. Kang, H.-G. Jun, K.-C. Ryoo, H. Jeong, and H. Sohn, “Emula- tion of spike-timing dependent plasticity in nano-scale phase change memory,” Neurocomputing, vol. 155, pp. 153–158, 2015. [2085] S. Kim, M. Ishii, S. Lewis, T. Perri, M. BrightSky, W. Kim, R. Jordan, G. Burr, N. Sosa, A. Ray et al., “Nvm neuromorphic core with 64k-cell (256-by-256) phase change memory synaptic array with on- chip neuron circuits for continuous in-situ learning,” in 2015 IEEE International Electron Devices Meeting (IEDM). IEEE, 2015, pp. 17–1. [2086] R. M. Shelby, G. W. Burr, I. Boybat, and C. di Nolfo, “Non-volatile memory as hardware synapse in neuromorphic computing: A ﬁrst look at reliability issues,” in Reliability Physics Symposium (IRPS), 2015 IEEE International. IEEE, 2015, pp. 6A–1. [2087] M. Suri, O. Bichler, D. Querlioz, O. Cueto, L. Perniola, V. Sousa, D. Vuillaume, C. Gamrat, and B. DeSalvo, “Phase change memory as synapse for ultra-dense neuromorphic systems: Application to com- plex visual pattern extraction,” in Electron Devices Meeting (IEDM), 2011 IEEE International. IEEE, 2011, pp. 4–4. [2088] M. Suri, O. Bichler, D. Querlioz, B. Traor´e, O. Cueto, L. Perniola, V. Sousa, D. Vuillaume, C. Gamrat, and B. DeSalvo, “Physical aspects of low power synapses based on phase change memory devices,” Journal of Applied Physics, vol. 112, no. 5, p. 054904, 2012. [2089] M. Suri, D. Garbin, O. Bichler, D. Querlioz, D. Vuillaume, C. Gamrat, and B. DeSalvo, “Impact of pcm resistance-drift in neuromorphic systems and drift-mitigation strategy,” in Nanoscale Architectures (NANOARCH), 2013 IEEE/ACM International Symposium on. IEEE, 2013, pp. 140–145. [2090] M. Suri, O. Bichler, Q. Hubert, L. Perniola, V. Sousa, C. Jahan, D. Vuillaume, C. Gamrat, and B. DeSalvo, “Addition of hfo 2 interface layer for improved synaptic performance of phase change memory (pcm) devices,” Solid-State Electronics, vol. 79, pp. 227–232, 2013. [2091] L. Wang, S.-D. Gong, C.-H. Yang, Y.-X. Peng, and S. Gai, “Asso- ciative memory function found on phase-change nanoscale device,” Current Nanoscience, vol. 11, no. 6, pp. 797–804, 2015. [2092] Y. Zhong, Y. Li, L. Xu, and X. Miao, “Simple square pulses for implementing spike-timing-dependent plasticity in phase-change memory,” physica status solidi (RRL)-Rapid Research Letters, vol. 9, no. 7, pp. 414–419, 2015. [2093] S. B. Eryilmaz, D. Kuzum, R. G. Jeyasingh, S. Kim, M. BrightSky, C. Lam, and H.-S. P. Wong, “Experimental demonstration of array- level learning with phase change synaptic devices,” in Electron Devices Meeting (IEDM), 2013 IEEE International. IEEE, 2013, pp. 25–5. [2094] O. Bichler, M. Suri, D. Querlioz, D. Vuillaume, B. DeSalvo, and C. Gamrat, “Visual pattern extraction using energy-efﬁcient ?2-pcm synapse? neuromorphic architecture,” Electron Devices, IEEE Trans- actions on, vol. 59, no. 8, pp. 2206–2214, 2012. [2095] D. Kuzum, R. G. Jeyasingh, B. Lee, and H.-S. P. Wong, “Nanoelec- tronic programmable synapses based on phase change materials for brain-inspired computing,” Nano letters, vol. 12, no. 5, pp. 2179– 2186, 2011. [2096] D. Kuzum, R. G. Jeyasingh, and H.-S. Wong, “Energy efﬁcient programming of nanoelectronic synaptic devices for large-scale imple- mentation of associative and temporal sequence learning,” in Electron Devices Meeting (IEDM), 2011 IEEE International. IEEE, 2011, pp. 30–3. [2097] D. Kuzum, R. G. D. Jeyasingh, S. Yu, and H.-S. Wong, “Low-energy robust neuromorphic computation using synaptic devices,” Electron Devices, IEEE Transactions on, vol. 59, no. 12, pp. 3489–3494, 2012. [2098] T. H. Lee, D. Loke, K.-J. Huang, W.-J. Wang, and S. R. Elliott, “Tai- loring transient-amorphous states: Towards fast and power-efﬁcient phase-change memory and neuromorphic computing,” Advanced Ma- terials, vol. 26, no. 44, pp. 7493–7498, 2014. [2099] J. M. Skelton, D. Loke, T. Lee, and S. R. Elliott, “Ab initio molecular- dynamics simulation of neuromorphic computing in phase-change memory materials,” ACS applied materials & interfaces, vol. 7, no. 26, pp. 14 223–14 230, 2015. [2100] G. Burr, R. Shelby, C. di Nolfo, J. Jang, R. Shenoy, P. Narayanan, K. Virwani, E. Giacometti, B. Kurdi, and H. Hwang, “Experimen- 74 tal demonstration and tolerancing of a large-scale neural network (165,000 synapses), using phase-change memory as the synaptic weight element,” in Electron Devices Meeting (IEDM), 2014 IEEE International. IEEE, 2014, pp. 29–5. [2101] G. W. Burr, R. M. Shelby, S. Sidler, C. Di Nolfo, J. Jang, I. Boybat, R. S. Shenoy, P. Narayanan, K. Virwani, E. U. Giacometti et al., “Experimental demonstration and tolerancing of a large-scale neural network (165 000 synapses) using phase-change memory as the synaptic weight element,” Electron Devices, IEEE Transactions on, vol. 62, no. 11, pp. 3498–3507, 2015. [2102] G. Burr, P. Narayanan, R. Shelby, S. Sidler, I. Boybat, C. di Nolfo, and Y. Leblebici, “Large-scale neural networks implemented with non-volatile memory as the synaptic weight element: Comparative performance analysis (accuracy, speed, and power),” in Electron Devices Meeting (IEDM), 2015 IEEE International. IEEE, 2015, pp. 4–4. [2103] S. Sidler, I. Boybat, R. M. Shelby, P. Narayanan, J. Jang, A. Fu- marola, K. Moon, Y. Leblebici, H. Hwang, and G. W. Burr, “Large- scale neural networks implemented with non-volatile memory as the synaptic weight element: impact of conductance response,” in Solid- State Device Research Conference (ESSDERC), 2016 46th European. IEEE, 2016, pp. 440–443. [2104] T. Tuma, M. Le Gallo, A. Sebastian, and E. Eleftheriou, “Detecting correlations using phase-change neurons and synapses,” IEEE Elec- tron Device Letters, vol. 37, no. 9, pp. 1238–1241, 2016. [2105] T. Tuma, A. Pantazi, M. Le Gallo, A. Sebastian, and E. Eleftheriou, “Stochastic phase-change neurons,” Nature nanotechnology, vol. 11, no. 8, pp. 693–699, 2016. [2106] C. D. Wright, P. Hosseini, and J. A. V. Diosdado, “Beyond von- neumann computing with nanoscale phase-change memory devices,” Advanced Functional Materials, vol. 23, no. 18, pp. 2248–2254, 2013. [2107] K. Roy, M. Sharad, D. Fan, and K. Yogendra, “Brain-inspired com- puting with spin torque devices,” in Design, Automation and Test in Europe Conference and Exhibition (DATE), 2014. IEEE, 2014, pp. 1–6. [2108] N. Locatelli, A. Mizrahi, A. Accioly, D. Querlioz, J.-V. Kim, V. Cros, and J. Grollier, “Spin torque nanodevices for bio-inspired computing,” in Cellular Nanoscale Networks and their Applications (CNNA), 2014 14th International Workshop on. IEEE, 2014, pp. 1–2. [2109] J. Grollier, D. Querlioz, and M. D. Stiles, “Spintronic nanodevices for bioinspired computing,” Proceedings of the IEEE, vol. 104, no. 10, pp. 2024–2039, 2016. [2110] K. Roy, D. Fan, X. Fong, Y. Kim, M. Sharad, S. Paul, S. Chatterjee, S. Bhunia, and S. Mukhopadhyay, “Exploring spin transfer torque devices for unconventional computing,” Emerging and Selected Topics in Circuits and Systems, IEEE Journal on, vol. 5, no. 1, pp. 5–16, 2015. [2111] A. Sengupta, P. Panda, A. Raghunathan, and K. Roy, “Neuromorphic computing enabled by spin-transfer torque devices,” in 2016 29th International Conference on VLSI Design and 2016 15th International Conference on Embedded Systems (VLSID). IEEE, 2016, pp. 32–37. [2112] C. M. Liyanagedera, K. Yogendra, K. Roy, and D. Fan, “Spin torque nano-oscillator based oscillatory neural network,” in Neural Networks (IJCNN), 2016 International Joint Conference on. IEEE, 2016, pp. 1387–1394. [2113] K. Nakada and K. Miura, “Pulse-coupled spin torque nano oscillators with dynamic synapses for neuromorphic computing,” in Nanotech- nology (IEEE-NANO), 2016 IEEE 16th International Conference on. IEEE, 2016, pp. 397–400. [2114] D. Fan, Y. Shim, A. Raghunathan, and K. Roy, “Stt-snn: A spin- transfer-torque based soft-limiting non-linear neuron for low-power artiﬁcial neural networks,” Nanotechnology, IEEE Transactions on, vol. 14, no. 6, pp. 1013–1023, 2015. [2115] A. Sengupta, S. H. Choday, Y. Kim, and K. Roy, “Spin orbit torque based electronic neuron,” Applied Physics Letters, vol. 106, no. 14, p. 143701, 2015. [2116] A. Sengupta, P. Panda, P. Wijesinghe, Y. Kim, and K. Roy, “Magnetic tunnel junction mimics stochastic cortical spiking neurons,” Scientiﬁc reports, vol. 6, 2016. [2117] A. Sengupta, M. Parsa, B. Han, and K. Roy, “Probabilistic deep spiking neural systems enabled by magnetic tunnel junction,” IEEE Transactions on Electron Devices, vol. 63, no. 7, pp. 2963–2970, 2016. [2118] M. Sharad, C. Augustine, G. Panagopoulos, and K. Roy, “Ultra low energy analog image processing using spin based neurons,” in Nanoscale Architectures (NANOARCH), 2012 IEEE/ACM Interna- tional Symposium on. IEEE, 2012, pp. 211–217. [2119] A. Sengupta, Z. Al Azim, X. Fong, and K. Roy, “Spin-orbit torque induced spike-timing dependent plasticity,” Applied Physics Letters, vol. 106, no. 9, p. 093704, 2015. [2120] A. Sengupta and K. Roy, “Short-term plasticity and long-term po- tentiation in magnetic tunnel junctions: Towards volatile synapses,” Physical Review Applied, vol. 5, no. 2, p. 024012, 2016. [2121] M. Adachi, M. Seki, H. Yamahara, H. Nasu, and H. Tabata, “Long-term potentiation of magnonic synapses by photocontrolled spin current mimicked in reentrant spin-glass garnet ferrite lu3fe5- 2xcoxsixo12 thin ﬁlms,” Applied Physics Express, vol. 8, no. 4, p. 043002, 2015. [2122] A. Sengupta, A. Banerjee, and K. Roy, “Hybrid spintronic-cmos spiking neural network with on-chip learning: Devices, circuits, and systems,” Physical Review Applied, vol. 6, no. 6, p. 064003, 2016. [2123] S. Lequeux, J. Sampaio, V. Cros, K. Yakushiji, A. Fukushima, R. Mat- sumoto, H. Kubota, S. Yuasa, and J. Grollier, “A magnetic synapse: multilevel spin-torque memristor with perpendicular anisotropy,” Sci- entiﬁc reports, vol. 6, 2016. [2124] D. I. Suh, J. P. Kil, Y. Choi, G. Y. Bae, and W. Park, “An associative memory device using a magnetic tunnel junction,” Magnetics, IEEE Transactions on, vol. 51, no. 11, pp. 1–4, 2015. [2125] A. F. Vincent, J. Larroque, W. S. Zhao, N. Ben Romdhane, O. Bichler, C. Gamrat, J.-O. Klein, S. Galdin-Retailleau, and D. Querlioz, “Spin- transfer torque magnetic memory as a stochastic memristive synapse,” in Circuits and Systems (ISCAS), 2014 IEEE International Symposium on. IEEE, 2014, pp. 1074–1077. [2126] D. Zhang, L. Zeng, Y. Qu, Z. M. Wang, W. Zhao, T. Tang, Y. Wang et al., “Energy-efﬁcient neuromorphic computation based on com- pound spin synapse with stochastic learning,” in Circuits and Systems (ISCAS), 2015 IEEE International Symposium on. IEEE, 2015, pp. 1538–1541. [2127] T. Roska, A. Horvath, A. Stubendek, F. Corinto, G. Csaba, W. Porod, T. Shibata, and G. Bourianoff, “An associative memory with oscilla- tory cnn arrays using spin torque oscillator cells and spin-wave inter- actions architecture and end-to-end simulator,” in Cellular Nanoscale Networks and Their Applications (CNNA), 2012 13th International Workshop on. IEEE, 2012, pp. 1–3. [2128] M. Sharad, C. Augustine, G. Panagopoulos, and K. Roy, “Spin-based neuron model with domain-wall magnets as synapse,” Nanotechnol- ogy, IEEE Transactions on, vol. 11, no. 4, pp. 843–853, 2012. [2129] A. Sengupta, K. Yogendra, D. Fan, and K. Roy, “Prospects of efﬁcient neural computing with arrays of magneto-metallic neurons and synapses,” in 2016 21st Asia and South Paciﬁc Design Automation Conference (ASP-DAC). IEEE, 2016, pp. 115–120. [2130] S. G. Ramasubramanian, R. Venkatesan, M. Sharad, K. Roy, and A. Raghunathan, “Spindle: Spintronic deep learning engine for large- scale neuromorphic computing,” in Proceedings of the 2014 interna- tional symposium on Low power electronics and design. ACM, 2014, pp. 15–20. [2131] A. Sengupta, K. Yogendra, and K. Roy, “Spintronic devices for ultra- low power neuromorphic computation (special session paper),” in Circuits and Systems (ISCAS), 2016 IEEE International Symposium on. IEEE, 2016, pp. 922–925. [2132] M. Sharad, D. Fan, and K. Roy, “Spin-neurons: A possible path to energy-efﬁcient neuromorphic computers,” Journal of Applied Physics, vol. 114, no. 23, p. 234906, 2013. [2133] ——, “Ultra low power associative computing with spin neurons and resistive crossbar memory,” in Design Automation Conference (DAC), 2013 50th ACM/EDAC/IEEE. IEEE, 2013, pp. 1–6. [2134] N. Locatelli, A. F. Vincent, A. Mizrahi, J. S. Friedman, D. Vodeni- carevic, J.-V. Kim, J.-O. Klein, W. Zhao, J. Grollier, and D. Querlioz, “Spintronic devices as key elements for energy-efﬁcient neuroinspired architectures,” in Proceedings of the 2015 Design, Automation & Test in Europe Conference & Exhibition. EDA Consortium, 2015, pp. 994–999. [2135] A. Sengupta, Y. Shim, and K. Roy, “Proposal for an all-spin arti- ﬁcial neural network: Emulating neural and synaptic functionalities through domain wall motion in ferromagnets,” IEEE transactions on biomedical circuits and systems, vol. 10, no. 6, pp. 1152–1160, 2016. [2136] A. Sengupta and K. Roy, “A vision for all-spin neural networks: A device to system perspective,” IEEE Transactions on Circuits and Systems I: Regular Papers, vol. 63, no. 12, pp. 2267–2277, 2016. [2137] A. Sengupta, B. Han, and K. Roy, “Toward a spintronic deep learning spiking neural processor,” in Proc. Biomed. Circuits Syst. Conf.(BioCAS), 2016. [2138] D. Zhang, L. Zeng, K. Cao, M. Wang, S. Peng, Y. Zhang, Y. Zhang, J.-O. Klein, Y. Wang, and W. Zhao, “All spin artiﬁcial neural 75 networks based on compound spintronic synapse and neuron,” IEEE transactions on biomedical circuits and systems, vol. 10, no. 4, pp. 828–836, 2016. [2139] A. Sengupta, K. Yogendra, D. Fan, and K. Roy, “Prospects of efﬁcient neural computing with arrays of magneto-metallic neurons and synapses,” in Design Automation Conference (ASP-DAC), 2016 21st Asia and South Paciﬁc. IEEE, 2016, pp. 115–120. [2140] M. Sharad, C. Augustine, G. Panagopoulos, and K. Roy, “Spin based neuron-synapse module for ultra low power programmable computational networks,” in Neural Networks (IJCNN), The 2012 International Joint Conference on. IEEE, 2012, pp. 1–7. [2141] ——, “Cognitive computing with spin-based neural networks,” in Proceedings of the 49th Annual Design Automation Conference. ACM, 2012, pp. 1262–1263. [2142] L. Zeng, D. Zhang, Y. Zhang, F. Gong, T. Gao, S. Tu, H. Yu, and W. Zhao, “Spin wave based synapse and neuron for ultra low power neuromorphic computation system,” in Circuits and Systems (ISCAS), 2016 IEEE International Symposium on. IEEE, 2016, pp. 918–921. [2143] D. Zhang, L. Zeng, Y. Zhang, W. Zhao, and J. O. Klein, “Stochastic spintronic device based synapses and spiking neurons for neuromor- phic computation,” in Nanoscale Architectures (NANOARCH), 2016 IEEE/ACM International Symposium on. IEEE, 2016, pp. 173–178. [2144] V. Q. Diep, B. Sutton, B. Behin-Aein, and S. Datta, “Spin switches for compact implementation of neuron and synapse,” Applied Physics Letters, vol. 104, no. 22, p. 222405, 2014. [2145] P. Hasler and T. S. Lande, “Overview of ﬂoating-gate devices, circuits, and systems,” IEEE Transactions on Circuits and Systems II: Analog and Digital Signal Processing, vol. 48, no. 1, pp. 1–3, 2001. [2146] S. Aunet and M. Hartmann, “Real-time reconﬁgurable linear threshold elements and some applications to neural hardware,” in Evolvable Systems: From Biology to Hardware. Springer, 2003, pp. 365–376. [2147] D. Durfee, F. Shoucair et al., “Comparison of ﬂoating gate neural network memory cells in standard vlsi cmos technology,” Neural Networks, IEEE Transactions on, vol. 3, no. 3, pp. 347–353, 1992. [2148] V. Fabbrizio, F. Raynal, X. Mariaud, A. Kramer, and G. Colli, “Low power, low voltage conductance-mode cmos analog neuron,” in Microelectronics for Neural Networks, 1996., Proceedings of Fifth International Conference on. IEEE, 1996, pp. 111–115. [2149] B. Fowler and A. El Gamal, “Pulse-modulated analog neuron circuits,” Journal of VLSI signal processing systems for signal, image and video technology, vol. 8, no. 1, pp. 45–51, 1994. [2150] O. Fujita and Y. Amemiya, “A ﬂoating-gate analog memory device for neural networks,” Electron Devices, IEEE Transactions on, vol. 40, no. 11, pp. 2029–2035, 1993. [2151] P. Haﬂiger and C. Rasche, “Floating gate analog memory for param- eter and variable storage in a learning silicon neuron,” in Circuits and Systems, 1999. ISCAS’99. Proceedings of the 1999 IEEE International Symposium on, vol. 2. IEEE, 1999, pp. 416–419. [2152] R. R. Harrison, J. A. Bragg, P. Hasler, B. A. Minch, and S. P. Deweerth, “A cmos programmable analog memory-cell array using ﬂoating-gate circuits,” Circuits and Systems II: Analog and Digital Signal Processing, IEEE Transactions on, vol. 48, no. 1, pp. 4–11, 2001. [2153] K. Rahimi, C. Diorio, C. Hernandez, and M. D. Brockhausen, “A sim- ulation model for ﬂoating-gate mos synapse transistors,” in Circuits and Systems, 2002. ISCAS 2002. IEEE International Symposium on, vol. 2. IEEE, 2002, pp. II–532. [2154] C.-K. Sin, A. Kramer, V. Hu, R. R. Chu, and P. K. Ko, “Eeprom as an analog storage device, with particular applications in neutral networks,” Electron Devices, IEEE Transactions on, vol. 39, no. 6, pp. 1410–1419, 1992. [2155] T. Morie, T. Matsuura, M. Nagata, and A. Iwata, “A multinanodot ﬂoating-gate mosfet circuit for spiking neuron models,” Nanotechnol- ogy, IEEE Transactions on, vol. 2, no. 3, pp. 158–164, 2003. [2156] S. Brink, S. Nease, and P. Hasler, “Computing with networks of spiking neurons on a biophysically motivated ﬂoating-gate based neuromorphic integrated circuit,” Neural Networks, vol. 45, pp. 39– 49, 2013. [2157] C. Diorio, P. Hasler, B. A. Minch, and C. A. Mead, “A ﬂoating-gate mos learning array with locally computed weight updates,” Electron Devices, IEEE Transactions on, vol. 44, no. 12, pp. 2281–2289, 1997. [2158] C. Diorio, P. Hasler, B. A. Minch, and C. Mead, “A complementary pair of four-terminal silicon synapses,” Analog Integrated Circuits and Signal Processing, vol. 13, no. 1-2, pp. 153–166, 1997. [2159] ——, “Floating-gate mos synapse transistors,” in Neuromorphic sys- tems engineering. Springer, 1998, pp. 315–337. [2160] P. Gupta and C. Markan, “An adaptable neuromorphic model of orientation selectivity based on ﬂoating gate dynamics,” Frontiers in neuroscience, vol. 8, 2014. [2161] P. Hasler, C. Diorio, and B. A. Minch, “A four-quadrant ﬂoating-gate synapse,” in Circuits and Systems, 1998. ISCAS’98. Proceedings of the 1998 IEEE International Symposium on, vol. 3. IEEE, 1998, pp. 29–32. [2162] P. Hasler and J. Dugger, “Correlation learning rule in ﬂoating-gate pfet synapses,” Circuits and Systems II: Analog and Digital Signal Processing, IEEE Transactions on, vol. 48, no. 1, pp. 65–73, 2001. [2163] H. Kosaka, T. Shibata, H. Ishii, and T. Ohmi, “An excellent weight- updating-linearity eeprom synapse memory cell for self-learning neuron-mos neural networks,” Electron Devices, IEEE Transactions on, vol. 42, no. 1, pp. 135–143, 1995. [2164] B. W. Lee, B. J. Sheu, and H. Yang, “Analog ﬂoating-gate synapses for general-purpose vlsi neural computation,” IEEE transactions on circuits and systems, vol. 38, no. 6, pp. 654–658, 1991. [2165] C. Markan, P. Gupta, and M. Bansal, “An adaptive neuromorphic model of ocular dominance map using ﬂoating gate ?synapse?” Neural Networks, vol. 45, pp. 117–133, 2013. [2166] M. Pankaala, M. Laiho, and P. Hasler, “Compact ﬂoating-gate learning array with stdp,” in Neural Networks, 2009. IJCNN 2009. Interna- tional Joint Conference on. IEEE, 2009, pp. 2409–2415. [2167] C. Riggert, M. Ziegler, D. Schroeder, W. Krautschneider, and H. Kohlstedt, “Memﬂash device: ﬂoating gate transistors as mem- ristive devices for neuromorphic computing,” Semiconductor Science and Technology, vol. 29, no. 10, p. 104011, 2014. [2168] T. Shibata, H. Kosaka, H. Ishii, and T. Ohmi, “A neuron-mos neural network using self-learning-compatible synapse circuits,” Solid-State Circuits, IEEE Journal of, vol. 30, no. 8, pp. 913–922, 1995. [2169] Y. L. Wong, P. Xu, and P. Abshire, “Ultra-low spike rate silicon neu- ron,” in Biomedical Circuits and Systems Conference, 2007. BIOCAS 2007. IEEE. IEEE, 2007, pp. 95–98. [2170] S. Brink, S. Koziol, S. Ramakrishnan, and P. Hasler, “A biophysically based dendrite model using programmable ﬂoating-gate devices,” in 2008 IEEE International Symposium on Circuits and Systems, 2008. [2171] S. Nease and E. Chicca, “Power-efﬁcient estimation of silicon neuron ﬁring rates with ﬂoating-gate transistors,” in Circuit Theory and Design (ECCTD), 2015 European Conference on. IEEE, 2015, pp. 1–4. [2172] D. Brunner, S. Reitzenstein, and I. Fischer, “All-optical neuromorphic computing in optical networks of semiconductor lasers,” in Rebooting Computing (ICRC), IEEE International Conference on. IEEE, 2016, pp. 1–2. [2173] W. Coomans, L. Gelens, S. Beri, J. Danckaert, and G. Van der Sande, “Solitary and coupled semiconductor ring lasers as optical spiking neurons,” Physical Review E, vol. 84, no. 3, p. 036209, 2011. [2174] R. C. Frye, E. A. Rietman, and C. C. Wong, “Back-propagation learning and nonidealities in analog neural network hardware,” Neural Networks, IEEE Transactions on, vol. 2, no. 1, pp. 110–117, 1991. [2175] K. D. Maier, C. Beckstein, R. Blickhan, W. Erhard, and D. Fey, “A multi-layer-perceptron neural network hardware based on 3d massively parallel optoelectronic circuits,” in Parallel Interconnects, 1999.(PI’99) Proceedings. The 6th International Conference on. IEEE, 1999, pp. 73–80. [2176] L. Neiberg and D. Casasent, “High-capacity neural networks on nonideal hardware,” Applied optics, vol. 33, no. 32, pp. 7665–7675, 1994. [2177] Y. Paquot, F. Duport, A. Smerieri, J. Dambre, B. Schrauwen, M. Hael- terman, and S. Massar, “Optoelectronic reservoir computing,” Scien- tiﬁc Reports, vol. 2, 2012. [2178] P. R. Prucnal, M. P. Fok, D. Rosenbluth, and K. Kravtsov, “Lightwave neuromorphic signal processing,” in Information Photonics (IP), 2011 ICO International Conference on. IEEE, 2011, pp. 1–2. [2179] T. F. de Lima, B. J. Shastri, M. A. Nahmias, A. N. Tait, and P. R. Prucnal, “Physical modeling of photonic neural networks,” in Photonics Society Summer Topical Meeting Series (SUM), 2016 IEEE. IEEE, 2016, pp. 222–223. [2180] Y. Shen, S. Skirlo, N. C. Harris, D. Englund, and M. Soljaˇci´c, “On- chip optical neuromorphic computing,” in Lasers and Electro-Optics (CLEO), 2016 Conference on. IEEE, 2016, pp. 1–2. [2181] M. P. Fok, H. Deming, M. Nahmias, N. Raﬁdi, D. Rosenbluth, A. Tait, Y. Tian, and P. R. Prucnal, “Signal feature recognition based on lightwave neuromorphic signal processing,” Optics letters, vol. 36, no. 1, pp. 19–21, 2011. 76 [2182] D. R. Collins and P. A. Penz, “Considerations for neural network hardware implementations,” in Circuits and Systems, 1989., IEEE International Symposium on. IEEE, 1989, pp. 834–836. [2183] B. J. Shastri, A. N. Tait, M. A. Nahmias, T. F. de Lima, and P. R. Prucnal, “Brain-inspired photonic hardware platforms,” in Photonics Society Summer Topical Meeting Series (SUM), 2016 IEEE. IEEE, 2016, pp. 181–182. [2184] M. P. Fok, J. Ge, R. Toole, R. Lin, Q. Zhou, A. James, and A. Mathews, “Wideband dynamic microwave photonic systems: From photonics to neuromorphic,” in Asia Communications and Photonics Conference. Optical Society of America, 2016, pp. ATh3H–1. [2185] G. Cauwenberghs, C. F. Neugebauer, A. J. Agranat, and A. Yariv, “Large scale optoelectronic integration of asynchronous analog neural networks,” in International Neural Network Conference. Springer, 1990, pp. 551–554. [2186] E. A. Rietman, R. Frye, C. Wong, and C. Kornfeld, “Amorphous silicon photoconductive arrays for artiﬁcial neural networks,” Applied optics, vol. 28, no. 16, pp. 3474–3478, 1989. [2187] B. Gholipour, P. Bastock, C. Craig, K. Khan, D. Hewak, and C. Soci, “Amorphous metal-sulphide microﬁbers enable photonic synapses for brain-like computing,” Advanced Optical Materials, vol. 3, no. 5, pp. 635–641, 2015. [2188] D. Livingston, S. Albin, and S. Park, “A new method for storing weights in analog neural hardware,” in Southeastcon’91., IEEE Pro- ceedings of. IEEE, 1991, pp. 86–88. [2189] P. Maier, F. Hartmann, M. Emmerling, C. Schneider, M. Kamp, S. H¨oﬂing, and L. Worschech, “Electro-photo-sensitive memristor for neuromorphic and arithmetic computing,” Physical Review Applied, vol. 5, no. 5, p. 054011, 2016. [2190] S. Qin, Y. Liu, X. Wang, Y. Xu, Y. Shi, R. Zhang, and F. Wang, “Light-activated artiﬁcial synapses based on graphene hybrid photo- transistors,” in Lasers and Electro-Optics (CLEO), 2016 Conference on. IEEE, 2016, pp. 1–2. [2191] Q. Ren, Y. Zhang, R. Wang, and J. Zhao, “Optical spike-timing- dependent plasticity with weight-dependent learning window and reward modulation,” Optics Express, vol. 23, no. 19, pp. 25 247– 25 258, 2015. [2192] M. P. Fok, Y. Tian, D. Rosenbluth, and P. R. Prucnal, “Asynchronous spiking photonic neuron for lightwave neuromorphic signal process- ing,” Optics letters, vol. 37, no. 16, pp. 3309–3311, 2012. [2193] A. Hurtado, K. Schires, I. D. Henning, and M. Adams, “Investigation of vertical cavity surface emitting laser dynamics for neuromorphic photonic systems,” Applied Physics Letters, vol. 100, no. 10, p. 103703, 2012. [2194] M. Nahmias, B. J. Shastri, A. N. Tait, P. R. Prucnal et al., “A leaky integrate-and-ﬁre laser neuron for ultrafast cognitive computing,” Selected Topics in Quantum Electronics, IEEE Journal of, vol. 19, no. 5, pp. 1–12, 2013. [2195] M. A. Nahmias, A. N. Tait, L. Tolias, M. P. Chang, T. Ferreira de Lima, B. J. Shastri, and P. R. Prucnal, “An integrated analog o/e/o link for multi-channel laser neurons,” Applied Physics Letters, vol. 108, no. 15, p. 151106, 2016. [2196] B. Romeira, R. Av´o, J. M. Figueiredo, S. Barland, and J. Javaloyes, “Regenerative memory in time-delayed neuromorphic photonic res- onators,” Scientiﬁc reports, vol. 6, p. 19510, 2016. [2197] J. M. Shainline, S. M. Buckley, R. P. Mirin, and S. W. Nam, “Neuro- morphic computing with integrated photonics and superconductors,” in Rebooting Computing (ICRC), IEEE International Conference on. IEEE, 2016, pp. 1–8. [2198] D. Mahalanabis, H. Barnaby, Y. Gonzalez-Velo, M. Kozicki, S. Vrud- hula, and P. Dandamudi, “Incremental resistance programming of programmable metallization cells for use as electronic synapses,” Solid-State Electronics, vol. 100, pp. 39–44, 2014. [2199] S. La Barbera, A. Vincent, D. Vuillaume, D. Querlioz, and F. Alibart, “Short-term to long-term plasticity transition in ﬁlamentary switching for memory applications,” in Memristive Systems (MEMRISYS) 2015 International Conference on. IEEE, 2015, pp. 1–2. [2200] S. La Barbera, A. F. Vincent, D. Vuillaume, D. Querlioz, and F. Al- ibart, “Interplay of multiple synaptic plasticity features in ﬁlamentary memristive devices for neuromorphic computing,” Scientiﬁc Reports, vol. 6, 2016. [2201] S. La Barbera, D. Vuillaume, and F. Alibart, “Filamentary switching: Synaptic plasticity through device volatility,” ACS nano, vol. 9, no. 1, pp. 941–949, 2015. [2202] E. Covi, S. Brivio, A. Serb, T. Prodromakis, M. Fanciulli, and S. Spiga, “Hfo2-based memristors for neuromorphic applications,” in Circuits and Systems (ISCAS), 2016 IEEE International Symposium on. IEEE, 2016, pp. 393–396. [2203] B. Gao, Y. Bi, H.-Y. Chen, R. Liu, P. Huang, B. Chen, L. Liu, X. Liu, S. Yu, H.-S. P. Wong et al., “Ultra-low-energy three-dimensional oxide-based electronic synapses for implementation of robust high- accuracy neuromorphic computation systems,” ACS nano, vol. 8, no. 7, pp. 6998–7004, 2014. [2204] B. Gao, L. Liu, and J. Kang, “Investigation of the synaptic device based on the resistive switching behavior in hafnium oxide,” Progress in Natural Science: Materials International, vol. 25, no. 1, pp. 47–50, 2015. [2205] R. Jha and S. Mandal, “Nanoelectronic synaptic devices and materials for brain-inspired computational architectures,” in SPIE NanoScience+ Engineering. International Society for Optics and Photonics, 2014, pp. 91 740S–91 740S. [2206] Y. Matveyev, R. Kirtaev, A. Fetisova, S. Zakharchenko, D. Ne- grov, and A. Zenkevich, “Crossbar nanoscale hfo 2-based electronic synapses,” Nanoscale Research Letters, vol. 11, no. 1, p. 1, 2016. [2207] S. Yu, Y. Wu, R. Jeyasingh, D. Kuzum, and H.-S. Wong, “An electronic synapse device based on metal oxide resistive switching memory for neuromorphic computation,” Electron Devices, IEEE Transactions on, vol. 58, no. 8, pp. 2729–2737, 2011. [2208] Y. Matveyev, K. Egorov, A. Markeev, and A. Zenkevich, “Resistive switching and synaptic properties of fully atomic layer deposition grown tin/hfo2/tin devices,” Journal of Applied Physics, vol. 117, no. 4, p. 044901, 2015. [2209] J. Woo, K. Moon, J. Song, M. Kwak, J. Park, and H. Hwang, “Optimized programming scheme enabling linear potentiation in ﬁlamentary hfo 2 rram synapse for neuromorphic systems,” IEEE Transactions on Electron Devices, vol. 63, no. 12, pp. 5064–5067, 2016. [2210] H. Jia, N. Deng, and H. Pang, “Threshold adaptive transistor realized with rrams for neuromorphic circuits,” in Junction Technology (IWJT), 2014 International Workshop on. IEEE, 2014, pp. 1–4. [2211] V. Demin, A. Emelyanov, D. Lapkin, V. Erokhin, P. Kashkarov, and M. Kovalchuk, “Neuromorphic elements and systems as the basis for the physical implementation of artiﬁcial intelligence technologies,” Crystallography Reports, vol. 61, no. 6, pp. 992–1001, 2016. [2212] T. Dongale, N. Desai, K. Khot, N. Mullani, P. Pawar, R. Tikke, V. Patil, P. Waifalkar, P. Patil, R. Kamat et al., “Effect of surfactants on the data directionality and learning behaviour of al/tio2/fto thin ﬁlm memristor-based electronic synapse,” Journal of Solid State Electrochemistry, pp. 1–5, 2016. [2213] M. Hu, Y. Wang, Q. Qiu, Y. Chen, and H. Li, “The stochastic modeling of tio 2 memristor and its usage in neuromorphic system design,” in Design Automation Conference (ASP-DAC), 2014 19th Asia and South Paciﬁc. IEEE, 2014, pp. 831–836. [2214] X. Hu, G. Feng, H. Li, Y. Chen, and S. Duan, “An adjustable memristor model and its application in small-world neural networks,” in Neural Networks (IJCNN), 2014 International Joint Conference on. IEEE, 2014, pp. 7–14. [2215] J. Park, M. Kwak, K. Moon, J. Woo, D. Lee, and H. Hwang, “Tio x-based rram synapse with 64-levels of conductance and symmetric conductance change by adopting a hybrid pulse scheme for neuro- morphic computing,” IEEE Electron Device Letters, vol. 37, no. 12, pp. 1559–1562, 2016. [2216] C. J. O’Kelly, J. A. Fairﬁeld, D. McCloskey, H. G. Manning, J. F. Donegan, and J. J. Boland, “Associative enhancement of time corre- lated response to heterogeneous stimuli in a neuromorphic nanowire device,” Advanced Electronic Materials, 2016. [2217] T. Chang, P. Sheridan, and W. Lu, “Modeling and implementation of oxide memristors for neuromorphic applications,” in 2012 13th International Workshop on Cellular Nanoscale Networks and their Applications, 2012. [2218] C. Du, W. Ma, T. Chang, P. Sheridan, and W. D. Lu, “Biorealistic implementation of synaptic functions with oxide memristors through internal ionic dynamics,” Advanced Functional Materials, vol. 25, no. 27, pp. 4290–4299, 2015. [2219] Z.-H. Tan, R. Yang, K. Terabe, X.-B. Yin, X.-D. Zhang, and X. Guo, “Synaptic metaplasticity realized in oxide memristive devices,” Ad- vanced Materials, 2015. [2220] T. Shi, X.-B. Yin, R. Yang, and X. Guo, “Pt/wo 3/fto memristive de- vices with recoverable pseudo-electroforming for time-delay switches in neuromorphic computing,” Physical Chemistry Chemical Physics, vol. 18, no. 14, pp. 9338–9343, 2016. 77 [2221] S. Thakoor, A. Moopenn, T. Daud, and A. Thakoor, “Solid-state thin- ﬁlm memistor for electronic neural networks,” Journal of Applied Physics, vol. 67, no. 6, pp. 3132–3135, 1990. [2222] Y.-F. Chang, B. Fowler, Y.-C. Chen, F. Zhou, C.-H. Pan, T.-C. Chang, and J. C. Lee, “Demonstration of synaptic behaviors and resistive switching characterizations by proton exchange reactions in silicon oxide,” Scientiﬁc reports, vol. 6, 2016. [2223] L. Guo, Q. Wan, C. Wan, L. Zhu, and Y. Shi, “Short-term memory to long-term memory transition mimicked in izo homojunction synaptic transistors,” Electron Device Letters, IEEE, vol. 34, no. 12, pp. 1581– 1583, 2013. [2224] L. Gao, I.-T. Wang, P.-Y. Chen, S. Vrudhula, J.-s. Seo, Y. Cao, T.-H. Hou, and S. Yu, “Fully parallel write/read in resistive synaptic array for accelerating on-chip learning,” Nanotechnology, vol. 26, no. 45, p. 455204, 2015. [2225] Y.-F. Wang, Y.-C. Lin, I.-T. Wang, T.-P. Lin, and T.-H. Hou, “Char- acterization and modeling of nonﬁlamentary ta/taox/tio2/ti analog synaptic device,” Scientiﬁc reports, vol. 5, 2015. [2226] S. Hu, Y. Liu, T. Chen, Z. Liu, Q. Yu, L. Deng, Y. Yin, and S. Hosaka, “Emulating the ebbinghaus forgetting curve of the human brain with a nio-based memristor,” Applied Physics Letters, vol. 103, no. 13, p. 133701, 2013. [2227] ——, “Emulating the paired-pulse facilitation of a biological synapse with a niox-based memristor,” Applied Physics Letters, vol. 102, no. 18, p. 183510, 2013. [2228] S. Hu, Y. Liu, Z. Liu, T. Chen, Q. Yu, L. Deng, Y. Yin, and S. Hosaka, “Synaptic long-term potentiation realized in pavlov’s dog model based on a niox-based memristor,” Journal of Applied Physics, vol. 116, no. 21, p. 214502, 2014. [2229] X. Yang, Y. Cai, Z. Zhang, M. Yu, and R. Huang, “An electronic synapse based on tantalum oxide material,” in 2015 15th Non-Volatile Memory Technology Symposium (NVMTS). IEEE, 2015, pp. 1–4. [2230] Z. Wang, M. Yin, T. Zhang, Y. Cai, Y. Wang, Y. Yang, and R. Huang, “Engineering incremental resistive switching in tao x based memris- tors for brain-inspired computing,” Nanoscale, 2016. [2231] A. Thomas, S. Nieh¨orster, S. Fabretti, N. Shepheard, O. Kuschel, K. K¨upper, J. Wollschl¨ager, P. Krzysteczko, and E. Chicca, “Tunnel junction based memristors as artiﬁcial synapses,” Frontiers in neuro- science, vol. 9, 2015. [2232] C. Wang, W. He, Y. Tong, and R. Zhao, “Investigation and manipula- tion of different analog behaviors of memristor as electronic synapse for neuromorphic applications,” Scientiﬁc reports, vol. 6, 2016. [2233] Y. Wu, S. Yu, H.-S. Wong, Y.-S. Chen, H.-Y. Lee, S.-M. Wang, P.-Y. Gu, F. Chen, and M.-J. Tsai, “Alox-based resistive switching device with gradual resistance modulation for neuromorphic device appli- cation,” in Memory Workshop (IMW), 2012 4th IEEE International. IEEE, 2012, pp. 1–4. [2234] B. Sarkar, B. Lee, and V. Misra, “Understanding the gradual reset in pt/al2o3/ni rram for synaptic applications,” Semiconductor Science and Technology, vol. 30, no. 10, p. 105014, 2015. [2235] L.-G. Wang, W. Zhang, Y. Chen, Y.-Q. Cao, A.-D. Li, and D. Wu, “Synaptic plasticity and learning behaviors mimicked in single in- organic synapses of pt/hfox/znox/tin memristive system,” Nanoscale Research Letters, vol. 12, no. 1, p. 65, 2017. [2236] K. Moon, E. Cha, J. Park, S. Gi, M. Chu, K. Baek, B. Lee, S. Oh, and H. Hwang, “High density neuromorphic system with mo/pr0. 7ca0. 3mno3 synapse and nbo2 imt oscillator neuron,” in 2015 IEEE International Electron Devices Meeting (IEDM). IEEE, 2015, pp. 17–6. [2237] J.-W. Jang, S. Park, Y.-H. Jeong, and H. Hwang, “Reram-based synaptic device for neuromorphic computing,” in Circuits and Systems (ISCAS), 2014 IEEE International Symposium on. IEEE, 2014, pp. 1054–1057. [2238] J.-W. Jang, S. Park, G. W. Burr, H. Hwang, and Y.-H. Jeong, “Optimization of conductance change in pr 1–x ca x mno 3-based synaptic devices for neuromorphic systems,” Electron Device Letters, IEEE, vol. 36, no. 5, pp. 457–459, 2015. [2239] D. Lee, J. Park, K. Moon, J. Jang, S. Park, M. Chu, J. Kim, J. Noh, M. Jeon, B. H. Lee et al., “Oxide based nanoscale analog synapse device for neural signal recognition system,” in 2015 IEEE International Electron Devices Meeting (IEDM). IEEE, 2015, pp. 4–7. [2240] K. Moon, E. Cha, D. Lee, J. Jang, J. Park, and H. Hwang, “Reram- based analog synapse and imt neuron device for neuromorphic sys- tem,” in VLSI Technology, Systems and Application (VLSI-TSA), 2016 International Symposium on. IEEE, 2016, pp. 1–2. [2241] K. Moon, E. Cha, J. Park, S. Gi, M. Chu, K. Baek, B. Lee, S. H. Oh, and H. Hwang, “Analog synapse device with 5-b mlc and improved data retention for neuromorphic system,” IEEE Electron Device Letters, vol. 37, no. 8, pp. 1067–1070, 2016. [2242] P. Krzysteczko, J. M¨unchenberger, M. Sch¨afers, G. Reiss, and A. Thomas, “The memristive magnetic tunnel junction as a nanoscopic synapse-neuron system,” Advanced Materials, vol. 24, no. 6, pp. 762– 766, 2012. [2243] Y. Li, Y. Zhong, L. Xu, J. Zhang, X. Xu, H. Sun, and X. Miao, “Ultrafast synaptic events in a chalcogenide memristor,” Scientiﬁc reports, vol. 3, 2013. [2244] Y. Li, Y. Zhong, J. Zhang, L. Xu, Q. Wang, H. Sun, H. Tong, X. Cheng, and X. Miao, “Activity-dependent synaptic plasticity of a chalcogenide electronic synapse for neuromorphic systems,” Scientiﬁc reports, vol. 4, 2014. [2245] J. Tranchant, E. Janod, B. Corraze, P. Stoliar, M. Rozenberg, M.- P. Besland, and L. Cario, “Control of resistive switching in am4q8 narrow gap mott insulators: A ﬁrst step towards neuromorphic ap- plications,” physica status solidi (a), vol. 212, no. 2, pp. 239–244, 2015. [2246] Y. Chen, G. Liu, C. Wang, W. Zhang, R.-W. Li, and L. Wang, “Polymer memristor for information storage and neuromorphic ap- plications,” Materials Horizons, vol. 1, no. 5, pp. 489–506, 2014. [2247] L. J. Juarez-Hernandez, N. Cornella, L. Pasquardini, S. Battistoni, L. Vidalino, L. Vanzetti, S. Caponi, M. Dalla Serra, S. Iannotta, C. Pederzolli et al., “Bio-hybrid interfaces to study neuromorphic functionalities: New multidisciplinary evidences of cell viability on poly (anyline)(pani), a semiconductor polymer with memristive prop- erties,” Biophysical chemistry, vol. 208, pp. 40–47, 2016. [2248] S. Li, F. Zeng, C. Chen, H. Liu, G. Tang, S. Gao, C. Song, Y. Lin, F. Pan, and D. Guo, “Synaptic plasticity and learning behaviours mim- icked through ag interface movement in an ag/conducting polymer/ta memristive system,” Journal of Materials Chemistry C, vol. 1, no. 34, pp. 5292–5298, 2013. [2249] W. Luo, F.-Y. Yuan, H. Wu, L. Pan, N. Deng, F. Zeng, R. Wei, and X. Cai, “Synaptic learning behaviors achieved by metal ion migration in a cu/pedot: Pss/ta memristor,” in 2015 15th Non-Volatile Memory Technology Symposium (NVMTS). IEEE, 2015, pp. 1–4. [2250] W. Luo, X. Wu, F.-Y. Yuan, H. Wu, L. Pan, and N. Deng, “Synaptic learning behavior based on a ag/pedot: Pss/ta memristor,” in Next- Generation Electronics (ISNE), 2016 5th International Symposium on. IEEE, 2016, pp. 1–2. [2251] R. Nawrocki, R. M. Voyles, S. E. Shaheen et al., “Neurons in polymer: Hardware neural units based on polymer memristive devices and polymer transistors,” Electron Devices, IEEE Transactions on, vol. 61, no. 10, pp. 3513–3519, 2014. [2252] Z. Xiao and J. Huang, “Energy-efﬁcient hybrid perovskite memristors and synaptic devices,” Advanced Electronic Materials, 2016. [2253] X. Yang, C. Wang, J. Shang, C. Zhang, H. Tan, X. Yi, L. Pan, W. Zhang, F. Fan, Y. Liu et al., “An organic terpyridyl-iron poly- mer based memristor for synaptic plasticity and learning behavior simulation,” RSC Advances, vol. 6, no. 30, pp. 25 179–25 184, 2016. [2254] C. Zhang, Y.-T. Tai, J. Shang, G. Liu, K.-L. Wang, C. Hsu, X. Yi, X. Yang, W. Xue, H. Tan et al., “Synaptic plasticity and learning behaviours in ﬂexible artiﬁcial synapse based on polymer/viologen system,” Journal of Materials Chemistry C, vol. 4, no. 15, pp. 3217– 3223, 2016. [2255] C. H. Bennett, D. Chabi, T. Cabaret, B. Jousselme, V. Derycke, D. Querlioz, and J.-O. Klein, “Supervised learning with organic mem- ristor devices and prospects for neural crossbar arrays,” in Nanoscale Architectures (NANOARCH), 2015 IEEE/ACM International Sympo- sium on. IEEE, 2015, pp. 181–186. [2256] T. Cabaret, L. Fillaud, B. Jousselme, J.-O. Klein, and V. Derycke, “Electro-grafted organic memristors: Properties and prospects for artiﬁcial neural networks based on stdp,” in Nanotechnology (IEEE- NANO), 2014 IEEE 14th International Conference on. IEEE, 2014, pp. 499–504. [2257] C. Chang, F. Zeng, X. Li, W. Dong, S. Lu, S. Gao, and F. Pan, “Simulation of synaptic short-term plasticity using ba (cf3so3) 2- doped polyethylene oxide electrolyte ﬁlm,” Scientiﬁc reports, vol. 6, 2016. [2258] V. Erokhin, T. Berzina, A. Smerieri, P. Camorani, S. Erokhina, and M. P. Fontana, “Bio-inspired adaptive networks based on organic memristors,” Nano Communication Networks, vol. 1, no. 2, pp. 108– 117, 2010. [2259] V. Erokhin, “Organic memristive devices: Architecture, properties and applications in neuromorphic networks,” in Electronics, Circuits, 78 and Systems (ICECS), 2013 IEEE 20th International Conference on. IEEE, 2013, pp. 305–308. [2260] S. Erokhina, “Layer-by-layer technique for the fabrication of or- ganic memristors and neuromorphic systems,” in Memristive Systems (MEMRISYS) 2015 International Conference on. IEEE, 2015, pp. 1–2. [2261] C.-H. Kim, S. Sung, and M.-H. Yoon, “Synaptic organic transistors with a vacuum-deposited charge-trapping nanosheet,” Scientiﬁc Re- ports, vol. 6, 2016. [2262] L.-a. Kong, J. Sun, C. Qian, G. Gou, Y. He, J. Yang, and Y. Gao, “Ion- gel gated ﬁeld-effect transistors with solution-processed oxide semi- conductors for bioinspired artiﬁcial synapses,” Organic Electronics, vol. 39, pp. 64–70, 2016. [2263] Y.-P. Lin, C. H. Bennett, T. Cabaret, D. Vodenicarevic, D. Chabi, D. Querlioz, B. Jousselme, V. Derycke, and J.-O. Klein, “Physical realization of a supervised learning system built with organic mem- ristive synapses,” Scientiﬁc Reports, vol. 6, 2016. [2264] G. Liu, C. Wang, W. Zhang, L. Pan, C. Zhang, X. Yang, F. Fan, Y. Chen, and R.-W. Li, “Organic biomimicking memristor for in- formation storage and processing applications,” Advanced Electronic Materials, 2015. [2265] R. A. Nawrocki, R. M. Voyles, and S. E. Shaheen, “Simulating hardware neural networks with organic memristors and organic ﬁeld effect transistors,” Intelligent Engineering Systems through Artiﬁcial Neural Networks, vol. 20, 2010. [2266] L. Wang, Z. Wang, J. Lin, J. Yang, L. Xie, M. Yi, W. Li, H. Ling, C. Ou, and W. Huang, “Long-term homeostatic properties comple- mentary to hebbian rules in cupc-based multifunctional memristor,” Scientiﬁc reports, vol. 6, 2016. [2267] H. Ishiwara, “Proposal of adaptive-learning neuron circuits with ferroelectric analog-memory weights,” Japanese journal of applied physics, vol. 32, no. 1S, p. 442, 1993. [2268] S.-M. Yoon, E. Tokumitsu, and H. Ishiwara, “An electrically modi- ﬁable synapse array composed of metal-ferroelectric-semiconductor (mfs) fet’s using srbi/sub 2/ta/sub 2/o/sub 9/thin ﬁlms,” Electron Device Letters, IEEE, vol. 20, no. 5, pp. 229–231, 1999. [2269] ——, “Adaptive-learning neuron integrated circuits using metal- ferroelectric (srbi/sub 2/ta/sub 2/o/sub 9/)-semiconductor (mfs) fet’s,” Electron Device Letters, IEEE, vol. 20, no. 10, pp. 526–528, 1999. [2270] ——, “Realization of adaptive learning function in a neuron circuit using metal/ferroelectric (srbi2ta2o9)/semiconductor ﬁeld effect tran- sistor (mfsfet),” Japanese journal of applied physics, vol. 38, no. 4S, p. 2289, 1999. [2271] ——, “Ferroelectric neuron integrated circuits using srbi 2 ta 2 o 9-gate fet’s and cmos schmitt-trigger oscillators,” Electron Devices, IEEE Transactions on, vol. 47, no. 8, pp. 1630–1635, 2000. [2272] ——, “Improvement of memory retention characteristics in ferroelec- tric neuron circuits using a pt/srbi 2ta 2o 9/pt/ti/sio 2/si structure-ﬁeld effect transistor as a synapse device,” Japanese Journal of Applied Physics, vol. 39, no. 4S, p. 2119, 2000. [2273] E. Kim, K. Kim, and S. Yoon, “Investigation of the ferroelectric switching behavior of p (vdf-trfe)-pmma blended ﬁlms for synaptic device applications,” Journal of Physics D: Applied Physics, vol. 49, no. 7, p. 075105, 2016. [2274] Y. Nishitani, Y. Kaneko, M. Ueda, T. Morie, and E. Fujii, “Three- terminal ferroelectric synapse device with concurrent learning func- tion for artiﬁcial neural networks,” Journal of Applied Physics, vol. 111, no. 12, p. 124108, 2012. [2275] Y. Nishitani, Y. Kaneko, M. Ueda, E. Fujii, and A. Tsujimura, “Dynamic observation of brain-like learning in a ferroelectric synapse device,” Japanese Journal of Applied Physics, vol. 52, no. 4S, p. 04CE06, 2013. [2276] S.-M. Yoon and H. Ishiwara, “Adaptive-learning synaptic devices using ferroelectric-gate ﬁeld-effect transistors for neuromorphic ap- plications,” in Ferroelectric-Gate Field Effect Transistor Memories. Springer, 2016, pp. 311–333. [2277] Y. Nishitani, Y. Kaneko, and M. Ueda, “Artiﬁcial synapses using ferroelectric memristors embedded with cmos circuit for image recog- nition,” in Device Research Conference (DRC), 2014 72nd Annual. IEEE, 2014, pp. 297–298. [2278] ——, “Supervised learning using spike-timing-dependent plasticity of memristive synapses,” 2015. [2279] Z. Wang, W. Zhao, W. Kang, Y. Zhang, J.-O. Klein, D. Ravelosona, and C. Chappert, “Compact modelling of ferroelectric tunnel memris- tor and its use for neuromorphic simulation,” Applied Physics Letters, vol. 104, no. 5, p. 053505, 2014. [2280] C. J. Wan, Y. H. Liu, P. Feng, W. Wang, L. Q. Zhu, Z. P. Liu, Y. Shi, and Q. Wan, “Flexible metal oxide/graphene oxide hybrid neuromorphic transistors on ﬂexible conducting graphene substrates,” Advanced Materials, 2016. [2281] C. J. Wan, L. Q. Zhu, Y. H. Liu, P. Feng, Z. P. Liu, H. L. Cao, P. Xiao, Y. Shi, and Q. Wan, “Proton-conducting graphene oxide-coupled neuron transistors for brain-inspired cognitive systems,” Advanced Materials, vol. 28, no. 18, pp. 3557–3563, 2016. [2282] Y. Yang, J. Wen, L. Guo, X. Wan, P. Du, P. Feng, Y. Shi, and Q. Wan, “Long-term synaptic plasticity emulated in modiﬁed graphene oxide electrolyte gated izo-based thin-ﬁlm transistors,” ACS Applied Mate- rials & Interfaces, vol. 8, no. 44, pp. 30 281–30 286, 2016. [2283] M. Darwish, V. Calayir, L. Pileggi, and J. Weldon, “Ultra-compact graphene multigate variable resistor for neuromorphic computing,” 2016. [2284] H. Tian, W. Mi, X.-F. Wang, H. Zhao, Q.-Y. Xie, C. Li, Y.-X. Li, Y. Yang, and T.-L. Ren, “Graphene dynamic synapse with modulatable plasticity,” Nano letters, vol. 15, no. 12, pp. 8013–8019, 2015. [2285] L. Wang, Z. Wang, W. Zhao, B. Hu, L. Xie, M. Yi, H. Ling, C. Zhang, Y. Chen, J. Lin et al., “Controllable multiple depression in a graphene oxide artiﬁcial synapse,” Advanced Electronic Materials, vol. 3, no. 1, 2017. [2286] C.-C. Hsu, A. C. Parker, and J. Joshi, “Dendritic computations, dendritic spiking and dendritic plasticity in nanoelectronic neurons,” in Circuits and Systems (MWSCAS), 2010 53rd IEEE International Midwest Symposium on. IEEE, 2010, pp. 89–92. [2287] C.-C. Hsu and A. C. Parker, “Border ownership in a nano- neuromorphic circuit using nonlinear dendritic computations,” in Neural Networks (IJCNN), 2014 International Joint Conference on. IEEE, 2014, pp. 3442–3449. [2288] ——, “Dynamic spike threshold and nonlinear dendritic computation for coincidence detection in neuromorphic circuits,” in Engineering in Medicine and Biology Society (EMBC), 2014 36th Annual Inter- national Conference of the IEEE. IEEE, 2014, pp. 461–464. [2289] J. Joshi, C. Hsu, A. C. Parker, and P. Deshmukh, “A carbon nanotube cortical neuron with excitatory and inhibitory dendritic computations,” in IEEE/NIH LIfe Science Systems and Applications Workshop, 2009. [2290] A. C. Parker, J. Joshi, C.-C. Hsu, and N. A. D. Singh, “A carbon nanotube implementation of temporal and spatial dendritic computa- tions,” in Circuits and Systems, 2008. MWSCAS 2008. 51st Midwest Symposium on. IEEE, 2008, pp. 818–821. [2291] S. Barzegarjalali and A. C. Parker, “A hybrid neuromorphic circuit demonstrating schizophrenic symptoms,” in Biomedical Circuits and Systems Conference (BioCAS), 2015 IEEE. IEEE, 2015, pp. 1–4. [2292] ——, “Neuromorphic circuit modeling directional selectivity in the visual cortex,” in Engineering in Medicine and Biology Society (EMBC), 2016 IEEE 38th Annual International Conference of the. IEEE, 2016, pp. 6130–6133. [2293] ——, “A neuromorphic circuit mimicking biological short-term mem- ory,” in Engineering in Medicine and Biology Society (EMBC), 2016 IEEE 38th Annual International Conference of the. IEEE, 2016, pp. 1401–1404. [2294] ——, “A bio-inspired electronic mechanism for unsupervised learning using structural plasticity,” in Future Technologies Conference (FTC). IEEE, 2016, pp. 806–815. [2295] K. Gacem, J.-M. Retrouvey, D. Chabi, A. Filoramo, W. Zhao, J.- O. Klein, and V. Derycke, “Neuromorphic function learning with carbon nanotube based synapses,” Nanotechnology, vol. 24, no. 38, p. 384013, 2013. [2296] J. Joshi, A. C. Parker, and C.-C. Hsu, “A carbon nanotube cortical neuron with spike-timing-dependent plasticity,” in Engineering in Medicine and Biology Society, 2009. EMBC 2009. Annual Interna- tional Conference of the IEEE. IEEE, 2009, pp. 1651–1654. [2297] J. Joshi, J. Zhang, C. Wang, C.-C. Hsu, A. C. Parker, C. Zhou, and U. Ravishankar, “A biomimetic fabricated carbon nanotube synapse for prosthetic applications,” in Life Science Systems and Applications Workshop (LiSSA), 2011 IEEE/NIH. IEEE, 2011, pp. 139–142. [2298] K. Kim, C.-L. Chen, Q. Truong, A. M. Shen, and Y. Chen, “A carbon nanotube synapse with dynamic logic and learning,” Advanced Materials, vol. 25, no. 12, pp. 1693–1698, 2013. [2299] K. Kim, A. Tudor, C.-L. Chen, D. Lee, A. M. Shen, and Y. Chen, “Bioinspired neuromorphic module based on carbon nan- otube/c60/polymer composite,” Journal of Composite Materials, p. 0021998315573559, 2015. [2300] S. Kim, J. Yoon, H.-D. Kim, and S.-J. Choi, “Carbon nanotube synaptic transistor network for pattern recognition,” ACS applied materials & interfaces, vol. 7, no. 45, pp. 25 479–25 486, 2015. 79 [2301] S.-Y. Liao, J.-M. Retrouvey, G. Agnus, W. Zhao, C. Maneux, S. Fr´egon`ese, T. Zimmer, D. Chabi, A. Filoramo, V. Derycke et al., “Design and modeling of a neuro-inspired learning circuit using nanotube-based memory devices,” Circuits and Systems I: Regular Papers, IEEE Transactions on, vol. 58, no. 9, pp. 2172–2181, 2011. [2302] M. Mahvash and A. C. Parker, “Modeling intrinsic ion-channel and synaptic variability in a cortical neuromorphic circuit,” in Biomedical Circuits and Systems Conference (BioCAS), 2011 IEEE. IEEE, 2011, pp. 69–72. [2303] A. M. Shen, C.-L. Chen, K. Kim, B. Cho, A. Tudor, and Y. Chen, “Analog neuromorphic module based on carbon nanotube synapses,” ACS nano, vol. 7, no. 7, pp. 6117–6122, 2013. [2304] A. M. Shen, K. Kim, A. Tudor, D. Lee, and Y. Chen, “Doping modulated carbon nanotube synapstors for a spike neuromorphic module,” Small, vol. 11, no. 13, pp. 1571–1579, 2015. [2305] C. Yin, Y. Li, J. Wang, X. Wang, Y. Yang, and T.-L. Ren, “Carbon nanotube transistor with short-term memory,” Tsinghua Science and Technology, vol. 21, no. 4, pp. 442–448, 2016. [2306] W. Zhao, G. Agnus, V. Derycke, A. Filoramo, J. Bourgoin, and C. Gamrat, “Nanotube devices based crossbar architecture: toward neuromorphic computing,” Nanotechnology, vol. 21, no. 17, p. 175202, 2010. [2307] P. Feng, W. Xu, Y. Yang, X. Wan, Y. Shi, Q. Wan, J. Zhao, and Z. Cui, “Printed neuromorphic devices based on printed carbon nanotube thin- ﬁlm transistors,” Advanced Functional Materials, 2016. [2308] J. Joshi, A. Parker, and C. Hsu, “A carbon nanotube spiking cortical neuron with tunable refractory period and spiking duration,” in IEEE Latin American Symp. on Circuits and Systems (LASCAS), 2010. [2309] M. Mahvash and A. C. Parker, “Synaptic variability in a cortical neuromorphic circuit,” Neural Networks and Learning Systems, IEEE Transactions on, vol. 24, no. 3, pp. 397–409, 2013. [2310] M. Najari, T. El-Grour, S. Jelliti, O. M. Hakami, A. Al-Kamli, N. Can, G. O. Souadi, M. Fadhali, A. Mahdy, and M. Mahgoub, “Simulation of a spiking neuron circuit using carbon nanotube transistors,” in AIP Conference Proceedings, vol. 1742, no. 1. AIP Publishing, 2016, p. 030013. [2311] H. Kim, J. Park, M. Kwon, J. Lee, and B. Park, “Silicon-based ﬂoating-body synaptic transistor with frequency dependent short- and long-term memories,” Electron Device Letters, IEEE, vol. PP, no. 99, pp. 1–1, 2016. [2312] H. Kim, S. Cho, M.-C. Sun, J. Park, S. Hwang, and B.-G. Park, “Sim- ulation study on silicon-based ﬂoating body synaptic transistor with short-and long-term memory functions and its spike timing-dependent plasticity,” JOURNAL OF SEMICONDUCTOR TECHNOLOGY AND SCIENCE, vol. 16, no. 5, pp. 657–663, 2016. [2313] F. Shao, Y. Yang, L. Q. Zhu, P. Feng, and Q. Wan, “Oxide-based synaptic transistors gated by sol-gel silica electrolytes,” ACS Applied Materials & Interfaces, 2016. [2314] J. Shi, S. D. Ha, Y. Zhou, F. Schoofs, and S. Ramanathan, “A cor- related nickelate synaptic transistor,” Nature communications, vol. 4, 2013. [2315] C. J. Wan, L. Q. Zhu, J. M. Zhou, Y. Shi, and Q. Wan, “Memory and learning behaviors mimicked in nanogranular sio 2-based proton conductor gated oxide-based synaptic transistors,” Nanoscale, vol. 5, no. 21, pp. 10 194–10 199, 2013. [2316] ——, “Inorganic proton conducting electrolyte coupled oxide-based dendritic transistors for synaptic electronics,” Nanoscale, vol. 6, no. 9, pp. 4491–4497, 2014. [2317] C. Wan, L. Zhu, Y. Liu, Y. Shi, and Q. Wan, “Laterally coupled synaptic transistors gated by proton conducting sodium alginate ﬁlms,” Electron Device Letters, IEEE, vol. 35, no. 6, pp. 672–674, 2014. [2318] X. Wan, P. Feng, G. D. Wu, Y. Shi, and Q. Wan, “Simulation of laterally coupled ingazno 4-based electric-double-layer transistors for synaptic electronics,” Electron Device Letters, IEEE, vol. 36, no. 2, pp. 204–206, 2015. [2319] X. Wan, Y. Yang, P. Feng, Y. Shi, and Q. Wan, “Short-term plasticity and synaptic ﬁltering emulated in electrolyte gated igzo transistors,” Electron Device Letters, IEEE, vol. PP, no. 99, pp. 1–1, 2016. [2320] C. Wan, Y. H. Liu, L. Q. Zhu, P. Feng, Y. Shi, and Q. Wan, “Short- term synaptic plasticity regulation in solution-gated indium-gallium- zinc-oxide electric-double-layer transistors,” ACS Applied Materials & Interfaces, 2016. [2321] J. Wang, Y. Li, C. Yin, Y. Yang, and T.-L. Ren, “Long-term depression mimicked in an igzo-based synaptic transistor,” IEEE Electron Device Letters, 2016. [2322] J. Zhou, N. Liu, L. Zhu, Y. Shi, and Q. Wan, “Energy-efﬁcient artiﬁ- cial synapses based on ﬂexible igzo electric-double-layer transistors,” Electron Device Letters, IEEE, vol. 36, no. 2, pp. 198–200, 2015. [2323] L. Q. Zhu, H. Xiao, Y. H. Liu, C. J. Wan, Y. Shi, and Q. Wan, “Multi- gate synergic modulation in laterally coupled synaptic transistors,” Applied Physics Letters, vol. 107, no. 14, p. 143502, 2015. [2324] L. Q. Zhu, C. J. Wan, P. Q. Gao, Y. H. Liu, H. Xiao, J. C. Ye, and Q. Wan, “Flexible proton-gated oxide synaptic transistors on si membrane,” ACS Applied Materials & Interfaces, vol. 8, no. 33, pp. 21 770–21 775, 2016. [2325] J. Zhou, C. Wan, L. Zhu, Y. Shi, and Q. Wan, “Synaptic behaviors mimicked in ﬂexible oxide-based transistors on plastic substrates,” Electron Device Letters, IEEE, vol. 34, no. 11, pp. 1433–1435, 2013. [2326] P. Gkoupidenis, N. Schaefer, B. Garlan, and G. G. Malliaras, “Neuro- morphic functions in pedot: Pss organic electrochemical transistors,” Advanced Materials, vol. 27, no. 44, pp. 7176–7180, 2015. [2327] P. Gkoupidenis, N. Schaefer, X. Strakosas, J. A. Fairﬁeld, and G. G. Malliaras, “Synaptic plasticity functions in an organic electrochemical transistor,” Applied Physics Letters, vol. 107, no. 26, p. 263302, 2015. [2328] C. Qian, J. Sun, L.-a. Kong, G. Gou, J. Yang, J. He, Y. Gao, and Q. Wan, “Artiﬁcial synapses based on in-plane gate organic electrochemical transistors,” ACS Applied Materials & Interfaces, vol. 8, no. 39, pp. 26 169–26 175, 2016. [2329] C. J. Wan, L. Q. Zhu, X. Wan, Y. Shi, and Q. Wan, “Organic/inorganic hybrid synaptic transistors gated by proton conducting methylcellulose ﬁlms,” Applied Physics Letters, vol. 108, no. 4, p. 043508, 2016. [2330] R. Wood, I. Bruce, and P. Mascher, “Modeling of spiking analog neural circuits with hebbian learning, using amorphous semiconductor thin ﬁlm transistors with silicon oxide nitride semiconductor split gates,” in Artiﬁcial Neural Networks and Machine Learning–ICANN 2012. Springer, 2012, pp. 89–96. [2331] W. Xu, S.-Y. Min, H. Hwang, and T.-W. Lee, “Organic core-sheath nanowire artiﬁcial synapses with femtojoule energy consumption,” Science advances, vol. 2, no. 6, p. e1501326, 2016. [2332] F. Alibart, S. Pleutin, D. Gu´erin, C. Novembre, S. Lenfant, K. Lmi- mouni, C. Gamrat, and D. Vuillaume, “An organic nanoparticle tran- sistor behaving as a biological spiking synapse,” Advanced Functional Materials, vol. 20, no. 2, pp. 330–337, 2010. [2333] F. Alibart, S. Pleutin, O. Bichler, C. Gamrat, T. Serrano-Gotarredona, B. Linares-Barranco, and D. Vuillaume, “A memristive nanoparti- cle/organic hybrid synapstor for neuroinspired computing,” Advanced Functional Materials, vol. 22, no. 3, pp. 609–616, 2012. [2334] O. Bichler, W. Zhao, F. Alibart, S. Pleutin, D. Vuillaume, and C. Gamrat, “Functional model of a nanoparticle organic memory transistor for use as a spiking synapse,” Electron Devices, IEEE Transactions on, vol. 57, no. 11, pp. 3115–3122, 2010. [2335] K.-C. Kwon, J.-S. Lee, C. G. Kim, and J.-G. Park, “Biological synapse behavior of nanoparticle organic memory ﬁeld effect transistor fabri- cated by curing,” Applied Physics Express, vol. 6, no. 6, p. 067001, 2013. [2336] Y. H. Liu, L. Q. Zhu, P. Feng, Y. Shi, and Q. Wan, “Freestanding artiﬁcial synapses based on laterally proton-coupled transistors on chitosan membranes,” Advanced Materials, vol. 27, no. 37, pp. 5599– 5604, 2015. [2337] G. Wu, J. Zhang, X. Wan, Y. Yang, and S. Jiang, “Chitosan-based biopolysaccharide proton conductors for synaptic transistors on paper substrates,” Journal of Materials Chemistry C, vol. 2, no. 31, pp. 6249–6255, 2014. [2338] G. Wu, C. Wan, J. Zhou, L. Zhu, and Q. Wan, “Low-voltage protonic/electronic hybrid indium zinc oxide synaptic transistors on paper substrates,” Nanotechnology, vol. 25, no. 9, p. 094001, 2014. [2339] G. Wu, P. Feng, X. Wan, L. Zhu, Y. Shi, and Q. Wan, “Artiﬁcial synaptic devices based on natural chicken albumen coupled electric- double-layer transistors,” Scientiﬁc Reports, vol. 6, 2016. [2340] J. Zhou, Y. Liu, Y. Shi, and Q. Wan, “Solution-processed chitosan- gated izo-based transistors for mimicking synaptic plasticity,” Electron Device Letters, IEEE, vol. 35, no. 2, pp. 280–282, 2014. [2341] M. Drouhard, C. D. Schuman, J. D. Birdwell, and M. E. Dean, “Visual analytics for neuroscience-inspired dynamic architectures,” in Foundations of Computational Intelligence (FOCI), 2014 IEEE Symposium on. IEEE, 2014, pp. 106–113. [2342] A. Disney, J. Reynolds, C. D. Schuman, A. Klibisz, A. Young, and J. S. Plank, “Danna: A neuromorphic software ecosystem,” Biologically Inspired Cognitive Architectures, vol. 17, pp. 49–56, 2016. 80 [2343] K. A. Boahen, “Communicating neuronal ensembles between neu- romorphic chips,” in Neuromorphic systems engineering. Springer, 1998, pp. 229–259. [2344] K. Boahen, “A throughput-on-demand address-event transmitter for neuromorphic chips,” in Advanced Research in VLSI, 1999. Proceed- ings. 20th Anniversary Conference on. IEEE, 1999, pp. 72–86. [2345] K. A. Boahen, “Point-to-point connectivity between neuromorphic chips using address events,” Circuits and Systems II: Analog and Digital Signal Processing, IEEE Transactions on, vol. 47, no. 5, pp. 416–434, 2000. [2346] M. Jablonski, T. Serrano-Gotarredona, and B. Linares-Barranco, “High-speed serial interfaces for event-driven neuromorphic systems,” in Event-based Control, Communication, and Signal Processing (EBCCSP), 2015 International Conference on. IEEE, 2015, pp. 1–4. [2347] S.-C. Liu, T. Delbruck, G. Indiveri, A. Whatley, and R. Douglas, “On-chip aer communication circuits,” Event-Based Neuromorphic Systems, pp. 285–303, 2015. [2348] P. A. Merolla, J. V. Arthur, B. E. Shi, and K. A. Boahen, “Expandable networks for neuromorphic chips,” Circuits and Systems I: Regular Papers, IEEE Transactions on, vol. 54, no. 2, pp. 301–311, 2007. [2349] S. Ramakrishnan, R. Wunderlich, J. Hasler, and S. George, “Neuron array with plastic synapses and programmable dendrites,” Biomedical Circuits and Systems, IEEE Transactions on, vol. 7, no. 5, pp. 631– 642, 2013. [2350] C. Zamarre˜no-Ramos, A. Linares-Barranco, T. Serrano-Gotarredona, and B. Linares-Barranco, “Multicasting mesh aer: a scalable assembly approach for reconﬁgurable neuromorphic structured aer systems. application to convnets,” Biomedical Circuits and Systems, IEEE Transactions on, vol. 7, no. 1, pp. 82–102, 2013. [2351] E. Chicca, A. M. Whatley, P. Lichtsteiner, V. Dante, T. Delbruck, P. Del Giudice, R. J. Douglas, and G. Indiveri, “A multichip pulse- based neuromorphic infrastructure and its application to a model of orientation selectivity,” Circuits and Systems I: Regular Papers, IEEE Transactions on, vol. 54, no. 5, pp. 981–993, 2007. [2352] R. Paz-Vicente, A. Linares-Barranco, D. Cascado, M. Rodriguez, G. Jimenez, A. Civit, and J. L. Sevillano, “Pci-aer interface for neuro- inspired spiking systems,” in Circuits and Systems, 2006. ISCAS 2006. Proceedings. 2006 IEEE International Symposium on. IEEE, 2006, pp. 4–pp. [2353] V. Thanasoulis, J. Partzsch, S. Hartmann, C. Mayr, and R. Schuffny, “Dedicated fpga communication architecture and design for a large- scale neuromorphic system,” in Electronics, Circuits and Systems (ICECS), 2012 19th IEEE International Conference on. IEEE, 2012, pp. 877–880. [2354] V. Thanasoulis, J. Partzsch, B. Vogginger, C. Mayr, and R. Schuffny, “Long-term pulse stimulation and recording in an accelerated neuro- morphic system,” in Electronics, Circuits and Systems (ICECS), 2012 19th IEEE International Conference on. IEEE, 2012, pp. 590–592. [2355] V. Thanasoulis, B. Vogginger, J. Partzsch, and R. Schuffny, “A pulse communication ﬂow ready for accelerated neuromorphic exper- iments,” in Circuits and Systems (ISCAS), 2014 IEEE International Symposium on. IEEE, 2014, pp. 265–268. [2356] V. Thanasoulis, J. Partzsch, B. Vogginger, and R. Schuffny, “Con- ﬁgurable pulse routing architecture for accelerated multi-node neuro- morphic systems,” in Electronics, Circuits and Systems (ICECS), 2014 21st IEEE International Conference on. IEEE, 2014, pp. 738–741. [2357] S. Scholze, H. Eisenreich, S. H¨oppner, G. Ellguth, S. Henker, M. Ander, S. H¨anzsche, J. Partzsch, C. Mayr, and R. Sch¨uffny, “A 32gbit/s communication soc for a waferscale neuromorphic system,” INTEGRATION, the VLSI journal, vol. 45, no. 1, pp. 61–75, 2012. [2358] S. Davies, J. Navaridas, F. Galluppi, and S. Furber, “Population- based routing in the spinnaker neuromorphic architecture,” in Neural Networks (IJCNN), The 2012 International Joint Conference on. IEEE, 2012, pp. 1–8. [2359] S. Furber, S. Temple, and A. Brown, “On-chip and inter-chip networks for modeling large-scale neural systems,” in Circuits and Systems, 2006. ISCAS 2006. Proceedings. 2006 IEEE International Symposium on. IEEE, 2006, pp. 4–pp. [2360] J. Navaridas, M. Luj´an, J. Miguel-Alonso, L. A. Plana, and S. Furber, “Understanding the interconnection network of spinnaker,” in Pro- ceedings of the 23rd international conference on Supercomputing. ACM, 2009, pp. 286–295. [2361] J. Navaridas, M. Luj´an, L. A. Plana, J. Miguel-Alonso, and S. B. Furber, “Analytical assessment of the suitability of multicast com- munications for the spinnaker neuromimetic system,” in High Per- formance Computing and Communication & 2012 IEEE 9th Inter- national Conference on Embedded Software and Systems (HPCC- ICESS), 2012 IEEE 14th International Conference on. IEEE, 2012, pp. 1–8. [2362] J. Navaridas, M. Luj´an, L. A. Plana, S. Temple, and S. B. Furber, “Spinnaker: Enhanced multicast routing,” Parallel Computing, vol. 45, pp. 49–66, 2015. [2363] C. Patterson, J. Garside, E. Painkras, S. Temple, L. A. Plana, J. Navaridas, T. Sharp, and S. Furber, “Scalable communications for a million-core neural processing architecture,” Journal of Parallel and Distributed Computing, vol. 72, no. 11, pp. 1507–1520, 2012. [2364] L. A. Plana, J. Bainbridge, S. Furber, S. Salisbury, Y. Shi, and J. Wu, “An on-chip and inter-chip communications network for the spinnaker massively-parallel neural net simulator,” in Proceedings of the second ACM/IEEE International Symposium on Networks-on-Chip. IEEE Computer Society, 2008, pp. 215–216. [2365] A. D. Rast, S. Yang, M. Khan, and S. B. Furber, “Virtual synaptic interconnect using an asynchronous network-on-chip,” in Neural Net- works, 2008. IJCNN 2008.(IEEE World Congress on Computational Intelligence). IEEE International Joint Conference on. IEEE, 2008, pp. 2727–2734. [2366] A. D. Rast, J. Navaridas, X. Jin, F. Galluppi, L. A. Plana, J. Miguel- Alonso, C. Patterson, M. Luj´an, and S. Furber, “Managing burstiness and scalability in event-driven models on the spinnaker neuromimetic system,” International Journal of Parallel Programming, vol. 40, no. 6, pp. 553–582, 2012. [2367] J. Park, T. Yu, C. Maier, S. Joshi, and G. Cauwenberghs, “Live demonstration: Hierarchical address-event routing architecture for reconﬁgurable large scale neuromorphic systems,” in Circuits and Systems (ISCAS), 2012 IEEE International Symposium on. IEEE, 2012, pp. 707–711. [2368] J. Park, T. Yu, S. Joshi, C. Maier, and G. Cauwenberghs, “Hierarchical address event routing for reconﬁgurable large-scale neuromorphic systems,” IEEE Transactions on Neural Networks and Learning Systems, 2016. [2369] S. Philipp, A. Gr¨ubl, K. Meier, and J. Schemmel, “Interconnecting vlsi spiking neural networks using isochronous connections,” in Computational and Ambient Intelligence. Springer, 2007, pp. 471– 478. [2370] S. Philipp, J. Schemmel, and K. Meier, “A qos network architecture to interconnect large-scale vlsi neural networks,” in Neural Networks, 2009. IJCNN 2009. International Joint Conference on. IEEE, 2009, pp. 2525–2532. [2371] S. R. Deiss, R. J. Douglas, A. M. Whatley et al., “A pulse-coded communications infrastructure for neuromorphic systems,” Pulsed neural networks, pp. 157–178, 1999. [2372] F. Sargeni and V. Bonaiuto, “An interconnection architecture for integrate and ﬁre neuromorphic multi-chip networks,” in Circuits and Systems, 2009. MWSCAS’09. 52nd IEEE International Midwest Symposium on. IEEE, 2009, pp. 877–880. [2373] A. Cassidy, T. Murray, A. G. Andreou, and J. Georgiou, “Evaluating on-chip interconnects for low operating frequency silicon neuron arrays,” in Circuits and Systems (ISCAS), 2011 IEEE International Symposium on. IEEE, 2011, pp. 2437–2440. [2374] A. Mortara and E. A. Vittoz, “A communication architecture tailored for analog vlsi artiﬁcial neural networks: intrinsic performance and limitations,” Neural Networks, IEEE Transactions on, vol. 5, no. 3, pp. 459–466, 1994. [2375] A. Mortara, E. A. Vittoz, and P. Venier, “A communication scheme for analog vlsi perceptive systems,” Solid-State Circuits, IEEE Journal of, vol. 30, no. 6, pp. 660–669, 1995. [2376] D. Vainbrand and R. Ginosar, “Network-on-chip architectures for neural networks,” in Proceedings of the 2010 Fourth ACM/IEEE International Symposium on Networks-on-Chip. IEEE Computer Society, 2010, pp. 135–144. [2377] ——, “Scalable network-on-chip architecture for conﬁgurable neural networks,” Microprocessors and Microsystems, vol. 35, no. 2, pp. 152–166, 2011. [2378] S. Pande, F. Morgan, G. Smit, T. Bruintjes, J. Rutgers, B. McGinley, S. Cawley, J. Harkin, and L. McDaid, “Fixed latency on-chip inter- connect for hardware spiking neural network architectures,” Parallel computing, vol. 39, no. 9, pp. 357–371, 2013. [2379] S. Pande, F. Morgan, S. Cawley, T. Bruintjes, G. Smit, B. McGinley, S. Carrillo, J. Harkin, and L. McDaid, “Modular neural tile architec- ture for compact embedded hardware spiking neural network,” Neural processing letters, vol. 38, no. 2, pp. 131–153, 2013. [2380] Y. Suzuki and L. Atlas, “A study of regular architectures for digital implementation of neural networks,” in Circuits and Systems, 1989., IEEE International Symposium on. IEEE, 1989, pp. 82–85. 81 [2381] R. Hasan and T. M. Taha, “On-chip static vs. dynamic routing for feed forward neural networks on multicore neuromorphic architectures,” in Advances in Electrical Engineering (ICAEE), 2013 International Conference on. IEEE, 2013, pp. 329–334. [2382] ——, “Routing bandwidth model for feed forward neural networks on multicore neuromorphic architectures,” in Neural Networks (IJCNN), The 2013 International Joint Conference on. IEEE, 2013, pp. 1–8. [2383] T. Duong, S. Kemeny, M. Tran, T. Daud, A. Thakoor, D. Ludwig, C. Saunders, and J. Carson, “Low power analog neurosynapse chips for a 3-d ?sugarcube? neuroprocessor,” in Neural Networks, 1994. IEEE World Congress on Computational Intelligence., 1994 IEEE International Conference on, vol. 3. IEEE, 1994, pp. 1907–1911. [2384] A. Bermak, “A highly scalable 3d chip for binary neural network classiﬁcation applications,” in Circuits and Systems, 2003. ISCAS’03. Proceedings of the 2003 International Symposium on, vol. 5. IEEE, 2003, pp. V–685. [2385] M. A. Ehsan, Z. Zhou, and Y. Yi, “Three dimensional integration tech- nology applied to neuromorphic hardware implementation,” in 2015 IEEE International Symposium on Nanoelectronic and Information Systems. IEEE, 2015, pp. 203–206. [2386] M. A. Ehsan, H. An, Z. Zhou, and Y. Yi, “Design challenges and methodologies in 3d integration for neuromorphic computing sys- tems,” in Quality Electronic Design (ISQED), 2016 17th International Symposium on. IEEE, 2016, pp. 24–28. [2387] M. A. Ehsan, Z. Zhou, and Y. Yi, “Modeling and optimization of tsv for crosstalk mitigation in 3d neuromorphic system,” in Electromag- netic Compatibility (EMC), 2016 IEEE International Symposium on. IEEE, 2016, pp. 621–626. [2388] A. Joubert, M. Duranton, B. Belhadj, O. Temam, and R. H´eliot, “Capacitance of tsvs in 3-d stacked chips a problem?: not for neuromorphic systems!” in Proceedings of the 49th Annual Design Automation Conference. ACM, 2012, pp. 1264–1265. [2389] B. Belhadj, A. Valentian, P. Vivet, M. Duranton, L. He, and O. Temam, “The improbable but highly appropriate marriage of 3d stacking and neuromorphic accelerators,” in Compilers, Architecture and Synthesis for Embedded Systems (CASES), 2014 International Conference on. IEEE, 2014, pp. 1–9. [2390] H. Li, K.-S. Li, C.-H. Lin, J.-L. Hsu, W.-C. Chiu, M.-C. Chen, T.- T. Wu, J. Sohn, S. B. Eryilmaz, J.-M. Shieh et al., “Four-layer 3d vertical rram integrated with ﬁnfet as a versatile computing unit for brain-inspired cognitive information processing,” in VLSI Technology, 2016 IEEE Symposium on. IEEE, 2016, pp. 1–2. [2391] I.-T. Wang, Y.-C. Lin, Y.-F. Wang, C.-W. Hsu, and T.-H. Hou, “3d synaptic architecture with ultralow sub-10 fj energy per spike for neuromorphic computation,” in Electron Devices Meeting (IEDM), 2014 IEEE International. IEEE, 2014, pp. 28–5. [2392] I.-T. Wang, C.-C. Chang, L.-W. Chiu, T. Chou, and T.-H. Hou, “3d ta/tao x/tio2/ti synaptic array and linearity tuning of weight update for hardware neural network applications,” Nanotechnology, vol. 27, no. 36, p. 365204, 2016. [2393] G. Piccolboni, G. Molas, J. Portal, R. Coquand, M. Bocquet, D. Garbin, E. Vianello, C. Carabasse, V. Delaye, C. Pellissier et al., “Investigation of the potentialities of vertical resistive ram (vrram) for neuromorphic applications,” in 2015 IEEE International Electron Devices Meeting (IEDM). IEEE, 2015, pp. 17–2. [2394] K. Ryan, S. Tanachutiwat, and W. Wang, “3d cmol crossnet for neuromorphic network applications,” in Nano-Net. Springer, 2009, pp. 1–5. [2395] A. Achyuthan and M. I. Elmasry, “Mixed analog/digital hardware synthesis of artiﬁcial neural networks,” Computer-Aided Design of Integrated Circuits and Systems, IEEE Transactions on, vol. 13, no. 9, pp. 1073–1087, 1994. [2396] ——, “A design automation environment for mixed analog/digital anns,” in VLSI Artiﬁcial Neural Networks Engineering. Springer, 1994, pp. 91–137. [2397] A. L. Braga, C. H. Llanos, M. Ayala-Rinc´on, and R. P. Jacobi, “Vanngen: a ﬂexible cad tool for hardware implementation of artiﬁcial neural networks,” in Reconﬁgurable Computing and FPGAs, 2005. ReConFig 2005. International Conference on. IEEE, 2005, pp. 8– pp. [2398] D. J. Chen and B. J. Sheu, “Automatic layout generation for mixed analog-digital vlsi neural chips,” in Computer Design: VLSI in Com- puters and Processors, 1990. ICCD’90. Proceedings, 1990 IEEE International Conference on. IEEE, 1990, pp. 29–32. [2399] H. Mostafa, F. Corradi, M. Osswald, and G. Indiveri, “Automated synthesis of asynchronous event-based interfaces for neuromorphic systems,” in Circuit Theory and Design (ECCTD), 2013 European Conference on. IEEE, 2013, pp. 1–4. [2400] M. E. Nigri and P. C. Treleaven, “High level synthesis of neural network chips,” in New Trends in Neural Computation. Springer, 1993, pp. 448–453. [2401] T. C. Stewart and C. Eliasmith, “Large-scale synthesis of functional spiking neural circuits,” Proceedings of the IEEE, vol. 102, no. 5, pp. 881–898, 2014. [2402] Y. Dong, Y. Wang, Z. Lin, and T. Watanabe, “High performance and low latency mapping for neural network into network on chip architecture,” in ASIC, 2009. ASICON’09. IEEE 8th International Conference on. IEEE, 2009, pp. 891–894. [2403] M. Ehrlich, K. Wendt, L. Z¨uhl, R. Sch¨uffny, D. Br¨uderle, E. M¨uller, and B. Vogginger, “A software framework for mapping neural net- works to a wafer-scale neuromorphic hardware system.” in ANNIIP, 2010, pp. 43–52. [2404] F. Galluppi, S. Davies, S. Furber, T. Stewart, and C. Eliasmith, “Real time on-chip implementation of dynamical systems with spiking neurons,” in Neural Networks (IJCNN), The 2012 International Joint Conference on. IEEE, 2012, pp. 1–8. [2405] F. Galluppi, S. Davies, A. Rast, T. Sharp, L. A. Plana, and S. Furber, “A hierachical conﬁguration system for a massively parallel neural hardware platform,” in Proceedings of the 9th conference on Com- puting Frontiers. ACM, 2012, pp. 183–192. [2406] P. Gao, B. V. Benjamin, and K. Boahen, “Dynamical system guided mapping of quantitative neuronal models onto neuromorphic hard- ware,” Circuits and Systems I: Regular Papers, IEEE Transactions on, vol. 59, no. 10, pp. 2383–2394, 2012. [2407] Y. Ji, Y. Zhang, S. Li, P. Chi, C. Jiang, P. Qu, Y. Xie, and W. Chen, “Neutrams: Neural network transformation and co-design under neuromorphic hardware constraints,” in Microarchitecture (MI- CRO), 2016 49th Annual IEEE/ACM International Symposium on. IEEE, 2016, pp. 1–13. [2408] Y. Ji, Y. Zhang, H. Liu, and W. Zheng, “Optimized mapping spiking neural networks onto network-on-chip,” in Algorithms and Architec- tures for Parallel Processing. Springer, 2016, pp. 38–52. [2409] C. Mayr, M. Ehrlich, S. Henker, K. Wendt, and R. Sch¨uffny, “Mapping complex, large-scale spiking networks on neural vlsi,” Int. J. Appl. Sci. Eng. Technol, vol. 4, pp. 37–42, 2007. [2410] M. Nigri, P. Treleaven, and M. Vellasco, “Silicon compilation of neural networks,” in CompEuro’91. Advanced Computer Technology, Reliable Systems and Applications. 5th Annual European Computer Conference. Proceedings. IEEE, 1991, pp. 541–546. [2411] R. E. Pino, G. Genello, M. Bishop, M. J. Moore, and R. Linderman, “Emerging neuromorphic computing architectures & enabling hard- ware for cognitive information processing applications,” in Cognitive Information Processing (CIP), 2010 2nd International Workshop on. IEEE, 2010, pp. 35–39. [2412] G. Urgese, F. Barchi, E. Macii, and A. Acquaviva, “Optimizing network trafﬁc for spiking neural network simulations on densely in- terconnected many-core neuromorphic platforms,” IEEE Transactions on Emerging Topics in Computing, 2016. [2413] Y. Zhang, Y. Ji, W. Chen, and Y. Xie, “Neural network transformation under hardware constraints,” in Compliers, Architectures, and Sythesis of Embedded Systems (CASES), 2016 International Conference on. IEEE, 2016, pp. 1–1. [2414] A. D. Brown, S. B. Furber, J. S. Reeve, J. D. Garside, K. J. Dugan, L. A. Plana, and S. Temple, “Spinnaker?programming model,” Computers, IEEE Transactions on, vol. 64, no. 6, pp. 1769–1782, 2015. [2415] D. Br¨uderle, A. Gr¨ubl, K. Meier, E. Mueller, and J. Schemmel, “A software framework for tuning the dynamics of neuromorphic silicon towards biology,” in Computational and Ambient Intelligence. Springer, 2007, pp. 479–486. [2416] N. Kasabov, N. Scott, E. Tu, S. Marks, N. Sengupta, E. Capecci, M. Othman, M. G. Doborjeh, N. Murli, J. I. Espinosa-Ramos et al., “Evolving spatio-temporal data machines based on the neucube neuro- morphic framework: Design methodology and selected applications,” Neural Networks, 2015. [2417] M. S. Kulkarni and C. Teuscher, “Memristor-based reservoir com- puting,” in Nanoscale Architectures (NANOARCH), 2012 IEEE/ACM International Symposium on. IEEE, 2012, pp. 226–232. [2418] O. Neopane, S. Das, E. Arias-Castro, and K. Kreutz-Delgado, “A nonparametric framework for quantifying generative inference on neuromorphic systems,” in Circuits and Systems (ISCAS), 2016 IEEE International Symposium on. IEEE, 2016, pp. 1346–1349. 82 [2419] N. Scott, N. Kasabov, and G. Indiveri, “Neucube neuromorphic frame- work for spatio-temporal brain data and its python implementation,” in Neural Information Processing. Springer, 2013, pp. 78–84. [2420] S. Sheik, F. Stefanini, E. Neftci, E. Chicca, and G. Indiveri, “System- atic conﬁguration and automatic tuning of neuromorphic systems,” in Circuits and Systems (ISCAS), 2011 IEEE International Symposium on. IEEE, 2011, pp. 873–876. [2421] J. R. Shinde and S. Salankar, “Multi-objective optimization for vlsi implementation of artiﬁcial neural network,” in Advances in Comput- ing, Communications and Informatics (ICACCI), 2015 International Conference on. IEEE, 2015, pp. 1694–1700. [2422] O. Temam and R. Heliot, “Implementation of signal processing tasks on neuromorphic hardware,” in Neural Networks (IJCNN), The 2011 International Joint Conference on. IEEE, 2011, pp. 1120–1125. [2423] W. Wen, C.-R. Wu, X. Hu, B. Liu, T.-Y. Ho, X. Li, and Y. Chen, “An eda framework for large scale hybrid neuromorphic computing systems,” in Proceedings of the 52nd Annual Design Automation Conference. ACM, 2015, p. 12. [2424] M. R. Wilby, A. B. R. Gonz´alez, J. J. V. D´ıaz, and J. R. Carri´on, “Neuromorphic sensor network platform: a bioinspired tool to grow applications in wireless sensor networks,” International Journal of Distributed Sensor Networks, vol. 2015, p. 93, 2015. [2425] A. Amir, P. Datta, W. P. Risk, A. S. Cassidy, J. Kusnitz, S. K. Esser, A. Andreopoulos, T. M. Wong, M. Flickner, R. Alvarez-Icaza et al., “Cognitive computing programming paradigm: a corelet language for composing networks of neurosynaptic cores,” in Neural Networks (IJCNN), The 2013 International Joint Conference on. IEEE, 2013, pp. 1–10. [2426] A. P. Davison, D. Br¨uderle, J. Eppler, J. Kremkow, E. Muller, D. Pecevski, L. Perrinet, and P. Yger, “Pynn: a common interface for neuronal network simulators,” Frontiers in neuroinformatics, vol. 2, 2008. [2427] A. Davison, E. Muller, D. Br¨uderle, and J. Kremkow, “A common lan- guage for neuronal networks in software and hardware,” Neuromorph. Eng, 2010. [2428] F. Stefanini, E. O. Neftci, S. Sheik, and G. Indiveri, “Pyncs: a mi- crokernel for high-level deﬁnition and conﬁguration of neuromorphic electronic systems,” Frontiers in neuroinformatics, vol. 8, 2014. [2429] A. Hashmi, A. Nere, J. J. Thomas, and M. Lipasti, “A case for neuromorphic isas,” in ACM SIGARCH Computer Architecture News, vol. 39, no. 1. ACM, 2011, pp. 145–158. [2430] M. Ad´e, R. Lauwereins, and J. Peperstraete, “A fast simulator for neural networks on dsps or fpgas,” in Neural Networks for Signal Processing [1992] II., Proceedings of the 1992 IEEE-SP Workshop. IEEE, 1992, pp. 597–605. [2431] L. E. Atlas and Y. Suzuki, “Digital systems for artiﬁcial neural networks,” IEEE Circuits and Devices;(USA), vol. 5, no. 6, 1989. [2432] F. Galluppi, A. Rast, S. Davies, and S. Furber, “A general-purpose model translation system for a universal neural chip,” in Neural information processing. Theory and algorithms. Springer, 2010, pp. 58–65. [2433] Y. Ji, Y.-H. Zhang, and W.-M. Zheng, “Modelling spiking neural network from the architecture evaluation perspective,” Journal of Computer Science and Technology, vol. 31, no. 1, pp. 50–59, 2016. [2434] M. Khalil-Hani, V. P. Nambiar, and M. Marsono, “Co-simulation methodology for improved design and veriﬁcation of hardware neural networks,” in Industrial Electronics Society, IECON 2013-39th Annual Conference of the IEEE. IEEE, 2013, pp. 2226–2231. [2435] M. Kolasa, R. Dlugosz, and W. Pedrycz, “Problem of efﬁcient initialization of large self-organizing maps implemented in the cmos technology,” in Cybernetics (CYBCONF), 2015 IEEE 2nd Interna- tional Conference on. IEEE, 2015, pp. 36–41. [2436] M. Kolasa and R. Dlugosz, “An advanced software model for opti- mization of self-organizing neural networks oriented on implementa- tion in hardware,” in Mixed Design of Integrated Circuits & Systems (MIXDES), 2015 22nd International Conference. IEEE, 2015, pp. 266–271. [2437] M. Plagge, C. D. Carothers, and E. Gonsiorowski, “Nemo: A massively parallel discrete-event simulation model for neuromorphic architectures,” in Proceedings of the 2016 annual ACM Conference on SIGSIM Principles of Advanced Discrete Simulation. ACM, 2016, pp. 233–244. [2438] H. E. Plesser, J. M. Eppler, A. Morrison, M. Diesmann, and M.- O. Gewaltig, “Efﬁcient parallel simulation of large-scale neuronal networks on clusters of multiprocessor computers,” in Euro-Par 2007 parallel processing. Springer, 2007, pp. 672–681. [2439] R. Preissl, T. M. Wong, P. Datta, M. Flickner, R. Singh, S. K. Esser, W. P. Risk, H. D. Simon, and D. S. Modha, “Compass: a scalable simulator for an architecture for cognitive computing,” in Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis. IEEE Computer Society Press, 2012, p. 54. [2440] L. Xia, B. Li, T. Tang, P. Gu, X. Yin, W. Huangfu, P.-Y. Chen, S. Yu, Y. Cao, Y. Wang et al., “Mnsim: Simulation platform for memristor- based neuromorphic computing system,” in 2016 Design, Automation & Test in Europe Conference & Exhibition (DATE). IEEE, 2016, pp. 469–474. [2441] J. P. Dominguez-Morales, A. Jimenez-Fernandez, M. Dominguez- Morales, and G. Jimenez-Moreno, “Navis: Neuromorphic auditory visualizer tool,” Neurocomputing, 2016. [2442] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation with deep convolutional neural networks,” in Advances in neural information processing systems, 2012, pp. 1097–1105. [2443] A. Graves, A.-r. Mohamed, and G. Hinton, “Speech recognition with deep recurrent neural networks,” in Acoustics, speech and signal processing (icassp), 2013 ieee international conference on. IEEE, 2013, pp. 6645–6649. [2444] L. Prechelt et al., “Proben1: A set of neural network benchmark problems and benchmarking rules,” 1994. [2445] M. T. Hagan, H. B. Demuth, and O. D. Jes´us, “An introduction to the use of neural networks in control systems,” International Journal of Robust and Nonlinear Control, vol. 12, no. 11, pp. 959–985, 2002. [2446] J. Ryan, M.-J. Lin, and R. Miikkulainen, “Intrusion detection with neural networks,” Advances in neural information processing systems, pp. 943–949, 1998. [2447] S. V. Adams, A. D. Rast, C. Patterson, F. Galluppi, K. Brohan, J.-A. P´erez-Carrasco, T. Wennekers, S. Furber, and A. Cangelosi, “Towards real-world neurorobotics: integrated neuromorphic visual attention,” in Neural Information Processing. Springer, 2014, pp. 563–570. [2448] A. G. Andreou, A. A. Dykman, K. D. Fischl, G. Garreau, D. R. Mendat, G. Orchard, A. S. Cassidy, P. Merolla, J. Arthur, R. Alvarez- Icaza et al., “Real-time sensory information processing using the truenorth neurosynaptic system,” in Circuits and Systems (ISCAS), 2016 IEEE International Symposium on. IEEE, 2016, pp. 2911– 2911. [2449] F. Barranco, C. Fermuller, Y. Aloimonos, and T. Delbruck, “A dataset for visual navigation with neuromorphic methods,” Frontiers in neuroscience, vol. 10, 2016. [2450] C. Bartolozzi, F. Rea, C. Clercq, M. Hofst¨atter, D. B. Fasnacht, G. In- diveri, and G. Metta, “Embedded neuromorphic vision for humanoid robots,” in Computer Vision and Pattern Recognition Workshops (CVPRW), 2011 IEEE Computer Society Conference on. IEEE, 2011, pp. 129–135. [2451] C. Bartolozzi, C. Clercq, N. Mandloi, F. Rea, G. Indiveri, D. Fas- nacht, G. Metta, M. Hofst¨atter, and R. Benosman, “emorph: Towards neuromorphic robotic vision,” Procedia Computer Science, vol. 7, pp. 163–165, 2011. [2452] V. Beˇcanovi´c, R. Hosseiny, and G. Indiveri, “Object tracking using multiple neuromorphic vision sensors,” in RoboCup 2004: Robot Soccer World Cup VIII. Springer, 2005, pp. 426–433. [2453] K. Boahen, “Neuromorphic microchips,” Scientiﬁc American, vol. 292, no. 5, pp. 56–63, 2005. [2454] C.-T. Chiang and C.-Y. Wu, “Implantable neuromorphic vision chips,” Electronics Letters, vol. 40, no. 6, pp. 361–363, 2004. [2455] L. Chotard, X. Lagorce, and C. Posch, “Ultra-low bandwidth video streaming using a neuromorphic, scene-driven image sensor,” in Circuits and Systems (ISCAS), 2016 IEEE International Symposium on. IEEE, 2016, pp. 455–455. [2456] L. Chua, Memristor, Hodgkin-Huxley, and edge of chaos. Springer, 2014. [2457] J. Cosp, J. Madrenas, and D. Fernandez, “Design and basic blocks of a neuromorphic vlsi analogue vision system,” Neurocomputing, vol. 69, no. 16, pp. 1962–1970, 2006. [2458] J. Costas-Santos, T. Serrano-Gotarredona, R. Serrano-Gotarredona, and B. Linares-Barranco, “A spatial contrast retina with on-chip calibration for neuromorphic spike-based aer vision systems,” Circuits and Systems I: Regular Papers, IEEE Transactions on, vol. 54, no. 7, pp. 1444–1458, 2007. [2459] E. Culurciello, R. Etienne-Cummings, and K. Boahen, “Arbitrated address-event representation digital image sensor,” Electronics Letters, vol. 37, no. 24, pp. 1443–1445, 2001. 83 [2460] E. Culurciello, R. Etienne-Cummings, and K. A. Boahen, “A biomor- phic digital image sensor,” Solid-State Circuits, IEEE Journal of, vol. 38, no. 2, pp. 281–294, 2003. [2461] V. Dante, P. Del Giudice, and M. Mattia, “Implementation of neuro- morphic systems: from discrete components to analog vlsi chips (test- ing and communication issues),” ANNALI-ISTITUTO SUPERIORE DI SANITA, vol. 37, no. 2, pp. 231–240, 2001. [2462] T. Delbruck and P. Lichtsteiner, “Fast sensory motor control based on event-based hybrid neuromorphic-procedural system,” in Circuits and Systems, 2007. ISCAS 2007. IEEE International Symposium on. IEEE, 2007, pp. 845–848. [2463] R. Douglas, M. Mahowald, and C. Mead, “Neuromorphic analogue vlsi,” Annual review of neuroscience, vol. 18, pp. 255–281, 1995. [2464] F. Eibensteiner, J. Kogler, C. Sulzbachner, and J. Scharinger, “Stereo- vision algorithm based on bio-inspired silicon retinas for implemen- tation in hardware,” in Computer Aided Systems Theory–EUROCAST 2011. Springer, 2012, pp. 624–631. [2465] R. Etienne-Cummings, S. Mehta, R. Philipp, and V. Gruev, “Neuro- morphic vision systems for mobile applications,” in Custom Integrated Circuits Conference, 2006. CICC’06. IEEE. IEEE, 2006, pp. 531– 534. [2466] C. Farabet, B. Martini, P. Akselrod, S. Talay, Y. LeCun, and E. Cu- lurciello, “Hardware accelerated convolutional neural networks for synthetic vision systems,” in Circuits and Systems (ISCAS), Proceed- ings of 2010 IEEE International Symposium on. IEEE, 2010, pp. 257–260. [2467] M. Firouzi and J. Conradt, “Asynchronous event-based cooperative stereo matching using neuromorphic silicon retinas,” Neural Process- ing Letters, pp. 1–16, 2015. [2468] ——, “Asynchronous event-based cooperative stereo matching using neuromorphic silicon retinas,” Neural Processing Letters, vol. 43, no. 2, pp. 311–326, 2016. [2469] F. Galluppi, K. Brohan, S. Davidson, T. Serrano-Gotarredona, J.-A. P. Carrasco, B. Linares-Barranco, and S. Furber, “A real-time, event- driven neuromorphic system for goal-directed attentional selection,” in Neural Information Processing. Springer, 2012, pp. 226–233. [2470] B. Gao, J. Kang, Z. Zhou, Z. Chen, P. Huang, L. Liu, and X. Liu, “Metal oxide resistive random access memory based synaptic devices for brain-inspired computing,” Japanese Journal of Applied Physics, vol. 55, no. 4S, p. 04EA06, 2016. [2471] W. J. Han and I. S. Han, “Bio-inspired visual information processing– the neuromorphic approach,” in Bio-Inspired Models of Network, Information, and Computing Systems. Springer, 2012, pp. 621–628. [2472] I. S. Han and W.-S. Han, “Application of neuromorphic visual process- ing in pedestrian detection technology,” in Science and Information Conference (SAI), 2013. IEEE, 2013, pp. 536–541. [2473] W.-S. Han and I.-S. Han, “All weather human detection using neu- romorphic visual processing,” in Intelligent Systems for Science and Information. Springer, 2014, pp. 25–44. [2474] ——, “Application of neuromorphic visual processing in railway en- vironment,” in Industrial Technology (ICIT), 2015 IEEE International Conference on. IEEE, 2015, pp. 1639–1643. [2475] W. Han and I. Han, “Neuromorphic visual information processing for vulnerable road user detection and driver monitoring,” in SAI Intelligent Systems Conference (IntelliSys), 2015. IEEE, 2015, pp. 798–803. [2476] ——, “Neuromorphic visual object detection for enhanced driving safety,” in Science and Information Conference (SAI), 2015. IEEE, 2015, pp. 721–726. [2477] W.-S. Han and I. S. Han, “Enhanced neuromorphic visual processing by segmented neuron for intelligent vehicle,” in SAI Computing Conference (SAI), 2016. IEEE, 2016, pp. 307–311. [2478] C. Huyck, C. Evans, and I. Mitchell, “A comparison of simple agents implemented in simulated neurons,” Biologically Inspired Cognitive Architectures, vol. 12, pp. 9–19, 2015. [2479] G. Indiveri, L. Raffo, S. P. Sabatini, and G. M. Bisio, “A neuromorphic architecture for cortical multilayer integration of early visual tasks,” Machine vision and applications, vol. 8, no. 5, pp. 305–314, 1995. [2480] G. Indiveri, R. Murer, and J. Kramer, “Active vision using an analog vlsi model of selective attention,” Circuits and Systems II: Analog and Digital Signal Processing, IEEE Transactions on, vol. 48, no. 5, pp. 492–500, 2001. [2481] G. Indiveri, “Neuromorphic selective attention systems,” in Circuits and Systems, 2003. ISCAS’03. Proceedings of the 2003 International Symposium on, vol. 3. IEEE, 2003, pp. III–770. [2482] ——, “Neuromorphic vlsi models of selective attention: from single chip vision sensors to multi-chip systems,” Sensors, vol. 8, no. 9, pp. 5352–5375, 2008. [2483] O. Kavehei and E. Skaﬁdas, “Highly scalable neuromorphic hardware with 1-bit stochastic nano-synapses,” in Circuits and Systems (ISCAS), 2014 IEEE International Symposium on. IEEE, 2014, pp. 1648–1651. [2484] T. Kawasetsu, R. Ishida, T. Sanada, and H. Okuno, “A hardware system for emulating the early vision utilizing a silicon retina and spinnaker chips,” in Biomedical Circuits and Systems Conference (BioCAS), 2014 IEEE. IEEE, 2014, pp. 552–555. [2485] J. Kramer and G. Indiveri, “Neuromorphic vision sensors and pre- processors in system applications,” in SYBEN-Broadband European Networks and Electronic Image Capture and Publishing. Interna- tional Society for Optics and Photonics, 1998, pp. 134–146. [2486] Y. Kudo, Y. Hayashida, R. Ishida, H. Okuno, and T. Yagi, “A retino- morphic hardware system simulating the graded and action potentials in retinal neuronal layers,” in International Conference on Neural Information Processing. Springer, 2016, pp. 326–333. [2487] P. Lichtsteiner, C. Posch, and T. Delbruck, “A 128× 128 120 db 15 µs latency asynchronous temporal contrast vision sensor,” Solid-State Circuits, IEEE Journal of, vol. 43, no. 2, pp. 566–576, 2008. [2488] J.-H. Liu, C.-Y. Wang, and Y.-Y. An, “A survey of neuromorphic vision system:–biological nervous systems realized on silicon,” in Industrial Mechatronics and Automation, 2009. ICIMA 2009. Inter- national Conference on. IEEE, 2009, pp. 154–157. [2489] S.-C. Liu and T. Delbruck, “Neuromorphic sensory systems,” Current opinion in neurobiology, vol. 20, no. 3, pp. 288–295, 2010. [2490] Z. Luo, H. Liu, and X. Wu, “Artiﬁcial neural network computation on graphic process unit,” in Neural Networks, 2005. IJCNN’05. Proceedings. 2005 IEEE International Joint Conference on, vol. 1. IEEE, 2005, pp. 622–626. [2491] M. Mahowald, Evolving analog VLSI neurons. Chap, 1992, vol. 15. [2492] J. N. Martel and Y. Sandamirskaya, “A neuromorphic approach for tracking using dynamic neural ﬁelds on a programmable vision-chip,” in Proceedings of the 10th International Conference on Distributed Smart Camera. ACM, 2016, pp. 148–154. [2493] J. N. Martel, Y. Sandamirskaya, and P. Dudek, “A demonstration of tracking using dynamic neural ﬁelds on a programmable vision chip: Demo,” in Proceedings of the 10th International Conference on Distributed Smart Camera. ACM, 2016, pp. 212–213. [2494] Y. Meng and B. E. Shi, “Adaptive gain control for spike-based map communication in a neuromorphic vision system,” Neural Networks, IEEE Transactions on, vol. 19, no. 6, pp. 1010–1021, 2008. [2495] A. Mishra, R. Ghosh, A. Goyal, N. V. Thakor, and S. L. Kukreja, “Real-time robot tracking and following with neuromorphic vision sensor,” in Biomedical Robotics and Biomechatronics (BioRob), 2016 6th IEEE International Conference on. IEEE, 2016, pp. 13–18. [2496] E. Mueller, A. Censi, and E. Frazzoli, “Low-latency heading feedback control with neuromorphic vision sensors using efﬁcient approximated incremental inference,” in Decision and Control (CDC), 2015 IEEE 54th Annual Conference on. IEEE, 2015, pp. 992–999. [2497] ——, “Efﬁcient high speed signal estimation with neuromorphic vision sensors,” in Event-based Control, Communication, and Signal Processing (EBCCSP), 2015 International Conference on. IEEE, 2015, pp. 1–8. [2498] T. Netter and N. Francheschini, “A robotic aircraft that follows terrain using a neuromorphic eye,” in Intelligent Robots and Systems, 2002. IEEE/RSJ International Conference on, vol. 1. IEEE, 2002, pp. 129– 134. [2499] H. Okuno, T. Kawasetsu, L. A. Plana, S. B. Furber, and T. Yagi, “A real-time simulator of a biological visual system composed of a silicon retina and spinnaker chips,” in Proc. of the International Symposium on Artiﬁcial Life and Robotics 2014. Citeseer, 2014, pp. 347–350. [2500] M. Osswald, S.-H. Ieng, R. Benosman, and G. Indiveri, “A spiking neural network model of 3d perception for event-based neuromorphic stereo vision systems,” Scientiﬁc Reports, vol. 7, 2017. [2501] Y. Sandamirskaya, “Dynamic neural ﬁelds as a step toward cognitive neuromorphic architectures,” Frontiers in neuroscience, vol. 7, 2013. [2502] R. Serrano-Gotarredona, T. Serrano-Gotarredona, A. Acosta-Jim´enez, and B. Linares-Barranco, “A neuromorphic cortical-layer microchip for spike-based event processing vision systems,” Circuits and Systems I: Regular Papers, IEEE Transactions on, vol. 53, no. 12, pp. 2548– 2566, 2006. [2503] R. Serrano-Gotarredona, M. Oster, P. Lichtsteiner, A. Linares- Barranco, R. Paz-Vicente, F. G´omez-Rodr´ıguez, L. Camu˜nas-Mesa, R. Berner, M. Rivas-P´erez, T. Delbruck et al., “Caviar: A 45k neu- ron, 5m synapse, 12g connects/s aer hardware sensory–processing– 84 learning–actuating system for high-speed visual object recognition and tracking,” Neural Networks, IEEE Transactions on, vol. 20, no. 9, pp. 1417–1438, 2009. [2504] K. Shimonomura and T. Yagi, “Neuromorphic vlsi vision system for real-time texture segregation,” Neural Networks, vol. 21, no. 8, pp. 1197–1204, 2008. [2505] P. Singh, S. Z. Yong, J. Gregoire, A. Censi, and E. Frazzoli, “Stabi- lization of linear continuous-time systems using neuromorphic vision sensors,” in Decision and Control (CDC), 2016 IEEE 55th Conference on. IEEE, 2016, pp. 3030–3036. [2506] G. Snider, R. Amerson, D. Carter, H. Abdalla, M. S. Qureshi, J. L´eveill´e, M. Versace, H. Ames, S. Patrick, B. Chandler et al., “From synapses to circuitry: Using memristive memory to explore the electronic brain,” Computer, vol. 44, no. 2, p. 21, 2011. [2507] D. Sonnleithner and G. Indiveri, “A neuromorphic saliency-map based active vision system,” in Information Sciences and Systems (CISS), 2011 45th Annual Conference on. IEEE, 2011, pp. 1–6. [2508] C. Tan, S. Lallee, and G. Orchard, “Benchmarking neuromorphic vi- sion: lessons learnt from computer vision,” Frontiers in neuroscience, vol. 9, 2015. [2509] T. Yagi, “Quest for visual system of the brain to create artiﬁcial vision,” in Future of Electron Devices, Kansai (IMFEDK), 2016 IEEE International Meeting for. IEEE, 2016, pp. 1–2. [2510] Z. Yang, A. Murray, F. Worgotter, K. Cameron, and V. Boonsobhak, “A neuromorphic depth-from-motion vision model with stdp adap- tation,” Neural Networks, IEEE Transactions on, vol. 17, no. 2, pp. 482–495, 2006. [2511] K. A. Zaghloul and K. Boahen, “Optic nerve signals in a neuromor- phic chip i: Outer and inner retina models,” Biomedical Engineering, IEEE Transactions on, vol. 51, no. 4, pp. 657–666, 2004. [2512] K. Zaghloul, K. Boahen et al., “Optic nerve signals in a neuromorphic chip ii: Testing and results,” Biomedical Engineering, IEEE Transac- tions on, vol. 51, no. 4, pp. 667–675, 2004. [2513] C. Zamarre˜no-Ramos, L. A. Camu˜nas-Mesa, J. A. P´erez-Carrasco, T. Masquelier, T. Serrano-Gotarredona, and B. Linares-Barranco, “On spike-timing-dependent-plasticity, memristive devices, and building a self-learning visual cortex,” Frontiers in neuroscience, vol. 5, 2011. [2514] J. P. Dominguez-Morales, A. Jimenez-Fernandez, A. Rios-Navarro, E. Cerezuela-Escudero, D. Gutierrez-Galan, M. J. Dominguez- Morales, and G. Jimenez-Moreno, “Multilayer spiking neural network for audio samples classiﬁcation using spinnaker,” in International Conference on Artiﬁcial Neural Networks. Springer, 2016, pp. 45–53. [2515] T. J. Koickal, R. Latif, L. Gouveia, E. Mastropaolo, S. Wang, A. Hamilton, R. Cheung, M. Newton, and L. Smith, “Design of a spike event coded rgt microphone for neuromorphic auditory systems,” in Circuits and Systems (ISCAS), 2011 IEEE International Symposium on. IEEE, 2011, pp. 2465–2468. [2516] J. Lazzaro, J. Wawrzynek, M. Mahowald, M. Sivilotti, and D. Gille- spie, “Silicon auditory processors as computer peripherals,” Neural Networks, IEEE Transactions on, vol. 4, no. 3, pp. 523–528, 1993. [2517] R. Sarpeshkar, “Brain power-borrowing from biology makes for low power computing [bionic ear],” Spectrum, IEEE, vol. 43, no. 5, pp. 24–29, 2006. [2518] B. J. Sheu and J. Choi, “Design methodologies of vlsi neural net- works,” in Neural Information Processing and VLSI. Springer, 1995, pp. 185–220. [2519] L. S. Smith, “Toward a neuromorphic microphone,” Frontiers in neuroscience, vol. 9, 2015. [2520] A. Van Schaik, E. Fragni`ere, and E. Vittoz, “An analogue electronic model of ventral cochlear nucleus neurons,” in Microelectronics for Neural Networks, 1996., Proceedings of Fifth International Confer- ence on. IEEE, 1996, pp. 52–59. [2521] K. Voutsas and J. Adamy, “A biologically inspired spiking neural network for sound source lateralization,” Neural Networks, IEEE Transactions on, vol. 18, no. 6, pp. 1785–1799, 2007. [2522] H.-Y. Hsieh and K.-T. Tang, “A spiking neural network chip for odor data classiﬁcation,” in Circuits and Systems (APCCAS), 2012 IEEE Asia Paciﬁc Conference on. IEEE, 2012, pp. 88–91. [2523] ——, “Vlsi implementation of a bio-inspired olfactory spiking neural network,” Neural Networks and Learning Systems, IEEE Transactions on, vol. 23, no. 7, pp. 1065–1073, 2012. [2524] N. Imam, T. A. Cleland, R. Manohar, P. A. Merolla, J. V. Arthur, F. Akopyan, and D. S. Modha, “Implementation of olfactory bulb glomerular-layer computations in a digital neurosynaptic core,” Fron- tiers in neuroscience, vol. 6, 2012. [2525] T. J. Koickal, A. Hamilton, T. C. Pearce, S.-L. Tan, J. A. Covington, and J. W. Gardner, “Analog vlsi design of an adaptive neuromorphic chip for olfactory systems,” in Circuits and Systems, 2006. ISCAS 2006. Proceedings. 2006 IEEE International Symposium on. IEEE, 2006, pp. 4–pp. [2526] T. J. Koickal, A. Hamilton, S. L. Tan, J. A. Covington, J. W. Gardner, and T. C. Pearce, “Analog vlsi circuit implementation of an adaptive neuromorphic olfaction chip,” Circuits and Systems I: Regular Papers, IEEE Transactions on, vol. 54, no. 1, pp. 60–73, 2007. [2527] D. R. Muir, G. Indiveri, and R. Douglas, “Form speciﬁes func- tion: Robust spike-based computation in analog vlsi without precise synaptic weights,” in Circuits and Systems, 2005. ISCAS 2005. IEEE International Symposium on. IEEE, 2005, pp. 5150–5153. [2528] T. C. Pearce, C. Fulvi-Mari, J. A. Covington, F. S. Tan, J. W. Gardner, T. J. Koickal, and A. Hamilton, “Silicon-based neuromorphic imple- mentation of the olfactory pathway,” in Neural Engineering, 2005. Conference Proceedings. 2nd International IEEE EMBS Conference on. IEEE, 2005, pp. 307–312. [2529] J. C. Principe, V. G. Tavares, J. G. Harris, and W. J. Freeman, “Design and implementation of a biologically realistic olfactory cortex in analog vlsi,” Proceedings of the IEEE, vol. 89, no. 7, pp. 1030–1051, 2001. [2530] N. Y.-M. Shen, Z. Liu, C. Lee, B. Minch, E. C.-C. Kan et al., “Charge- based chemical sensors: a neuromorphic approach with chemore- ceptive neuron mos (cνmos) transistors,” Electron Devices, IEEE Transactions on, vol. 50, no. 10, pp. 2171–2178, 2003. [2531] N. Y. Shen, Z. Liu, B. C. Jacquot, B. A. Minch, and E. C. Kan, “Integration of chemical sensing and electrowetting actuation on chemoreceptive neuron mos (cνmos) transistors,” Sensors and actua- tors B: chemical, vol. 102, no. 1, pp. 35–43, 2004. [2532] U. B. Rongala, A. Mazzoni, and C. M. Oddo, “Neuromorphic artiﬁcial touch for categorization of naturalistic textures,” 2015. [2533] P. M. Ros, M. Crepaldi, and D. Demarchi, “A hybrid quasi- digital/neuromorphic architecture for tactile sensing in humanoid robots,” in Advances in Sensors and Interfaces (IWASI), 2015 6th IEEE International Workshop on. IEEE, 2015, pp. 126–130. [2534] J. Joshi, A. C. Parker, and T. Celikel, “Neuromorphic network implementation of the somatosensory cortex,” in Neural Engineering (NER), 2013 6th International IEEE/EMBS Conference on. IEEE, 2013, pp. 907–910. [2535] W. W. Lee, S. L. Kukreja, and N. V. Thakor, “A kilohertz kilotaxel tactile sensor array for investigating spatiotemporal features in neu- romorphic touch,” in Biomedical Circuits and Systems Conference (BioCAS), 2015 IEEE. IEEE, 2015, pp. 1–4. [2536] ——, “Discrimination of dynamic tactile contact by temporally pre- cise event sensing in spiking neuromorphic networks,” Frontiers in Neuroscience, vol. 11, p. 5, 2017. [2537] A. Rios-Navarro, E. Cerezuela-Escudero, M. Dominguez-Morales, A. Jimenez-Fernandez, G. Jimenez-Moreno, and A. Linares-Barranco, “Real-time motor rotation frequency detection with event-based visual and spike-based auditory aer sensory integration for fpga,” in Event- based Control, Communication, and Signal Processing (EBCCSP), 2015 International Conference on. IEEE, 2015, pp. 1–6. [2538] S. Harrer, I. Kiral-Kornek, R. Kerr, B. Mashford, J. Tang, A. J. Yepes, and H. Deligianni, “From wearables to thinkables-deep learning, nano-biosensors and the next generation of mobile devices,” White Paper, ICONN, 2016. [2539] F. Corradi, D. Bontrager, and G. Indiveri, “Toward neuromorphic intelligent brain-machine interfaces: An event-based neural recording and processing system,” in Biomedical Circuits and Systems Confer- ence (BioCAS), 2014 IEEE. IEEE, 2014, pp. 584–587. [2540] F. Corradi and G. Indiveri, “A neuromorphic event-based neural recording system for smart brain-machine-interfaces,” Biomedical Circuits and Systems, IEEE Transactions on, vol. 9, no. 5, pp. 699– 709, 2015. [2541] R. George, C. Mayr, G. Indiveri, and S. Vassanelli, “Event-based softcore processor in a biohybrid setup applied to structural plastic- ity,” in Event-based Control, Communication, and Signal Processing (EBCCSP), 2015 International Conference on. IEEE, 2015, pp. 1–4. [2542] M.-C. Hsiao, C.-H. Chan, V. Srinivasan, A. Ahuja, G. Erinjippu- rath, T. P. Zanos, G. Gholmieh, D. Song, J. D. Wills, J. LaCoss et al., “Vlsi implementation of a nonlinear neuronal model: A” neural prosthesis” to restore hippocampal trisynaptic dynamics,” in Engineering in Medicine and Biology Society, 2006. EMBS’06. 28th Annual International Conference of the IEEE. IEEE, 2006, pp. 4396– 4399. [2543] S. Micera, J. Carpaneto, S. Raspopovic, G. Granata, A. Mazzoni, C. M. Oddo, C. Cipriani, T. Stieglitz, M. Mueller, X. Navarro et al., 85 “Toward the development of a neuro-controlled bidirectional hand prosthesis,” in Symbiotic Interaction. Springer, 2015, pp. 105–110. [2544] C. M. Thibeault, “A role for neuromorphic processors in therapeutic nervous system stimulation,” Frontiers in systems neuroscience, vol. 8, 2014. [2545] Z. S. Zumsteg, C. Kemere, S. O’Driscoll, G. Santhanam, R. E. Ahmed, K. V. Shenoy, and T. H. Meng, “Power feasibility of im- plantable digital spike sorting circuits for neural prosthetic systems,” Neural Systems and Rehabilitation Engineering, IEEE Transactions on, vol. 13, no. 3, pp. 272–279, 2005. [2546] N. Gaspar, A. Sondhi, B. Evans, and K. Nikolic, “Live demonstration: A low-power neuromorphic system for retinal implants and sensory substitution,” in Biomedical Circuits and Systems Conference (Bio- CAS), 2015 IEEE. IEEE, 2015, pp. 1–1. [2547] J. Zhang, C. Wang, C.-C. Hsu, C. Zhou et al., “A biomimetic fabricated carbon nanotube synapse for prosthetic applications,” in 2011 IEEE/NIH Life Science Systems and Applications Workshop (LiSSA), 2011, pp. 139–142. [2548] J. Daly, J. Brown, and J. Weng, “Neuromorphic motivated systems,” in Neural Networks (IJCNN), The 2011 International Joint Conference on. IEEE, 2011, pp. 2917–2924. [2549] T. Yamazaki and J. Igarashi, “Realtime cerebellum: A large-scale spiking network model of the cerebellum that runs in realtime using a graphics processing unit,” Neural Networks, vol. 47, pp. 103–111, 2013. [2550] S. Davies, C. Patterson, F. Galluppi, A. Rast, D. Lester, and S. B. Furber, “Interfacing real-time spiking i/o with the spinnaker neu- romimetic architecture,” in Proceedings of the 17th International Conference on Neural Information Processing: Australian Journal of Intelligent Information Processing Systems, 2010, pp. 7–11. [2551] C. M. Niu, K. Jalaleddini, W. J. Sohn, J. Rocamora, T. Sanger, and F. Valero-Cuevas, “Neuromorphic meets neuromechanics part i: The methodology and implementation,” Journal of Neural Engineering, 2017. [2552] T. Belpaeme, S. Adams, J. de Greeff, A. di Nuovo, A. Morse, and A. Cangelosi, “Social development of artiﬁcial cognition,” in Toward Robotic Socially Believable Behaving Systems-Volume I. Springer, 2016, pp. 53–72. [2553] R. Cingolani and G. Metta, “Nanotechnology for humans and hu- manoids a vision of the use of nanotechnology in future robotics,” in Nanotechnology (IEEE-NANO), 2015 IEEE 15th International Conference on. IEEE, 2015, pp. 600–603. [2554] F. Galluppi, C. Denk, M. C. Meiner, T. C. Stewart, L. Plana, C. Elia- smith, S. Furber, J. Conradt et al., “Event-based neural computing on an autonomous mobile platform,” in Robotics and Automation (ICRA), 2014 IEEE International Conference on. IEEE, 2014, pp. 2862–2867. [2555] H. Ames, M. Versace, A. Gorchetchnikov, B. Chandler, G. Livitz, J. L´eveill´e, E. Mingolla, D. Carter, H. Abdalla, and G. Snider, “Persuading computers to act more like brains,” in Advances in Neuromorphic Memristor Science and Applications. Springer, 2012, pp. 37–61. [2556] M. Beyeler, N. Oros, N. Dutt, and J. L. Krichmar, “A gpu-accelerated cortical neural network model for visually guided robot navigation,” Neural Networks, vol. 72, pp. 75–87, 2015. [2557] C. Denk, F. Llobet-Blandino, F. Galluppi, L. A. Plana, S. Furber, and J. Conradt, “Real-time interface board for closed-loop robotic tasks on the spinnaker neural computing system,” in Artiﬁcial Neural Networks and Machine Learning–ICANN 2013. Springer, 2013, pp. 467–474. [2558] L. Reichel, D. Liechti, K. Presser, and S.-C. Liu, “Robot guidance with neuromorphic motion sensors,” in Robotics and Automation, 2005. ICRA 2005. Proceedings of the 2005 IEEE International Conference on. IEEE, 2005, pp. 3540–3544. [2559] ——, “Range estimation on a robot using neuromorphic motion sensors,” Robotics and Autonomous Systems, vol. 51, no. 2, pp. 167– 174, 2005. [2560] T. C. Stewart, A. Kleinhans, A. Mundy, and J. Conradt, “Serendipitous ofﬂine learning in a neuromorphic robot,” Frontiers in neurorobotics, vol. 10, 2016. [2561] R. Bayindir and A. Gorgun, “Hardware implementation of a real- time neural network controller set for reactive power compensation systems,” in Machine Learning and Applications (ICMLA), 2010 Ninth International Conference on. IEEE, 2010, pp. 699–703. [2562] T. Fukuda, T. Shibata, M. Tokita, and T. Mitsuoka, “Neuromorphic control: adaptation and learning,” Industrial Electronics, IEEE Trans- actions on, vol. 39, no. 6, pp. 497–503, 1992. [2563] G. Grossi, R. Posenato, M. Costa et al., “Fast prototyping for hardware neural networks,” 1995. [2564] S. Li, S. Dasmahapatra, and K. Maharatna, “Dynamical system approach for edge detection using coupled ﬁtzhugh–nagumo neurons,” Image Processing, IEEE Transactions on, vol. 24, no. 12, pp. 5206– 5219, 2015. [2565] D. Querlioz, O. Bichler, A. F. Vincent, and C. Gamrat, “Theoretical analysis of spike-timing-dependent plasticity learning with memristive devices,” in Advances in Neuromorphic Hardware Exploiting Emerg- ing Nanoscale Devices. Springer, 2017, pp. 197–210. [2566] V. Roychowdhury, D. Janes, S. Bandyopadhyay, and X. Wang, “Col- lective computational activity in self-assembled arrays of quantum dots: a novel neuromorphic architecture for nanoelectronics,” Electron Devices, IEEE Transactions on, vol. 43, no. 10, pp. 1688–1699, 1996. [2567] F. Bernhard and R. Keriven, “Spiking neurons on gpus,” in Compu- tational Science–ICCS 2006. Springer, 2006, pp. 236–243. [2568] H. P. Graf, E. Sackinger, and L. D. Jackel, “Recent developments of electronic neural nets in north america,” The Journal of VLSI Signal Processing, vol. 6, no. 1, pp. 19–31, 1993. [2569] G. Seguin-Godin, F. Mailhot, and J. Rouat, “Efﬁcient event-driven approach using synchrony processing for hardware spiking neural networks,” in Circuits and Systems (ISCAS), 2015 IEEE International Symposium on. IEEE, 2015, pp. 2696–2699. [2570] G. Indiveri, B. Linares Barranco, T. Masquelier, M. T. Serrano Go- tarredona, and T. Prodromakis, “Stdp and stdp variations with mem- ristors for spiking neuromorphic learning systems,” 2013. [2571] P. Knag, J. K. Kim, T. Chen, and Z. Zhang, “A sparse coding neural network asic with on-chip learning for feature extraction and encoding,” Solid-State Circuits, IEEE Journal of, vol. 50, no. 4, pp. 1070–1079, 2015. [2572] T. Serrano-Gotarredona, T. Masquelier, T. Prodromakis, G. Indiveri, and B. Linares-Barranco, “Stdp and stdp variations with memristors for spiking neuromorphic learning systems,” Frontiers in neuro- science, vol. 7, 2013. [2573] H.-P. Cheng, W. Wen, C. Song, B. Liu, H. Li, and Y. Chen, “Exploring the optimal learning technique for ibm truenorth platform to overcome quantization loss,” in Nanoscale Architectures (NANOARCH), 2016 IEEE/ACM International Symposium on. IEEE, 2016, pp. 185–190. [2574] G. K. Cohen, G. Orchard, S. H. Ieng, J. Tapson, R. B. Benosman, and A. van Schaik, “Skimming digits: Neuromorphic classiﬁcation of spike-encoded images,” Frontiers in Neuroscience, vol. 10, p. 184, 2016. [2575] A. Diamond, T. Nowotny, and M. Schmuker, “Comparing neuromor- phic solutions in action: implementing a bio-inspired solution to a benchmark classiﬁcation task on three parallel-computing platforms,” Frontiers in neuroscience, vol. 9, 2015. [2576] Z. Du, D. D. Ben-Dayan Rubin, Y. Chen, L. He, T. Chen, L. Zhang, C. Wu, and O. Temam, “Neuromorphic accelerators: a comparison between neuroscience and machine-learning approaches,” in Pro- ceedings of the 48th International Symposium on Microarchitecture. ACM, 2015, pp. 494–507. [2577] S. K. Esser, R. Appuswamy, P. Merolla, J. V. Arthur, and D. S. Modha, “Backpropagation for energy-efﬁcient neuromorphic computing,” in Advances in Neural Information Processing Systems, 2015, pp. 1117– 1125. [2578] Y. Ji, Y.-H. Zhang, and W.-M. Zheng, “Modelling spiking neural network from the architecture evaluation perspective,” Journal of Computer Science and Technology, vol. 31, no. 1, pp. 50–59, 2016. [2579] J. Kung, D. Kim, and S. Mukhopadhyay, “A power-aware digital feedforward neural network platform with backpropagation driven ap- proximate synapses,” in Low Power Electronics and Design (ISLPED), 2015 IEEE/ACM International Symposium on. IEEE, 2015, pp. 85– 90. [2580] V. Mrazek, S. S. Sarwar, L. Sekanina, Z. Vasicek, and K. Roy, “Design of power-efﬁcient approximate multipliers for approximate artiﬁcial neural networks,” in 2016 International Conference On Computer Aided Design (ICCAD)(prijato), 2016, p. 7. [2581] E. Neftci, S. Das, B. Pedroni, K. Kreutz-Delgado, and G. Cauwen- berghs, “Event-driven contrastive divergence for spiking neuromor- phic systems,” Frontiers in neuroscience, vol. 7, 2013. [2582] B. U. Pedroni, S. Das, E. Neftci, K. Kreutz-Delgado, and G. Cauwen- berghs, “Neuromorphic adaptations of restricted boltzmann machines and deep belief networks,” in Neural Networks (IJCNN), The 2013 International Joint Conference on. IEEE, 2013, pp. 1–6. [2583] M. Pietras, “Error analysis in the hardware neural networks ap- plications using reduced ﬂoating-point numbers representation,” in PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON NUMERICAL ANALYSIS AND APPLIED MATHEMATICS 2014 (ICNAAM-2014), vol. 1648. AIP Publishing, 2015, p. 660005. 86 [2584] T. E. Potok, C. D. Schuman, S. R. Young, R. M. Patton, F. Spedalieri, J. Liu, K.-T. Yao, G. Rose, and G. Chakma, “A study of complex deep learning networks on high performance, neuromorphic, and quantum computers,” in Proceedings of the Workshop on Machine Learning in High Performance Computing Environments. IEEE Press, 2016, pp. 47–55. [2585] S. S. Sarwar, S. Venkataramani, A. Raghunathan, and K. Roy, “Multiplier-less artiﬁcial neurons exploiting error resiliency for energy-efﬁcient neural computing,” in 2016 Design, Automation & Test in Europe Conference & Exhibition (DATE). IEEE, 2016, pp. 145–150. [2586] W. Shi, Y. Wen, Z. Liu, X. Zhao, D. Boumber, R. Vilalta, and L. Xu, “Fault resilient physical neural networks on a single chip,” in Compilers, Architecture and Synthesis for Embedded Systems (CASES), 2014 International Conference on. IEEE, 2014, pp. 1– 10. [2587] G. Srinivasan, P. Wijesinghe, S. S. Sarwar, A. Jaiswal, and K. Roy, “Signiﬁcance driven hybrid 8t-6t sram for energy-efﬁcient synaptic storage in artiﬁcial neural networks,” in Design, Automation & Test in Europe Conference & Exhibition (DATE), 2016. IEEE, 2016, pp. 151–156. [2588] D. Strigl, K. Koﬂer, and S. Podlipnig, “Performance and scalability of gpu-based convolutional neural networks,” in Parallel, Distributed and Network-Based Processing (PDP), 2010 18th Euromicro International Conference on. IEEE, 2010, pp. 317–324. [2589] T. M. Taha, P. Yalamanchili, M. Bhuiyan, R. Jalasutram, S. K. Mohan et al., “Parallelizing two classes of neuromorphic models on the cell multicore architecture,” in Neural Networks, 2009. IJCNN 2009. International Joint Conference on. IEEE, 2009, pp. 3046–3053. [2590] M. D. Tissera and M. D. McDonnell, “Deep extreme learning machines: supervised autoencoding architecture for classiﬁcation,” Neurocomputing, vol. 174, pp. 42–49, 2016. [2591] S. Venkataramani, A. Ranjan, K. Roy, and A. Raghunathan, “Axnn: energy-efﬁcient neuromorphic systems using approximate comput- ing,” in Proceedings of the 2014 international symposium on Low power electronics and design. ACM, 2014, pp. 27–32. [2592] W. Wen, C. Wu, Y. Wang, K. Nixon, Q. Wu, M. Barnell, H. Li, and Y. Chen, “A new learning method for inference accuracy, core occupation, and performance co-optimization on truenorth chip,” in Design Automation Conference (DAC), 2016 53nd ACM/EDAC/IEEE. IEEE, 2016, pp. 1–6. [2593] C. Yang, B. Liu, H. Li, Y. Chen, W. Wen, M. Barnell, Q. Wu, and J. Rajendran, “Security of neuromorphic computing: thwarting learn- ing attacks using memristor’s obsolescence effect,” in Proceedings of the 35th International Conference on Computer-Aided Design. ACM, 2016, p. 97. [2594] A. J. Yepes, J. Tang, S. Saxena, T. Brosch, and A. Amir, “Weighted population code for low power neuromorphic image classiﬁcation,” in Neural Networks (IJCNN), 2016 International Joint Conference on. IEEE, 2016, pp. 4294–4301. [2595] S. Duan, Z. Dong, X. Hu, L. Wang, and H. Li, “Small-world hopﬁeld neural networks with weight salience priority and memristor synapses for digit recognition,” Neural Computing and Applications, vol. 27, no. 4, pp. 837–844, 2016. [2596] C. Merkel and D. Kudithipudi, “Comparison of off-chip training methods for neuromemristive systems,” in VLSI Design (VLSID), 2015 28th International Conference on. IEEE, 2015, pp. 99–104. [2597] A. D. Rast, L. A. Plana, S. R. Welbourne, and S. B. Furber, “Event- driven mlp implementation on neuromimetic hardware,” in Neural Networks (IJCNN), The 2012 International Joint Conference on. IEEE, 2012, pp. 1–8. [2598] W. Zhao, D. Querlioz, J.-O. Klein, D. Chabi, and C. Chappert, “Nanodevice-based novel computing paradigms and the neuromorphic approach,” in Circuits and Systems (ISCAS), 2012 IEEE International Symposium on. IEEE, 2012, pp. 2509–2512. [2599] Y. Zhu, X. Wang, T. Huang, and Z. Zeng, “Memristor-based neu- romorphic system with content addressable memory structure,” in International Symposium on Neural Networks. Springer, 2016, pp. 681–690. [2600] A. J. Agranat, C. F. Neugebauer, and A. Yariv, “A ccd based neural network integrated circuit with 64k analog programmable synapses,” in Neural Networks, 1990., 1990 IJCNN International Joint Confer- ence on. IEEE, 1990, pp. 551–555. [2601] M. Bishop, M. J. Moore, D. J. Burns, R. E. Pino, and R. Lin- derman, “Affordable emerging computer hardware for neuromorphic computing applications,” in Neural Networks (IJCNN), The 2010 International Joint Conference on. IEEE, 2010, pp. 1–5. [2602] B. Han and T. M. Taha, “Neuromorphic models on a gpgpu cluster,” in Neural Networks (IJCNN), The 2010 International Joint Conference on. IEEE, 2010, pp. 1–8. [2603] ——, “Acceleration of spiking neural network based pattern recogni- tion on nvidia graphics processors,” Applied Optics, vol. 49, no. 10, pp. B83–B91, 2010. [2604] J.-W. Jang, B. Attarimashalkoubeh, A. Prakash, H. Hwang, and Y.- H. Jeong, “Scalable neuron circuit using conductive-bridge ram for pattern reconstructions,” IEEE Transactions on Electron Devices, vol. 63, no. 6, pp. 2610–2613, 2016. [2605] E. Kim and D. Y. Cho, “Multilayer perceptron with on-chip learning using stochastic ratio pulse arithmetic,” in Neural Information Pro- cessing, 1999. Proceedings. ICONIP’99. 6th International Conference on, vol. 2. IEEE, 1999, pp. 566–569. [2606] Y. Kim, Y. Zhang, and P. Li, “Energy efﬁcient approximate arithmetic for error resilient neuromorphic computing,” 2014. [2607] K.-S. Oh and K. Jung, “Gpu implementation of neural networks,” Pattern Recognition, vol. 37, no. 6, pp. 1311–1314, 2004. [2608] Q. Qiu, Q. Wu, M. Bishop, R. E. Pino, and R. W. Linderman, “A parallel neuromorphic text recognition system and its implementation on a heterogeneous high-performance computing cluster,” Computers, IEEE Transactions on, vol. 62, no. 5, pp. 886–899, 2013. [2609] L. M. Reyneri and E. Filippi, “An analysis on the performance of silicon implementations of backpropagation algorithms for artiﬁcial neural networks,” Computers, IEEE Transactions on, vol. 40, no. 12, pp. 1380–1389, 1991. [2610] T. M. Taha, P. Yalamanchili, M. Bhuiyan, R. Jalasutram, C. Chen, and R. Linderman, “Neuromorphic algorithms on clusters of playstation 3s,” in Neural Networks (IJCNN), The 2010 International Joint Conference on. IEEE, 2010, pp. 1–10. [2611] Q. Wang, Y. Kim, and P. Li, “Architectural design exploration for neu- romorphic processors with memristive synapses,” in Nanotechnology (IEEE-NANO), 2014 IEEE 14th International Conference on. IEEE, 2014, pp. 962–966. [2612] H. P. Graf, L. D. Jackel, and W. E. Hubbard, “A cmos implementation of a neural net-work model,” in Advanced Research in VLSI, Proc. StanfordConf. Citeseer, 1987. [2613] D. Ielmini, S. Ambrogio, V. Milo, S. Balatti, and Z.-Q. Wang, “Neuromorphic computing with hybrid memristive/cmos synapses for real-time learning,” in Circuits and Systems (ISCAS), 2016 IEEE International Symposium on. IEEE, 2016, pp. 1386–1389. [2614] R. Ranjan, A. Kyrmanidis, W. L. Hellweg, P. M. Ponce, L. A. Saleh, D. Schroeder, and W. H. Krautschneider, “Integrated circuit with memristor emulator array and neuron circuits for neuromorphic pattern recognition,” in Telecommunications and Signal Processing (TSP), 2016 39th International Conference on. IEEE, 2016, pp. 265–268. [2615] H. R. Mahdiani, S. M. Fakhraie, and C. Lucas, “Relaxed fault- tolerant hardware implementation of neural networks in the presence of multiple transient errors,” Neural Networks and Learning Systems, IEEE Transactions on, vol. 23, no. 8, pp. 1215–1228, 2012. [2616] H. Bolouri, P. Morgan, and C. Peacock, “A ram-based neural network architecture for wafer scale integration,” in Proc. IEEE Int. Conf. on Wafer Scale Integration (San Francisco). Citeseer, 1995, pp. 82–90. [2617] A. Andreopoulos, R. Alvarez-Icaza, A. S. Cassidy, and M. D. Flickner, “A low-power neurosynaptic implementation of local binary patterns for texture analysis,” in Neural Networks (IJCNN), 2016 International Joint Conference on. IEEE, 2016, pp. 4308–4316. [2618] Y. LeCun, C. Cortes, and C. J. Burges, “The mnist database of handwritten digits,” 1998. [2619] A. Krizhevsky, V. Nair, and G. Hinton, “The cifar-10 dataset,” 2014. [2620] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng, “Reading digits in natural images with unsupervised feature learning,” in NIPS workshop on deep learning and unsupervised feature learning, vol. 2011, no. 2, 2011, p. 5. [2621] M. Coath, S. Sheik, E. Chicca, G. Indiveri, S. L. Denham, and T. Wen- nekers, “A robust sound perception model suitable for neuromorphic implementation,” Frontiers in neuroscience, vol. 7, 2013. [2622] P. G´omez-Vilda, J. M. Ferr´andez-Vicente, V. Rodellar-Biarge, A. ´Alvarez-Marquina, L. M. Mazaira-Fern´andez, R. Mart´ınez-Olalla, and C. Mu˜noz-Mulas, “Neuromorphic detection of vowel repre- sentation spaces,” in New Challenges on Bioinspired Applications. Springer, 2011, pp. 1–11. [2623] S. S. Modi, P. R. Wilson, and A. D. Brown, “Power scalable implementation of artiﬁcial neural networks,” in Electronics, Circuits and Systems, 2005. ICECS 2005. 12th IEEE International Conference on. IEEE, 2005, pp. 1–4. 87 [2624] K. Vandoorne, P. Mechet, T. Van Vaerenbergh, M. Fiers, G. Morthier, D. Verstraeten, B. Schrauwen, J. Dambre, and P. Bienstman, “Exper- imental demonstration of reservoir computing on a silicon photonics chip,” Nature communications, vol. 5, 2014. [2625] H. Akolkar, C. Meyer, X. Clady, O. Marre, C. Bartolozzi, S. Panzeri, and R. Benosman, “What can neuromorphic event-driven precise timing add to spike-based pattern recognition?” Neural computation, 2015. [2626] Y. Chen, D. Khosla, D. Huber, K. Kim, and S. Y. Cheng, “A neuromor- phic approach to object detection and recognition in airborne videos with stabilization,” in Advances in Visual Computing. Springer, 2011, pp. 126–135. [2627] D. Khosla, Y. Chen, D. J. Huber, D. J. Van Buer, K. Kim, and S. Y. Cheng, “Real-time low-power neuromorphic hardware for au- tonomous object recognition,” in SPIE Defense, Security, and Sensing. International Society for Optics and Photonics, 2013, pp. 871 313– 871 313. [2628] D. Khosla, D. J. Huber, and C. Kanan, “A neuromorphic system for visual object recognition,” Biologically Inspired Cognitive Architec- tures, vol. 8, pp. 33–45, 2014. [2629] J.-Y. Kim, M. Kim, S. Lee, J. Oh, K. Kim, and H.-J. Yoo, “A 201.4 gops 496 mw real-time multi-object recognition processor with bio- inspired neural perception engine,” Solid-State Circuits, IEEE Journal of, vol. 45, no. 1, pp. 32–45, 2010. [2630] G. Orchard, X. Lagorce, C. Posch, S. B. Furber, R. Benosman, and F. Galluppi, “Real-time event-driven spiking neural network object recognition on the spinnaker platform,” in Circuits and Systems (ISCAS), 2015 IEEE International Symposium on. IEEE, 2015, pp. 2413–2416. [2631] M. Suri and B. DeSalvo, “Phase change memory and chalcogenide materials for neuromorphic applications: Emphasis on synaptic plas- ticity,” in Advances in Neuromorphic Memristor Science and Appli- cations. Springer, 2012, pp. 155–178. [2632] T. Brosch and H. Neumann, “Event-based optical ﬂow on neuromor- phic hardware,” in proceedings of the 9th EAI International Confer- ence on Bio-inspired Information and Communications Technologies (formerly BIONETICS) on 9th EAI International Conference on Bio- inspired Information and Communications Technologies (formerly BIONETICS). ICST (Institute for Computer Sciences, Social- Informatics and Telecommunications Engineering), 2016, pp. 551– 558. [2633] J. Garcia-Rodriguez, S. Orts-Escolano, A. Angelopoulou, A. Psarrou, J. Azorin-Lopez, and J. M. Garcia-Chamizo, “Real time motion estimation using a neural architecture implemented on gpus,” Journal of Real-Time Image Processing, pp. 1–19, 2014. [2634] D. Reverter Valeiras, X. Lagorce, X. Clady, C. Bartolozzi, S.-H. Ieng, and R. Benosman, “An asynchronous neuromorphic event-driven visual part-based shape tracking,” Neural Networks and Learning Systems, IEEE Transactions on, vol. 26, no. 12, pp. 3045–3059, 2015. [2635] Q. Liu and S. Furber, “Real-time recognition of dynamic hand postures on a neuromorphic system,” World Academy of Science, Engineering and Technology, International Journal of Electrical, Computer, Energetic, Electronic and Communication Engineering, vol. 9, no. 5, pp. 432–439, 2015. [2636] J. Kung, D. Kim, and S. Mukhopadhyay, “Dynamic approximation with feedback control for energy-efﬁcient recurrent neural network hardware,” in Proceedings of the 2016 International Symposium on Low Power Electronics and Design. ACM, 2016, pp. 168–173. [2637] B. Li, E. Zhou, B. Huang, J. Duan, Y. Wang, N. Xu, J. Zhang, and H. Yang, “Large scale recurrent neural network on gpu,” in Neural Networks (IJCNN), 2014 International Joint Conference on. IEEE, 2014, pp. 4062–4069. [2638] M. E. Phillips, N. D. Stepp, J. Cruz-Albrecht, V. De Sapio, T.-C. Lu, and V. Sritapan, “Neuromorphic and early warning behavior-based authentication for mobile devices,” in Technologies for Homeland Security (HST), 2016 IEEE Symposium on. IEEE, 2016, pp. 1–5. [2639] M. Otahal, M. Najman, and O. Stepankova, “Design of neuromorphic cognitive module based on hierarchical temporal memory and demon- strated on anomaly detection,” Procedia Computer Science, vol. 88, pp. 232–238, 2016. [2640] R. E. Pino and A. Kott, “Neuromorphic computing for cognitive augmentation in cyber defense,” in Cybersecurity Systems for Human Cognition Augmentation. Springer, 2014, pp. 19–45. [2641] S. Draghici, “Using information entropy bounds to design vlsi friendly neural networks,” in Neural Networks Proceedings, 1998. IEEE World Congress on Computational Intelligence. The 1998 IEEE Interna- tional Joint Conference on, vol. 1. IEEE, 1998, pp. 547–552. [2642] ——, “On the complexity of vlsi-friendly neural networks for classi- ﬁcation problems,” in Advances in Artiﬁcial Intelligence. Springer, 1998, pp. 285–297. [2643] L. A. Pastur-Romay, F. Cedr´on, A. Pazos, and A. B. Porto-Pazos, “Deep artiﬁcial neural networks and neuromorphic chips for big data analysis: pharmaceutical and bioinformatics applications,” Interna- tional Journal of Molecular Sciences, vol. 17, no. 8, p. 1313, 2016. [2644] C. Blake and C. J. Merz, “{UCI} repository of machine learning databases,” 1998. [2645] S. Hussain, S.-C. Liu, and A. Basu, “Hardware-amenable structural learning for spike-based pattern classiﬁcation using a simple model of active dendrites,” Neural computation, 2015. [2646] T. Chen, Y. Chen, M. Duranton, Q. Guo, A. Hashmi, M. Lipasti, A. Nere, S. Qiu, M. Sebag, and O. Temam, “Benchnn: On the broad potential application scope of hardware neural network accelerators,” in Workload Characterization (IISWC), 2012 IEEE International Symposium on. IEEE, 2012, pp. 36–45. [2647] I. C. Goknar, M. Yildiz, S. Minaei, and E. Deniz, “Neural cmos- integrated circuit and its application to data classiﬁcation,” Neural Networks and Learning Systems, IEEE Transactions on, vol. 23, no. 5, pp. 717–724, 2012. [2648] B. Girau, “Fpna: applications and implementations,” in FPGA Imple- mentations of Neural Networks. Springer, 2006, pp. 103–136. [2649] V. Beiu and J. G. Taylor, “Optimal mapping of neural networks onto fpgas,” in From Natural to Artiﬁcial Neural Computation. Springer, 1995, pp. 822–829. [2650] ——, “Direct synthesis of neural networks,” in Microelectronics for Neural Networks, 1996., Proceedings of Fifth International Confer- ence on. IEEE, 1996, pp. 257–264. [2651] M. N. Devi, N. Mohankumar, and J. P. Nair, “Vlsi realization of artiﬁcial neural networks with improved fault tolerance,” International Journal of Computer Applications, vol. 2, no. 9, 2010. [2652] C. Alippi and G. Storti-Gajani, “Simple approximation of sigmoidal functions: realistic design of digital neural networks capable of learn- ing,” in Circuits and Systems, 1991., IEEE International Sympoisum on. IEEE, 1991, pp. 1505–1508. [2653] H.-J. K. Chiang, J.-H. R. Jiang, and F. Fages, “Reconﬁgurable neuromorphic computation in biochemical systems,” in Engineering in Medicine and Biology Society (EMBC), 2015 37th Annual Inter- national Conference of the IEEE. IEEE, 2015, pp. 937–940. [2654] S. Davies, T. Stewart, C. Eliasmith, and S. Furber, “Spike-based learn- ing of transfer functions with the spinnaker neuromimetic simulator,” in Neural Networks (IJCNN), The 2013 International Joint Conference on. IEEE, 2013, pp. 1–8. [2655] E. Z. Farsa, S. Nazari, and M. Gholami, “Function approximation by hardware spiking neural network,” Journal of Computational Electronics, vol. 14, no. 3, pp. 707–716, 2015. [2656] R. C. Frye and E. A. Reitman, “Experimental studies of analog neural networks,” in Frontiers of Computing Systems Research. Springer, 1992, pp. 239–290. [2657] D. Korkmaz, ¨O. F. Alc¸in, and S. Ekici, “Modeling of memristor by using artiﬁcial neural network,” Global Journal on Technology, vol. 4, no. 2, 2013. [2658] F. Galluppi, X. Lagorce, E. Stromatias, M. Pfeiffer, L. A. Plana, S. B. Furber, and R. B. Benosman, “A framework for plasticity implementation on the spinnaker neural architecture,” Frontiers in neuroscience, vol. 8, 2014. [2659] C. Zhao, B. T. Wysocki, Y. Liu, C. D. Thiem, N. R. McDonald, and Y. Yi, “Spike-time-dependent encoding for neuromorphic proces- sors,” ACM Journal on Emerging Technologies in Computing Systems (JETC), vol. 12, no. 3, p. 23, 2015. [2660] N. Kasabov, N. M. Scott, E. Tu, S. Marks, N. Sengupta, E. Capecci, M. Othman, M. G. Doborjeh, N. Murli, R. Hartono et al., “Evolving spatio-temporal data machines based on the neucube neuromorphic framework: design methodology and selected applications,” Neural Networks, vol. 78, pp. 1–14, 2016. [2661] W. W. Lee, H. Yu, and N. V. Thakor, “Gait event detection through neuromorphic spike sequence learning,” in Biomedical Robotics and Biomechatronics (2014 5th IEEE RAS & EMBS International Con- ference on. IEEE, 2014, pp. 899–904. [2662] C. Rasche, “Neuromorphic excitable maps for visual processing,” Neural Networks, IEEE Transactions on, vol. 18, no. 2, pp. 520–529, 2007. [2663] S. Roy, A. Basu, and S. Hussain, “Hardware efﬁcient, neuromorphic dendritically enhanced readout for liquid state machines,” in Biomed- ical Circuits and Systems Conference (BioCAS), 2013 IEEE. IEEE, 2013, pp. 302–305. 88 [2664] S. Roy, A. Banerjee, and A. Basu, “Liquid state machine with dendritically enhanced readout for low-power, neuromorphic vlsi im- plementations,” Biomedical Circuits and Systems, IEEE Transactions on, vol. 8, no. 5, pp. 681–695, 2014. [2665] R. Serrano-Gotarredona, M. Oster, P. Lichtsteiner, A. Linares- Barranco, R. Paz-Vicente, F. Gomez-Rodriguez, H. K. Riis, T. Delbr¨uck, S.-C. Liu, S. Zahnd et al., “Aer building blocks for multi-layer multi-chip neuromorphic vision systems,” in NIPS, 2005. [2666] S. Sheik, M. Pfeiffer, F. Stefanini, and G. Indiveri, “Spatio-temporal spike pattern classiﬁcation in neuromorphic systems,” in Biomimetic and Biohybrid Systems. Springer, 2013, pp. 262–273. [2667] E. Gale, B. de Lacy Costello, and A. Adamatzky, “Is spiking logic the route to memristor-based computers?” in Electronics, Circuits, and Systems (ICECS), 2013 IEEE 20th International Conference on. IEEE, 2013, pp. 297–300. [2668] H. H. Li, M. Hu, and R. E. Pino, “Statistical memristor model and its applications in neuromorphic computing,” in Advances in Neuromorphic Memristor Science and Applications. Springer, 2012, pp. 107–131. [2669] S. Aunet and V. Beiu, “Ultra low power fault tolerant neural inspired cmos logic,” in Neural Networks, 2005. IJCNN’05. Proceedings. 2005 IEEE International Joint Conference on, vol. 5. IEEE, 2005, pp. 2843–2848. [2670] N. Joye, A. Schmid, Y. Leblebici, T. Asai, and Y. Amemiya, “Fault- tolerant logic gates using neuromorphic cmos circuits,” in Research in Microelectronics and Electronics Conference, 2007. PRIME 2007. Ph. D. IEEE, 2007, pp. 249–252. [2671] M. J. Shariﬁ and Y. M. Banadaki, “General spice models for memris- tor and application to circuit simulation of memristor-based synapses and memory cells,” Journal of Circuits, Systems, and Computers, vol. 19, no. 02, pp. 407–424, 2010. [2672] H. Card, B. Dolenko, D. McNeill, C. Schneider, and R. Schneider, “Is vlsi neural learning robust against circuit limitations?” in Neural Net- works, 1994. IEEE World Congress on Computational Intelligence., 1994 IEEE International Conference on, vol. 3. IEEE, 1994, pp. 1889–1893. [2673] H. C. Card, D. K. McNeill, C. R. Schneider, and R. S. Schneider, “The impact of vlsi fabrication on neural learning,” in Circuits and Systems, 1995. ISCAS’95., 1995 IEEE International Symposium on, vol. 2. IEEE, 1995, pp. 985–988. [2674] E. Gale, “Memristors in unconventional computing: How a biomimetic circuit element can be used to do bioinspired compu- tation,” in Advances in Unconventional Computing. Springer, 2017, pp. 497–542. [2675] B. Girau, “Fpna: concepts and properties,” FPGA Implementations of Neural Networks, p. 63, 2006. [2676] K. Kollmann, K.-R. Riemschneider, and H. C. Zeidler, “On-chip backpropagation training using parallel stochastic bit streams,” in Microelectronics for Neural Networks, 1996., Proceedings of Fifth International Conference on. IEEE, 1996, pp. 149–156. [2677] D. Kudithipudi, C. Merkel, M. Soltiz, G. S. Rose, and R. E. Pino, “Design of neuromorphic architectures with memristors,” in Network Science and Cybersecurity. Springer, 2014, pp. 93–103. [2678] K. P. Lakshmi and M. Subadra, “A survey on fpga based mlp realization for on-chip learning,” Int. J. Sci. & Eng. Res, vol. 4, no. 1, pp. 1–9, 2013. [2679] K. Madani, “From integrated circuits technology to silicon grey matter: Hardware implementation of artiﬁcial neural networks,” in Information Processing and Security Systems. Springer, 2005, pp. 327–351. [2680] K. Mathia and J. Clark, “On neural network hardware and program- ming paradigms,” in Neural Networks, 2002. IJCNN’02. Proceedings of the 2002 International Joint Conference on, vol. 3. IEEE, 2002, pp. 2692–2697. [2681] J. L. Meador, A. Wu, C. Cole, N. Nintunze, and P. Chintrakulchai, “Programmable impulse neural circuits.” IEEE transactions on neural networks/a publication of the IEEE Neural Networks Council, vol. 2, no. 1, pp. 101–109, 1990. [2682] S. Sato, K. Nemoto, S. Akimoto, M. Kinjo, and K. Nakajima, “Implementation of a new neurochip using stochastic logic,” Neural Networks, IEEE Transactions on, vol. 14, no. 5, pp. 1122–1127, 2003.","libVersion":"0.2.3","langs":""}