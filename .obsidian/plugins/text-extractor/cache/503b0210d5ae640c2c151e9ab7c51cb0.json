{"path":"2-仓库/0-代数/_assets/AJBR-Advanced-Linear-AJBR.pdf","text":"Graduate Texts in Mathematics 135 Editorial Board S. Axler K.A. Ribet Graduate Texts in Mathematics 1TAKEUTI/ZARING. Introduction to Axiomatic Set Theory. 2nd ed. 2OXTOBY. Measure and Category. 2nd ed. 3SCHAEFER. Topological Vector Spaces. 2nd ed. 4HILTON/STAMMBACH. A Course in Homological Algebra. 2nd ed. 5MAC LANE. Categories for the Working Mathematician. 2nd ed. 6HUGHES/PIPER. Projective Planes. 7J.-P. SERRE. A Course in Arithmetic. 8TAKEUTI/ZARING. Axiomatic Set Theory. 9HUMPHREYS. Introduction to Lie Algebras and Representation Theory. 10 COHEN. A Course in Simple Homotopy Theory. 11 CONWAY. Functions of One Complex Variable I. 2nd ed. 12 BEALS. Advanced Mathematical Analysis. 13 ANDERSON/FULLER. Rings and Categories of Modules. 2nd ed. 14 GOLUBITSKY/GUILLEMIN. Stable Mappings and Their Singularities. 15 BERBERIAN. Lectures in Functional Analysis and Operator Theory. 16 WINTER. The Structure of Fields. 17 ROSENBLATT. Random Processes. 2nd ed. 18 HALMOS. Measure Theory. 19 HALMOS. A Hilbert Space Problem Book. 2nd ed. 20 HUSEMOLLER. Fibre Bundles. 3rd ed. 21 HUMPHREYS. Linear Algebraic Groups. 22 BARNES/MACK. An Algebraic Introduction to Mathematical Logic. 23 GREUB. Linear Algebra. 4th ed. 24 HOLMES. Geometric Functional Analysis and Its Applications. 25 HEWITT/STROMBERG. Real and Abstract Analysis. 26 MANES. Algebraic Theories. 27 KELLEY. General Topology. 28 ZARISKI/SAMUEL. Commutative Algebra. Vol. I. 29 ZARISKI/SAMUEL. Commutative Algebra. Vol. II. 30 JACOBSON. Lectures in Abstract Algebra I. Basic Concepts. 31 JACOBSON. Lectures in Abstract Algebra II. Linear Algebra. 32 JACOBSON. Lectures in Abstract Algebra III. Theory of Fields and Galois Theory. 33 HIRSCH. Differential Topology. 34 SPITZER. Principles of Random Walk. 2nd ed. 35 ALEXANDER/WERMER. Several Complex Variables and Banach Algebras. 3rd ed. 36 KELLEY/NAMIOKA et al. Linear Topological Spaces. 37 MONK. Mathematical Logic. 38 GRAUERT/FRITZSCHE. Several Complex Variables. 39 ARVESON. An Invitation to C∗-Algebras. 40 KEMENY/SNELL/KNAPP. Denumerable Markov Chains. 2nd ed. 41 APOSTOL. Modular Functions and Dirichlet Series in Number Theory. 2nd ed. 42 J.-P. SERRE. Linear Representations of Finite Groups. 43 GILLMAN/JERISON. Rings of Continuous Functions. 44 KENDIG. Elementary Algebraic Geometry. 45 LOÈVE. Probability Theory I. 4th ed. 46 LOÈVE. Probability Theory II. 4th ed. 47 MOISE. Geometric Topology in Dimensions 2 and 3. 48 SACHS/WU. General Relativity for Mathematicians. 49 GRUENBERG/WEIR. Linear Geometry. 2nd ed. 50 EDWARDS. Fermat’s Last Theorem. 51 KLINGENBERG. A Course in Differential Geometry. 52 HARTSHORNE. Algebraic Geometry. 53 MANIN. A Course in Mathematical Logic. 54 GRAVER/WATKINS. Combinatorics with Emphasis on the Theory of Graphs. 55 BROWN/PEARCY. Introduction to Operator Theory I: Elements of Functional Analysis. 56 MASSEY. Algebraic Topology: An Introduction. 57 CROWELL/FOX. Introduction to Knot Theory. 58 KOBLITZ. p-adic Numbers, p-adic Analysis, and Zeta-Functions. 2nd ed. 59 LANG. Cyclotomic Fields. 60 ARNOLD. Mathematical Methods in Classical Mechanics. 2nd ed. 61 WHITEHEAD. Elements of Homotopy Theory. 62 KARGAPOLOV/MERIZJAKOV. Fundamentals of the Theory of Groups. 63 BOLLOBAS. Graph Theory. 64 EDWARDS. Fourier Series. Vol. I. 2nd ed. 65 WELLS. Differential Analysis on Complex Manifolds. 3rd ed. 66 WATERHOUSE. Introduction to Afﬁne Group Schemes. 67 SERRE. Local Fields. 68 WEIDMANN. Linear Operators in Hilbert Spaces. 69 LANG. Cyclotomic Fields II. 70 MASSEY. Singular Homology Theory. 71 FARKAS/KRA. Riemann Surfaces. 2nd ed. 72 STILLWELL. Classical Topology and Combinatorial Group Theory. 2nd ed. 73 HUNGERFORD.Algebra. 74 DAVENPORT. Multiplicative Number Theory. 3rd ed. 75 HOCHSCHILD. Basic Theory of Algebraic Groups and Lie Algebras. (continued after index) Steven Roman Advanced Linear Algebra Third Edition Steven Roman 8 Night Star Irvine, CA 92603 USA sroman@romanpress.com Editorial Board S. Axler K.A. Ribet Mathematics Department Mathematics Department San Francisco State University University of California at Berkeley San Francisco, CA 94132 Berkeley, CA 94720-3840 USA USA axler@sfsu.edu ribet@math.berkeley.edu ISBN-13: 978-0-387-72828-5 e-ISBN-13: 978-0-387-72831-5 Library of Congress Control Number: 2007934001 Mathematics Subject Classiﬁcation (2000): 15-01 c⃝ 2008 Springer Science+Business Media, LLC All rights reserved. This work may not be translated or copied in whole or in part without the written permission of the publisher (Springer Science+Business Media, LLC, 233 Spring Street, New York, NY 10013, USA), except for brief excerpts in connection with reviews or scholarly analysis. Use in connection with any form of information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology now known or hereafter developed is forbidden. The use in this publication of trade names, trademarks, service marks, and similar terms, even if they are not identiﬁed as such, is not to be taken as an expression of opinion as to whether or not they are subject to proprietary rights. Printed on acid-free paper. 98 765 43 21 springer.com To Donna and to Rashelle, Carol and Dan Preface to the Third Edition Let me begin by thanking the readers of the second edition for their many helpful comments and suggestions, with special thanks to Joe Kidd and Nam Trang. For the third edition, I have corrected all known errors, polished and refined some arguments (such as the discussion of reflexivity, the rational canonical form, best approximations and the definitions of tensor products) and upgraded some proofs that were originally done only for finite-dimensional/rank cases. I have also moved some of the material on projection operators to an earlier position in the text. A few new theorems have been added in this edition, including the spectral mapping theorem and a theorem to the effect that , withdim dim equality if and only if is finite-dimensional. I have also added a new chapter on associative algebras that includes the well- known characterizations of the finite-dimensional division algebras over the real field (a theorem of Frobenius) and over a finite field (Wedderburn's theorem). The reference section has been enlarged considerably, with over a hundred references to books on linear algebra. Steven Roman Irvine, California, May 2007 Preface to the Second Edition Let me begin by thanking the readers of the first edition for their many helpful comments and suggestions. The second edition represents a major change from the first edition. Indeed, one might say that it is a totally new book, with the exception of the general range of topics covered. The text has been completely rewritten. I hope that an additional 12 years and roughly 20 books worth of experience has enabled me to improve the quality of my exposition. Also, the exercise sets have been completely rewritten. The second edition contains two new chapters: a chapter on convexity, separation and positive solutions to linear systems Chapter 15) and a chapter on( the QR decomposition, singular values and pseudoinverses Chapter 17). The( treatments of tensor products and the umbral calculus have been greatly expanded and I have included discussions of determinants in the chapter on( tensor products), the complexification of a real vector space, Schur's theorem and Geršgorin disks. Steven Roman Irvine, California February 2005 Preface to the First Edition This book is a thorough introduction to linear algebra, for the graduate or advanced undergraduate student. Prerequisites are limited to a knowledge of the basic properties of matrices and determinants. However, since we cover the basics of vector spaces and linear transformations rather rapidly, a prior course in linear algebra even at the sophomore level), along with a certain measure of( “mathematical maturity,” is highly desirable. Chapter 0 contains a summary of certain topics in modern algebra that are required for the sequel. This chapter should be skimmed quickly and then used primarily as a reference. Chapters 1–3 contain a discussion of the basic properties of vector spaces and linear transformations. Chapter 4 is devoted to a discussion of modules, emphasizing a comparison between the properties of modules and those of vector spaces. Chapter 5 provides more on modules. The main goals of this chapter are to prove that any two bases of a free module have the same cardinality and to introduce Noetherian modules. However, the instructor may simply skim over this chapter, omitting all proofs. Chapter 6 is devoted to the theory of modules over a principal ideal domain, establishing the cyclic decomposition theorem for finitely generated modules. This theorem is the key to the structure theorems for finite-dimensional linear operators, discussed in Chapters 7 and 8. Chapter 9 is devoted to real and complex inner product spaces. The emphasis here is on the finite-dimensional case, in order to arrive as quickly as possible at the finite-dimensional spectral theorem for normal operators, in Chapter 10. However, we have endeavored to state as many results as is convenient for vector spaces of arbitrary dimension. The second part of the book consists of a collection of independent topics, with the one exception that Chapter 13 requires Chapter 12. Chapter 11 is on metric vector spaces, where we describe the structure of symplectic and orthogonal geometries over various base fields. Chapter 12 contains enough material on metric spaces to allow a unified treatment of topological issues for the basic xii Preface Hilbert space theory of Chapter 13. The rather lengthy proof that every metric space can be embedded in its completion may be omitted. Chapter 14 contains a brief introduction to tensor products. In order to motivate the universal property of tensor products, without getting too involved in categorical terminology, we first treat both free vector spaces and the familiar direct sum, in a universal way. Chapter 15 (Chapter 16 in the second edition) is on affine geometry, emphasizing algebraic, rather than geometric, concepts. The final chapter provides an introduction to a relatively new subject, called the umbral calculus. This is an algebraic theory used to study certain types of polynomial functions that play an important role in applied mathematics. We give only a brief introduction to the subject emphasizing the algebraic aspects, rather than the applications. This is the first time that this subject has appeared in a true textbook. One final comment. Unless otherwise mentioned, omission of a proof in the text is a tacit suggestion that the reader attempt to supply one. Steven Roman Irvine, California Contents Preface to the Third Edition, vii Preface to the Second Edition, ix Preface to the First Edition, xi Preliminaries, 1 Part 1: Preliminaries, 1 Part 2: Algebraic Structures, 17 Part I—Basic Linear Algebra, 33 1 Vector Spaces, 35 Vector Spaces, 35 Subspaces, 37 Direct Sums, 40 Spanning Sets and Linear Independence, 44 The Dimension of a Vector Space, 48 Ordered Bases and Coordinate Matrices, 51 The Row and Column Spaces of a Matrix, 52 The Complexification of a Real Vector Space, 53 Exercises, 55 2 Linear Transformations, 59 Linear Transformations, 59 The Kernel and Image of a Linear Transformation, 61 Isomorphisms, 62 The Rank Plus Nullity Theorem, 63 Linear Transformations from to , 64 Change of Basis Matrices, 65 The Matrix of a Linear Transformation, 66 Change of Bases for Linear Transformations, 68 Equivalence of Matrices, 68 Similarity of Matrices, 70 Similarity of Operators, 71 Invariant Subspaces and Reducing Pairs, 72 Projection Operators, 73 xiv Contents Topological Vector Spaces, 79 Linear Operators on , 82 Exercises, 83 3 The Isomorphism Theorems, 87 Quotient Spaces, 87 The Universal Property of Quotients and the First Isomorphism Theorem, 90 Quotient Spaces, Complements and Codimension, 92 Additional Isomorphism Theorems, 93 Linear Functionals, 94 Dual Bases, 96 Reflexivity, 100 Annihilators, 101 Operator Adjoints, 104 Exercises, 106 4 Modules I: Basic Properties, 109 Motivation, 109 Modules, 109 Submodules, 111 Spanning Sets, 112 Linear Independence, 114 Torsion Elements, 115 Annihilators, 115 Free Modules, 116 Homomorphisms, 117 Quotient Modules, 117 The Correspondence and Isomorphism Theorems, 118 Direct Sums and Direct Summands, 119 Modules Are Not as Nice as Vector Spaces, 124 Exercises, 125 5 Modules II: Free and Noetherian Modules, 127 The Rank of a Free Module, 127 Free Modules and Epimorphisms, 132 Noetherian Modules, 132 The Hilbert Basis Theorem, 136 Exercises, 137 6 Modules over a Principal Ideal Domain, 139 Annihilators and Orders, 139 Cyclic Modules, 140 Free Modules over a Principal Ideal Domain, 142 Torsion-Free and Free Modules, 145 The Primary Cyclic Decomposition Theorem, 146 The Invariant Factor Decomposition, 156 Characterizing Cyclic Modules, 158 Contents xv Indecomposable Modules, 158 Exercises, 159 7 The Structure of a Linear Operator, 163 The Module Associated with a Linear Operator, 164 The Primary Cyclic Decomposition of , 167 The Characteristic Polynomial, 170 Cyclic and Indecomposable Modules, 171 The Big Picture, 174 The Rational Canonical Form, 176 Exercises, 182 8 Eigenvalues and Eigenvectors, 185 Eigenvalues and Eigenvectors, 185 Geometric and Algebraic Multiplicities, 189 The Jordan Canonical Form, 190 Triangularizability and Schur's Theorem, 192 Diagonalizable Operators, 196 Exercises, 198 9 Real and Complex Inner Product Spaces, 205 Norm and Distance, 208 Isometries, 210 Orthogonality, 211 Orthogonal and Orthonormal Sets, 212 The Projection Theorem and Best Approximations, 219 The Riesz Representation Theorem, 221 Exercises, 223 10 Structure Theory for Normal Operators, 227 The Adjoint of a Linear Operator, 227 Unitary Diagonalizability, 233 Normal Operators, 234 Special Types of Normal Operators, 238 Self-Adjoint Operators, 239 Unitary Operators and Isometries, 240 The Structure of Normal Operators, 245 Functional Calculus, 247 Positive Operators, 250 The Polar Decomposition of an Operator, 252 Exercises, 254 Part II—Topics, 257 11 Metric Vector Spaces: The Theory of Bilinear Forms, 259 Symmetric, Skew-Symmetric and Alternate Forms, 259 The Matrix of a Bilinear Form, 261 Orthogonal Projections, 231 xvi Contents Quadratic Forms, 264 Orthogonality, 265 Linear Functionals, 268 Orthogonal Complements and Orthogonal Direct Sums, 269 Isometries, 271 Hyperbolic Spaces, 272 Nonsingular Completions of a Subspace, 273 The Witt Theorems: A Preview, 275 The Classification Problem for Metric Vector Spaces, 276 Symplectic Geometry, 277 The Structure of Orthogonal Geometries: Orthogonal Bases, 282 The Classification of Orthogonal Geometries: Canonical Forms, 285 The Orthogonal Group, 291 The Witt Theorems for Orthogonal Geometries, 294 Maximal Hyperbolic Subspaces of an Orthogonal Geometry, 295 Exercises, 297 12 Metric Spaces, 301 The Definition, 301 Open and Closed Sets, 304 Convergence in a Metric Space, 305 The Closure of a Set, 306 Dense Subsets, 308 Continuity, 310 Completeness, 311 Isometries, 315 The Completion of a Metric Space, 316 Exercises, 321 13 Hilbert Spaces, 325 A Brief Review, 325 Hilbert Spaces, 326 Infinite Series, 330 An Approximation Problem, 331 Hilbert Bases, 335 Fourier Expansions, 336 A Characterization of Hilbert Bases, 346 Hilbert Dimension, 346 A Characterization of Hilbert Spaces, 347 The Riesz Representation Theorem, 349 Exercises, 352 14 Tensor Products, 355 Universality, 355 Bilinear Maps, 359 Tensor Products, 361 Contents xvii When Is a Tensor Product Zero?, 367 Coordinate Matrices and Rank, 368 Characterizing Vectors in a Tensor Product, 371 Defining Linear Transformations on a Tensor Product, 374 The Tensor Product of Linear Transformations, 375 Change of Base Field, 379 Multilinear Maps and Iterated Tensor Products, 382 Tensor Spaces, 385 Special Multilinear Maps, 390 Graded Algebras, 392 The Symmetric and Antisymmetric The Determinant, 403 Exercises, 406 15 Positive Solutions to Linear Systems: Convexity and Separation, 411 Convex, Closed and Compact Sets, 413 Convex Hulls, 414 Linear and Affine Hyperplanes, 416 Separation, 418 Exercises, 423 16 Affine Geometry, 427 Affine Geometry, 427 Affine Combinations, 428 Affine Hulls, 430 The Lattice of Flats, 431 Affine Independence, 433 Affine Transformations, 435 Projective Geometry, 437 Exercises, 440 17 Singular Values and the Moore–Penrose Inverse, 443 Singular Values, 443 The Moore–Penrose Generalized Inverse, 446 Least Squares Approximation, 448 Exercises, 449 18 An Introduction to Algebras, 451 Motivation, 451 Associative Algebras, 451 Division Algebras, 462 Exercises, 469 19 The Umbral Calculus, 471 Formal Power Series, 471 The Umbral Algebra, 473 Tensor Algebras, 392 xviii Contents Formal Power Series as Linear Operators, 477 Sheffer Sequences, 480 Examples of Sheffer Sequences, 488 Umbral Operators and Umbral Shifts, 490 Continuous Operators on the Umbral Algebra, 492 Operator Adjoints, 493 Umbral Operators and Automorphisms of the Umbral Algebra, 494 Umbral Shifts and Derivations of the Umbral Algebra, 499 The Transfer Formulas, 504 A Final Remark, 505 Exercises, 506 References, 507 Index of Symbols, 513 Index, 515 Preliminaries In this chapter, we briefly discuss some topics that are needed for the sequel. This chapter should be skimmed quickly and used primarily as a reference. Part 1 Preliminaries Multisets The following simple concept is much more useful than its infrequent appearance would indicate. Definition Let be a nonempty set. A with is a multiset underlying set set of ordered pairs for where . The number is referred to as the of the multiplicity elements in . If the underlying set of a multiset is finite, we say that the multiset is . The of a finite multiset is the sum of the multiplicitiesfinite size of all of its elements. For example, is a multiset with underlying set . The element has multiplicity . One often writes out the elements of a multiset according to multiplicities, as in . Of course, two mutlisets are equal if their underlying sets are equal and if the multiplicity of each element in the common underlying set is the same in both multisets. Matrices The set of matrices with entries in a field is denoted by or by when the field does not require mention. The set is denoted by or If , the th entry of will be denoted by . The identity matrix of size is denoted by . The elements of the base 2 Advanced Linear Algebra field are called . We expect that the reader is familiar with the basic scalars properties of matrices, including matrix addition and multiplication. The of an matrix is the sequence of entriesmain diagonal where . min Definition The of is the matrix defined bytranspose A matrix is if and if . symmetric skew-symmetric Theorem 0.1 Properties of the transpose Let , . Then() 1) 2) 3 for all ) 4 provided that the product is defined) 5 .)det det Partitioning and Matrix Multiplication Let be a matrix of size . If and , then the is the matrix obtained from by keeping only thesubmatrix rows with index in and the columns with index in . Thus, all other rows and columns are discarded and has size . Suppose that and . Let 1) be a partition of 2) be a partition of 3) be a partition of (Partitions are defined formally later in this chapter.) Then it is a very useful fact that matrix multiplication can be performed at the block level as well as at the entry level. In particular, we have When the partitions in question contain only single-element blocks, this is precisely the usual formula for matrix multiplication Preliminaries 3 Block Matrices It will be convenient to introduce the notational device of a block matrix. If are matrices of the appropriate sizes, then by the block matrix block we mean the matrix whose upper left is , and so on. Thus, thesubmatrix 's are of and not entries. A square matrix of the formsubmatrices block where each is square and is a zero submatrix, is said to be a block diagonal matrix. Elementary Row Operations Recall that there are three types of elementary row operations. Type 1 operations consist of multiplying a row of by a nonzero scalar. Type 2 operations consist of interchanging two rows of . Type 3 operations consist of adding a scalar multiple of one row of to another row of . If we perform an elementary operation of type to an identity matrix , the result is called an of type . It is easy to see that allelementary matrix elementary matrices are invertible. In order to perform an elementary row operation on we can perform that operation on the identity , to obtain an elementary matrix and then take the product . Note that multiplying on the right by has the effect of performing column operations. Definition A matrix is said to be in if reduced row echelon form 1 All rows consisting only of 's appear at the bottom of the matrix.) 2 In any nonzero row, the first nonzero entry is a . This entry is called a) leading entry. 3 For any two consecutive rows, the leading entry of the lower row is to the) right of the leading entry of the upper row. 4 Any column that contains a leading entry has 's in all other positions.) Here are the basic facts concerning reduced row echelon form. 4 Advanced Linear Algebra Theorem 0.2 Matrices are , denoted by , row equivalent if either one can be obtained from the other by a series of elementary row operations. 1 Row equivalence is an equivalence relation. That is,) a ) b ) c , .) 2 A matrix is row equivalent to one and only one matrix that is in) reduced row echelon form. The matrix is called the reduced row echelon form of . Furthermore, where are the elementary matrices required to reduce to reduced row echelon form. 3 is invertible if and only if its reduced row echelon form is an identity) matrix. Hence, a matrix is invertible if and only if it is the product of elementary matrices. The following definition is probably well known to the reader. Definition A square matrix is if all of its entries below theupper triangular main diagonal are . Similarly, a square matrix is if all of its lower triangular entries above the main diagonal are . A square matrix is if all of its diagonal entries off the main diagonal are . Determinants We assume that the reader is familiar with the following basic properties of determinants. Theorem 0.3 Let . Then is an element of . Furthermore, det 1 For any ,) det det det 2 is nonsingular invertible if and only if .)( ) det 3 The determinant of an upper triangular or lower triangular matrix is the) product of the entries on its main diagonal. 4 If a square matrix has the block diagonal form) block then .det det Preliminaries 5 Polynomials The set of all polynomials in the variable with coefficients from a field is denoted by . If , we say that is a polynomial . If over is a polynomial with , then is called the of leading coefficient and the of is , written . For convenience, the degreedegree deg of the zero polynomial is . A polynomial is if its leading coefficient monic is . Theorem 0.4 Let where .()Division algorithm deg Then there exist unique polynomials for which where or . deg deg If , that is, if there exists a polynomial for which divides then we write . A nonzero polynomial is said to split over if can be written as a product of linear factors where . Theorem 0.5 Let . The of and greatest common divisor , denoted by , is the unique monic polynomial over gcd for which 1 and ) 2 if and then .) Furthermore, there exist polynomials and over for which gcd Definition The polynomials are if relatively prime gcd . In particular, and are relatively prime if and only if there exist polynomials and over for which Definition A nonconstant polynomial is if whenever irreducible , then one of and must be constant. The following two theorems support the view that irreducible polynomials behave like prime numbers. 6 Advanced Linear Algebra Theorem 0.6 A nonconstant polynomial is irreducible if and only if it has the property that whenever , then either or . Theorem 0.7 Every nonconstant polynomial in can be written as a product of irreducible polynomials. Moreover, this expression is unique up to order of the factors and multiplication by a scalar. Functions To set our notation, we should make a few comments about functions. Definition Let be a function from a set to a set . 1 The of is the set and the of is .) domain range 2 The of is the set .)imimage 3 is , or an , if .)( ) injective one-to-one injection 4 is , or a , if .)( ) im surjective onto surjection 5 is , or a , if it is both injective and surjective.) bijective bijection 6 Assuming that , the of is) support supp If is injective, then its inverse exists and is well- im defined as a function on .im It will be convenient to apply to subsets of and . In particular, if and if , we set and Note that the latter is defined even if is not injective. Let . If , the of to is the function restriction defined by for all . Clearly, the restriction of an injective map is injective. In the other direction, if and if , then an of to is extension a function for which . Preliminaries 7 Equivalence Relations The concept of an equivalence relation plays a major role in the study of matrices and linear transformations. Definition Let be a nonempty set. A binary relation on is called an equivalence relation on if it satisfies the following conditions: 1)( )Reflexivity for all . 2)( )Symmetry for all . 3)( )Transitivity for all . Definition Let be an equivalence relation on . For , the set of all elements equivalent to is denoted by and called the of .equivalence class Theorem 0.8 Let be an equivalence relation on . Then 1) 2 For any , we have either or .) Definition A of a nonempty set is a collection ofpartition nonempty subsets of , called the of the partition, for which blocks 1 for all ) 2 .) The following theorem sheds considerable light on the concept of an equivalence relation. Theorem 0.9 1 Let be an equivalence relation on . Then the set of equivalence) distinct classes with respect to are the blocks of a partition of . 2 Conversely, if is a partition of , the binary relation defined by) if and lie in the same block of 8 Advanced Linear Algebra is an equivalence relation on , whose equivalence classes are the blocks of . This establishes a one-to-one correspondence between equivalence relations on and partitions of . The most important problem related to equivalence relations is that of finding an efficient way to determine when two elements are equivalent. Unfortunately, in most cases, the definition does not provide an efficient test for equivalence and so we are led to the following concepts. Definition Let be an equivalence relation on . A function , where is any set, is called an of if it is constant on the equivalenceinvariant classes of , that is, and a if it is constant and distinct on the equivalencecomplete invariant classes of , that is, A collection of invariants is called a complete system of invariants if for all Definition Let be an equivalence relation on . A subset is said to be a set of or just a for if for every ,canonical forms canonical form() there is such that . Put another way, each equivalenceexactly one class under contains member of .exactly one Example 0.1 Define a binary relation on by letting if and only if for some nonzero constant . This is easily seen to be an equivalence relation. The function that assigns to each polynomial its degree is an invariant, since deg deg However, it is not a complete invariant, since there are inequivalent polynomials with the same degree. The set of all monic polynomials is a set of canonical forms for this equivalence relation. Example 0.2 We have remarked that row equivalence is an equivalence relation on . Moreover, the subset of reduced row echelon form matrices is a set of canonical forms for row equivalence, since every matrix is row equivalent to a unique matrix in reduced row echelon form. Preliminaries 9 Example 0.3 Two matrices , are row equivalent if and only if there is an invertible matrix such that . Similarly, and are column equivalent, that is, can be reduced to using elementary column operations, if and only if there exists an invertible matrix such that . Two matrices and are said to be if there exist invertible equivalent matrices and for which Put another way, and are equivalent if can be reduced to by performing a series of elementary row and/or column operations. The use of the( term equivalent is unfortunate, since it applies to all equivalence relations, not just this one. However, the terminology is standard, so we use it here.) It is not hard to see that an matrix that is in both reduced row echelon form and reduced column echelon form must have the block form block We leave it to the reader to show that every matrix in is equivalent to exactly one matrix of the form and so the set of these matrices is a set of canonical forms for equivalence. Moreover, the function defined by , where , is a complete invariant for equivalence. Since the rank of is and since neither row nor column operations affect the rank, we deduce that the rank of is . Hence, rank is a complete invariant for equivalence. In other words, two matrices are equivalent if and only if they have the same rank. Example 0.4 Two matrices , are said to be if there exists similar an invertible matrix such that Similarity is easily seen to be an equivalence relation on . As we will learn, two matrices are similar if and only if they represent the same linear operators on a given -dimensional vector space . Hence, similarity is extremely important for studying the structure of linear operators. One of the main goals of this book is to develop canonical forms for similarity. We leave it to the reader to show that the determinant function and the trace function are invariants for similarity. However, these two invariants do not, in general, form a complete system of invariants. Example 0.5 Two matrices , are said to be if there congruent exists an invertible matrix for which 10 Advanced Linear Algebra where is the transpose of . This relation is easily seen to be an equivalence relation and we will devote some effort to finding canonical forms for congruence. For some base fields such as , or a finite field , this is () relatively easy to do, but for other base fields such as , it is extremely() difficult. Zorn's Lemma In order to show that any vector space has a basis, we require a result known as Zorn's lemma. To state this lemma, we need some preliminary definitions. Definition A is a pair where is a nonempty setpartially ordered set and is a binary relation called a , read “less than or equal to,” partial order with the following properties: 1 For all ,)( )Reflexivity 2 For all ,)( )Antisymmetry and implies 3 For all ,)( )Transitivity and implies Partially ordered sets are also called .posets It is customary to use a phrase such as “Let be a partially ordered set” when the partial order is understood. Here are some key terms related to partially ordered sets. Definition Let be a partially ordered set. 1 The , element of , should it exist, is an element)( )maximum largest top with the property that all elements of are less than or equal to , that is, Similarly, the , , element of , should itmimimum least smallest bottom() exist, is an element with the property that all elements of are greater than or equal to , that is, 2 A is an element with the property that there is no) maximal element larger element in , that is, Preliminaries 11 Similarly, a is an element with the property thatminimal element there is no smaller element in , that is, 3 Let . Then is an for and if) upper bound and The unique smallest upper bound for and , if it exists, is called the least upper bound of and and is denoted by . lub 4 Let . Then is a for and if) lower bound and The unique largest lower bound for and , if it exists, is called the greatest lower bound of and and is denoted by . glb Let be a subset of a partially ordered set . We say that an element is an for if for all . Lower bounds are definedupper bound similarly. Note that in a partially ordered set, it is possible that not all elements are comparable. In other words, it is possible to have with the property that and . Definition A partially ordered set in which every pair of elements is comparable is called a , or a . Anytotally ordered set linearly ordered set totally ordered subset of a partially ordered set is called a in .chain Example 0.6 1 The set of real numbers, with the usual binary relation , is a partially) ordered set. It is also a totally ordered set. It has no maximal elements. 2 The set of natural numbers, together with the binary) relation of divides, is a partially ordered set. It is customary to write to indicate that divides . The subset of consisting of all powers of is a totally ordered subset of , that is, it is a chain in . The set is a partially ordered set under . It has two maximal elements, namely and . The subset is a partially ordered set in which every element is both maximal and minimal! 3 Let be any set and let be the power set of , that is, the set of all) subsets of . Then , together with the subset relation , is a partially ordered set. Now we can state Zorn's lemma, which gives a condition under which a partially ordered set has a maximal element. 12 Advanced Linear Algebra Theorem 0.10 If is a partially ordered set in which every()Zorn's lemma chain has an upper bound, then has a maximal element. We will use Zorn's lemma to prove that every vector space has a basis. Zorn's lemma is equivalent to the famous axiom of choice. As such, it is not subject to proof from the other axioms of ordinary (ZF) set theory. Zorn's lemma has many important equivalancies, one of which is the . A well-ordering principle well ordering on a nonempty set is a total order on with the property that every nonempty subset of has a least element. Theorem 0.11 Every nonempty set has a well()Well-ordering principle ordering. Cardinality Two sets and have the same , written cardinality if there is a bijective function a one-to-one correspondence between the sets.() The reader is probably aware of the fact that and where denotes the natural numbers, the integers and the rational numbers. If is in one-to-one correspondence with a of , we write . If subset is in one-to-one correspondence with a subset of but not all of ,proper then we write . The second condition is necessary, since, for instance, is in one-to-one correspondence with a proper subset of and yet is also in one-to-one correspondence with itself. Hence, . This is not the place to enter into a detailed discussion of cardinal numbers. The intention here is that the cardinality of a set, whatever that is, represents the “size” of the set. It is actually easier to talk about two sets having the same, or different, size cardinality than it is to explicitly define the size cardinality of() () a given set. Be that as it may, we associate to each set a cardinal number, denoted by or , that is intended to measure the size of the set. Actually, cardinalcard numbers are just very special types of sets. However, we can simply think of them as vague amorphous objects that measure the size of sets. Definition 1 A set is if it can be put in one-to-one correspondence with a set of the) finite form , for some nonnegative integer . A set that is Preliminaries 13 not finite is . The or of a finite set isinfinite cardinal number cardinality() just the number of elements in the set. 2 The of the set of natural numbers is read “aleph)(cardinal number nought” , where is the first letter of the Hebrew alphabet. Hence,) 3 Any set with cardinality is called a set and any finite) countably infinite or countably infinite set is called a set. An infinite set that is notcountable countable is said to be .uncountable Since it can be shown that , the real numbers are uncountable. If and are sets, then it is well known that finite and The first part of the next theorem tells us that this is also true for infinite sets. The reader will no doubt recall that the of a set is the set ofpower set all subsets of . For finite sets, the power set of is always bigger than the set itself. In fact, The second part of the next theorem says that the power set of any set is bigger has larger cardinality than itself. On the other hand, the third part of() this theorem says that, for infinite sets , the set of all subsets of is thefinite same size as . Theorem 0.12 1 – For any sets and ,)( )Schroder Bernstein theorem¨ and 2 If denotes the power set of , then)( )Cantor's theorem 3 If denotes the set of all finite subsets of and if is an infinite set,) then Proof. We prove only parts 1 and 2 . Let be an injective function)) from into and let be an injective function from into . We want to use these functions to create a bijective function from to . For this purpose, we make the following definitions. The of an elementdescendants are the elements obtained by repeated alternate applications of the functions and , namely 14 Advanced Linear Algebra If is a descendant of , then is an of . Descendants and ancestors ancestor of elements of are defined similarly. Now, by tracing an element's ancestry to its beginning, we find that there are three possibilities: the element may originate in , or in , or it may have no point of origin. Accordingly, we can write as the union of three disjoint sets originates in originates in has no originator Similarly, is the disjoint union of , and . Now, the restriction is a bijection. To see this, note that if , then originated in and therefore must have the form for some . But and its ancestor have the same point of origin and so implies . Thus, is surjective and hence bijective. We leave it to the reader to show that the functions and are also bijections. Putting these three bijections together gives a bijection between and . Hence, , as desired. We now prove Cantor's theorem. The map defined by is an injection from to and so . To complete the proof we must show that no injective map can be surjective. To this end, let We claim that is not in . For suppose that for some . im Then if , we have by the definition of that . On the other hand, if , we have again by the definition of that . This contradiction implies that and so is not surjective. im Cardinal Arithmetic Now let us define addition, multiplication and exponentiation of cardinal numbers. If and are sets, the is the set of all cartesian product ordered pairs The set of all functions from to is denoted by . Preliminaries 15 Definition Let and denote cardinal numbers. Let and be disjoint sets for which and . 1 The is the cardinal number of .) sum 2 The is the cardinal number of .) product 3 The is the cardinal number of .) power We will not go into the details of why these definitions make sense. For( instance, they seem to depend on the sets and , but in fact they do not. It ) can be shown, using these definitions, that cardinal addition and multiplication are associative and commutative and that multiplication distributes over addition. Theorem 0.13 Let , and be cardinal numbers. Then the following properties hold: 1)( )Associativity and 2)( )Commutativity and 3)( )Distributivity 4 Properties of Exponents)( ) a ) b ) c ) On the other hand, the arithmetic of cardinal numbers can seem a bit strange, as the next theorem shows. Theorem 0.14 Let and be cardinal numbers, at least one of which is infinite. Then max It is not hard to see that there is a one-to-one correspondence between the power set of a set and the set of all functions from to . This leads to the following theorem. Theorem 0.15 For any cardinal 1 If , then ) 2) 16 Advanced Linear Algebra We have already observed that . It can be shown that is the smallest infinite cardinal, that is, 0 is a natural number It can also be shown that the set of real numbers is in one-to-one correspondence with the power set of the natural numbers. Therefore, The set of all points on the real line is sometimes called the and socontinuum is sometimes called the and denoted by .power of the continuum Theorem 0.14 shows that cardinal addition and multiplication have a kind of “absorption” quality, which makes it hard to produce larger cardinals from smaller ones. The next theorem demonstrates this more dramatically. Theorem 0.16 1 Addition applied a countable number of times or multiplication applied a) finite number of times to the cardinal number , does not yield anything more than . Specifically, for any nonzero , we have and 2 Addition and multiplication applied a countable number of times to the) cardinal number does not yield more than . Specifically, we have and Using this theorem, we can establish other relationships, such as which, by the Schro¨der–Bernstein theorem, implies that We mention that the problem of evaluating in general is a very difficult one and would take us far beyond the scope of this book. We will have use for the following reasonable-sounding result, whose proof is omitted. Theorem 0.17 Let be a collection of sets, indexed by the set , with . If for all , then Let us conclude by describing the cardinality of some famous sets. Preliminaries 17 Theorem 0.18 1 The following sets have cardinality .) a The rational numbers .) b The set of all finite subsets of .) c The union of a countable number of countable sets.) d The set of all ordered -tuples of integers.) 2 The following sets have cardinality .) a The set of all points in .) b The set of all infinite sequences of natural numbers.) c The set of all infinite sequences of real numbers.) d The set of all finite subsets of .) e The set of all irrational numbers.) Part 2 Algebraic Structures We now turn to a discussion of some of the many algebraic structures that play a role in the study of linear algebra. Groups Definition A is a nonempty set , together with a binary operationgroup denoted by *, that satisfies the following properties: 1 For all ,)( )Associativity 2 There exists an element for which)( )Identity for all . 3 For each , there is an element for which)( )Inverses Definition A group is , or , if abelian commutative for all . When a group is abelian, it is customary to denote the operation by +, thus writing as . It is also customary to refer to the identity as the and to denote the inverse by , referred to aszero element the of .negative Example 0.7 The set of all bijective functions from a set to is a group under composition of functions. However, in general, it is not abelian. Example 0.8 The set is an abelian group under addition of matrices. The identity is the zero matrix 0 of size . The set is not a group under multiplication of matrices, since not all matrices have multiplicative 18 Advanced Linear Algebra inverses. However, the set of invertible matrices of size is a nonabelian () group under multiplication. A group is if it contains only a finite number of elements. The finite cardinality of a finite group is called its and is denoted by ororder simply . Thus, for example, is a finite group under addition modulo , but is not finite. Definition A of a group is a nonempty subset of that is asubgroup group in its own right, using the same operations as defined on . Cyclic Groups If is a formal symbol, we can define a group to be the set of all integral powers of : where the product is defined by the formal rules of exponents: This group is denoted by and called the . The cyclic group generated by identity of is . In general, a group is if it has the form cyclic for some . We can also create a finite group of arbitrary positive order by declaring that . Thus, where the product is defined by the formal rules of exponents, followed by reduction modulo : mod This defines a group of order , called a . The inversecyclic group of order of is . mod Rings Definition A is a nonempty set , together with two binary operations,ring called denoted by and denoted by juxtaposition ,addition multiplication() ( ) for which the following hold: 1 is an abelian group under addition) 2 For all ,)( )Associativity Preliminaries 19 3 For all ,)( )Distributivity and A ring is said to be if for all . If a ring commutative contains an element with the property that for all , we say that is a . The identity is usually ring with identity denoted by . A is a commutative ring with identity in which each nonzero elementfield has a multiplicative inverse, that is, if is nonzero, then there is a for which . Example 0.9 The set is a commutative ring under addition and multiplication modulo mod mod The element is the identity. Example 0.10 The set of even integers is a commutative ring under the usual operations on , but it has no identity. Example 0.11 The set is a noncommutative ring under matrix addition and multiplication. The identity matrix is the identity for . Example 0.12 Let be a field. The set of all polynomials in a single variable , with coefficients in , is a commutative ring under the usual operations of polynomial addition and multiplication. What is the identity for ? Similarly, the set of polynomials in variables is a commutative ring under the usual addition and multiplication of polynomials. Definition If and are rings, then a function is a ring homomorphism if for all . Definition A of a ring is a subset of that is a ring in its ownsubring right, using the same operations as defined on and having the same multiplicative identity as . 20 Advanced Linear Algebra The condition that a subring have the same multiplicative identity as is required. For example, the set of all matrices of the form for is a ring under addition and multiplication of matrices isomorphic to ( ). The multiplicative identity in is the matrix , which is not the identity of . Hence, is a ring under the same operations as but it is not a subring of . Applying the definition is not generally the easiest way to show that a subset of a ring is a subring. The following characterization is usually easier to apply. Theorem 0.19 A nonempty subset of a ring is a subring if and only if 1 The multiplicative identity of is in ) 2 is closed under subtraction, that is,) 3 is closed under multiplication, that is,) Ideals Rings have another important substructure besides subrings. Definition Let be a ring. A nonempty subset of is called an if ideal 1 is a subgroup of the abelian group , that is, is closed under) subtraction: 2 is closed under multiplication by ring element, that is,) any and Note that if an ideal contains the unit element , then . Example 0.13 Let be a polynomial in . The set of all multiples of , is an ideal in , called the . ideal generated by Definition Let be a subset of a ring with identity. The set Preliminaries 21 of all finite linear combinations of elements of , with coefficients in , is an ideal in , called the . It is the smallest in the sense of setideal generated by ( inclusion ideal of containing . If is a finite set, we write) Note that in the previous definition, we require that have an identity. This is to ensure that . Theorem 0.20 Let be a ring. 1 The intersection of any collection of ideals is an ideal.) 2 If is an ascending sequence of ideals, each one contained in) the next, then the union is also an ideal. 3 More generally, if) is a chain of ideals in , then the union is also an ideal in . Proof. To prove 1 , let . Then if , we have for all) . Hence, for all and so . Hence, is closed under subtraction. Also, if , then for all and so . Of course, part 2 is a special case of part 3 . To prove 3 , if , then )) ) and for some . Since one of and is contained in the other, we may assume that . It follows that and so and if , then . Thus is an ideal. Note that in general, the union of ideals is not an ideal. However, as we have just proved, the union of any of ideals is an ideal. chain Quotient Rings and Maximal Ideals Let be a subset of a commutative ring with identity. Let be the binary relation on defined by It is easy to see that is an equivalence relation. When , we say that and are . The term “mod” is used as a colloquialism forcongruent modulo modulo and is often written mod As shorthand, we write . 22 Advanced Linear Algebra To see what the equivalence classes look like, observe that for some The set is called a of in . The element is called a forcoset coset representative . Thus, the equivalence classes for congruence mod are the cosets of in . The set of all cosets is denoted by This is read “ mod .” We would like to place a ring structure on . Indeed, if is a subgroup of the abelian group , then is easily seen to be an abelian group as well under coset addition defined by In order for the product to be well-defined, we must have or, equivalently, But may be any element of and may be any element of and so this condition implies that must be an ideal. Conversely, if is an ideal, then coset multiplication is well defined. Theorem 0.21 Let be a commutative ring with identity. Then the quotient is a ring under coset addition and multiplication if and only if is an ideal of . In this case, is called the of , where quotient ring modulo addition and multiplication are defined by Preliminaries 23 Definition An ideal in a ring is a if and if whenever maximal ideal is an ideal satisfying , then either or . Here is one reason why maximal ideals are important. Theorem 0.22 Let be a commutative ring with identity. Then the quotient ring is a field if and only if is a maximal ideal. Proof. First, note that for any ideal of , the ideals of are precisely the quotients where is an ideal for which . It is clear that is an ideal of . Conversely, if is an ideal of , then let It is easy to see that is an ideal of for which . Next, observe that a commutative ring with identity is a field if and only if has no nonzero proper ideals. For if is a field and is an ideal of containing a nonzero element , then and so . Conversely, if has no nonzero proper ideals and , then the ideal must be and so there is an for which . Hence, is a field. Putting these two facts together proves the theorem. The following result says that maximal ideals always exist. Theorem 0.23 Any nonzero commutative ring with identity contains a maximal ideal. Proof. Since is not the zero ring, the ideal is a proper ideal of . Hence, the set of all proper ideals of is nonempty. If is a chain of proper ideals in , then the union is also an ideal. Furthermore, if is not proper, then and so , for some , which implies that is not proper. Hence, . Thus, any chain in has an upper bound in and so Zorn's lemma implies that has a maximal element. This shows that has a maximal ideal. Integral Domains Definition Let be a ring. A nonzero element r is called a if zero divisor there exists a nonzero for which . A commutative ring with identity is called an if it contains no zero divisors.integral domain Example 0.14 If is not a prime number, then the ring has zero divisors and so is not an integral domain. To see this, observe that if is not prime, then in , where . But in , we have 24 Advanced Linear Algebra mod and so and are both zero divisors. As we will see later, if is a prime, then is a field which is an integral domain, of course .() Example 0.15 The ring is an integral domain, since implies that or . If is a ring and where , then we cannot in general cancel the 's and conclude that . For instance, in , we have , but canceling the 's gives . However, it is precisely the integral domains in which we can cancel. The simple proof is left to the reader. Theorem 0.24 Let be a commutative ring with identity. Then is an integral domain if and only if the cancellation law holds. The Field of Quotients of an Integral Domain Any integral domain can be embedded in a field. The or quotient field field( of quotients) of is a field that is constructed from just as the field of rational numbers is constructed from the ring of integers. In particular, we set where if and only if . Addition and multiplication of fractions is defined by and It is customary to write in the form . Note that if has zero divisors, then these definitions do not make sense, because may be even if and are not. This is why we require that be an integral domain. Principal Ideal Domains Definition Let be a ring with identity and let . The principal ideal generated by is the ideal An in which every ideal is a principal ideal is called aintegral domain principal ideal domain. Preliminaries 25 Theorem 0.25 The integers form a principal ideal domain. In fact, any ideal in is generated by the smallest positive integer a that is contained in . Theorem 0.26 The ring is a principal ideal domain. In fact, any ideal is generated by the unique monic polynomial of smallest degree contained in . Moreover, for polynomials , gcd Proof. Let be an ideal in and let be a monic polynomial of smallest degree in . First, we observe that there is only one such polynomial in . For if is monic and , then deg deg and since , we must have and sodeg deg . We show that . Since , we have . To establish the reverse inclusion, if , then dividing by gives where or deg deg . But since is an ideal, and so is impossible. Hence, and deg deg This shows that and so . To prove the second statement, let . Then, by what we have just shown, where is the unique monic polynomial in of smallest degree. In particular, since , we have for each . In other words, is a common divisor of the 's. Moreover, if for all , then for all , which implies that and so . This shows that is the common divisor of the greatest 's and completes the proof. 26 Advanced Linear Algebra Example 0.16 The ring of polynomials in two variables and is not a principal ideal domain. To see this, observe that the set of all polynomials with zero constant term is an ideal in . Now, suppose that is the principal ideal . Since , there exist polynomials and for which and 0.1() But cannot be a constant, for then we would have . Hence, deg and so and must both be constants, which implies that 0.1 cannot hold.() Theorem 0.27 Any principal ideal domain satisfies the ascending chain condition, that is, cannot have a strictly increasing sequence of ideals where each ideal is properly contained in the next one. Proof. Suppose to the contrary that there is such an increasing sequence of ideals. Consider the ideal which must have the form for some . Since for some , we have for all , contradicting the fact that the inclusions are proper. Prime and Irreducible Elements We can define the notion of a prime element in any integral domain. For , we say that written if there exists an fordivides () which . Definition Let be an integral domain. 1 An invertible element of is called a . Thus, is a unit if ) unit for some . 2 Two elements are said to be if there exists a unit for) associates which . We denote this by writing . 3 A nonzero nonunit is said to be if) prime or 4 A nonzero nonunit is said to be if) irreducible or is a unit Note that if is prime or irreducible, then so is for any unit . The property of being associate is clearly an equivalence relation. Preliminaries 27 Definition We will refer to the equivalence classes under the relation of being associate as the of .associate classes Theorem 0.28 Let be a ring. 1 An element is a unit if and only if .) 2 if and only if .) 3 divides if and only if .) 4 , that is, where is not a unit, if and only if) properly divides . In the case of the integers, an integer is prime if and only if it is irreducible. In any integral domain, prime elements are irreducible, but the converse need not hold. In the ring the irreducible element ( divides the product but does not divide either factor.) However, in principal ideal domains, the two concepts are equivalent. Theorem 0.29 Let be a principal ideal domain. 1 An is irreducible if and only if the ideal is maximal.) 2 An element in is prime if and only if it is irreducible.) 3 The elements are , that is, have no common) relatively prime nonunit factors, if and only if there exist for which This is denoted by writing . Proof. To prove 1 , suppose that is irreducible and that . Then) and so for some . The irreducibility of implies that or is a unit. If is a unit, then and if is a unit, then . This shows that is maximal. We have , since is not a unit. () Conversely, suppose that is not irreducible, that is, where neither nor is a unit. Then . But if , then , which implies that is a unit. Hence . Also, if , then must be a unit. So we conclude that is not maximal, as desired. To prove 2 , assume first that is prime and . Then or . We) may assume that . Therefore, . Canceling 's gives and so is a unit. Hence, is irreducible. Note that this argument applies in ( any integral domain.) Conversely, suppose that is irreducible and let . We wish to prove that or . The ideal is maximal and so or . In the former case, and we are done. In the latter case, we have 28 Advanced Linear Algebra for some . Thus, and since divides both terms on the right, we have . To prove 3 , it is clear that if , then and are relatively prime. For) the converse, consider the ideal , which must be principal, say . Then and and so must be a unit, which implies that . Hence, there exist for which . Unique Factorization Domains Definition An integral domain is said to be a unique factorization domain if it has the following factorization properties: 1 Every nonzero nonunit element can be written as a product of a finite) number of irreducible elements . 2 The factorization into irreducible elements is unique in the sense that if) and are two such factorizations, then and after a suitable reindexing of the factors, . Unique factorization is clearly a desirable property. Fortunately, principal ideal domains have this property. Theorem 0.30 Every principal ideal domain is a unique factorization domain. Proof. Let be a nonzero nonunit. If is irreducible, then we are done. If not, then , where neither factor is a unit. If and are irreducible, we are done. If not, suppose that is not irreducible. Then , where neither nor is a unit. Continuing in this way, we obtain a factorization of the form after renumbering if necessary() Each step is a factorization of into a product of nonunits. However, this process must stop after a finite number of steps, for otherwise it will produce an infinite sequence of nonunits of for which properly divides . But this gives the ascending chain of ideals where the inclusions are proper. But this contradicts the fact that a principal ideal domain satisfies the ascending chain condition. Thus, we conclude that every nonzero nonunit has a factorization into irreducible elements. As to uniqueness, if and are two such factorizations, then because is an integral domain, we may equate them and cancel like factors, so let us assume this has been done. Thus, for all . If there are no factors on either side, we are done. If exactly one side has no factors left, Preliminaries 29 then we have expressed as a product of irreducible elements, which is not possible since irreducible elements are nonunits. Suppose that both sides have factors left, that is, where . Then , which implies that for some . We can assume by reindexing if necessary that . Since is irreducible must be a unit. Replacing by and canceling gives This process can be repeated until we run out of 's or 's. If we run out of 's first, then we have an equation of the form where is a unit, which is not possible since the 's are not units. By the same reasoning, we cannot run out of 's first and so and the 's and 's can be paired off as associates. Fields For the record, let us give the definition of a field a concept that we have been( using .) Definition A is a set , containing at least two elements, together with twofield binary operations, called denoted by and addition multiplication() ()denoted by juxtaposition , for which the following hold: 1 is an abelian group under addition.) 2 The set of all elements in is an abelian group under) nonzero multiplication. 3 For all ,)( )Distributivity and We require that have at least two elements to avoid the pathological case in which . Example 0.17 The sets , and , of all rational, real and complex numbers, respectively, are fields, under the usual operations of addition and multiplication of numbers. Example 0.18 The ring is a field if and only if is a prime number. We have already seen that is not a field if is not prime, since a field is also an integral domain. Now suppose that is a prime. We have seen that is an integral domain and so it remains to show that every nonzero element in has a multiplicative inverse. Let . Since , we know that and are relatively prime. It follows that there exist integers and for which 30 Advanced Linear Algebra Hence, mod and so in , that is, is the multiplicative inverse of . The previous example shows that not all fields are infinite sets. In fact, finite fields play an extremely important role in many areas of abstract and applied mathematics. A field is said to be if every nonconstant polynomial algebraically closed over has a root in . This is equivalent to saying that every nonconstant polynomial splits over . For example, the complex field is algebraically closed but the real field is not. We mention without proof that every field is contained in an algebraically closed field , called the of .algebraic closure For example, the algebraic closure of the real field is the complex field. The Characteristic of a Ring Let be a ring with identity. If is a positive integer, then by , we simply mean terms Now, it may happen that there is a positive integer for which For instance, in , we have . On the other hand, in , the equation implies and so no such positive integer exists. Notice that in any ring, there must exist such a positive integer , since thefinite members of the infinite sequence of numbers cannot be distinct and so for some , whence . Definition Let be a ring with identity. The smallest positive integer for which is called the of . If no such number exists, we characteristic say that has characteristic . The characteristic of is denoted by char. If , then for any , we havechar terms terms Preliminaries 31 Theorem 0.31 Any finite ring has nonzero characteristic. Any finite integral domain has prime characteristic. Proof. We have already seen that a finite ring has nonzero characteristic. Let be a finite integral domain and suppose that . If , wherechar , then . Hence, , implying that or . In either case, we have a contradiction to the fact that is the smallest positive integer such that . Hence, must be prime. Notice that in any field of characteristic , we have for all . Thus, in , for all This property takes a bit of getting used to and makes fields of characteristic quite exceptional. As it happens, there are many important uses for fields of( characteristic . It can be shown that all finite fields have size equal to a ) positive integral power of a prime and for each prime power , there is a finite field of size . In fact, up to isomorphism, there is exactly one finite field of size . Algebras The final algebraic structure of which we will have use is a combination of a vector space and a ring. We have not yet officially defined vector spaces, but( we will do so before needing the following definition, which is placed here for easy reference.) Definition An over a field is a nonempty set , together withalgebra three operations, called denoted by , denoted byaddition multiplication() ( juxtaposition and also denoted by juxtaposition , for)( )scalar multiplication which the following properties hold: 1 is a vector space over under addition and scalar multiplication.) 2 is a ring under addition and multiplication.) 3 If and , then) Thus, an algebra is a vector space in which we can take the product of vectors, or a ring in which we can multiply each element by a scalar subject, of course,( to additional requirements as given in the definition .) Part I—Basic Linear AlgebraChapter 1 Vector Spaces Vector Spaces Let us begin with the definition of one of our principal objects of study. Definition Let be a field, whose elements are referred to as . A scalars vector space vectors over is a nonempty set , whose elements are referred to as , together with two operations. The first operation, called and denotedaddition by , assigns to each pair of vectors in a vector in . The second operation, called and denoted by juxtaposition,scalar multiplication assigns to each pair a vector in . Furthermore, the following properties must be satisfied: 1 For all vectors ,)( )Associativity of addition 2 For all vectors ,)( )Commutativity of addition 3 There is a vector with the property that)( )Existence of a zero for all vectors . 4 For each vector , there is a vector)( )Existence of additive inverses in , denoted by , with the property that 36 Advanced Linear Algebra 5 For all scalars F and for all)( )Properties of scalar multiplication vectors , Note that the first four properties in the definition of vector space can be summarized by saying that is an abelian group under addition. A vector space over a field is sometimes called an . A vector space -space over the real field is called a and a vector space over thereal vector space complex field is called a .complex vector space Definition Let be a nonempty subset of a vector space . A linear combination of vectors in is an expression of the form where and . The scalars are called the coefficients trivial of the linear combination. A linear combination is if every coefficient is zero. Otherwise, it is . nontrivial Examples of Vector Spaces Here are a few examples of vector spaces. Example 1.1 1 Let be a field. The set of all functions from to is a vector space) over , under the operations of ordinary addition and scalar multiplication of functions: and 2 The set of all matrices with entries in a field is a vector) space over , under the operations of matrix addition and scalar multiplication. 3 The set of all ordered -tuples whose components lie in a field , is a) vector space over , with addition and scalar multiplication defined componentwise: and Vector Spaces 37 When convenient, we will also write the elements of in column form. When is a finite field with elements, we write for . 4 Many sequence spaces are vector spaces. The set Seq of all infinite) sequences with members from a field is a vector space under the componentwise operations and In a similar way, the set of all sequences of complex numbers that converge to is a vector space, as is the set of all bounded complex sequences. Also, if is a positive integer, then the set of all complex sequences for which is a vector space under componentwise operations. To see that addition is a binary operation on , one verifies Minkowski's inequality which we will not do here. Subspaces Most algebraic structures contain substructures, and vector spaces are no exception. Definition A of a vector space is a subset of that is a vectorsubspace space in its own right under the operations obtained by restricting the operations of to . We use the notation to indicate that is a subspace of and to indicate that is a of , that is, proper subspace but . The of is .zero subspace Since many of the properties of addition and scalar multiplication hold a fortiori in a nonempty subset , we can establish that is a subspace merely by checking that is closed under the operations of . Theorem 1.1 A nonempty subset of a vector space is a subspace of if and only if is closed under addition and scalar multiplication or, equivalently, 38 Advanced Linear Algebra is closed under linear combinations, that is, Example 1.2 Consider the vector space of all binary -tuples, that is, -tuples of 's and 's. The of a vector is the numberweight of nonzero coordinates in . For instance, . Let be the set of all vectors in of even weight. Then is a subspace of . To see this, note that where is the vector in whose th component is the product of the th components of and , that is, Hence, if and are both even, so is . Finally, scalar multiplication over is trivial and so is a subspace of , known as the of .even weight subspace Example 1.3 Any subspace of the vector space is called a . linear code Linear codes are among the most important and most studied types of codes, because their structure allows for efficient encoding and decoding of information. The Lattice of Subspaces The set of all subspaces of a vector space is partially ordered by set inclusion. The zero subspace is the smallest element in and the entire space is the largest element. If , then is the largest subspace of that is contained in both and . In terms of set inclusion, is the of greatest lower bound and : glb Similarly, if is any collection of subspaces of , then their intersection is the greatest lower bound of the subspaces: glb On the other hand, if and is infinite , then if () and only if or . Thus, the union of two subspaces is never a subspace in any “interesting” case. We also have the following. Vector Spaces 39 Theorem 1.2 A nontrivial vector space over an infinite field is not the union of a finite number of proper subspaces. Proof. Suppose that , where we may assume that Let and let . Consider the infinite set which is the “line” through , parallel to . We want to show that each contains at most one vector from the infinite set , which is contrary to the fact that . This will prove the theorem. If for , then implies , contrary to assumption. Next, suppose that and , for , where . Then and so , which is also contrary to assumption. To determine the smallest subspace of containing the subspaces and , we make the following definition. Definition Let and be subspaces of . The is defined by sum More generally, the of any collection of subspaces is the setsum of all finite sums of vectors from the union : It is not hard to show that the sum of any collection of subspaces of is a subspace of and that the sum is the least upper bound under set inclusion: lub More generally, lub If a partially ordered set has the property that every pair of elements has a least upper bound and greatest lower bound, then is called a . If haslattice a smallest element and a largest element and has the property that every collection of elements has a least upper bound and greatest lower bound, then 40 Advanced Linear Algebra is called a . The least upper bound of a collection is also calledcomplete lattice the of the collection and the greatest lower bound is called the .join meet Theorem 1.3 The set of all subspaces of a vector space is a complete lattice under set inclusion, with smallest element , largest element , meet glb and join lub Direct Sums As we will see, there are many ways to construct new vector spaces from old ones. External Direct Sums Definition Let be vector spaces over a field . The external direct sum of , denoted by is the vector space whose elements are ordered -tuples: with componentwise operations and for all . Example 1.4 The vector space is the external direct sum of copies of , that is, where there are summands on the right-hand side. This construction can be generalized to any collection of vector spaces by generalizing the idea that an ordered -tuple is just a function from the to the union of the spacesindex set with the property that . Vector Spaces 41 Definition Let be any family of vector spaces over . The direct product of is the vector space thought of as a subspace of the vector space of all functions from to . It will prove more useful to restrict the set of functions to those with finite support. Definition Let be a family of vector spaces over . The support of a function is the set supp Thus, a function has if for all but a finite number of finite support . The of the family is the vector spaceexternal direct sum ext , has finite support thought of as a subspace of the vector space of all functions from to . An important special case occurs when for all . If we let denote the set of all functions from to and denote the set of all functions in that have finite support, then and ext Note that the direct product and the external direct sum are the same for a finite family of vector spaces. Internal Direct Sums An internal version of the direct sum construction is often more relevant. Definition A vector space is the of a family ()internal direct sum of subspaces of , written or if the following hold: 42 Advanced Linear Algebra 1 is the sum join of the family :)( ) ( )Join of the family 2 For each ,)( )Independence of the family In this case, each is called a of . If is a direct summand finite family, the direct sum is often written Finally, if , then is called a of in . complement Note that the condition in part 2) of the previous definition is thanstronger saying simply that the members of are pairwise disjoint: for all . A word of caution is in order here: If and are subspaces of , then we may always say that the sum exists. However, to say that the direct sum of and exists or to write is to imply that . Thus, while the sum of two subspaces always exists, the sum of two subspaces does notdirect always exist. Similar statements apply to families of subspaces of . The reader will be asked in a later chapter to show that the concepts of internal and external direct sum are essentially equivalent isomorphic . For this reason,() the term “direct sum” is often used without qualification. Once we have discussed the concept of a basis, the following theorem can be easily proved. Theorem 1.4 Any subspace of a vector space has a complement, that is, if is a subspace of , then there exists a subspace for which . It should be emphasized that a subspace generally has many complements ()although they are isomorphic . The reader can easily find examples of this in . We can characterize the uniqueness part of the definition of direct sum in other useful ways. First a remark. If and are distinct subspaces of and if , then the sum can be thought of as a sum of vectors from the Vector Spaces 43 same subspace (say ) or from different subspaces—one from and one from . When we say that a vector cannot be written as a sum of vectors from the distinct subspaces and , we mean that cannot be written as a sum where and as coming from different subspaces, even if can be interpreted they can also be interpreted as coming from the same subspace. Thus, if , then express as a sum of vectors from distinctdoes subspaces. Theorem 1.5 Let be a family of distinct subspaces of . The following are equivalent: 1 For each ,)( )Independence of the family 2 The zero vector cannot be written as a)( )Uniqueness of expression for sum of nonzero vectors from distinct subspaces of . 3 Every nonzero has a unique, except for)( )Uniqueness of expression order of terms, expression as a sum of nonzero vectors from distinct subspaces in . Hence, a sum is direct if and only if any one of 1 3 holds.)– ) Proof. Suppose that 2) fails, that is, where the nonzero 's are from distinct subspaces . Then and so which violates 1). Hence, 1) implies 2). If 2) holds and and where the terms are nonzero and the 's belong to distinct subspaces in and similarily for the 's, then By collecting terms from the same subspaces, we may write 44 Advanced Linear Algebra Then 2) implies that and for all . Hence, 2) implies 3). Finally, suppose that 3) holds. If then and where are nonzero. But this violates 3). Example 1.5 Any matrix can be written in the form ()1.1 where is the transpose of . It is easy to verify that is symmetric and is skew-symmetric and so 1.1 is a decomposition of as the sum of a symmetric() matrix and a skew-symmetric matrix. Since the sets Sym and SkewSym of all symmetric and skew-symmetric matrices in are subspaces of , we have Sym SkewSym Furthermore, if , where and are symmetric and and are skew-symmetric, then the matrix is both symmetric and skew-symmetric. Hence, provided that , wechar must have and so and . Thus, Sym SkewSym Spanning Sets and Linear Independence A set of vectors a vector space if every vector can be written as a linearspans combination of some of the vectors in that set. Here is the formal definition. Definition The or by a nonempty setsubspace spanned subspace generated() of vectors in is the set of all linear combinations of vectors from : span Vector Spaces 45 When is a finite set, we use the notation or span . A set of vectors in is said to , or , ifspan generate span . It is clear that any superset of a spanning set is also a spanning set. Note also that all vector spaces have spanning sets, since spans itself. Linear Independence Linear independence is a fundamental concept. Definition Let be a vector space. A nonempty set of vectors in is linearly independent if for any distinct vectors in , for all In words, is linearly independent if the only linear combination of vectors from that is equal to is the trivial linear combination, all of whose coefficients are . If is not linearly independent, it is said to be linearly dependent. It is immediate that a linearly independent set of vectors cannot contain the zero vector, since then violates the condition of linear independence. Another way to phrase the definition of linear independence is to say that is linearly independent if the zero vector has an “as unique as possible” expression as a linear combination of vectors from . We can never prevent the zero vector from being written in the form , but we can prevent from being written in any other way as a linear combination of the vectors in . For the introspective reader, the expression has two interpretations. One is where and , but this does not involve distinct vectors so is not relevant to the question of linear independence. The other interpretation is where (assuming that ). Thus, if is linearly independent, then cannot contain both and . Definition Let be a nonempty set of vectors in . To say that a nonzero vector is an linear combination of the vectors in is essentially unique to say that, up to order of terms, there is one and only one way to express as a linear combination where the 's are distinct vectors in and the coefficients are nonzero. More explicitly, is an essentially unique linear combination of the vectors in if and if whenever 46 Advanced Linear Algebra and where the 's are distinct, the 's are distinct and all coefficients are nonzero, then and after a reindexing of the 's if necessary, we have and for all . Note that this is stronger than saying that( .) We may characterize linear independence as follows. Theorem 1.6 Let be a nonempty set of vectors in . The following are equivalent: 1 is linearly independent.) 2 Every nonzero vector is an essentially unique linear)span combination of the vectors in . 3 No vector in is a linear combination of other vectors in .) Proof. Suppose that 1 holds and that) where the 's are distinct, the 's are distinct and the coefficients are nonzero. By subtracting and grouping 's and 's that are equal, we can write and so 1 implies that and and for all .) Thus, 1 implies 2 .)) If 2) holds and can be written as where are different from , then we may collect like terms on the right and then remove all terms with coefficient. The resulting expression violates 2). Hence, 2) implies 3). If 3) holds and where the 's are distinct and , then and we may write which violates 3 .) The following key theorem relates the notions of spanning set and linear independence. Vector Spaces 47 Theorem 1.7 Let be a set of vectors in . The following are equivalent: 1 is linearly independent and spans .) 2 Every nonzero vector is an essentially unique linear combination of) vectors in . 3 is a minimal spanning set, that is, spans but any proper subset of ) does not span . 4 is a maximal linearly independent set, that is, is linearly independent,) but any proper superset of is not linearly independent. A set of vectors in that satisfies any and hence all of these conditions is () called a for .basis Proof. We have seen that 1 and 2 are equivalent. Now suppose 1 holds. Then)) ) is a spanning set. If some proper subset of also spanned , then any vector in would be a linear combination of the vectors in , contradicting the fact that the vectors in are linearly independent. Hence 1 ) implies 3 .) Conversely, if is a minimal spanning set, then it must be linearly independent. For if not, some vector would be a linear combination of the other vectors in and so would be a proper spanning subset of , which is not possible. Hence 3 implies 1 .)) Suppose again that 1 holds. If were not maximal, there would be a vector) for which the set is linearly independent. But then is not in the span of , contradicting the fact that is a spanning set. Hence, is a maximal linearly independent set and so 1 implies 4 .)) Conversely, if is a maximal linearly independent set, then must span , for if not, we could find a vector that is not a linear combination of the vectors in . Hence, would be a linearly independent proper superset of , which is a contradiction. Thus, 4 implies 1 .)) Theorem 1.8 A finite set of vectors in is a basis for if and only if Example 1.6 The th in is the vector that has 's in all standard vector coordinate positions except the th, where it has a . Thus, The set is called the for . standard basis The proof that every nontrivial vector space has a basis is a classic example of the use of Zorn's lemma. 48 Advanced Linear Algebra Theorem 1.9 Let be a nonzero vector space. Let be a linearly independent set in and let be a spanning set in containing . Then there is a basis for for which . In particular, 1 Any vector space, except the zero space , has a basis.) 2 Any linearly independent set in is contained in a basis.) 3 Any spanning set in contains a basis.) Proof. Consider the collection of all linearly independent subsets of containing and contained in . This collection is not empty, since . Now, if is a chain in , then the union is linearly independent and satisfies , that is, . Hence, every chain in has an upper bound in and according to Zorn's lemma, must contain a maximal element , which is linearly independent. Now, is a basis for the vector space , for if any is not a linear combination of the elements of , then is linearly independent, contradicting the maximality of . Hence and so . The reader can now show, using Theorem 1.9, that any subspace of a vector space has a complement. The Dimension of a Vector Space The next result, with its classical elegant proof, says that if a vector space has a spanning set , then the size of any linearly independent set cannotfinite exceed the size of . Theorem 1.10 Let be a vector space and assume that the vectors are linearly independent and the vectors span . Then . Proof. First, we list the two sets of vectors: the spanning set followed by the linearly independent set: Then we move the first vector to the front of the first list: Since span , is a linear combination of the 's. This implies that we may remove one of the 's, which by reindexing if necessary can be , from the first list and still have a spanning set Vector Spaces 49 Note that the first set of vectors still spans and the second set is still linearly independent. Now we repeat the process, moving from the second list to the first list As before, the vectors in the first list are linearly dependent, since they spanned before the inclusion of . However, since the 's are linearly independent, any nontrivial linear combination of the vectors in the first list that equals must involve at least one of the 's. Hence, we may remove that vector, which again by reindexing if necessary may be taken to be and still have a spanning set Once again, the first set of vectors spans and the second set is still linearly independent. Now, if , then this process will eventually exhaust the 's and lead to the list where span , which is clearly not possible since is not in the span of . Hence, . Corollary 1.11 If has a spanning set, then any two bases of have thefinite same size. Now let us prove the analogue of Corollary 1.11 for arbitrary vector spaces. Theorem 1.12 If is a vector space, then any two bases for have the same cardinality. Proof. We may assume that all bases for are infinite sets, for if any basis is finite, then has a finite spanning set and so Corollary 1.11 applies. Let be a basis for and let be another basis for . Then any vector can be written as a finite linear combination of the vectors in , where all of the coefficients are nonzero, say But because is a basis, we must have 50 Advanced Linear Algebra for if the vectors in can be expressed as finite linear combinations of the vectors in a subset of , then spans , which is not the case.proper Since for all , Theorem 0.17 implies that But we may also reverse the roles of and , to conclude that and so the Schro¨der–Bernstein theorem implies that . Theorem 1.12 allows us to make the following definition. Definition A vector space is if it is the zero space , or finite-dimensional if it has a finite basis. All other vector spaces are . Theinfinite-dimensional dimension of the zero space is and the of any nonzero vector dimension space is the cardinality of any basis for . If a vector space has a basis of cardinality , we say that is and write . -dimensional dim It is easy to see that if is a subspace of , then . If in dim dim addition, , then .dim dim Theorem 1.13 Let be a vector space. 1 If is a basis for and if and , then) 2 Let . If is a basis for and is a basis for , then) and is a basis for . Theorem 1.14 Let and be subspaces of a vector space . Then dim dim dim dim In particular, if is any complement of in , then dim dim dim that is, dim dim dim Proof. Suppose that is a basis for . Extend this to a basis for where is disjoint from . Also, extend to a basis for where is disjoint from . We claim that is a basis for . It is clear that . To see that is linearly independent, suppose to the contrary that Vector Spaces 51 where and for all . There must be vectors in this expression from both and , since and are linearly independent. Isolating the terms involving the vectors from on one side of the equality shows that there is a nonzero vector in . But then and so , which implies that , a contradiction. Hence, is linearly independent and a basis for . Now, dim dim dim dim dim as desired. It is worth emphasizing that while the equation dim dim dim dim holds for all vector spaces, we cannot write dim dim dim dim unless is finite-dimensional. Ordered Bases and Coordinate Matrices It will be convenient to consider bases that have an order imposed on their members. Definition Let be a vector space of dimension . An for is ordered basis an ordered -tuple of vectors for which the set is a basis for . If is an ordered basis for , then for each there is a unique ordered -tuple of scalars for which Accordingly, we can define the bycoordinate map ()1.3 52 Advanced Linear Algebra where the column matrix is known as the of with coordinate matrix respect to the ordered basis . Clearly, knowing is equivalent to knowing ()assuming knowledge of . Furthermore, it is easy to see that the coordinate map is bijective and preserves the vector space operations, that is, or equivalently Functions from one vector space to another that preserve the vector space operations are called and form the objects of study in thelinear transformations next chapter. The Row and Column Spaces of a Matrix Let be an matrix over . The rows of span a subspace of known as the of and the columns of span a subspace of known asrow space the of . The dimensions of these spaces are called the column space row rank and , respectively. We denote the row space and row rank bycolumn rank rs rrk cs crk and and the column space and column rank by and . It is a remarkable and useful fact that the row rank of a matrix is always equal to its column rank, despite the fact that if , the row space and column space are not even in the same vector space! Our proof of this fact hinges on the following simple observation about matrices. Lemma 1.15 Let be an matrix. Then elementary column operations do not affect the row rank of . Similarly, elementary row operations do not affect the column rank of . Proof. The second statement follows from the first by taking transposes. As to the first, the row space of is rs where are the standard basis vectors in . Performing an elementary column operation on is equivalent to multiplying on the right by an elementary matrix . Hence the row space of is rs and since is invertible, Vector Spaces 53 rrk rs rs rrk dim dim as desired. Theorem 1.16 If , then . This number is called the rrk crk rank of and is denoted by .rk Proof. According to the previous lemma, we may reduce to reduced column echelon form without affecting the row rank. But this reduction does not affect the column rank either. Then we may further reduce to reduced row echelon form without affecting either rank. The resulting matrix has the same row and column ranks as . But is a matrix with 's followed by 's on the main diagonal entries and 's elsewhere. Hence,() rrk rrk crk crk as desired. The Complexification of a Real Vector Space If is a complex vector space that is, a vector space over , then we can () think of as a real vector space simply by restricting all scalars to the field . Let us denote this real vector space by and call it the of . real version On the other hand, to each real vector space , we can associate a complex vector space . This “complexification” process will play a useful role when we discuss the structure of linear operators on a real vector space. Throughout( our discussion will denote a real vector space. ) Definition If is a real vector space, then the set of ordered pairs, with componentwise addition and scalar multiplication over defined by for is a complex vector space, called the of . complexification It is convenient to introduce a notation for vectors in that resembles the notation for complex numbers. In particular, we denote by and so Addition now looks like ordinary addition of complex numbers, and scalar multiplication looks like ordinary multiplication of complex numbers, 54 Advanced Linear Algebra Thus, for example, we immediately have for , The of is and the of is .real part imaginary part The essence of the fact that is really an ordered pair is that is if and only if its real and imaginary parts are both . We can define the bycomplexification map cpx cpx Let us refer to as the , or of . complexification complex version Note that this map is a group homomorphism, that is, cpx cpx cpx cpx and and it is injective: cpx cpx Also, it preserves multiplication by scalars:real cpx cpx for . However, the complexification map is not surjective, since it gives only “real” vectors in . The complexification map is an injective linear transformation defined in the( next chapter from the real vector space to the real version of the) complexification , that is, to the complex vector space provided that scalars are restricted to real numbers. In this way, we see that contains an embedded copy of . The Dimension of The vector-space dimensions of and are the same. This should not necessarily come as a surprise because although may seem “bigger” than , the field of scalars is also “bigger.” Theorem 1.17 If is a basis for over , then the complexification of , cpx Vector Spaces 55 is a basis for the vector space over . Hence, dim dim Proof. To see that spans over , let . Then cpx and so there exist real numbers and some of which may be for which () To see that is linearly independent, ifcpx then the previous computations show that and The independence of then implies that and for all . If and is a basis for , then we may write for . Since the coefficients are real, we have and so the coordinate matrices are equal: cpx Exercises 1. Let be a vector space over . Prove that and for all and . Describe the different 's in these equations. Prove that if , then or . Prove that implies that or . 56 Advanced Linear Algebra 2. Prove Theorem 1.3. 3. a Find an abelian group and a field for which is a vector space) over in at least two different ways, that is, there are two different definitions of scalar multiplication making a vector space over . b Find a vector space over and a subset of that is 1 a)( ) subspace of and 2 a vector space using operations that differ from () those of . 4. Suppose that is a vector space with basis and is a subspace of . Let be a partition of . Then is it true that What if for all ? 5. Prove Theorem 1.8. 6. Let . Show that if , then This is called the for the lattice .modular law 7. For what vector spaces does the distributive law of subspaces hold? 8. A vector is called if for all strongly positive . a Suppose that is strongly positive. Show that any vector that is “close) enough” to is also strongly positive. Formulate carefully what “close ( enough” should mean.) b Prove that if a subspace of contains a strongly positive vector,) then has a basis of strongly positive vectors. 9. Let be an matrix whose rows are linearly independent. Suppose that the columns of span the column space of . Let be the matrix obtained from by deleting all columns except . Show that the rows of are also linearly independent. 10. Prove that the first two statements in Theorem 1.7 are equivalent. 11. Show that if is a subspace of a vector space , then . dim dim Furthermore, if then . Give an example todim dim show that the finiteness is required in the second statement. 12. Let and suppose that . What can youdim say about the relationship between and ? What can you say if ? 13. What is the relationship between and ? Is the direct sum operation commutative? Formulate and prove a similar statement concerning associativity. Is there an “identity” for direct sum? What about “negatives”? Vector Spaces 57 14. Let be a finite-dimensional vector space over an infinite field . Prove that if are subspaces of of equal dimension, then there is a subspace of for which for all . In other words, is a common complement of the subspaces . 15. Prove that the vector space of all continuous functions from to is infinite-dimensional. 16. Show that Theorem 1.2 need not hold if the base field is finite. 17. Let be a subspace of . The set is called an affine subspace of . a Under what conditions is an affine subspace of a subspace of ?) b Show that any two affine subspaces of the form and are) either equal or disjoint. 18. If and are vector spaces over for which , then does it follow that ?dim dim 19. Let be an -dimensional real vector space and suppose that is a subspace of with . Define an equivalence relation on dim the set by if the “line segment” has the property that . Prove that is an equivalence relation and that it has exactly two equivalence classes. 20. Let be a field. A of is a subset of that is a field in its subfield own right using the same operations as defined on . a Show that is a vector space over any subfield of .) b Suppose that is an -dimensional vector space over a subfield of) . If is an -dimensional vector space over , show that is also a vector space over . What is the dimension of as a vector space over ? 21. Let be a finite field of size and let be an -dimensional vector space over . The purpose of this exercise is to show that the number of subspaces of of dimension is The expressions are called and have properties Gaussian coefficients similar to those of the binomial coefficients. Let be the number of -dimensional subspaces of . a Let be the number of -tuples of linearly independent vectors) in . Show that b Now, each of the -tuples in a can be obtained by first choosing a)) subspace of of dimension and then selecting the vectors from this subspace. Show that for any -dimensional subspace of , the number 58 Advanced Linear Algebra of -tuples of independent vectors in this subspace is c Show that) How does this complete the proof? 22. Prove that any subspace of is a closed set or, equivalently, that its set complement is open, that is, for any there is an open ball centered at with radius for which . 23. Let and be bases for a vector space . Let . Show that there is a permutation of such that and are both bases for . : You may use the fact that if is an invertibleHint matrix and if , then it is possible to reorder the rows so that the upper left submatrix and the lower right submatrix are both invertible. This follows, for example, from the general( Laplace expansion theorem for determinants.) 24. Let be an -dimensional vector space over an infinite field and suppose that are subspaces of with . Prove dim that there is a subspace of of dimension for which for all . 25. What is the dimension of the complexification thought of as a real vector space? 26. When is a subspace of a complex vector space a complexification? Let () be a real vector space with complexification and let be a subspace of . Prove that there is a subspace of for which if and only if is closed under complex conjugation defined by . Chapter 2 Linear Transformations Linear Transformations Loosely speaking, a linear transformation is a function from one vector space to another that the vector space operations. Let us be more precise.preserves Definition Let and be vector spaces over a field . A function is a iflinear transformation for all scalars and vectors , . The set of all linear transformations from to is denoted by . 1 A linear transformation from to is called a on . The) linear operator set of all linear operators on is denoted by . A linear operator on a real vector space is called a and a linear operator on areal operator complex vector space is called a .complex operator 2 A linear transformation from to the base field thought of as a vector)( space over itself is called a on . The set of all linear) linear functional functionals on is denoted by and called the of . dual space We should mention that some authors use the term linear operator for any linear transformation from to . Also, the application of a linear transformation on a vector is denoted by or by , parentheses being used when necessary, as in , or to improve readability, as in rather than . Definition The following terms are also employed: 1 for linear transformation) homomorphism 2 for linear operator) endomorphism 3 or for injective linear transformation)( )monomorphism embedding 4 for surjective linear transformation) epimorphism 5 for bijective linear transformation.) isomorphism 60 Advanced Linear Algebra 6 for bijective linear operator.) automorphism Example 2.1 1 The derivative is a linear operator on the vector space of all) infinitely differentiable functions on . 2 The integral operator defined by) is a linear operator on . 3 Let be an matrix over . The function defined by) , where all vectors are written as column vectors, is a linear transformation from to . This function is just multiplication by . 4 The coordinate map of an -dimensional vector space is a) linear transformation from to . The set is a vector space in its own right and has the structure of an algebra, as defined in Chapter 0. Theorem 2.1 1 The set is a vector space under ordinary addition of functions) and scalar multiplication of functions by elements of . 2 If and , then the composition is in .) 3 If is bijective then .) 4 The vector space is an algebra, where multiplication is composition) of functions. The identity map is the multiplicative identity and the zero map is the additive identity. Proof. We prove only part 3 . Let be a bijective linear) transformation. Then is a well-defined function and since any two vectors and in have the form and , we have which shows that is linear. One of the easiest ways to define a linear transformation is to give its values on a basis. The following theorem says that we may assign these values arbitrarily and obtain a unique linear transformation by linear extension to the entire domain. Theorem 2.2 Let and be vector spaces and let be a basis for . Then we can define a linear transformation by Linear Transformations 61 specifying the values of for all and extending to by arbitrarily linearity, that is, This process defines a unique linear transformation, that is, if satisfy for all then . Proof. The crucial point is that the extension by linearity is well-defined, since each vector in has an essentially unique representation as a linear combination of a finite number of vectors in . We leave the details to the reader. Note that if and if is a subspace of , then the restriction of to is a linear transformation from to . The Kernel and Image of a Linear Transformation There are two very important vector spaces associated with a linear transformation from to . Definition Let . The subspace ker is called the of and the subspacekernel im is called the of . The dimension of is called the of and isimage nullity ker denoted by . The dimension of is called the of and isnull im rank denoted by .rk It is routine to show that is a subspace of and is a subspace ofker im . Moreover, we have the following. Theorem 2.3 Let . Then 1 is surjective if and only if )im 2 is injective if and only if ) ker Proof. The first statement is merely a restatement of the definition of surjectivity. To see the validity of the second statement, observe that ker Hence, if , then , which shows that is injective.ker Conversely, if is injective and , then and so . This ker shows that .ker 62 Advanced Linear Algebra Isomorphisms Definition A bijective linear transformation is called an isomorphism from to . When an isomorphism from to exists, we say that and are and write . isomorphic Example 2.2 Let . For any ordered basis of , the coordinatedim map that sends each vector to its coordinate matrix is an isomorphism. Hence, any -dimensional vector space over is isomorphic to . Isomorphic vector spaces share many properties, as the next theorem shows. If and we write Theorem 2.4 Let be an isomorphism. Let . Then 1 spans if and only if spans .) 2 is linearly independent in if and only if is linearly independent in) . 3 is a basis for if and only if is a basis for .) An isomorphism can be characterized as a linear transformation that maps a basis for to a basis for . Theorem 2.5 A linear transformation is an isomorphism if and only if there is a basis for for which is a basis for . In this case, maps any basis of to a basis of . The following theorem says that, up to isomorphism, there is only one vector space of any given dimension over a given field. Theorem 2.6 Let and be vector spaces over . Then if and only if .dim dim In Example 2.2, we saw that any -dimensional vector space is isomorphic to . Now suppose that is a set of cardinality and let be the vector space of all functions from to with finite support. We leave it to the reader to show that the functions defined for all by if if form a basis for , called the . Hence, . standard basis dim It follows that for any cardinal number , there is a vector space of dimension . Also, any vector space of dimension is isomorphic to . Linear Transformations 63 Theorem 2.7 If is a natural number, then any -dimensional vector space over is isomorphic to . If is any cardinal number and if is a set of cardinality , then any -dimensional vector space over is isomorphic to the vector space of all functions from to with finite support. The Rank Plus Nullity Theorem Let . Since any subspace of has a complement, we can write ker ker where is a complement of in . It follows thatker ker dim dim ker dim ker Now, the restriction of to ,ker ker is injective, since ker ker ker Also, . For the reverse inclusion, if , then sinceim im im for and , we haveker ker im Thus . It follows thatim im ker im From this, we deduce the following theorem. Theorem 2.8 Let . 1 Any complement of is isomorphic to )imker 2)( )The rank plus nullity theorem dim ker dim dim im or, in other notation, rk null dim Theorem 2.8 has an important corollary. Corollary 2.9 Let , where . Then is dim dim injective if and only if it is surjective. Note that this result fails if the vector spaces are not finite-dimensional. The reader is encouraged to find an example to support this statement. 64 Advanced Linear Algebra Linear Transformations from to Recall that for any matrix over the multiplication map is a linear transformation. In fact, any linear transformation has this form, that is, is just multiplication by a matrix, for we have and so , where Theorem 2.10 1 If is an matrix over then .) 2 If then , where) The matrix is called the of . matrix Example 2.3 Consider the linear transformation defined by Then we have, in column form, and so the standard matrix of is If , then since the image of is the column space of , we have dim ker dim rk This gives the following useful result. Theorem 2.11 Let be an matrix over . 1 is injective if and only if n.)rk 2 is surjective if and only if m. )rk Linear Transformations 65 Change of Basis Matrices Suppose that and are ordered bases for a vector space . It is natural to ask how the coordinate matrices and are related. Referring to Figure 2.1, V F n F n B C C(B)-1 Figure 2.1 the map that takes to is and is called the change of basis operator change of coordinates operator or . Since is an operator on() , it has the form , where We denote by and call it the from to ., change of basis matrix Theorem 2.12 Let and be ordered bases for a vector space . Then the change of basis operator is an automorphism of , whose standard matrix is , Hence and . , Consider the equation or equivalently, Then given any two of an invertible matrix an ordered basis for () ( )( ) and an ordered basis for , the third component is uniquely determined by this equation. This is clear if and are given or if and are 66 Advanced Linear Algebra given. If and are given, then there is a unique for which and so there is a unique for which . Theorem 2.13 If we are given any two of the following: 1 an invertible matrix ) 2 an ordered basis for ) 3 an ordered basis for .) then the third is uniquely determined by the equation The Matrix of a Linear Transformation Let be a linear transformation, where and dim dim and let be an ordered basis for and an ordered basis for . Then the map is a of as a linear transformation from to , in the senserepresentation that knowing along with and , of course is equivalent to knowing . Of () course, this representation depends on the choice of ordered bases and . Since is a linear transformation from to , it is just multiplication by an matrix , that is, Indeed, since , we get the columns of as follows: Theorem 2.14 Let and let and be ordered bases for and , respectively. Then can be represented with respect to and as matrix multiplication, that is, , where , is called the and . When andmatrix of with respect to the bases , we denote by and so , Example 2.4 Let be the derivative operator, defined on the vector space of all polynomials of degree at most . Let . Then Linear Transformations 67 , and so Hence, for example, if , then and so . The following result shows that we may work equally well with linear transformations or with the matrices that represent them with respect to fixed( ordered bases and . This applies not only to addition and scalar) multiplication, but also to matrix multiplication. Theorem 2.15 Let and be finite-dimensional vector spaces over , with ordered bases and , respectively. 1 The map defined by) , is an isomorphism and so . Hence, dim dim 2 If and and if , and are ordered bases for) , and , respectively, then ,, , Thus, the matrix of the product composition is the product of the() matrices of and . In fact, this is the primary motivation for the definition of matrix multiplication. Proof. To see that is linear, observe that for all , 68 Advanced Linear Algebra and since is a standard basis vector, we conclude that and so is linear. If , we define by the condition , whence and is surjective. Also, since ker implies that . Hence, the map is an isomorphism. To prove part 2 , we ) have , Change of Bases for Linear Transformations Since the matrix that represents depends on the ordered bases and , it , is natural to wonder how to choose these bases in order to make this matrix as simple as possible. For instance, can we always choose the bases so that is represented by a diagonal matrix? As we will see in Chapter 7, the answer to this question is no. In that chapter, we will take up the general question of how best to represent a linear operator by a matrix. For now, let us take the first step and describe the relationship between the matrices and of with respect to two different pairs and of ordered bases. Multiplication by sends to . This can be reproduced by first switching from to , then applying and finally switching from to , that is, ,, , Theorem 2.16 Let , and let and be pairs of ordered bases of and , respectively. Then (2.1) When is a linear operator on , it is generally more convenient to represent by matrices of the form , where the ordered bases used to represent vectors in the domain and image are the same. When , Theorem 2.16 takes the following important form. Corollary 2.17 Let and let and be ordered bases for . Then the matrix of with respect to can be expressed in terms of the matrix of with respect to as follows: (2.2) Equivalence of Matrices Since the change of basis matrices are precisely the invertible matrices, 2.1 has() the form Linear Transformations 69 where and are invertible matrices. This motivates the following definition. Definition Two matrices and are if there exist invertible equivalent matrices and for which We have remarked that is equivalent to if and only if can be obtained from by a series of elementary row and column operations. Performing the row operations is equivalent to multiplying the matrix on the left by and performing the column operations is equivalent to multiplying on the right by . In terms of 2.1 , we see that performing row operations premultiplying by () ( ) is equivalent to changing the basis used to represent vectors in the image and performing column operations postmultiplying by is equivalent to() changing the basis used to represent vectors in the domain. According to Theorem 2.16, if and are matrices that represent with respect to possibly different ordered bases, then and are equivalent. The converse of this also holds. Theorem 2.18 Let and be vector spaces with and dim dim . Then two matrices and are equivalent if and only if they represent the same linear transformation , but possibly with respect to different ordered bases. In this case, and represent exactly the same set of linear transformations in . Proof. If and represent , that is, if ,,and for ordered bases and , then Theorem 2.16 shows that and are equivalent. Now suppose that and are equivalent, say where and are invertible. Suppose also that represents a linear transformation for some ordered bases and , that is, Theorem 2.9 implies that there is a unique ordered basis for for which and a unique ordered basis for for which . Hence 70 Advanced Linear Algebra Hence, also represents . By symmetry, we see that and represent the same set of linear transformations. This completes the proof. We remarked in Example 0.3 that every matrix is equivalent to exactly one matrix of the block form block Hence, the set of these matrices is a set of canonical forms for equivalence. Moreover, the rank is a complete invariant for equivalence. In other words, two matrices are equivalent if and only if they have the same rank. Similarity of Matrices When a linear operator is represented by a matrix of the form , equation 2.2 has the form() where is an invertible matrix. This motivates the following definition. Definition Two matrices and are , denoted by , if there similar exists an invertible matrix for which The equivalence classes associated with similarity are called similarity classes. The analog of Theorem 2.18 for square matrices is the following. Theorem 2.19 Let be a vector space of dimension . Then two matrices and are similar if and only if they represent the same linear operator , but possibly with respect to different ordered bases. In this case, and represent exactly the same set of linear operators in . Proof. If and represent , that is, if and for ordered bases and , then Corollary 2.17 shows that and are similar. Now suppose that and are similar, say Suppose also that represents a linear operator for some ordered basis , that is, Theorem 2.9 implies that there is a unique ordered basis for for which Linear Transformations 71 . Hence Hence, also represents . By symmetry, we see that and represent the same set of linear operators. This completes the proof. We will devote much effort in Chapter 7 to finding a canonical form for similarity. Similarity of Operators We can also define similarity of operators. Definition Two linear operators are , denoted by , if similar there exists an automorphism for which The equivalence classes associated with similarity are called similarity classes. Note that if and are ordered bases for , then Now, the map defined by is an automorphism of and Conversely, if is an automorphism and is an ordered basis for , then is also a basis: The analog of Theorem 2.19 for linear operators is the following. Theorem 2.20 Let be a vector space of dimension . Then two linear operators and on are similar if and only if there is a matrix that represents both operators, but with respect to possibly different ordered bases. In this case, and are represented by exactly the same set of matrices in . Proof. If and are represented by , that is, if for ordered bases and , then As remarked above, if is defined by , then 72 Advanced Linear Algebra and so from which it follows that and are similar. Conversely, suppose that and are similar, say where is an automorphism of . Suppose also that is represented by the matrix , that is, for some ordered basis . Then and so It follows that and so also represents . By symmetry, we see that and are represented by the same set of matrices. This completes the proof. We can summarize the sitiation with respect to similarity in Figure 2.2. Each similarity class in corresponds to a similarity class in : is the set of all matrices that represent any and is the set of all operators in that are represented by any . similarity classes of L(V) []B []C Similarity classes of matrices []B []C Figure 2.2 Invariant Subspaces and Reducing Pairs The restriction of a linear operator to a subspace of is not necessarily a linear operator on . This prompts the following definition. Linear Transformations 73 Definition Let . A subspace of is said to be or invariant under - if , that is, if for all . Put another way, isinvariant invariant under if the restriction is a linear operator on . If then the fact that is -invariant does not imply that the complement is also -invariant. The reader may wish to supply a simple example with .() Definition Let . If and if both and are -invariant, we say that the pair . reduces A reducing pair can be used to decompose a linear operator into a direct sum as follows. Definition Let . If reduces we write and call the of and . Thus, the expression direct sum means that there exist subspaces and of for which reduces and and The concept of the direct sum of linear operators will play a key role in the study of the structure of a linear operator. Projection Operators We will have several uses for a special type of linear operator that is related to direct sums. Definition Let . The linear operator defined by where and is called onto . projection along Whenever we say that the operator is a projection, it is with the understanding that . The following theorem describes a few basic properties of projection operators. We leave proof as an exercise. Theorem 2.21 Let be a vector space and let . 74 Advanced Linear Algebra 1 If then) 2 If then) im and ker and so im ker In other words, is projection onto its image along its kernel. Moreover, im 3 If has the property that) im ker and im then is projection onto along . im ker Projection operators are easy to characterize. Definition A linear operator is if . idempotent Theorem 2.22 A linear operator is a projection if and only if it is idempotent. Proof. If , then for any and , and so . Conversely, suppose that is idempotent. If , im ker then and so Hence . Also, if , thenim ker ker im and so . Finally, and so . ker im im Hence, is projection onto along . im ker Projections and Invariance Projections can be used to characterize invariant subspaces. Let and let be a subspace of . Let for any complement of . The key is that the elements of can be characterized as those vectors fixed by , that is, Linear Transformations 75 if and only if . Hence, the following are equivalent: for all for all for all Thus, is -invariant if and only if for all vectors . But this is also true for all vectors in , since both sides are equal to on . This proves the following theorem. Theorem 2.23 Let . Then a subspace of is -invariant if and only if there is a projection for which in which case this holds for all projections of the form . We also have the following relationship between projections and reducing pairs. Theorem 2.24 Let . Then reduces if and only if commutes with . Proof. Theorem 2.23 implies that and are -invariant if and only if and and a little algebra shows that this is equivalent to and which is equivalent to . Orthogonal Projections and Resolutions of the Identity Observe that if is a projection, then Definition Two projections are , written , if orthogonal Note that if and only if im im ker kerand The following example shows that it is not enough to have in the definition of orthogonality. In fact, it is possible for and yet is not even a projection. 76 Advanced Linear Algebra Example 2.5 Let and consider the - and -axes and the diagonal: Then From this we deduce that if and are projections, it may happen that both products and are projections, but that they are not equal. We leave it to the reader to show that (which is a projection), but that is not a projection. Since a projection is idempotent, we can write the identity operator as s sum of two orthogonal projections: Let us generalize this to more than two projections. Definition A on is a sum of the formresolution of the identity where the 's are pairwise orthogonal projections, that is, for . There is a connection between the resolutions of the identity on and direct sum decompositions of . In general terms, if for any linear operators , then for all , im im and so im im However, the sum need not be direct. Theorem 2.25 Let be a vector space. Resolutions of the identity on correspond to direct sum decompositions of as follows: 1 If is a resolution of the identity, then) im im Linear Transformations 77 and is projection onto alongim ker im 2 Conversely, if) and if is projection onto along the direct sum ,, then is a resolution of the identity. Proof. To prove 1), if is a resolution of the identity, then im im Moreover, if then applying gives and so the sum is direct. As to the kernel of , we have im im im ker and since , it follows that im ker and so equality must hold. For part 2), suppose that and is projection onto along . If , then im ker and so . Also, if for , then and so is a resolution of the identity. The Algebra of Projections If and are projections, it does not necessarily follow that , or is a projection. For example, the sum is a projection if and only if 78 Advanced Linear Algebra which is equivalent to Of course, this holds if , that is, if . But the converse is also true, provided that . To see this, we simply evaluate in twochar ways: and Hence, and so . It follows that and so . Thus, for , we have is a projection if and only ifchar . Now suppose that is a projection. For the kernel of , note that and similarly, . Hence, . But the reverse ker ker ker inclusion is obvious and so ker ker ker As to the image of , we have im im im and so . For the reverse inclusion, if ,im im im then and so . Thus, . Finally, im im im im implies that and so the sum is direct andim ker im im im The following theorem also describes the situation for the difference and product. Proof in these cases is left for the exercises. Theorem 2.26 Let be a vector space over a field of characteristic and let and be projections. 1 The sum is a projection if and only if , in which case) im im im and ker ker ker 2 The difference is a projection if and only if) Linear Transformations 79 in which case im im im ker ker kerand 3 If and commute, then is a projection, in which case) im im im and ker ker ker ()Example 2.5 shows that the converse may be false. Topological Vector Spaces This section is for readers with some familiarity with point-set topology. The Definition A pair where is a real vector space and is a topology on the set is called a if the operations of additiontopological vector space and scalar multiplication are continuous functions. The Standard Topology on The vector space is a topological vector space under the , standard topology which is the topology for which the set of open rectangles 's are open intervals in is a base, that is, a subset of is open if and only if it is a union of open rectangles. The standard topology is also the topology induced by the Euclidean metric on , since an open rectangle is the union of Euclidean open balls and an open ball is the union of open rectangles. The standard topology on has the property that the addition function and the scalar multiplication function are continuous and so is a topological vector space under this topology. Also, the linear functionals are continuous maps. For example, to see that addition is continuous, if 80 Advanced Linear Algebra then and so there is an for which for all . It follows that if and then The Natural Topology on Now let be a real vector space of dimension and fix an ordered basis for . We wish to show that there is precisely one topology on for which is a topological vector space and all linear functionals are continuous. This topology is called the on .natural topology Our plan is to show that if is a topological vector space and if all linear functionals on are continuous, then the coordinate map is a homeomorphism. This implies that if does exist, it must be unique. Then we use to move the standard topology from to , thus giving a topology for which is a homeomorphism. Finally, we show that is a topological vector space and that all linear functionals on are continuous. The first step is to show that if is a topological vector space, then is continuous. Since where is defined by it is sufficient to show that these maps are continuous. The sum of continuous( maps is continuous. ) Let be an open set in . Then is open in . This implies that if , then there is an open interval containing for which We need to show that the set is open. But In words, an -tuple is in if the th coordinate times is Linear Transformations 81 in . But if , then there is an open interval for which and . Hence, the entire open set where the factor is in the th position is in , that is, Thus, is open and , and therefore also , is continuous. Next we show that if every linear functional on is continuous under a topology on , then the coordinate map is continuous. If denote by the th coordinate of . The map defined by is a linear functional and so is continuous by assumption. Hence, for any open interval the set is open. Now, if are open intervals in , then is open. Thus, is continuous. We have shown that if a topology has the property that is a topological vector space under which every linear functional is continuous, then and are homeomorphisms. This means that if exists, its open sets must be the images under of the open sets in the standard topology of . It remains to prove that the topology on that makes a homeomorphism makes a topological vector space for which any linear functional on is continuous. The addition map on is a composition where is addition in and since each of the maps on the right is continuous, so is . Similarly, scalar multiplication in is where is scalar multiplication in . Hence, is continuous. Now let be a linear functional. Since is continuous if and only if is continuous, we can confine attention to . In this case, if is the standard basis for for any and for all , then 82 Advanced Linear Algebra , we have Now, if , then and so , which implies that is continuous at . According to the Riesz representation theorem (Theorem 9.18) and the Cauchy– Schwarz inequality, we have where . Hence, implies and so by linearity, implies and so is continuous at all . Theorem 2.27 Let be a real vector space of dimension . There is a unique topology on , called the , for which is a topological vectornatural topology space and for which all linear functionals on are continuous. This topology is determined by the fact that the coordinate map is a homeomorphism, where has the standard topology induced by the Euclidean metric. Linear Operators on A linear operator on a real vector space can be extended to a linear operator on the complexification by defining Here are the basic properties of this of .complexification Theorem 2.28 If , then 1 , ) 2) 3) 4 .) Let us recall that for any ordered basis for and any vector we have cpx Now, if is an ordered basis for , then the th column of is cpx cpx which is the th column of the coordinate matrix of with respect to the basis cpx . Thus we have the following theorem. Linear Transformations 83 Theorem 2.29 Let where is a real vector space. The matrix of with respect to the ordered basis is equal to the matrix of with respectcpx to the ordered basis : cpx Hence, if a real matrix represents a linear operator on , then also represents the complexification of on . Exercises 1. Let have rank . Prove that there are matrices and , both of rank , for which . Prove that has rank if and only if it has the form where and are row matrices. 2. Prove Corollary 2.9 and find an example to show that the corollary does not hold without the finiteness condition. 3. Let . Prove that is an isomorphism if and only if it carries a basis for to a basis for . 4. If and we define the external direct sum by Show that is a linear transformation. 5. Let . Prove that . Thus, internal and external direct sums are equivalent up to isomorphism. 6. Let and consider the external direct sum . Define a map by . Show that is linear. What is the kernel of ? When is an isomorphism? 7. Let where . Let . Suppose that dim there is an isomorphism with the property that . Prove that there is an ordered basis for which . 8. Let be a subset of . A subspace of is if is - -invariant invariant for every . Also, is if the only -invariant -irreducible subspaces of are and . Prove the following form of Schur's lemma. Suppose that and and is -irreducible and is -irreducible. Let satisfy , that is, for any there is a such that and for any there is a such that . Prove that or is an isomorphism. 9. Let where . If show that dim rk rk im ker . 10. Let , and . Show that minrk rk rk 11. Let and . Show that null null null 84 Advanced Linear Algebra 12. Let where is invertible. Show that rk rk rk 13. Let . Show that rk rk rk 14. Let be a subspace of . Show that there is a for which ker . Show also that there exists a for which .im 15. Suppose that . a Show that for some .)im im if and only if b Show that for some .) if and only if ker ker 16. Let and suppose that satisfies . Show thatdim rk dim . 17. Let be an matrix over . What is the relationship between the linear transformation and the system of equations ? Use your knowledge of linear transformations to state and prove various results concerning the system , especially when . 18. Let have basis and assume that the base field for has characteristic . Suppose that for each we define by if if Prove that the are invertible and form a basis for . 19. Let . If is a -invariant subspace of must there be a subspace of for which reduces ? 20. Find an example of a vector space and a proper subspace of for which . 21. Let . If , prove that implies that and dim are invertible and that for some polynomial . 22. Let . If for all show that , for some , where is the identity map. 23. Let be a vector space over a field of characteristic and let and be projections. Prove the following: a The difference is a projection if and only if) in which case im im im ker ker kerand Hint: is a projection if and only if is a projection and so is a projection if and only if Linear Transformations 85 is a projection. b If and commute, then is a projection, in which case) im im im and ker ker ker 24 be a continuous function with the property that. Let Prove that is a linear functional on . 25. Prove that any linear functional is a continuous map. 26. Prove that any subspace of is a closed set or, equivalently, that is open, that is, for any there is an open ball centered at with radius for which . 27. Prove that any linear transformation is continuous under the natural topologies of and . 28. Prove that any surjective linear transformation from to both finite- ( dimensional topological vector spaces under the natural topology is an) open map, that is, maps open sets to open sets. 29. Prove that any subspace of a finite-dimensional vector space is a closed set or, equivalently, that is open, that is, for any there is an open ball centered at with radius for which . 30. Let be a subspace of with . dim a Show that the subspace topology on inherited from is the natural) topology. b Show that the natural topology on is the topology for which the) natural projection map continuous and open. 31. If is a real vector space, then is a complex vector space. Thinking of as a vector space over , show that is isomorphic to the external direct product . 32. When is a complex linear map a complexification? Let be a real vector() space with complexification and let . Prove that is a complexification, that is, has the form for some if and only if commutes with the conjugate map defined by . 33. Let be a complex vector space. a Consider replacing the scalar multiplication on by the operation) where and . Show that the resulting set with the addition defined for the vector space and with this scalar multiplication is a complex vector space, which we denote by . b Show, without using dimension arguments, that .) Chapter 3 The Isomorphism Theorems Quotient Spaces Let be a subspace of a vector space . It is easy to see that the binary relation on defined by is an equivalence relation. When , we say that and are congruent modulo . The term is used as a colloquialism for modulo and is mod often written mod When the subspace in question is clear, we will simply write . To see what the equivalence classes look like, observe that for some The set is called a of in and is called a for .coset coset representative ()Thus, any member of a coset is a coset representative. The set of all cosets of in is denoted by This is read “ mod ” and is called the . Of quotient space of modulo 88 Advanced Linear Algebra course, the term space is a hint that we intend to define vector space operations on . The natural choice for these vector space operations is and but we must check that these operations are well-defined, that is, 1) 2) Equivalently, the equivalence relation must be with the vector consistent space operations on , that is, 3) 4) This senario is a recurring one in algebra. An equivalence relation on an algebraic structure, such as a group, ring, module or vector space is called a congruence relation if it preserves the algebraic operations. In the case of a vector space, these are conditions 3) and 4) above. These conditions follow easily from the fact that is a subspace, for if and , then which verifies both conditions at once. We leave it to the reader to verify that is indeed a vector space over under these well-defined operations. Actually, we are lucky here: For subspace of , the quotient is aany vector space under the natural operations. In the case of groups, not all subgroups have this property. Indeed, it is precisely the subgroups ofnormal that have the property that the quotient is a group. Also, for rings, it is precisely the (not the subrings) that have the property that the quotient isideals a ring. Let us summarize. The Isomorphism Theorems 89 Theorem 3.1 Let be a subspace of . The binary relation is an equivalence relation on , whose equivalence classes are the cosets of in . The set of all cosets of in , called the of quotient space modulo , is a vector space under the well-defined operations The zero vector in is the coset . The Natural Projection and the Correspondence Theorem If is a subspace of , then we can define a map by sending each vector to the coset containing it: This map is called the or of ontocanonical projection natural projection , or simply . (Not to be confused with the projectionprojection modulo operators .) It is easily seen to be linear, for we have writing for () The canonical projection is clearly surjective. To determine the kernel of , note that ker and so ker Theorem 3.2 The canonical projection defined by is a surjective linear transformation with .ker If is a subspace of , then the subspaces of the quotient space have the form for some intermediate subspace satisfying . In fact, as shown in Figure 3.1, the projection map provides a one-to-one correspondence between intermediate subspaces and subspaces of the quotient space . The proof of the following theorem is left as an exercise. 90 Advanced Linear Algebra V V/S {0} S T/S T {0} Figure 3.1: The correspondence theorem Theorem 3.3 The correspondence theorem() Let be a subspace of . Then the function that assigns to each intermediate subspace the subspace of is an order-preserving with respect to set inclusion () one-to-one correspondence between the set of all subspaces of containing and the set of all subspaces of . Proof. We prove only that the correspondence is surjective. Let be a subspace of and let be the union of all cosets in : We show that and that . If , then and are in and since , we have which implies that . Hence, is a subspace of containing . Moreover, if , then and so . Conversely, if , then and therefore . Thus, . The Universal Property of Quotients and the First Isomorphism Theorem Let be a subspace of . The pair has a very special property, known as the —a term that comes from the world of categoryuniversal property theory. Figure 3.2 shows a linear transformation , along with the canonical projection from to the quotient space . The Isomorphism Theorems 91 V W V/S s ' Figure 3.2: The universal property The universal property states that if , then there is a uniqueker for which Another way to say this is that any such can be factored through the canonical projection . Theorem 3.4 Let be a subspace of and let satisfy ker . Then, as pictured in Figure 3.2, there is a unique linear transformation with the property that Moreover, and .ker ker im im Proof. We have no other choice but to define by the condition , that is, This function is well-defined if and only if which is equivalent to each of the following statements: ker Thus, is well-defined. Also, im im and 92 Advanced Linear Algebra ker ker ker The uniqueness of is evident. Theorem 3.4 has a very important corollary, which is often called the first isomorphism theorem and is obtained by taking . ker Theorem 3.5 The Let be a linear()first isomorphism theorem transformation. Then the linear transformation defined by ker ker is injective and ker im According to Theorem 3.5, the image of any linear transformation on is isomorphic to a quotient space of . Conversely, any quotient space of is the image of a linear transformation on : the canonical projection . Thus, up to isomorphism, quotient spaces are equivalent to homomorphic images. Quotient Spaces, Complements and Codimension The first isomorphism theorem gives some insight into the relationship between complements and quotient spaces. Let be a subspace of and let be a complement of , that is, Applying the first isomorphism theorem to the projection operator gives Theorem 3.6 Let be a subspace of . All complements of in are isomorphic to and hence to each other. The previous theorem can be rephrased by writing On the other hand, quotients and complements do not behave as nicely with respect to isomorphisms as one might casually think. We leave it to the reader to show the following: The Isomorphism Theorems 93 1 It is possible that) with but . Hence, does imply that a complement not of is isomorphic to a complement of . 2 It is possible that and) and but . Hence, does imply that . However, not ( according to the previous theorem, if then . equals ) Corollary 3.7 Let be a subspace of a vector space . Then dim dim dim Definition If is a subspace of , then is called the of dim codimension in and is denoted by or .codim codim Thus, the codimension of in is the dimension of any complement of in and when is , we have finite-dimensional codim dim dim (This makes no sense, in general, if is not finite-dimensional, since infinite cardinal numbers cannot be subtracted.) Additional Isomorphism Theorems There are other isomorphism theorems that are direct consequences of the first isomorphism theorem. As we have seen, if then . This can be written This applies to nondirect sums as well. Theorem 3.7 The Let be a vector space()second isomorphism theorem and let and be subspaces of . Then Proof. Let be defined by We leave it to the reader to show that is a well-defined surjective linear transformation, with kernel . An application of the first isomorphism theorem then completes the proof. 94 Advanced Linear Algebra The following theorem demonstrates one way in which the expression behaves like a fraction. Theorem 3.8 The Let be a vector space and()third isomorphism theorem suppose that are subspaces of . Then Proof. Let be defined by . We leave it to the reader to show that is a well-defined surjective linear transformation whose kernel is . The rest follows from the first isomorphism theorem. The following theorem demonstrates one way in which the expression does not behave like a fraction. Theorem 3.9 Let be a vector space and let be a subspace of . Suppose that and with . Then Proof. Let be defined by This map is well-defined, since the sum is direct. We leave it to the reader to show that is a surjective linear transformation, whose kernel is . The rest follows from the first isomorphism theorem. Linear Functionals Linear transformations from to the base field thought of as a vector space ( over itself are extremely important.) Definition Let be a vector space over . A linear transformation whose values lie in the base field is called a linear functional () ( )or simply on . Some authors use the term . Thefunctional linear function vector space of all linear functionals on is denoted by and is called the * algebraic dual space of . The adjective is needed here, since there is another type of dual spacealgebraic that is defined on general normed vector spaces, where continuity of linear transformations makes sense. We will discuss the so-called continuous dual space briefly in Chapter 13. However, until then, the term “dual space” will refer to the algebraic dual space. The Isomorphism Theorems 95 To help distinguish linear functionals from other types of linear transformations, we will usually denote linear functionals by lowercase italic letters, such as , and . Example 3.1 The map defined by is a linear functional, known as .evaluation at Example 3.2 Let denote the vector space of all continuous functions on . Let be defined by Then . For any , the rank plus nullity theorem is * dim ker dim dim im But since , we have either , in which case is the zeroim im linear functional, or , in which case is surjective. In other words, aim nonzero linear functional is surjective. Moreover, if , then codim ker dim ker and if , thendim dim ker dim Thus, in dimensional terms, the kernel of a linear functional is a very “large” subspace of the domain . The following theorem will prove very useful. Theorem 3.10 1 For any nonzero vector , there exists a linear functional for) * which . 2 A vector is zero if and only if for all .) * 3 Let . If , then) ker 4 Two nonzero linear functionals have the same kernel if and only) if there is a nonzero scalar such that . Proof. For part 3 , if , then and for) ker , whence , which is false. Hence, andker the direct sum exists. Also, for any we have ker 96 Advanced Linear Algebra ker and so . ker For part 4 , if for , then . Conversely, if) ker ker ker ker , then for we have by part 3 ,) Of course, for any . Therefore, if , it follows that and hence . Dual Bases Let be a vector space with basis . For each , we can define a linear functional by the orthogonality condition * where is the , defined by Kronecker delta function if if Then the set is linearly independent, since applying the equation to the basis vector gives for all . Theorem 3.11 Let be a vector space with basis . 1 The set is linearly independent.) 2 If is finite-dimensional, then is a basis for , called the of) dual basis . Proof. For part 2 , for any , we have) and so is in the span of . Hence, is a basis for . * The Isomorphism Theorems 97 It follows from the previous theorem that if , thendim dim dim since the dual vectors also form a basis for . Our goal now is to show that the converse of this also holds. But first, let us consider an example. Example 3.3 Let be an infinite-dimensional vector space over the field , with basis . Since the only coefficients in are and , a finite linear combination over is just a finite sum. Hence, is the set of all finite sums of vectors in and so according to Theorem 0.12, On the other hand, each linear functional is uniquely defined by specifying its values on the basis . Since these values must be either or , specifying a linear functional is equivalent to specifying the subset of on which takes the value . In other words, there is a one-to-one correspondence between linear functionals on and all subsets of . Hence, This shows that cannot be isomorphic to , nor to any proper subset of . Hence, .dim dim We wish to show that the behavior in the previous example is typical, in particular, that dim dim with equality if and only if is finite-dimensional. The proof uses the concept of the of a field , which is defined as the smallest subfield ofprime subfield the field . Since , it follows that contains a copy of the integers If has prime characteristic , then and so contains the elements which form a subfield of . Since any subfield of contains and , we see that and so is the prime subfield of . On the other hand, if has characteristic , then contains a “copy” of the integers and therefore also the rational numbers , which is the prime subfield of . Our main interest in the prime subfield is that in either case, the prime subfield is .countable Theorem 3.12 Let be a vector space. Then dim dim with equality if and only if is finite-dimensional. 98 Advanced Linear Algebra Proof. For any vector space , we have dim dim since the dual vectors to a basis for are linearly independent in . We have already seen that if is finite-dimensional, then . We dim dim wish to show that if is infinite-dimensional, then . The dim dim ( author is indebted to Professor Richard Foote for suggesting this line of proof.) If is a basis for and if is the base field for , then Theorem 2.7 implies that where is the set of all functions with finite support from to and where is the set of all functions from to . Thus, we can work with the vector spaces and . The plan is to show that if is a countable subfield of and if is infinite, then dim dim dim dim Since we may take to be the prime subfield of , this will prove the theorem. The first equality follows from the fact that the -space and the -space each have a basis consisting of the “standard” linear functionals defined by for all , where is the Kronecker delta function. For the final inequality, suppose that is linearly independent over and that where . If is a basis for over , then for and so Evaluating at any gives The Isomorphism Theorems 99 and since the inner sums are in and is -independent, the inner sums must be zero: Since this holds for all , we have which implies that for all . Hence, is linearly independent over . This proves that .dim dim For the center inequality, it is clear that dim dim We will show that the inequality must be strict by showing that the cardinality of is whereas the cardinality of is greater than . To this end, the set can be partitioned into blocks based on the support of the function. In particular, for each finite subset of , if we let supp then finite where the union is disjoint. Moreover, if , then and so finite max But since the reverse inequality is easy to establish, we have As to the cardinality of , for each subset of , there is a function that sends every element of to and every element of to . Clearly, each distinct subset gives rise to a distinct function and so Cantor's 100 Advanced Linear Algebra theorem implies that This shows that dim dim and completes the proof. Reflexivity If is a vector space, then so is the dual space and so we may form the double algebraic dual space( ) , which consists of all linear functionals . In other words, an element of is a linear functional that ** assigns a scalar to each linear functional on . With this firmly in mind, there is one rather obvious way to obtain an element of . Namely, if , consider the map defined by which sends the linear functional to the scalar . The map is called evaluation at . To see that , if and , then and so is indeed linear. We can now define a map by This is called the or the from to . Thiscanonical map natural map() map is injective and hence in the finite-dimensional case, it is also surjective. Theorem 3.13 The canonical map defined by , where is evaluation at , is a monomorphism. If is finite-dimensional, then is an isomorphism. Proof. The map is linear since for all . To determine the kernel of , observe that for all for all by Theorem 3.10 and so . In the finite-dimensional case, sinceker The Isomorphism Theorems 101 dim dim dim it follows that is also surjective, hence an isomorphism. Note that if , then since the dimensions of and are the same,dim we deduce immediately that . This is not the point of Theorem 3.13. The point is that the is an isomorphism. Because of this, natural map is said to be . Theorem 3.13 and Theorem 3.12 togetheralgebraically reflexive imply that a vector space is algebraically reflexive if and only if it is finite- dimensional. If is finite-dimensional, it is customary to identify the double dual space with and to think of the elements of simply as vectors in . Let us consider a specific example to show how algebraic reflexivity fails in the infinite-dimensional case. Example 3.4 Let be the vector space over with basis where the is in the th position. Thus, is the set of all infinite binary sequences with a finite number of 's. Define the of any to be order the largest coordinate of with value . Then for all . Consider the dual vectors , defined as usual by () For any , the evaluation functional has the property that if However, since the dual vectors are linearly independent, there is a linear functional for which for all . Hence, does not have the form for any . This shows that the canonical map is not surjective and so is not algebraically reflexive. Annihilators The functions are defined on vectors in , but we may also define on subsets of by letting 102 Advanced Linear Algebra Definition Let be a nonempty subset of a vector space . The annihilator of is The term annihilator is quite descriptive, since consists of all linear functionals that send to every vector in . It is not hard to seeannihilate () that is a subspace of , even when is not a subspace of . The basic properties of annihilators are contained in the following theorem. Theorem 3.14 1 If )( )Order-reversing and are nonempty subsets of , then 2 If , then for any nonempty subset of the natural map)dim span is an isomorphism from . In particular, if is aspan onto subspace of , then . 3 If and are subspaces of , then) and Proof. We leave proof of part 1 for the reader. For part 2 , since)) span it is sufficient to prove that is an isomorphism, where is a subspace of . Now, we know that is a monomorphism, so it remains to prove that . If , then has the property that for all , and so , which implies that . Moreover, if , then for all we have and so every linear functional that annihilates also annihilates . But if , then there is a linear functional for which and . ()We leave proof of this as an exercise. Hence, and so and so . For part 3 , it is clear that annihilates if and only if annihilates both) and . Hence, . Also, if where and , then and so . Thus, The Isomorphism Theorems 103 For the reverse inclusion, suppose that . Write where and . Define by and define by It follows that , and . Annihilators and Direct Sums Consider a direct sum decomposition Then any linear functional can be extended to a linear functional on by setting . Let us call this . Clearly, and it is extension by easy to see that the extension by map is an isomorphism from to , whose inverse is the restriction to . Theorem 3.15 Let . a The extension by map is an isomorphism from to and so) b If is finite-dimensional, then) dim dim dim codim Example 3.5 Part b of Theorem 3.15 may fail in the infinite-dimensional case,) since it may easily happen that . As an example, let be the vector space over with a countably infinite ordered basis . Let and . It is easy to see that and that dim dim . The annihilator provides a way to describe the dual space of a direct sum. Theorem 3.16 A linear functional on the direct sum can be written as a sum of a linear functional that annihilates and a linear functional that annihilates , that is, 104 Advanced Linear Algebra Proof. Clearly , since any functional that annihilates both and must annihilate . Hence, the sum is direct. The rest follows from Theorem 3.14, since Alternatively, since is the identity map, if , then we can write and so . Operator Adjoints If , then we may define a map by * for . We will write composition as juxtaposition. Thus, for any , * () The map is called the of and can be described by the operator adjoint phrase “apply first.” Theorem 3.17 Properties of the Operator Adjoint() 1 For and ,) 2 For and ,) 3 For any invertible ,) Proof. Proof of part 1 is left for the reader. For part 2 , we have for all ,)) Part 3 follows from part 2 and)) and in the same way, . Hence . If , then and so . Of course, () is not equal to . However, in the finite-dimensional case, if we use the natural maps to identify with and with , then we can think of The Isomorphism Theorems 105 as being in . Using these identifications, we do have equality in the finite-dimensional case. Theorem 3.18 Let and be finite-dimensional and let . If we identify with and with using the natural maps, then is identified with . Proof. For any let the corresponding element of be denoted by and similarly for . Then before making any identifications, we have for , for all and so * Therefore, using the canonical identifications for both and we have for all . The next result describes the kernel and image of the operator adjoint. Theorem 3.19 Let . Then 1)imker 2)im ker Proof. For part 1 ,) ker im im For part 2 , if , then and so)im ker ker ker . For the reverse inclusion, let . We wish to show that ker for some . On , there is no problem since ker and agree on for any . Let be a complement of . ker Then maps a basis for to a linearly independent set in and so we can define on by setting and extending to all of . Then on and therefore on . Thus, im . 106 Advanced Linear Algebra Corollary 3.20 Let , where and are finite-dimensional. Then .rk rk In the finite-dimensional case, and can both be represented by matrices. Let and be ordered bases for and , respectively, and let and be the corresponding dual bases. Then and Comparing the last two expressions we see that they are the same except that the roles of and are reversed. Hence, the matrices in question are transposes. Theorem 3.21 Let , where and are finite-dimensional. If and are ordered bases for and , respectively, and and are the corresponding dual bases, then In words, the matrices of and its operator adjoint are transposes of one another. Exercises 1. If is infinite-dimensional and is an infinite-dimensional subspace, must the dimension of be finite? Explain. 2. Prove the correspondence theorem. 3. Prove the first isomorphism theorem. 4. Complete the proof of Theorem 3.9. 5. Let be a subspace of . Starting with a basis for how would you find a basis for ? 6. Use the first isomorphism theorem to prove the rank-plus-nullity theorem rk null dim for and . dim 7. Let and suppose that is a subspace of . Define a map by The Isomorphism Theorems 107 When is well-defined? If is well-defined, is it a linear transformation? What are and ?im ker 8. Show that for any nonzero vector , there exists a linear functional for which . 9. Show that a vector is zero if and only if for all . 10. Let be a proper subspace of a finite-dimensional vector space and let . Show that there is a linear functional for which and for all . 11. Find a vector space and decompositions with but . Hence, does imply that . not 12. Find isomorphic vectors spaces and with and but . Hence, does imply that . not 13. Let be a vector space with Prove that if and have finite codimension in , then so does and codim dim dim 14. Let be a vector space with Suppose that and have finite codimension. Hence, by the previous exercise, so does . Find a direct sum decomposition for which 1 has finite codimension, 2 and 3() () () . 15. Let be a basis for an infinite-dimensional vector space and define, for all , the map by if and otherwise, for all . Does form a basis for ? What do you conclude about the concept of a dual basis? 16. Prove that if and are subspaces of , then . 17. Prove that and where is the zero linear operator and is the identity. 18. Let be a subspace of . Prove that . 19. Verify that a for .) b for any and ) 20. Let , where and are finite-dimensional. Prove that rk rk . Chapter 4 Modules I: Basic Properties Motivation Let be a vector space over a field and let . Then for any polynomial , the operator is well-defined. For instance, if , then where is the identity operator and is the threefold composition . Thus, using the operator we can define the product of a polynomial and a vector by ()4.1 This product satisfies the usual properties of scalar multiplication, namely, for all and , Thus, for a fixed , we can think of as being endowed with the operations of addition and multiplication of an element of by a in polynomial . However, since is not a field, these two operations do not make into a vector space. Nevertheless, the situation in which the scalars form a ring but not a field is extremely important, not only in this context but in many others. Modules Definition Let be a commutative ring with identity, whose elements are called . An or a is a nonempty set ,scalars -module module over () 110 Advanced Linear Algebra together with two operations. The first operation, called and denotedaddition by , assigns to each pair , an element . The second operation, denoted by juxtaposition, assigns to each pair , an element . Furthermore, the following properties must hold: 1 is an abelian group under addition.) 2 For all and ) The ring is called the of .base ring Note that vector spaces are just special types of modules: a vector space is a module over a field. When we turn in a later chapter to the study of the structure of a linear transformation , we will think of as having the structure of a vector space over as well as a module over and we will use the notation . Put another way, is an abelian group under addition, with two scalar multiplications—one whose scalars are elements of and one whose scalars are polynomials over . This viewpoint will be of tremendous benefit for the study of . For now, we concentrate only on modules. Example 4.1 1 If is a ring, the set of all ordered -tuples whose components lie in ) is an -module, with addition and scalar multiplication defined componentwise just as in ,() and for , . For example, is the -module of all ordered -tuples of integers. 2 If is a ring, the set of all matrices of size is an -) module, under the usual operations of matrix addition and scalar multiplication over . Since is a ring, we can also take the product of matrices in . One important example is , whence is the -module of all matrices whose entries are polynomials. 3 Any commutative ring with identity is a module over itself, that is, is) an -module. In this case, scalar multiplication is just multiplication by Modules I: Basic Properties 111 elements of , that is, scalar multiplication is the ring multiplication. The defining properties of a ring imply that the defining properties of the - module are satisfied. We shall use this example many times in the sequel. Importance of the Base Ring Our definition of a module requires that the ring of scalars be commutative. Modules over noncommutative rings can exhibit quite a bit more unusual behavior than modules over commutative rings. Indeed, as one would expect, the general behavior of -modules improves as we impose more structure on the base ring . If we impose the very strict structure of a field, the result is the very well behaved vector space. To illustrate, we will give an example of a module over a ringnoncommutative that has a basis of size for every integer ! As another example, if the base ring is an integral domain, then whenever are linearly independent over so are for any nonzero . This can fail when is not an integral domain. We will also consider the property on the base ring that all of its ideals are finitely generated. In this case, any finitely generated -module has the property that all of its submodules are also finitely generated. This property of -modules fails if does not have the stated property. When is a principal ideal domain such as or , each of its ideals is () generated by a single element. In this case, the -modules are “reasonably” well behaved. For instance, in general, a module may have a basis and yet possess a submodule that has no basis. However, if is a principal ideal domain, this cannot happen. Nevertheless, even when is a principal ideal domain, -modules are less well behaved than vector spaces. For example, there are modules over a principal ideal domain that do not have any linearly independent elements. Of course, such modules cannot have a basis. Submodules Many of the basic concepts that we defined for vector spaces can also be defined for modules, although their properties are often quite different. We begin with submodules. Definition A of an -module is a nonempty subset of thatsubmodule is an -module in its own right, under the operations obtained by restricting the operations of to . We write to denote the fact that is a submodule of . 112 Advanced Linear Algebra Theorem 4.1 A nonempty subset of an -module is a submodule if and only if it is closed under the taking of linear combinations, that is, Theorem 4.2 If and are submodules of , then and are also submodules of . We have remarked that a commutative ring with identity is a module over itself. As we will see, this type of module provides some good examples of non- vector-space-like behavior. When we think of a ring as an -module rather than as a ring, multiplication is treated as multiplication. This has some important implications. Inscalar particular, if is a submodule of , then it is closed under scalar multiplication, which means that it is closed under multiplication by elements of the ring .all In other words, is an ideal of the ring . Conversely, if is an ideal of the ring , then is also a submodule of the module . Hence, the submodules of the -module are precisely the ideals of the ring . Spanning Sets The concept of spanning set carries over to modules as well. Definition The or by a subset of a modulesubmodule spanned generated() is the set of all of elements of :linear combinations A subset is said to or if . span generate We use a double angle bracket notation for the submodule generated by a set because when we study the -vector space/ -module , we will need to make a distinction between the subspace generated by and the submodule generated by . One very important point to note is that if a nontrivial linear combination of the elements in an -module is , where not all of the coefficients are , then we conclude, as we could in cannot a vector space, that one of the elements is a linear combination of the others. After all, this involves dividing by one of the coefficients, which may not be possible in a ring. For instance, for the -module we have but neither nor is an integer multiple of the other. Modules I: Basic Properties 113 The following simple submodules play a special role in the theory. Definition Let be an -module. A submodule of the form for is called the generated by . cyclic submodule Of course, any finite-dimensional vector space is the direct sum of cyclic submodules, that is, one-dimensional subspaces. One of our main goals is to show that a finitely generated module over a principal ideal domain has this property as well. Definition An -module is said to be if it contains a finitely generated finite set that generates . More specifically, is if it has a -generated generating set of size although it may have a smaller generating set as ( well .) Of course, a vector space is finitely generated if and only if it has a finite basis, that is, if and only if it is finite-dimensional. For modules, life is more complicated. The following is an example of a finitely generated module that has a submodule that is not finitely generated. Example 4.2 Let be the ring of all polynomials in infinitely many variables over a field . It will be convenient to use to denote and write a polynomial in in the form . Each polynomial in( , being a finite sum, involves only finitely many variables, however. Then ) is an -module and as such, is finitely generated by the identity element . Now consider the submodule of all polynomials with zero constant term. This module is generated by the variables themselves, However, is not finitely generated. To see this, suppose that is a finite generating set for . Choose a variable that does not appear in any of the polynomials in . Then no linear combination of the polynomials in can be equal to . For if 114 Advanced Linear Algebra then let where does not involve . This gives The last sum does not involve and so it must equal . Hence, the first sum must equal , which is not possible since has no constant term. Linear Independence The concept of linear independence also carries over to modules. Definition A subset of an -module is if for any linearly independent distinct and , we have for all A set that is not linearly independent is . linearly dependent It is clear from the definition that any subset of a linearly independent set is linearly independent. Recall that in a vector space, a set of vectors is linearly dependent if and only if some vector in is a linear combination of the other vectors in . For arbitrary modules, this is not true. Example 4.3 Consider as a -module. The elements are linearly dependent, since but neither one is a linear combination i.e., integer multiple of the other.() The problem in the previous example as noted earlier is that() implies that but in general, we cannot divide both sides by , since it may not have a multiplicative inverse in the ring . Modules I: Basic Properties 115 Torsion Elements In a vector space over a field , singleton sets where are linearly independent. Put another way, and imply . However, in a module, this need not be the case. Example 4.4 The abelian group is a -module, with scalar multiplication defined by , for all and . mod However, since for all , no singleton set is linearly independent. Indeed, has no linearly independent sets. This example motivates the following definition. Definition Let be an -module. A nonzero element for which for some nonzero is called a of . A module that has no torsion element nonzero torsion elements is said to be . If all elements of aretorsion-free torsion elements, then is a . The set of all torsion elements of torsion module , together with the zero element, is denoted by .tor If is a module over an , it is not hard to see that is aintegral domain tor submodule of and that is torsion-free. We will define quotient tor ( modules shortly: they are defined in the same way as for vector spaces.) Annihilators Closely associated with the notion of a torsion element is that of an annihilator. Definition Let be an -module. The of an element is annihilator ann and the of a submodule of isannihilator ann where . Annihilators are also called . order ideals It is easy to see that and are ideals of . Clearly, is aann ann torsion element if and only if . Also, if and are submodules ofann , then ann ann (note the reversal of order). Let be a finitely generated module over an integral domain and assume that each of the generators is torsion, that is, for each , there is a nonzero . Then, the nonzero product annihilates each ann generator of and therefore every element of , that is, . This ann 116 Advanced Linear Algebra shows that . On the other hand, this may fail if is not anann integral domain. Also, there are torsion modules whose annihilators are trivial. ()We leave verification of these statements as an exercise. Free Modules The definition of a basis for a module parallels that of a basis for a vector space. Definition Let be an -module. A subset of is a if is linearly basis independent and spans . An -module is said to be if or if free has a basis. If is a basis for , we say that is .free on We have the following analog of part of Theorem 1.7. Theorem 4.3 A subset of a module is a basis if and only if every nonzero is an essentially unique linear combination of the vectors in . In a vector space, a set of vectors is a basis if and only if it is a minimal spanning set, or equivalently, a maximal linearly independent set. For modules, the following is the best we can do in general. We leave proof to the reader. Theorem 4.4 Let be a basis for an -module . Then 1 is a minimal spanning set.) 2 is a maximal linearly independent set.) The -module has no basis since it has no linearly independent sets. But since the entire module is a spanning set, we deduce that a minimal spanning set need not be a basis. In the exercises, the reader is asked to give an example of a module that has a finite basis, but with the property that not every spanning set in contains a basis and not every linearly independent set in is contained in a basis. It follows in this case that a maximal linearly independent set need not be a basis. The next example shows that even free modules are not very much like vector spaces. It is an example of a free module that has a submodule that is not free. Example 4.5 The set is a free module over itself, using componentwise scalar multiplication with basis . But the submodule is not free since it has no linearly independent elements and hence no basis. Theorem 2.2 says that a linear transformation can be defined by specifying its values arbitrarily on a basis. The same is true for modules.free Modules I: Basic Properties 117 Theorem 4.5 Let and be -modules where is free with basis . Then we can define a unique -map by specifying the values of arbitrarily for all and then extending to by linearity, that is, Homomorphisms The term is special to vector spaces. However, thelinear transformation concept applies to most algebraic structures. Definition Let and be -modules. A function is an - homomorphism -map or if it preserves the module operations, that is, for all and . The set of all -homomorphisms from to is denoted by . The following terms are also employed:hom 1 An - is an -homomorphism from to itself.) endomorphism 2 An - or - is an injective -homomorphism.) monomorphism embedding 3 An - is a surjective -homomorphism.) epimorphism 4 An - is a bijective -homomorphism.) isomorphism It is easy to see that is itself an -module under addition ofhom functions and scalar multiplication defined by Theorem 4.6 Let . The kernel and image of , defined as for hom linear transformations by ker and im are submodules of and , respectively. Moreover, is a monomorphism if and only if .ker If is a submodule of the -module , then the map defined by is evidently an -monomorphism, called of into .injection Quotient Modules The procedure for defining quotient modules is the same as that for defining quotient vector spaces. We summarize in the following theorem. 118 Advanced Linear Algebra Theorem 4.7 Let be a submodule of an -module . The binary relation is an equivalence relation on , whose equivalence classes are the cosets of in . The set of all cosets of in , called the of quotient module , is an -module under the well-defined operationsmodulo The zero element in is the coset . One question that immediately comes to mind is whether a quotient module of a free module must be free. As the next example shows, the answer is no. Example 4.6 As a module over itself, is free on the set . For any , the set is a free cyclic submodule of , but the quotient - module is isomorphic to via the map mod and since is not free as a -module, neither is . The Correspondence and Isomorphism Theorems The correspondence and isomorphism theorems for vector spaces have analogs for modules. Theorem 4.8 The correspondence theorem() Let be a submodule of . Then the function that assigns to each intermediate submodule the quotient submodule of is an order-preserving with respect to set ( inclusion one-to-one correspondence between submodules of containing ) and all submodules of . Theorem 4.9 The Let be an -()first isomorphism theorem homomorphism. Then the map defined by ker ker is an -embedding and so ker im Modules I: Basic Properties 119 Theorem 4.10 The Let be an -module()second isomorphism theorem and let and be submodules of . Then Theorem 4.11 The Let be an -module and()third isomorphism theorem suppose that are submodules of . Then Direct Sums and Direct Summands The definition of direct sum of a family of submodules is a direct analog of the definition for vector spaces. Definition The of -modules , denoted byexternal direct sum is the -module whose elements are ordered -tuples with componentwise operations and for . We leave it to the reader to formulate the definition of external direct sums and products for arbitrary families of modules, in direct analogy with the case of vector spaces. Definition An -module is the of a family ()internal direct sum of submodules of , written or if the following hold: 1 is the sum join of the family :)( ) ( )Join of the family 120 Advanced Linear Algebra 2 For each ,)( )Independence of the family In this case, each is called a of . If is direct summand a finite family, the direct sum is often written Finally, if , then is said to be and is called a complemented complement of in . As with vector spaces, we have the following useful characterization of direct sums. Theorem 4.12 Let be a family of distinct submodules of an - module . The following are equivalent: 1 For each ,)( )Independence of the family 2 The zero element cannot be written as)( )Uniqueness of expression for a sum of nonzero elements from distinct submodules in . 3 Every nonzero has a unique, except for)( )Uniqueness of expression order of terms, expression as a sum of nonzero elements from distinct submodules in . Hence, a sum is direct if and only if any one of 1 3 holds.)– ) In the case of vector spaces, every subspace is a direct summand, that is, every subspace has a complement. However, as the next example shows, this is not true for modules. Example 4.7 The set of integers is a -module. Since the submodules of are precisely the ideals of the ring and since is a principal ideal domain, the submodules of are the sets Modules I: Basic Properties 121 Hence, any two nonzero proper submodules of have nonzero intersection, for if , then where . It follows that the only complemented submodules of lcm are and . In the case of vector spaces, there is an intimate connection between subspaces and quotient spaces, as we saw in Theorem 3.6. The problem we face in generalizing this to modules is that not all submodules are complemented. However, this is the only problem. Theorem 4.13 Let be a complemented submodule of . All complements of are isomorphic to and hence to each other. Proof. For any complement of , the first isomorphism theorem applied to the projection gives . Direct Summands and Extensions of Isomorphisms Direct summands play a role in questions relating to whether certain module homomorphisms can be extended from a submodule to the full module . The discussion will be a bit simpler if we restrict attention to epimorphisms. If , then a module epimorphism can be extended to an epimorphism simply by sending the elements of to zero, that is, by setting This is easily seen to be an -map with ker ker Moreover, if is another extension of with the same kernel as , then and agree on as well as on , whence . Thus, there is a extension of unique with kernel .ker Now suppose that is an isomorphism. If is complemented, that is, if then we have seen that there is a extension of for which .unique ker Thus, the correspondence , where ker from complements of to extensions of is an injection. To see that this correspondence is a bijection, if is an extension of , then 122 Advanced Linear Algebra ker To see this, we have ker ker and if , then there is a for which and so Thus, ker which shows that is a complement of .ker Theorem 4.14 Let and be -modules and let . 1 If , then any -epimorphism has a unique) extension to an epimorphism with ker ker 2 Let be an -isomorphism. Then the correspondence) , where ker is a bijection from complements of onto the extensions of . Thus, an isomorphism has an extension to if and only if is complemented. Definition Let . When the identity map has an extension to , the submodule is called a of and is called theretract retraction map. Corollary 4.15 A submodule is a retract of if and only if has a complement in . Direct Summands and One-Sided Invertibility Direct summands are also related to one-sided invertibility of -maps. Definition Let be a module homomorphism. 1 A of is a module homomorphism for which) left inverse . 2 A of is a module homomorphism for which) right inverse . Left and right inverses are called . An ordinary inverse isone-sided inverses called a .two-sided inverse Unlike a two-sided inverse, one-sided inverses need not be unique. Modules I: Basic Properties 123 A left-invertible homomorphism must be injective, since Also, a right-invertible homomorphism must be surjective, since if , then im For functions, the converses of these statements hold: is left-invertible ifset and only if it is injective and is right-invertible if and only if it is surjective. However, this is not the case for -maps. Let be an injective -map. Referring to Figure 4.1, M M1 im() H im()) -1 im() Figure 4.1 the map obtained from by restricting its range to is im im im an isomorphism and the left inverses of are precisely the extensions of im im to . Hence, Theorem 4.14 says that the correspondence extension of with kernel im is a bijection from the complements of onto the left inverses of . im Now let be a surjective -map. Referring to Figure 4.2, M M1 ker() H |H R=(|H)-1 Figure 4.2 if is complemented, that is, ifker ker 124 Advanced Linear Algebra then is an isomorphism. Thus, a map is a right inverse of if and only if is a of , the only range-extension difference being in the ranges of the two functions. Hence, is the only right inverse of with image . It follows that the correspondence is an injection from the complements of to the right inverses of . ker Moreover, this map is a bijection, since if is a right inverse of , then and is an extension of , which im im implies that im ker Theorem 4.16 Let and be -modules and let be an -map. 1 Let be injective. The map) extension of with kernel im is a bijection from the complements of onto the left inverses of . im Thus, there is exactly one left inverse of for each complement of im and that complement is the kernel of the left inverse. 2 Let be surjective. The map) is a bijection from the complements of to the right inverses of . ker Thus, there is exactly one right inverse of for each complement of ker and that complement is the image of the right inverse. Thus, ker ker im The last part of the previous theorem is worth further comment. Recall that if is a linear transformation on vector spaces, then ker im This holds for modules as well .provided that is a direct summandker Modules Are Not as Nice as Vector Spaces Here is a list of some of the properties of modules over commutative rings with( identity that emphasize the differences between modules and vector spaces.) 1 A submodule of a module need not have a complement.) 2 A submodule of a finitely generated module need not be finitely generated.) 3 There exist modules with no linearly independent elements and hence with) no basis. 4 A minimal spanning set or maximal linearly independent set is not) necessarily a basis. Modules I: Basic Properties 125 5 There exist free modules with submodules that are not free.) 6 There exist free modules with linearly independent sets that are not) contained in a basis and spanning sets that do not contain a basis. Recall also that a module over a ring may have bases ofnoncommutative different sizes. However, all bases for a free module over a commutative ring with identity have the same size, as we will prove in the next chapter. Exercises 1. Give the details to show that any commutative ring with identity is a module over itself. 2. Let be a subset of a module . Prove that is the submodule of containing . First you will need to formulatesmallest precisely what it means to be the smallest submodule of containing . 3. Let be an -module and let be an ideal in . Let be the set of all finite sums of the form where and . Is a submodule of ? 4. Show that if and are submodules of , then with respect to set ( inclusion) glb lub and 5. Let be an ascending sequence of submodules of an - module . Prove that the union is a submodule of . 6. Give an example of a module that has a finite basis but with the property that not every spanning set in contains a basis and not every linearly independent set in is contained in a basis. 7. Show that, just as in the case of vector spaces, an -homomorphism can be defined by assigning arbitrary values on the elements of a basis and extending by linearity. 8. Let be an -isomorphism. If is a basis for , prove hom that is a basis for . 9. Let be an -module and let be an -endomorphism. hom If is , that is, if , show that idempotent ker im Does the converse hold? 10. Consider the ring of polynomials in two variables. Show that the set consisting of all polynomials in that have zero constant term is an -module. Show that is not a free -module. 11. Prove that if is an integral domain, then all -modules have the following property: If is linearly independent over , then so is for any nonzero . 126 Advanced Linear Algebra 12. Prove that if a nonzero commutative ring with identity has the property that every finitely generated -module is free then is a field. 13. Let and be -modules. If is a submodule of and is a submodule of show that 14. If is a commutative ring with identity and is an ideal of , then is an -module. What is the maximum size of a linearly independent set in ? Under what conditions is free? 15. a the set of all) Show that for any module over an integral domaintor torsion elements in a module is a submodule of . b Find an example of a ring with the property that for some -module) the set is not a submodule.tor c ) Show that for any module over an integral domain, the quotient module is torsion-free.tor 16. a Find a module that is finitely generated by torsion elements but for) which .ann b Find a torsion module for which .)ann 17. Let be an abelian group together with a scalar multiplication over a ring that satisfies all of the properties of an -module except that does not necessarily equal for all . Show that can be written as a direct sum of an -module and another “pseudo -module” . 18. Prove that is an -module under addition of functions andhom scalar multiplication defined by 19. Prove that any -module is isomorphic to the -module . hom 20. Let and be commutative rings with identity and let be a ring homomorphism. Show that any -module is also an -module under the scalar multiplication 21. Prove that where .hom gcd 22. Suppose that is a commutative ring with identity. If and are ideals of for which as -modules, then prove that . Is the result true if as rings? Chapter 5 Modules II: Free and Noetherian Modules The Rank of a Free Module Since all bases for a vector space have the same cardinality, the concept of vector space dimension is well-defined. A similar statement holds for free - modules when the base ring is commutative but not otherwise .() Theorem 5.1 Let be a free module over a commutative ring with identity. 1 Then any two bases of have the same cardinality.) 2 The cardinality of a spanning set is greater than or equal to that of a basis.) Proof. The plan is to find a vector space with the property that, for any basis for , there is a basis of the same cardinality for . Then we can appeal to the corresponding result for vector spaces. Let be a maximal ideal of , which exists by Theorem 0.23. Then is a field. Our first thought might be that is a vector space over , but that is not the case. In fact, scalar multiplication using the field , is not even well-defined, since this would require that . On the other hand, we can fix precisely this problem by factoring out the submodule Indeed, is a vector space over , with scalar multiplication defined by To see that this is well-defined, we must show that the conditions imply 128 Advanced Linear Algebra But this follows from the fact that Hence, scalar multiplication is well-defined. We leave it to the reader to show that is a vector space over . Consider now a set and the corresponding set If spans over , then spans over . To see this, note that any has the form for and so which shows that spans . Now suppose that is a basis for over . We show that is a basis for over . We have seen that spans . Also, if then and so where . From the linear independence of we deduce that for all and so . Hence is linearly independent and therefore a basis, as desired. To see that , note that if , then where . If , then the coefficient of on the right must be equal to Modules II: Free and Noetherian Modules 129 and so , which is not possible since is a maximal ideal. Hence, . Thus, if is a basis for over , then dim and so all bases for over have the same cardinality, which proves part 1 . ) Finally, if spans over , then spans and so dim Thus, has cardinality at least as great as that of any basis for over . The previous theorem allows us to define the of a free module. The termrank ( dimension is not used for modules in general.) Definition Let be a commutative ring with identity. The of a rank rk nonzero free -module is the cardinality of any basis for . The rank of the trivial module is . Theorem 5.1 fails if the underlying ring of scalars is not commutative. The next example describes a module over a noncommutative ring that has the remarkable property of possessing a basis of size for any positive integer . Example 5.1 Let be a vector space over with a countably infinite basis . Let be the ring of linear operators on . Observe that is not commutative, since composition of functions is not commutative. The ring is an -module and as such, the identity map forms a basis for . However, we can also construct a basis for of any desired finite size . To understand the idea, consider the case and define the operators and by and These operators are linearly independent essentially because they are surjective and their supports are disjoint. In particular, if then 130 Advanced Linear Algebra and which shows that and . Moreover, if , then we define and by from which it follows easily that which shows that is a basis for . More generally, we begin by partitioning into blocks. For each , let mod Now we define elements by where and where is the Kronecker delta function. These functions are surjective and have disjoint support. It follows that is 0 linearly independent. For if where , then, applying this to gives for all . Hence, . Also, spans , for if , we define by + to get ++ + and so Thus, is a basis for of size . 0 Recall that if is a basis for a vector space over , then is isomorphic to the vector space of all functions from to that have finite support. A Modules II: Free and Noetherian Modules 131 similar result holds for free -modules. We begin with the fact that is a free -module. The simple proof is left to the reader. Theorem 5.2 Let be any set and let be a commutative ring with identity. The set of all functions from to that have finite support is a free - module of rank with basis where if if This basis is referred to as the for .standard basis Theorem 5.3 Let be an -module. If is a basis for , then is isomorphic to . Proof. Consider the map defined by setting where is defined in Theorem 5.2 and extending to by linearity. Since maps a basis for to a basis for , it follows that is an isomorphism from to . Theorem 5.4 Two free -modules over a commutative ring are isomorphic if () and only if they have the same rank. Proof. If , then any isomorphism from to maps a basis for to a basis for . Since is a bijection, we have . Conversely, rk rk suppose that . Let be a basis for and let be a basis for .rk rk Since , there is a bijective map . This map can be extended by linearity to an isomorphism of onto and so . We have seen that the cardinality of a minimal spanning set for a free module() is at least equal to . Let us now speak about the cardinality of maximalrk linearly independent sets. Theorem 5.5 Let be an integral domain and let be a free -module. Then all linearly independent sets have cardinality at most .rk Proof. Since we need only prove the result for . Let be the field of quotients of . Then is a vector space. Now, if is linearly independent over as a subset of , then is clearly linearly independent over as a subset of . Conversely, suppose that is linearly independent over and 132 Advanced Linear Algebra where for all and for some . Multiplying by produces a nontrivial linear dependency over , which implies that for all . Thus is linearly dependent over if and only if it is linearly dependent over . But in the vector space , all sets of cardinality greater than are linearly dependent over and hence all subsets of of cardinality greater than are linearly dependent over . Free Modules and Epimorphisms If is a module epimorphism where is free on , then it is easy to define a right inverse for , since we can define an -map by specifying its values arbitrarily on and extending by linearity. Thus, we take to be any member of . Then Theorem 4.16 implies that is aker direct summand of and ker This discussion applies to the canonical projection provided that the quotient is free. Theorem 5.6 Let be a commutative ring with identity. 1 If is an -epimorphism and is free, then is) ker complemented and ker ker where . 2 If is a submodule of and if is free, then is complemented and ) If and are free, then rk rk rk and if the ranks are all finite, then rk rk rk Noetherian Modules One of the most desirable properties of a finitely generated -module is that all of its submodules be finitely generated: Modules II: Free and Noetherian Modules 133 finitely generated, finitely generated Example 4.2 shows that this is not always the case and leads us to search for conditions on the ring that will guarantee this property for -modules. Definition An -module is said to satisfy the ascending chain condition ()abbreviated ACC on submodules if every ascending sequence of submodules of is eventually constant, that is, there exists an index for which k Modules with the ascending chain condition on submodules are also called Noetherian modules after Emmy Noether, one of the pioneers of module( theory .) Since a ring is a module over itself and since the submodules of the module are precisely the ideals of the ring , the preceding definition can be formulated for rings as follows. Definition A ring is said to satisfy the ascending chain condition ()abbreviated ACC on ideals if any ascending sequence of ideals of is eventually constant, that is, there exists an index for which 2 A ring that satisfies the ascending chain condition on ideals is called a Noetherian ring. The following theorem describes the relevance of this to the present discussion. Theorem 5.7 1 An -module is Noetherian if and only if every submodule of is) finitely generated. 2 In particular, a ring is Noetherian if and only if every ideal of is) finitely generated. Proof. Suppose that all submodules of are finitely generated and that contains an infinite ascending sequence 3 ()5.1 of submodules. Then the union 134 Advanced Linear Algebra is easily seen to be a submodule of . Hence, is finitely generated, say . Since , there exists an index such that . Therefore, if , we have max and so which shows that the chain 5.1 is eventually constant.() For the converse, suppose that satisfies the ACC on submodules and let be a submodule of . Pick and consider the submodule generated by . If , then is finitely generated. If , then there is a . Now let , . If , then is finitely generated. If , then pick and consider the submodule 3 33,, . Continuing in this way, we get an ascending chain of submodules If none of these submodules were equal to , we would have an infinite ascending chain of submodules, each properly contained in the next, which contradicts the fact that satisfies the ACC on submodules. Hence, for some and so is finitely generated. Our goal is to find conditions under which all finitely generated -modules are Noetherian. The very pleasing answer is that all finitely generated -modules are Noetherian if and only if is Noetherian as an -module, or equivalently, as a ring. Theorem 5.8 Let be a commutative ring with identity. 1 is Noetherian if and only if every finitely generated -module is) Noetherian. 2 Let be a principal ideal domain. If an -module is -generated, then) any submodule of is also -generated. Proof. For part 1 , one direction is evident. Assume that is Noetherian and) let be a finitely generated -module. Consider the epimorphism defined by Let be a submodule of . Then Modules II: Free and Noetherian Modules 135 is a submodule of and . If every submodule of is finitely generated, then is finitely generated and so . Then is finitely generated by . Thus, it is sufficient to prove the theorem for , which we do by induction on . If , any submodule of is an ideal of , which is finitely generated by assumption. Assume that every submodule of is finitely generated for all and let be a submodule of . If , we can extract from something that is isomorphic to an ideal of and so will be finitely generated. In particular, let be the “last coordinates” in , specifically, let for some The set is isomorphic to an ideal of and is therefore finitely generated, say , where is a finite subset of . Also, let for some be the set of all elements of that have last coordinate equal to . Note that is a submodule of and is isomorphic to a submodule of . Hence, the inductive hypothesis implies that is finitely generated, say , where is a finite subset of . By definition of , each has the form for where there is a of the form Let . We claim that is generated by the finite set . To see this, let . Then and so for . Consider now the sum 136 Advanced Linear Algebra The last coordinate of this sum is and so the difference has last coordinate and is thus in . Hence as desired. For part 2 , we leave it to the reader to review the proof and make the necessary) changes. The key fact is that is isomorphic to an ideal of , which is principal. Hence, is generated by a single element of . The Hilbert Basis Theorem Theorem 5.8 naturally leads us to ask which familiar rings are Noetherian. The following famous theorem describes one very important case. Theorem 5.9 Hilbert basis theorem() If a ring is Noetherian, then so is the polynomial ring . Proof. We wish to show that any ideal in is finitely generated. Let denote the set of all leading coefficients of polynomials in , together with the element of . Then is an ideal of . To see this, observe that if is the leading coefficient of and if , then either or else is the leading coefficient of . In either case, . Similarly, suppose that is the leading coefficient of . We may assume that and , with . Thendeg deg is in , has leading coefficient and has the same degree as . Hence, either is or is the leading coefficient of . In either case . Since is an ideal of the Noetherian ring , it must be finitely generated, say . Since , there exist polynomials with leading coefficient . By multiplying each by a suitable power of , we may assume that deg max deg for all . Modules II: Free and Noetherian Modules 137 Now for let be the set of all leading coefficients of polynomials in of degree , together with the element of . A similar argument shows that is an ideal of and so is also finitely generated. Hence, we can find polynomials in whose leading coefficients constitute a generating set for . Consider now the finite set If is the ideal generated by , then . An induction argument can be used to show that . If has degree , then it is a linear combination of the elements of which are constants and is thus in . () Assume that any polynomial in of degree less than is in and let have degree . If , then some linear combination over of the polynomials in has the same leading coefficient as and if , then some linear combination of the polynomials has the same leading coefficient as . In either case, there is a polynomial that has the same leading coefficient as . Since has degree strictly smaller than that of the induction hypothesis implies that and so This completes the induction and shows that is finitely generated. Exercises 1. If is a free -module and is an epimorphism, then must also be free? 2. Let be an ideal of . Prove that if is a free -module, then is the zero ideal. 3. Prove that the union of an ascending chain of submodules is a submodule. 4. Let be a submodule of an -module . Show that if is finitely generated, so is the quotient module . 5. Let be a submodule of an -module. Show that if both and are finitely generated, then so is . 6. Show that an -module satisfies the ACC for submodules if and only if the following condition holds. Every nonempty collection of submodules 138 Advanced Linear Algebra of has a maximal element. That is, for every nonempty collection of submodules of there is an with the property that . 7. Let be an -homomorphism. a Show that if is finitely generated, then so is .)im b Show that if and are finitely generated, then)imker ker where is a finitely generated submodule of . Hence, is finitely generated. 8. If is Noetherian and is an ideal of show that is also Noetherian. 9. Prove that if is Noetherian, then so is . 10. Find an example of a commutative ring with identity that does not satisfy the ascending chain condition. 11. a Prove that an -module is cyclic if and only if it is isomorphic to) where is an ideal of . b Prove that an -module is and has no proper)( simple nonzero submodules if and only if it is isomorphic to where is) a maximal ideal of . c Prove that for any nonzero commutative ring with identity, a simple) -module exists. 12. Prove that the condition that be a principal ideal domain in part 2 of ) Theorem 5.8 is required. 13. Prove Theorem 5.8 in the following way. a Show that if are submodules of and if and are) finitely generated, then so is . b The proof is again by induction. Assuming it is true for any module) generated by elements, let and let . Then let in part a .) 14. Prove that any -module is isomorphic to the quotient of a free module . If is finitely generated, then can also be taken to be finitely generated. 15 Prove that if and are isomorphic submodules of a module it does. not necessarily follow that the quotient modules and are isomorphic. Prove also that if as modules it does not necessarily follow that . Prove that these statements do hold if all modules are free and have finite rank. Chapter 6 Modules over a Principal Ideal Domain We remind the reader of a few of the basic properties of principal ideal domains. Theorem 6.1 Let be a principal ideal domain. 1 An element is irreducible if and only if the ideal is maximal.) 2 An element in is prime if and only if it is irreducible.) 3 is a unique factorization domain.) 4 satisfies the ascending chain condition on ideals. Hence, so does any) finitely generated -module . Moreover, if is -generated, then any submodule of is -generated. Annihilators and Orders When is a principal ideal domain, all annihilators are generated by a single element. This permits the following definition. Definition Let be a principal ideal domain and let be an -module. 1 If is a submodule of , then any generator of is called an )ann order of . 2 An of an element is an order of the submodule .) order For readers acquainted with group theory, we mention that the order of a module corresponds to the smallest exponent of a group, to the order of thenot group. Theorem 6.2 Let be a principal ideal domain and let be an -module. 1 If is an order of , then the orders of are precisely the) associates of . We denote any order of by and, as is customary, refer to as “the” order of . 2 If , then) lcm 140 Advanced Linear Algebra that is, the orders of are precisely the least common multiples of the orders of and . Proof. We leave proof of part 1) for the reader. For part 2), suppose that lcm Then and imply that and and so . On the other hand, annihilates both and and therefore also . Hence, and so is an order of . Cyclic Modules The simplest type of nonzero module is clearly a cyclic module. Despite their simplicity, cyclic modules will play a very important role in our study of linear operators on a finite-dimensional vector space and so we want to explore some of their basic properties, including their composition and decomposition. Theorem 6.3 Let be a principal ideal domain. 1 If is a cyclic -module with annihilator , then the multiplication) map defined by is an -epimorphism with kernel . Hence the induced map defined by is an isomorphism. In other words, cyclic -modules are isomorphic to quotient modules of the base ring . 2 Any submodule of a cyclic -module is cyclic.) 3 If is a cyclic submodule of of order , then for ,) gcd Also, Proof. We leave proof of part 1 as an exercise. For part 2 , let . Then)) is an ideal of and so for some . Thus, For part 3 , we have if and only if , that is, if and only if) , which is equivalent to Modules Over a Principal Ideal Domain 141 gcd Thus, if and only if and so . For the second ann ann statement, if then there exist for which and so and so . Of course, if then . Finally, if , then gcd and so . The Decomposition of Cyclic Modules The following theorem shows how cyclic modules can be composed and decomposed. Theorem 6.4 Let be an -module. 1 If have relatively prime)( )Composing cyclic modules orders, then and Consequently, if where the submodules have relatively prime orders, then the sum is direct. 2 If where the 's are)( )Decomposing cyclic modules pairwise relatively prime, then has the form where and so Proof. For part 1), let , and . Then since annihilates , the order of divides . If is a proper divisor of , then for some index , there is a prime for which annihilates . But annihilates each for . Thus, 142 Advanced Linear Algebra Since and are relatively prime, the order of is equal to , which contradicts the equation above. Hence, . It is clear that . For the reverse inclusion, since and are relatively prime, there exist for which Hence Similarly, for all and so we get the reverse inclusion. Finally, to see that the sum above is direct, note that if where , then each must be , for otherwise the order of the sum on the left would be different from . For part 2 , the scalars are relatively prime and so there exist ) for which Hence, Since and since and are relatively prime, gcd we have . The second statement follows from part 1 . ) Free Modules over a Principal Ideal Domain We have seen that a submodule of a free module need not be free: The submodule of the module over itself is not free. However, if is a principal ideal domain this cannot happen. Theorem 6.5 Let be a free module over a principal ideal domain . Then any submodule of is also free and . rk rk Proof. We will give the proof first for modules of finite rank and then generalize to modules of arbitrary rank. Since where is rk finite, we may in fact assume that . For each , let Modules Over a Principal Ideal Domain 143 for some Then it is easy to see that is an ideal of and so for some . Let We claim that and is a basis for . As to linear independence, suppose that and that Then comparing the th coordinates gives and since , it follows that . In a similar way, all coefficients are and so is linearly independent. To see that spans , we partition the elements according to the largest coordinate index with nonzero entry and induct on . If , then , which is in the span of . Suppose that all with are in the span of and let , that is, where . Then and so and for some . Hence, and so and therefore . Thus, is a basis for . The previous proof can be generalized in a more or less direct way to modules of arbitrary rank. In this case, we may assume that is the -module of functions with finite support from to , where is a cardinal number. We use the fact that is a well-ordered set, that is, is a totally ordered set in which any nonempty subset has a smallest element. If , the closed interval is Let . For each , let supp Then the set 144 Advanced Linear Algebra is an ideal of and so for some . We show that is a basis for . First, suppose that where for . Applying this to gives and since is an integral domain, . Similarly, for all and so is linearly independent. To show that spans , since any has finite support, there is a largest index for which . Now, if , then since is well- ordered, we may choose a for which is as small as possible. Then . Moreover, since , it follows that and for some . Then supp and and so , which implies that . But then a contradiction. Thus, is a basis for . In a vector space of dimension , any set of linearly independent vectors is a basis. This fails for modules. For example, is a -module of rank but the independent set is not a basis. On the other hand, the fact that a spanning set of size is a basis does hold for modules over a principal ideal domain, as we now show. Theorem 6.6 Let be a free -module of finite rank , where is a principal ideal domain. Let be a spanning set for . Then is a basis for . Proof. Let be a basis for and define the map by and extending to a surjective -homomorphism. Since is free, Theorem 5.6 implies that ker ker im Since is a submodule of the free module and since is a principal idealker domain, we know that is free of rank at most . It follows thatker Modules Over a Principal Ideal Domain 145 rk rk rk ker and so , that is, , which implies that is an -rk ker ker isomorphism and so is a basis. In general, a basis for a submodule of a free module over a principal ideal domain cannot be extended to a basis for the entire module. For example, the set is a basis for the submodule of the -module , but this set cannot be extended to a basis for itself. We state without proof the following result along these lines. Theorem 6.7 Let be a free -module of rank , where is a principal ideal domain. Let be a submodule of that is free of rank . Then there is a basis for that contains a subset for which is a basis for , for some nonzero elements of . Torsion-Free and Free Modules Let us explore the relationship between the concepts of torsion-free and free. It is not hard to see that any free module over an integral domain is torsion-free. The converse does not hold, unless we strengthen the hypotheses by requiring that the module be finitely generated. Theorem 6.8 A finitely generated module over a principal ideal domain is free if and only if it is torsion-free. Proof. We leave proof that a free module over an integral domain is torsion-free to the reader. Let be a generating set for . Consider first the case , whence . Then is a basis for since singleton sets are linearly independent in a torsion-free module. Hence, is free. Now suppose that is a generating set with . If is linearly independent, we are done. If not, then there exist nonzero for which . It follows that and so is a submodule of a free module and is therefore free by Theorem 6.5. But the map defined by is an isomorphism because is torsion-free. Thus is also free. Now we can do the general case. Write where is a maximal linearly independent subset of . Note ( that is nonempty because singleton sets are linearly independent. ) For each , the set is linearly dependent and so there exist and for which 146 Advanced Linear Algebra If , then and since the latter is a free module, so is , and therefore so is . The Primary Cyclic Decomposition Theorem The first step in the decomposition of a finitely generated module over a principal ideal domain is an easy one. Theorem 6.9 Any finitely generated module over a principal ideal domain is the direct sum of a finitely generated free -module and a finitely generated torsion -module free tor The torsion part is unique, since it must be the set of all torsion elements oftor , whereas the free part is unique only up to isomorphism, that is, thefree rank of the free part is unique. Proof. It is easy to see that the set of all torsion elements is a submodule oftor and the quotient is torsion-free. Moreover, since is finitelytor generated, so is . Hence, Theorem 6.8 implies that is free. tor tor Hence, Theorem 5.6 implies that tor where is free. tor As to the uniqueness of the torsion part, suppose that where is torsion and is free. Then . But if for and tor tor , then and so and . Thus, .tor tor For the free part, since , the submodules and tor tor are both complements of and hence are isomorphic.tor Note that if is a basis for we can write free tor where each cyclic submodule has zero annihilator. This is a partial decomposition of into a direct sum of cyclic submodules. The Primary Decomposition In view of Theorem 6.9, we turn our attention to the decomposition of finitely generated torsion modules over a principal ideal domain. The first step is to decompose into a direct sum of submodules, defined as follows. primary Modules Over a Principal Ideal Domain 147 Definition Let be a prime in . A - or just module is a primary primary() module whose order is a power of . Theorem 6.10 The primary decomposition theorem() Let be a torsion module over a principal ideal domain , with order where the 's are distinct nonassociate primes in . 1 is the direct sum) where is a primary submodule of order . This decomposition of into primary submodules is called the of .primary decomposition 2 The primary decomposition of is unique up to order of the summands.) That is, if where is primary of order and are distinct nonassociate primes, then and, after a possible reindexing, . Hence, and , for . 3 Two -modules and are isomorphic if and only if the summands in) their primary decompositions are pairwise isomorphic, that is, if and are primary decompositions, then and, after a possible reindexing, for . Proof. Let us write and show first that Since , we have . On the other hand, since and are relatively prime, there exist for which and so if then 148 Advanced Linear Algebra Hence . For part 1 , since , there exist scalars for which)gcd and so for any , Moreover, since the and the 's are pairwise relatively prime, it follows that the sum of the submodules is direct, that is, As to the annihilators, it is clear that . For the reverse ann inclusion, if , then and so , that is, ann ann and so . Thus .ann As to uniqueness, we claim that is an order of . It is clear that annihilates and so . On the other hand, contains an element of order and so the sum has order , which implies that . Hence, and are associates. Unique factorization in now implies that and, after a suitable reindexing, that and and are associates. Hence, is primary of order . For convenience, we can write as . Hence, But if and for all , we must have for all . For part 3), if and , then the map defined by is an isomorphism and so . Conversely, suppose that . Then and have the same annihilators and therefore the same order Hence, part 1) and part 2) imply that and after a suitable reindexing, Modules Over a Principal Ideal Domain 149 . Moreover, since it follows that . The Cyclic Decomposition of a Primary Module The next step in the decomposition process is to show that a primary module can be decomposed into a direct sum of cyclic submodules. While this decomposition is not unique see the exercises , the set of annihilators is unique,() as we will see. To establish this uniqueness, we use the following result. Lemma 6.11 Let be a module over a principal ideal domain and let be a prime. 1 If , then is a vector space over the field with scalar) multiplication defined by for all . 2 For any submodule of the set) is also a submodule of and if , then Proof. For part 1 , since is prime, the ideal is maximal and so is a) field. We leave the proof that is a vector space over to the reader. For part 2 , it is straightforward to show that is a submodule of . Since) and we see that . Also, if , then . But for some and and so . Since and we deduce that , whence . Thus, . But the reverse inequality is manifest. Theorem 6.12 The cyclic decomposition theorem of a primary module() Let be a primary finitely generated torsion module over a principal ideal domain , with order . 1 is a direct sum) ()6.1 of cyclic submodules with annihilators , which can beann arranged in ascending order ann ann 150 Advanced Linear Algebra or equivalently, 2 As to uniqueness, suppose that is also the direct sum) of cyclic submodules with annihilators , arranged inann ascending order ann ann or equivalently Then the two chains of annihilators are identical, that is, and ann ann for all . Thus, and for all . 3 Two -primary -modules) and are isomorphic if and only if they have the same annihilator chains, that is, if and only if and, after a possible reindexing, ann ann Proof. Let have order equal to the order of , that is, ann ann Such an element must exist since for all and if this inequality is strict, then will annihilate . If we show that is complemented, that is, for some submodule , then since is also a finitely generated primary torsion module over , we can repeat the process to get where . We can continue this decomposition:ann as long as . But the ascending sequence of submodules Modules Over a Principal Ideal Domain 151 must terminate since is Noetherian and so there is an integer for which eventually , giving 6.1 . () Let . The direct sum clearly exists. Suppose that the direct sum exists. We claim that if , then it is possible to find a submodule for which and for which the direct sum also exists. This process must also stop after a finite number of steps, giving as desired. If and let for . Then since . We wish to show that for some , the direct sum exists, that is, Now, there exist scalars and for which for and so if we find a scalar for which (6.2) then implies that and the proof of existence will be complete. Solving for gives so let us consider the ideal of all such scalars: Since and is principal, we have for some . Also, since implies that . 152 Advanced Linear Algebra Since , we have and there exist and for which Hence, Now we need more information about . Multiplying the expression for by gives and since , it follows that . Hence, , that is, and so for some . Now we can write and so Thus, we take to get (6.2) and that completes the proof of existence. For uniqueness, note first that has orders and and so and are associates and . Next we show that . According to part 2 of ) Lemma 6.10, and where all summands are nonzero. Since , it follows from Lemma 6.10 that is a vector space over and so each of the preceding decompositions expresses as a direct sum of one-dimensional vector subspaces. Hence, . dim Finally, we show that the exponents and are equal using induction on . If , then for all and since , we also have for all . Suppose the result is true whenever and let . Write and Modules Over a Principal Ideal Domain 153 Then and But is a cyclic submodule of with annihilator and so by the induction hypothesis and which concludes the proof of uniqueness. For part 3), suppose that and has annihilator chain ann ann and has annihilator chain ann ann Then and so and after a suitable reindexing, ann ann ann Conversely, suppose that and have the same annihilator chains, that is, and ann ann Then ann ann The Primary Cyclic Decomposition Now we can combine the various decompositions. Theorem 6.13 The primary cyclic decomposition theorem() Let be a finitely generated torsion module over a principal ideal domain . 154 Advanced Linear Algebra 1 If has order) where the 's are distinct nonassociate primes in , then can be uniquely decomposed up to the order of the summands into the direct sum() where is a primary submodule with annihilator . Finally, each primary submodule can be written as a direct sum of cyclic submodules, so that where and the terms in each cyclic decomposition canann be arranged so that, for each , ann ann or, equivalently, 2 As for uniqueness, suppose that) is also a primary cyclic decomposition of . Then, a The number of summands is the same in both decompositions; in fact,) and after possible reindexing, for all . b The primary submodules are the same; that is, after possible) reindexing, and c For each primary submodule pair , the cyclic submodules) have the same annihilator chains; that is, after possible reindexing, ann ann for all . In summary, the primary submodules and annihilator chains are uniquely determined by the module . 3 Two -modules and are isomorphic if and only if they have the same) annihilator chains. Modules Over a Principal Ideal Domain 155 Elementary Divisors Since the chain of annihilators ann is unique except for order, the multiset of generators is uniquely determined up to associate. The generators are called the elementary divisors of . Note that for each prime , the elementary divisor of largest exponent is precisely the factor of associated to . Let us write to denote the multiset of elementary divisors ofElemDiv all . Thus, if , then any associate of is also in .ElemDiv ElemDiv We can now say that is a complete invariant for isomorphism.ElemDiv Technically, the function is the complete invariant, but this ElemDiv hair is not worth splitting. Also, we could work with a system of distinct representatives for the associate classes of the elementary divisors, but in general, there is no way to single out a special representative. Theorem 6.14 Let be a principal ideal domain. The multiset is ElemDiv a complete invariant for isomorphism of finitely generated torsion -modules, that is, ElemDiv ElemDiv We have seen (Theorem 6.2) that if then lcm Let us now compare the elementary divisors of to those of and . Theorem 6.15 Let be a finitely generated torsion module over a principal ideal domain and suppose that 1 The primary cyclic decomposition of is the direct sum of the primary) cyclic decompositons of and ; that is, if and are the primary cyclic decompositions of and , respectively, then M is the primary cyclic decomposition of M. 156 Advanced Linear Algebra 2 The elementary divisors of are ) ElemDiv ElemDiv ElemDiv where the union is a multiset union; that is, we keep all duplicate members. The Invariant Factor Decomposition According to Theorem 6.4, if and are cyclic submodules with relatively prime orders, then is a cyclic submodule whose order is the product of the orders of and . Accordingly, in the primary cyclic decomposition of , with elementary divisors satisfying ()6.3 we can combine cyclic summands with relatively prime orders. One judicious way to do this is to take the leftmost highest-order cyclic submodules from() each group to get and repeat the process Of course, some summands may be missing here since different primary modules do not necessarily have the same number of summands. In any case, the result of this regrouping and combining is a decomposition of the form which is called an of .invariant factor decomposition For example, suppose that Then the resulting regrouping and combining gives As to the orders of the summands, referring to 6.3 , if has order , then() since the highest powers of each prime are taken for , the second–highest for and so on, we conclude that Modules Over a Principal Ideal Domain 157 ()6.4 or equivalently, ann ann The numbers are called of the decomposition. invariant factors For instance, in the example above suppose that the elementary divisors are Then the invariant factors are The process described above that passes from a sequence of elementary divisors in order 6.3 to a sequence of invariant factors in order 6.4 is() () reversible. The inverse process takes a sequence satisfying 6.4 , () factors each into a product of distinct nonassociate prime powers with the primes in the same order and then “peels off” like prime powers from the left. ()The reader may wish to try it on the example above. This fact, together with Theorem 6.4, implies that primary cyclic decompositions and invariant factor decompositions are essentially equivalent. Therefore, since the multiset of elementary divisors of is unique up to associate, the multiset of invariant factors of is also unique up to associate. Furthermore, the multiset of invariant factors is a complete invariant for isomorphism. Theorem 6.16 The invariant factor decomposition theorem() Let be a finitely generated torsion module over a principal ideal domain . Then where D is a cyclic submodule of , with order , where This decomposition is called an of and theinvariant factor decomposition scalars are called the of . invariant factors 1 The multiset of invariant factors is uniquely determined up to associate by) the module . 2 The multiset of invariant factors is a complete invariant for isomorphism.) The annihilators of an invariant factor decomposition are called the invariant ideals of . The chain of invariant ideals is unique, as is the chain of 158 Advanced Linear Algebra annihilators in the primary cyclic decomposition. Note that is an order of , that is, ann Note also that the product of the invariant factors of has some nice properties. For example, is the product of all the elementary divisors of . We will see in a later chapter that in the context of a linear operator on a vector space, is the characteristic polynomial of . Characterizing Cyclic Modules The primary cyclic decomposition can be used to characterize cyclic modules via their elementary divisors. Theorem 6.17 Let be a finitely generated torsion module over a principal ideal domain, with order The following are equivalent: 1 is cyclic.) 2 is the direct sum) of primary cyclic submodules of order . 3 The elementary divisors of are precisely the prime power factors of :) ElemDiv Proof. Suppose that is cyclic. Then the primary decomposition of is a primary decomposition, since any submodule of a cyclic module is cyclic.cyclic Hence, 1) implies 2). Conversely, if 2) holds, then since the orders are relatively prime, Theorem 6.4 implies that is cyclic. We leave the rest of the proof to the reader. Indecomposable Modules The primary cyclic decomposition of is a decomposition of into a direct sum of submodules that cannot be further decomposed. In fact, this characterizes the primary cyclic decomposition of . Before justifying these statements, we make the following definition. Definition A module is if it cannot be written as a direct indecomposable sum of proper submodules. Modules Over a Principal Ideal Domain 159 We leave proof of the following as an exercise. Theorem 6.18 Let be a finitely generated torsion module over a principal ideal domain. The following are equivalent: 1 is indecomposable) 2 is primary cyclic) 3 has only one elementary divisor:) ElemDiv Thus, the primary cyclic decomposition of is a decomposition of into a direct sum of indecomposable modules. Conversely, if is a decomposition of into a direct sum of indecomposable submodules, then each submodule is primary cyclic and so this is the primary cyclic decomposition of . Indecomposable Submodules of Prime Order Readers acquainted with group theory know that any group of prime order is cyclic. However, as mentioned earlier, the order of a module corresponds to the smallest exponent of a group, not to the order of a group. Indeed, there are modules of prime order that are not cyclic. Nevertheless, cyclic modules of prime order are important. Indeed, if is a finitely generated torsion module over a principal ideal domain, with order , then each prime factor of gives rise to a cyclic submodule of whose order is and so is also indecomposable. Unfortunately, need not be complemented and so we cannot use it to decompose . Nevertheless, the theorem is still useful, as we will see in a later chapter. Theorem 6.19 Let be a finitely generated torsion module over a principal ideal domain, with order . If is a prime divisor of , then has a cyclic ()equivalently, indecomposable submodule of prime order . Proof. If , then there is a for which but . Then is annihilated by and so . But is prime and and so . Since has prime order, Theorem 6.18 implies that is cyclic if and only if it is indecomposable. Exercises 1. Show that any free module over an integral domain is torsion-free. 2. Let be a finitely generated torsion module over a principal ideal domain. Prove that the following are equivalent: a) is indecomposable b) has only one elementary divisor (including multiplicity) 160 Advanced Linear Algebra c) is cyclic of prime power order. 3. Let be a principal ideal domain and the field of quotients. Then is an -module. Prove that any nonzero finitely generated submodule of is a free module of rank . 4. Let be a principal ideal domain. Let be a finitely generated torsion- free -module. Suppose that is a submodule of for which is a free -module of rank and is a torsion module. Prove that is a free -module of rank . 5. Show that the primary cyclic decomposition of a torsion module over a principal ideal domain is not unique even though the elementary divisors( are .) 6. Show that if is a finitely generated -module where is a principal ideal domain, then the free summand in the decomposition tor need not be unique. 7. If is a cyclic -module of order show that the map defined by is a surjective -homomorphism with kernel and so 8. If is an integral domain with the property that all submodules of cyclic -modules are cyclic, show that is a principal ideal domain. 9. Suppose that is a finite field and let be the set of all nonzero elements of . a Show that if is a nonconstant polynomial over and if) is a root of , then is a factor of . b Prove that a nonconstant polynomial of degree can have) at most distinct roots in . c Use the invariant factor or primary cyclic decomposition of a finite -) module to prove that is cyclic. 10. Let be a principal ideal domain. Let be a cyclic -module with order . We have seen that any submodule of is cyclic. Prove that for each such that there is a unique submodule of of order . 11. Suppose that is a free module of finite rank over a principal ideal domain . Let be a submodule of . If is torsion, prove that rk rk . 12. Let be the ring of polynomials over a field and let be the ring of all polynomials in that have coefficient of equal to . Then is an -module. Show that is finitely generated and torsion-free but not free. Is a principal ideal domain? 13. Show that the rational numbers form a torsion-free -module that is not free. More on Complemented Submodules 14. Let be a principal ideal domain and let be a free -module. Modules Over a Principal Ideal Domain 161 a Prove that a submodule of is complemented if and only if ) is free. b If is also finitely generated, prove that is complemented if and) only if is torsion-free. 15. Let be a free module of finite rank over a principal ideal domain . a Prove that if is a complemented submodule of , then) rk rk if and only if . b Show that this need not hold if is not complemented.) c Prove that is complemented if and only if any basis for can be) extended to a basis for . 16. Let and be free modules of finite rank over a principal ideal domain . Let be an -homomorphism. a Prove that is complemented.)ker b What about ?)im c Prove that) rk rk rk im rk rk ker ker ker d If is surjective, then is an isomorphism if and only if) rk rk . e If is a submodule of and if is free, then) rk rk rk 17. A submodule of a module is said to be if whenever pure in , then for all nonzero . a Show that is pure if and only if and for implies) . b Show that is pure if and only if is torsion-free.) c If is a principal ideal domain and is finitely generated, prove that) is pure if and only if is free. d If and are pure submodules of , then so are and .) What about ? e If is pure in , then show that is pure in for any) submodule of . 18. Let be a free module of finite rank over a principal ideal domain . Let and be submodules of with complemented in . Prove that rk rk rk rk Chapter 7 The Structure of a Linear Operator In this chapter, we study the structure of a linear operator on a finite- dimensional vector space, using the powerful module decomposition theorems of the previous chapter. Unless otherwise noted, all vector spaces will be assumed to be finite-dimensional. Let be a finite-dimensional vector space. Let us recall two earler theorems (Theorem 2.19 and Theorem 2.20). Theorem 7.1 Let be a vector space of dimension . 1 Two matrices and are similar written if and only if)( ) they represent the same linear operator , but possibly with respect to different ordered bases. In this case, the matrices and represent exactly the same set of linear operators in . 2 Then two linear operators and on are similar written if and)( ) only if there is a matrix that represents both operators, but with respect to possibly different ordered bases. In this case, and are represented by exactly the same set of matrices in . Theorem 7.1 implies that the matrices that represent a given linear operator are precisely the matrices that lie in one similarity class. Hence, in order to uniquely represent all linear operators on , we would like to find a set consisting of one simple representative of each similarity class, that is, a set of simple canonical forms for similarity. One of the simplest types of matrix is the diagonal matrix. However, these are too simple, since some operators cannot be represented by a diagonal matrix. A less simple type of matrix is the upper triangular matrix. However, these are not simple enough: Every operator (over an algebraically closed field) can be represented by an upper triangular matrix but some operators can be represented by more than one upper triangular matrix. 164 Advanced Linear Algebra This gives rise to two different directions for further study. First, we can search for a characterization of those linear operators that can be represented by diagonal matrices. Such operators are called . Second, we candiagonalizable search for a different type of “simple” matrix that does provide a set of canonical forms for similarity. We will pursue both of these directions. The Module Associated with a Linear Operator If , we will think of not only as a vector space over a field but also as a module over , with scalar multiplication defined by We will write to indicate the dependence on . Thus, and are modules with the same ring of scalars , although with different scalar multiplication if . Our plan is to interpret the concepts of the previous chapter for the module . First, if , then . This implies that is a torsiondim dim module. In fact, the vectors are linearly dependent in , which implies that for some nonzero polynomial . Hence, and so is a nonzero ann ann principal ideal of . Also, since is finitely generated as a vector space, it is, a fortiori, finitely generated as an -module. Thus, is a finitely generated torsion module over a principal ideal domain and so we may apply the decomposition theorems of the previous chapter. In the first part of this chapter, we embark on a “translation project” to translate the powerful results of the previous chapter into the language of the modules . Let us first characterize when two modules and are isomorphic. Theorem 7.2 If , then In particular, is a module isomorphism if and only if is a vector space automorphism of satisfying Proof. Suppose that is a module isomorphism. Then for , which is equivalent to The Structure of a Linear Operator 165 and since is bijective, this is equivalent to that is, . Since a module isomorphism from to is a vector space isomorphism as well, the result follows. For the converse, suppose that is a vector space automorphism of and , that is, . Then and the -linearity of implies that for any polynomial , Hence, is a module isomorphism from to . Submodules and Invariant Subspaces There is a simple connection between the submodules of the -module and the subspaces of the vector space . Recall that a subspace of is - invariant if . Theorem 7.3 A subset is a submodule of if and only if is a - invariant subspace of . Orders and the Minimal Polynomial We have seen that the annihilator of , ann is a nonzero principal ideal of , say ann Since the elements of the base ring of are polynomials, for the first time in our study of modules there is a logical choice among all scalars in a given associate class: Each associate class contains exactly one polynomial.monic Definition Let . The unique monic order of is called the minimal polynomial for and is denoted by or . Thus, min ann In treatments of linear algebra that do not emphasize the role of the module , the minimal polynomial of a linear operator is simply defined as the unique 166 Advanced Linear Algebra monic smallest degree polynomial of for which . This definition is equivalent to our definition. The concept of minimal polynomial is also defined for matrices. The minimal polynomial of matrix is defined as the minimal polynomial A of the multiplication operator . Equivalently, is the unique monic polynomial of smallest degree for which . Theorem 7.4 1 If are similar linear operators on , then . Thus, the) minimal polynomial is an invariant under similarity of operators. 2 If are similar matrices, then . Thus, the minimal) polynomial is an invariant under similarity of matrices. 3 The minimal polynomial of is the same as the minimal) polynomial of any matrix that represents . Cyclic Submodules and Cyclic Subspaces Let us now look at the cyclic submodules of : which are -invariant subspaces of . Let be the minimal polynomial of and suppose that . If , then writingdeg where givesdeg deg and so deg Hence, the set spans the . To see that is a basis for , note that any linearvector space combination of the vectors in has the form for and so is deg equal to if and only if . Thus, is an ordered basis for . Definition Let . A -invariant subspace of is - if has a cyclic basis of the form for some and . The basis is called a - for . cyclic basis The Structure of a Linear Operator 167 Thus, a cyclic submodule of with order of degree is a -cyclic subspace of of dimension . The converse is also true, for if is a basis for a -invariant subspace of , then is a submodule of . Moreover, the minimal polynomial of has degree , since if then satisfies the polynomial but none of smaller degree since is linearly independent. Theorem 7.5 Let be a finite-dimenional vector space and let . The following are equivalent: 1 is a cyclic submodule of with order of degree ) 2 is a -cyclic subspace of of dimension .) We will have more to say about cyclic modules a bit later in the chapter. Summary The following table summarizes the connection between the module concepts and the vector space concepts that we have discussed so far. - - Scalar multiplication: Action of : Submodule of -Invariant subspace of Annihil Module Vector Space ator: Annihilator: Monic order of : Minimal polynomial of : ann ann ann has smallest deg with Cyclic submodule of : -cyclic subspace of : deg deg d eg The Primary Cyclic Decomposition of We are now ready to translate the cyclic decomposition theorem into the language of . Definition Let . 1 The and of are the ) elementary divisors invariant factors monic elementary divisors and invariant factors, respectively, of the module . We denote the multiset of elementary divisors of by and theElemDiv multiset of invariant factors of by .InvFact 168 Advanced Linear Algebra 2 The and of a matrix are the) elementary divisors invariant factors elementary divisors and invariant factors, respectively, of the multiplication operator : ElemDiv ElemDiv InvFact InvFact and We emphasize that the elementary divisors and invariant factors of an operator or matrix are by definition. Thus, we no longer need to worry aboutmonic uniqueness up to associate. Theorem 7.6 Let be(The primary cyclic decomposition theorem for finite-dimensional and let have minimal polynomial where the polynomials are distinct monic primes. 1 The -module is the direct sum)( )Primary decomposition where is a primary submodule of of order . In vector space terms, is a -invariant subspace of and the minimal polynomial of is min 2 Each primary summand can be decomposed)( )Cyclic decomposition into a direct sum of -cyclic submodules of order with In vector space terms, is a -cyclic subspace of and the minimal polynomial of is min 3 This yields the decomposition of into a)( )The complete decomposition direct sum of -cyclic subspaces 4 The multiset of elementary divisors)( )Elementary divisors and dimensions is uniquely determined by . If , then the -deg The Structure of a Linear Operator 169 cyclic subspace has -cyclic basis and . Hence,dim deg dim deg We will call the basis for the for .elementary divisor basis Recall that if and if both and are -invariant subspaces of , the pair is said to . In module language, the pair reduces reduce if and are submodules of and We can now translate Theorem 6.15 into the current context. Theorem 7.7 Let and let 1 The minimal polynomial of is) lcm 2 The primary cyclic decomposition of is the direct sum of the primary) cyclic decompositons of and ; that is, if and are the primary cyclic decompositions of and , respectively, then is the primary cyclic decomposition of . 3 The elementary divisors of are ) ElemDiv ElemDiv ElemDiv where the union is a multiset union; that is, we keep all duplicate members. 170 Advanced Linear Algebra The Characteristic Polynomial To continue our translation project, we need a definition. Recall that in the characterization of cyclic modules in Theorem 6.17, we made reference to the product of the elementary divisors, one from each associate class. Now that we have singled out a special representative from each associate class, we can make a useful definition. Definition Let . The of is the characteristic polynomial product of all of the elementary divisors of : Hence, deg dim Similarly, the of a matrix is the product ofcharacteristic polynomial the elementary divisors of . The following theorem describes the relationship between the minimal and characteristic polynomials. Theorem 7.8 Let . 1 The minimal polynomial of divides the)( )The Cayley–Hamilton theorem characteristic polynomial of : Equivalently, satisfies its own characteristic polynomial, that is, 2 The minimal polynomial) and characteristic polynomial of have the same set of prime factors and hence the same set of roots not counting multiplicity .() We have seen that the multiset of elementary divisors forms a complete invariant for similarity. The reader should construct an example to show that the pair is a complete invariant for similarity, that is, this pair of not The Structure of a Linear Operator 171 polynomials does not uniquely determine the multiset of elementary divisors of the operator . In general, the minimal polynomial of a linear operator is hard to find. One of the virtues of the characteristic polynomial is that it is comparatively easy to find and we will discuss this in detail a bit later in the chapter. Note that since and both polynomials are monic, it follows that deg deg Definition A linear operator is if its minimal nonderogatory polynomial is equal to its characteristic polynomial: or equivalently, if deg deg or if deg dim Similar statements hold for matrices. Cyclic and Indecomposable Modules We have seen (Theorem 6.17) that cyclic submodules can be characterized by their elementary divisors. Let us translate this theorem into the language of (and add one more equivalence related to the characteristic polynomial). Theorem 7.9 Let have minimal polynomial where are distinct monic primes. The following are equivalent: 1 is cyclic.) 2 is the direct sum) of -cyclic submodules of order . 3 The elementary divisors of are) ElemDiv 4 is nonderogatory, that is,) 172 Advanced Linear Algebra Indecomposable Modules We have also seen (Theorem 6.19) that, in the language of , each prime factor of the minimal polynomial gives rise to a cyclic submodule of of prime order . Theorem 7.10 Let and let be a prime factor of . Then has a cyclic submodule of prime order . For a module of prime order, we have the following. Theorem 7.11 For a module of prime order , the following are equivalent: 1 is cyclic) 2 is indecomposable) 3 is irreducible) 4 is nonderogatory, that is, ) 5 .)dim deg Our translation project is now complete and we can begin to look at issues that are specific to the modules . Companion Matrices We can also characterize the cyclic modules via the matrix representations of the operator , which is obviously something that we could not do for arbitrary modules. Let be a cyclic module, with order and ordered -cyclic basis Then for and and so The Structure of a Linear Operator 173 2 This matrix is known as the for the polynomial .companion matrix Definition The of a monic polyomialcompanion matrix is the matrix 2 Note that companion matrices are defined only for polynomials.monic Companion matrices are nonderogatory. Also, companion matrices are precisely the matrices that represent operators on -cyclic subspaces. Theorem 7.12 Let . 1 A companion matrix is nonderogatory; in fact,) 2 is cyclic if and only if can be represented by a companion matrix, in) which case the representing basis is -cyclic. Proof. For part 1), let be the standard basis for . Since for , it follows that for any polynomial , for all If , then and so , whence . Also, if is nonzero and has degree , then 174 Advanced Linear Algebra since is linearly independent. Hence, has smallest degree among all polynomials satisfied by and so . Finally, deg deg deg For part 2), we have already proved that if is cyclic with -cyclic basis , then . For the converse, if , then part 1) implies that is nonderogatory. Hence, Theorem 7.11 implies that is cyclic. It is clear from the form of that is a -cyclic basis for . The Big Picture If , then Theorem 7.2 and the fact that the elementary divisors form a complete invariant for isomorphism imply that ElemDiv ElemDiv Hence, the multiset of elementary divisors is a complete invariant for similarity of operators. Of course, the same is true for matrices: ElemDiv ElemDiv where we write in place of . The connection between the elementary divisors of an operator and the elementary divisors of the matrix representations of is described as follows. If , then the coordinate map is also a isomorphism module . Specifically, we have and so preserves -scalar multiplication. Hence, for some For the converse, suppose that . If we define by , where is the th standard basis vector, then is an ordered basis for and is the coordinate map for . Hence, is a module isomorphism and so for all , that is, which shows that . Theorem 7.13 Let be a finite-dimensional vector space over . Let and let . The Structure of a Linear Operator 175 1 The multiset of elementary divisors or invariant factors is a complete)( ) invariant for similarity of operators, that is, ElemDiv ElemDiv InvFact InvFact A similar statement holds for matrices: ElemDiv ElemDiv InvFact InvFact 2 The connection between operators and their representing matrices is ) for some ElemDiv ElemDiv InvFact InvFact Theorem 7.13 can be summarized in Figure 7.1, which shows the big picture. similarity classes of L(V) V isomorphism classes of F[x]-modules V {ED1} Multisets of elementary divisors{ED2} []B []B []R []R Similarity classes of matrices Figure 7.1 Figure 7.1 shows that the similarity classes of are in one-to-one correspondence with the isomorphism classes of -modules and that these are in one-to-one correspondence with the multisets of elementary divisors, which, in turn, are in one-to-one correspondence with the similarity classes of matrices. We will see shortly that any multiset of prime power polynomials is the multiset of elementary divisors for some operator (or matrix) and so the third family in 176 Advanced Linear Algebra the figure could be replaced by the family of all multisets of prime power polynomials. The Rational Canonical Form We are now ready to determine a set of canonical forms for similarity. Let . The elementary divisor basis for that gives the primary cyclic decomposition of , is the union of the bases and so the matrix of with respect to is the block diagonal matrix diag with companion matrices on the block diagonal. This matrix has the following form. Definition A matrix is in the of elementary divisor form rational canonical form if diag where the are monic prime polynomials. Thus, as shown in Figure 7.1, each similarity class contains at least one matrix in the elementary divisor form of rational canonical form. On the other hand, suppose that is a rational canonical matrix diag of size . Then represents the matrix multiplication operator under the standard basis on . The basis can be partitioned into blocks corresponding to the position of each of the companion matrices on the block diagonal of . Since it follows from Theorem 7.12 that each subspace is -cyclic with monic order and so Theorem 7.9 implies that the multiset of elementary divisors of is . This shows two important things. First, any multiset of prime power polynomials is the multiset of elementary divisors for some matrix. Second, The Structure of a Linear Operator 177 lies in the similarity class that is associated with the elementary divisors . Hence, two matrices in the elementary divisor form of rational canonical form lie in the same similarity class if and only if they have the same multiset of elementary divisors. In other words, the elementary divisor form of rational canonical form is a set of canonical forms for similarity, up to order of blocks on the block diagonal. Theorem 7.14 Let()The rational canonical form: elementary divisor version be a finite-dimensional vector space and let have minimal polynomial where the 's are distinct monic prime polynomials. 1 If is an elementary divisor basis for , then is in the elementary) divisor form of rational canonical form: diag where are the elementary divisors of . This block diagonal matrix is called an of a of .elementary divisor version rational canonical form 2 Each similarity class of matrices contains a matrix in the elementary) divisor form of rational canonical form. Moreover, the set of matrices in that have this form is the set of matrices obtained from by reordering the block diagonal matrices. Any such matrix is called an elementary divisor verison rational canonical form of a of . 3 The dimension of is the sum of the degrees of the elementary divisors of) , that is, dim deg Example 7.1 Let be a linear operator on the vector space and suppose that7 has minimal polynomial Noting that and are elementary divisors and that the sum of the degrees of all elementary divisors must equal , we have two possibilities: 1 1 ) 2 1) These correspond to the following rational canonical forms: 178 Advanced Linear Algebra 1) 2) The rational canonical form may be far from the ideal of simplicity that we had in mind for a set of simple canonical forms. Indeed, the rational canonical form can be important as a theoretical tool, more so than a practical one. The Invariant Factor Version There is also an invariant factor version of the rational canonical form. We begin with the following simple result. Theorem 7.15 If are relatively prime polynomials, then block Proof. Speaking in general terms, if an matrix has minimal polynomial of degree equal to the size of the matrix, then Theorem 7.14 implies that the elementary divisors of are precisely Since the matrices and have the same size diag and the same minimal polynomial of degree , it follows that they have the same multiset of elementary divisors and so are similar. Definition A matrix is in the of invariant factor form rational canonical form if The Structure of a Linear Operator 179 diag where for . Theorem 7.15 can be used to rearrange and combine the companion matrices in an elementary divisor version of a rational canonical form to produce an invariant factor version of rational canonical form that is similar to . Also, this process is reversible. Theorem 7.16 The rational canonical form: invariant factor version () Let dim and suppose that has minimal polynomial where the monic polynomials are distinct prime irreducible polynomials () 1 has an , that is, a basis for which) invariant factor basis diag where the polynomials are the invariant factors of and . This block diagonal matrix is called an invariant factor version rational canonical form of a of . 2 Each similarity class of matrices contains a matrix in the invariant) factor form of rational canonical form. Moreover, the set of matrices in that have this form is the set of matrices obtained from by reordering the block diagonal matrices. Any such matrix is called an invariant factor verison rational canonical form of a of . 3 The dimension of is the sum of the degrees of the invariant factors of ,) that is, dim deg The Determinant Form of the Characteristic Polynomial In general, the minimal polynomial of an operator is hard to find. One of the virtues of the characteristic polynomial is that it is comparatively easy to find. This also provides a nice example of the theoretical value of the rational canonical form. Let us first take the case of a companion matrix. If is the companion matrix of a monic polynomial then how can we recover from by arithmetic operations? 180 Advanced Linear Algebra When , we can write as which looks suspiciously like a determinant: det det det So, let us define where is an independent variable. The determinant of this matrix is a polynomial in whose degree equals the number of parameters . We have just seen that det and this is also true for . As a basis for induction, if det then expanding along the first row gives det det det det We have proved the following. The Structure of a Linear Operator 181 Lemma 7.17 For any , det Now suppose that is a matrix in the elementary divisor form of rational canonical form. Since the determinant of a block diagonal matrix is the product of the determinants of the blocks on the diagonal, it follows that det Moreover, if , say , then det det det det det det det and so det det Hence, the fact that all matrices have a rational canonical form allows us to deduce the following theorem. Theorem 7.18 Let . If is any matrix that represents , then det Changing the Base Field A change in the base field will generally change the primeness of polynomials and therefore has an effect on the multiset of elementary divisors. It is perhaps a surprising fact that a change of base field has on the invariant factors—no effect hence the adjective .invariant Theorem 7.19 Let and be fields with . Suppose that the elementary divisors of a matrix are Suppose also that the polynomials can be further factored over , say where is prime over . Then the prime powers are the elementary divisors of over . 182 Advanced Linear Algebra Proof. Consider the companion matrix in the rational canonical form of over . This is a matrix over as well and Theorem 7.15 implies that diag Hence, is an elementary divisor basis for over . As mentioned, unlike the elementary divisors, the invariant factors are field independent. This is equivalent to saying that the invariant factors of a matrix are polynomials over the subfield of that contains thesmallest entries of Theorem 7.20 Let and let be the smallest subfield of that contains the entries of . 1 The invariant factors of are polynomials over .) 2 Two matrices are similar over if and only if they are) similar over . Proof. Part 1 follows immediately from Theorem 7.19, since using either or) to compute invariant factors gives the same result. Part 2) follows from the fact that two matrices are similar over a given field if and only if they have the same multiset of over that field.invariant factors Example 7.2 Over the real field, the matrix is the companion matrix for the polynomial , and so ElemDiv InvFact However, as a complex matrix, the rational canonical form for is and so ElemDiv InvFact and Exercises 1. We have seen that any can be used to make into an - module. Does every module over come from some ? Explain. 2. Let have minimal polynomial The Structure of a Linear Operator 183 where are distinct monic primes. Prove that the following are equivalent: a is -cyclic.) b .)deg dim c The elementary divisors of are the prime power factors and so) is a direct sum of -cyclic submodules of order . 3. Prove that a matrix is nonderogatory if and only if it is similar to a companion matrix. 4. Show that if and are block diagonal matrices with the same blocks, but in possibly different order, then and are similar. 5. Let . Justify the statement that the entries of any invariant factor version of a rational canonical form for are “rational” expressions in the coefficients of , hence the origin of the term rational canonical form. Is the same true for the elementary divisor version? 6. Let where is finite-dimensional. If is irreducible and if is not one-to-one, prove that divides the minimal polynomial of . 7. Prove that the minimal polynomial of is the least common multiple of its elementary divisors. 8. Let where is finite-dimensional. Describe conditions on the minimal polynomial of that are equivalent to the fact that the elementary divisor version of the rational canonical form of is diagonal. What can you say about the elementary divisors? 9. Verify the statement that the multiset of elementary divisors or invariant( factors is a complete invariant for similarity of matrices.) 10. Prove that given any multiset of monic prime power polynomials and given any vector space of dimension equal to the sum of the degrees of these polynomials, there is an operator whose multiset of elementary divisors is . 11. Find all rational canonical forms up to the order of the blocks on the diagonal for a linear operator on having minimal polynomial) 6 11 . 12. How many possible rational canonical forms up to order of blocks are() there for linear operators on with minimal polynomial 1 1 ?6 13. a Show that if and are matrices, at least one of which is) invertible, then and are similar. 184 Advanced Linear Algebra b What do the matrices) and have to do with this issue? c Show that even without the assumption on invertibility the matrices) and have the same characteristic polynomial. : WriteHint where and are invertible and is an matrix that has the identity in the upper left-hand corner and 's elsewhere. Write . Compute and and find their characteristic polynomials. 14. Let be a linear operator on with minimal polynomial 1 2 . Find the rational canonical form for if , or . 15. Suppose that the minimal polynomial of is irreducible. What can you say about the dimension of ? 16. Let where is finite-dimensional. Suppose that is an irreducible factor of the minimal polynomial of . Suppose further that have the property that . Prove that for some polyjomial if and only if for some polynomial . Chapter 8 Eigenvalues and Eigenvectors Unless otherwise noted, we will assume throughout this chapter that all vector spaces are finite-dimensional. Eigenvalues and Eigenvectors We have seen that for any , the minimal and characteristic polynomials have the same set of roots (but not generally the same ofmultiset roots). These roots are of vital importance. Let be a matrix that represents . A scalar is a root of the characteristic polynomial if and only if det det ()8.1 that is, if and only if the matrix is singular. In particular, if , dim then 8.1 holds if and only if there exists a nonzero vector for which() or equivalently, If , then this is equivalent to or in operator language, This prompts the following definition. Definition Let be a vector space over a field and let . 1 A scalar is an or of if there)( ) eigenvalue characteristic value exists a vector for whichnonzero 186 Advanced Linear Algebra In this case, is called an or of eigenvector characteristic vector() associated with . 2 A scalar is an for a matrix if there exists a ) eigenvalue nonzero column vector for which In this case, is called an or for eigenvector characteristic vector() associated with . 3 The set of all eigenvectors associated with a given eigenvalue , together) with the zero vector, forms a subspace of , called the of and eigenspace denoted by . This applies to both linear operators and matrices. 4 The set of all eigenvalues of an operator or matrix is called the ) spectrum of the operator or matrix. We denote the spectrum of by .Spec Theorem 8.1 Let have minimal polynomial and characteristic polynomial . 1 The spectrum of is the set of all roots of or of , not counting) multiplicity. 2 The eigenvalues of a matrix are invariants under similarity.) 3 The eigenspace of the matrix is the solution space to the homogeneous) system of equations One way to compute the eigenvalues of a linear operator is to first represent by a matrix and then solve the characteristic equation det Unfortunately, it is quite likely that this equation cannot be solved when dim . As a result, the art of approximating the eigenvalues of a matrix is a very important area of applied linear algebra. The following theorem describes the relationship between eigenspaces and eigenvectors of distinct eigenvalues. Theorem 8.2 Suppose that are distinct eigenvalues of a linear operator . 1 Eigenvectors associated with distinct eigenvalues are linearly independent;) that is, if , then the set is linearly independent. 2 The sum is direct; that is, exists.) Proof. For part 1), if is linearly dependent, then by renumbering if necessary, we may assume that among all nontrivial linear combinations of Eigenvalues and Eigenvectors 187 these vectors that equal , the equation ()8.2 has the fewest number of terms. Applying gives ()8.3 Multiplying (8.2) by and subtracting from (8.3) gives But this equation has fewer terms than (8.2) and so all of its coefficients must equal . Since the 's are distinct, for and so as well. This contradiction implies that the 's are linearly independent. The next theorem describes the spectrum of a polynomial in . Theorem 8.3 The Let be a vector space over()spectral mapping theorem an algebraically closed field . Let and let . Then Spec Spec Spec Proof. We leave it as an exercise to show that if is an eigenvalue of , then is an eigenvalue of . Hence, . For the reverseSpec Spec inclusion, let , that is, Spec for . If where , then writing this as a product of (not necessarily distinct) linear factors, we have (The operator is written for convenience.) We can remove factors from the left end of this equation one by one until we arrive at an operator (perhaps the identity) for which but . Then is an eigenvector for with eigenvalue . But since , it follows that Spec Spec Spec. Hence, . The Trace and the Determinant Let be algebraically closed and let have characteristic polynomial 188 Advanced Linear Algebra where are the eigenvalues of . Then det and setting gives det Hence, if is algebraically closed then, , is the constant termup to sign det of and the product of the eigenvalues of , including multiplicity. The of the eigenvalues of a matrix over an algebraically closed field is alsosum an interesting quantity. Like the determinant, this quantity is one of the coefficients of the characteristic polynomial (up to sign) and can also be computed directly from the entries of the matrix, without knowing the eigenvalues explicitly. Definition The of a matrix , denoted by , is the sum oftrace tr the elements on the main diagonal of . Here are the basic propeties of the trace. Proof is left as an exercise. Theorem 8.4 Let . 1 A , for .)tr tr 2 .)tr tr tr 3 .)tr tr 4 . However, may not equal) trtrtr tr tr. 5 The trace is an invariant under similarity.) 6 If is algebraically closed, then is the sum of the eigenvalues of ,)tr including multiplicity, and so tr where . Since the trace is invariant under similarity, we can make the following definition. Definition The of a linear operator is the trace of any matrixtrace that represents . As an aside, the reader who is familar with symmetric polynomials knows that the coefficients of any polynomial Eigenvalues and Eigenvectors 189 are the of the roots:elementary symmetric functions The most important elementary symmetric functions of the eigenvalues are the first and last ones: tr and det Geometric and Algebraic Multiplicities Eigenvalues actually have two forms of multiplicity, as described in the next definition. Definition Let be an eigenvalue of a linear operator . 1 The of is the multiplicity of as a root of the) algebraic multiplicity characteristic polynomial . 2 The of is the dimension of the eigenspace .) geometric multiplicity Theorem 8.5 The geometric multiplicity of an eigenvalue of is less than or equal to its algebraic multiplicity. Proof. We can extend any basis of to a basis for . Since is invariant under , the matrix of with respect to has the block form block where and are matrices of the appropriate sizes and so det det det det ()Here is the dimension of . Hence, the algebraic multiplicity of is at least equal to the the geometric multiplicity of . 190 Advanced Linear Algebra The Jordan Canonical Form One of the virtues of the rational canonical form is that every linear operator on a finite-dimensional vector space has a rational canonical form. However, as mentioned earlier, the rational canonical form may be far from the ideal of simplicity that we had in mind for a set of simple canonical forms and is really more of a theoretical tool than a practical tool. When the minimal polynomial of splits over , there is another set of canoncial forms that is arguably simpler than the set of rational canonical forms. In some sense, the complexity of the rational canonical form comes from the choice of basis for the cyclic submodules . Recall that the -cyclic bases have the form where . With this basis, all of the complexity comes at the end, deg so to speak, when we attempt to express as a linear combination of the basis vectors. However, since has the form any ordered set of the form where will also be a basis for . In particular, when deg splits over , the elementary divisors are and so the set is also a basis for . If we temporarily denote the th basis vector in by , then for , Eigenvalues and Eigenvectors 191 For , a similar computation, using the fact that gives Thus, for this basis, the complexity is more or less spread out evenly, and the matrix of with respect to is the matrix which is called a associated with the scalar . Note that a JordanJordan block block has 's on the main diagonal, 's on the subdiagonal and 's elsewhere. Let us refer to the basis as a for .Jordan basis Theorem 8.6 The Jordan canonical form() Suppose that the minimal polynomial of splits over the base field , that is, where . 1 The matrix of with respect to a Jordan basis is) diag where the polynomials are the elementary divisors of . This block diagonal matrix is said to be in and is calledJordan canonical form the .Jordan canonical form of 2 If is algebraically closed, then up to order of the block diagonal) matrices, the set of matrices in Jordan canonical form constitutes a set of canonical forms for similarity. Proof. For part 2), the companion matrix and corresponding Jordan block are similar: 192 Advanced Linear Algebra since they both represent the same operator on the subspace . It follows that the rational canonical matrix and the Jordan canonical matrix for are similar. Note that the diagonal elements of the Jordan canonical form of are precisely the eigenvalues of , each appearing a number of times equal to its algebraic multiplicity. In general, the rational canonical form does not “expose” the eigenvalues of the matrix, even when these eigenvalues lie in the base field. Triangularizability and Schur's Lemma We have discussed two different canonical forms for similarity: the rational canonical form, which applies in all cases and the Jordan canonical form, which applies only when the base field is algebraically closed. Moreover, there is an annoying sense in which these sets of canoncial forms leave something to be desired: One is too complex and the other does not always exist. Let us now drop the rather strict requirements of canonical forms and look at two classes of matrices that are too large to be canonical forms (the upper triangular matrices and the almost upper triangular matrices) and one class of matrices that is too small to be a canonical form (the diagonal matrices). The upper triangular matrices or lower triangular matrices have some nice() algebraic properties and it is of interest to know when an arbitrary matrix is similar to a triangular matrix. We confine our attention to upper triangular matrices, since there are direct analogs for lower triangular matrices as well. Definition A linear operator is if there is an upper triangularizable ordered basis of for which the matrix is upper triangular, or equivalently, if for all . As we will see next, when the base field is algebraically closed, all operators are upper triangularizable. However, since two distinct upper triangular matrices can be similar, the class of upper triangular matrices is not a canonical form for similarity. Simply put, there are just too many upper triangular matrices. Theorem 8.7 ()Schur's theorem Let be a finite-dimensional vector space over a field . 1 If the characteristic polynomial or minimal polynomial of splits)( ) over , then is upper triangularizable. 2 If is algebraically closed, then all operators are upper triangularizable.) Eigenvalues and Eigenvectors 193 Proof. Part 2) follows from part 1). The proof of part 1) is most easily accomplished by matrix means, namely, we prove that every square matrix whose characteristic polynomial splits over is similar to an upper triangular matrix. If there is nothing to prove, since all matrices are upper triangular. Assume the result is true for and let . Let be an eigenvector associated with the eigenvalue of and extend to an ordered basis for . The matrix of with respect to has the form block for some . Since and are similar, we have det det det Hence, the characteristic polynomial of also splits over and the induction hypothesis implies that there is an invertible matrix for which is upper triangular. Hence, if block then is invertible and is upper triangular. The Real Case When the base field is , an operator is upper triangularizable if and only if its characteristic polynomial splits over . (Why?) We can, however, always achieve a form that is close to triangular by permitting values on the first subdiagonal. Before proceeding, let us recall Theorem 7.11, which says that for a module of prime order , the following are equivalent: 1 is cyclic) 2 is indecomposable) 3 is irreducible) 4 is nonderogatory, that is, ) 5 .)dim deg 194 Advanced Linear Algebra Now suppose that and is an irreducible quadratic. If is a -cyclic basis for , then However, there is a more appealing matrix representation of . To this end, let be the matrix above. As a complex matrix, has two distinct eigenvalues: Now, a matrix of the form has characteristic polynomial and eigenvalues . So if we set and then has the same two distinct eigenvalues as and so and have the same Jordan canonical form over . It follows that and are similar over and therefore also over , by Theorem 7.20. Thus, there is an ordered basis for which . Theorem 8.8 If and is cyclic and , then there is an deg ordered basis for which Now we can proceed with the real version of Schur's theorem. For the sake of the exposition, we make the following definition. Definition A matrix is if it has the form almost upper triangular block where Eigenvalues and Eigenvectors 195 or for . A linear operator is if almost upper triangularizable there is an ordered basis for which is almost upper triangular. To see that every real linear operator is almost upper triangularizable, we use Theorem 7.19, which states that if is a prime factor of , then has a cyclic submodule of order . Hence, is a -cyclic subspace of dimension and has characteristic polynomial .deg Now, the minimal polynomial of a real operator factors into a product of linear and irreducible quadratic factors. If has a linear factor over , then has a one-dimensional -invariant subspace . If has an irreducible quadratic factor , then has a cyclic submodule of order and so a matrix representation of on is given by the matrix This is the basis for an inductive proof, as in the complex case. Theorem 8.9 If is a real vector space, then()Schur's theorem: real case every linear operator on is almost upper triangularizable. Proof. As with the complex case, it is simpler to proceed using matrices, by showing that any real matrix is similar to an almost upper triangular matrix. The result is clear if . Assume for the purposes of induction that any square matrix of size less than is almost upper triangularizable. We have just seen that has a one-dimensional -invariant subspace or a two-dimensional -cyclic subspace , where has irreducible characteristic polynomial on . Hence, we may choose a basis for for which the first one or first two vectors are a basis for . Then block where or and has size . The induction hypothesis applied to gives an invertible matrix for which 196 Advanced Linear Algebra is almost upper triangular. Hence, if block then is invertible and is almost upper triangular. Unitary Triangularizability Although we have not yet discussed inner product spaces and orthonormal bases, the reader may very well be familiar with these concepts. For those who are, we mention that when is a real or complex inner product space, then if an operator on can be triangularized (or almost triangularized) using an ordered basis , it can also be triangularized (or almost triangularized) using an orthonormal ordered basis . To see this, suppose we apply the Gram–Schmidt orthogonalization process to a basis that triangularizes (or almost triangularizes) . The resulting ordered orthonormal basis has the property that for all . Since is (almost) upper triangular, that is, for all , it follows that and so the matrix is also (almost) upper triangular. A linear operator is if there is an ordered unitarily upper triangularizable orthonormal basis with respect to which is upper triangular. Accordingly, when is an inner product space, we can replace the term “upper triangularizable” with “unitarily upper triangularizable” in Schur's theorem. (A similar statement holds for almost upper triangular matrices.) Diagonalizable Operators Definition A linear operator is if there is an ordered diagonalizable basis of for which the matrix is diagonal, or equivalently, if Eigenvalues and Eigenvectors 197 for all . The previous definition leads immediately to the following simple characterization of diagonalizable operators. Theorem 8.10 Let . The following are equivalent: 1 is diagonalizable.) 2 has a basis consisting entirely of eigenvectors of .) 3 has the form) where are the distinct eigenvalues of . Diagonalizable operators can also be characterized in a simple way via their minimal polynomials. Theorem 8.11 A linear operator on a finite-dimensional vector space is diagonalizable if and only if its minimal polynomial is the product of distinct linear factors. Proof. If is diagonalizable, then and Theorem 7.7 implies that is the least common multiple of the minimal polynomials of restricted to . Hence, is a product of distinct linear factors. Conversely, if is a product of distinct linear factors, then the primary decomposition of has the form where and so is diagonalizable. Spectral Resolutions We have seen (Theorem 2.25) that resolutions of the identity on a vector space correspond to direct sum decompositions of . We can do something similar for any linear operator on (not just the identity operator).diagonalizable Suppose that has the form where is a resolution of the identity and the are distinct. This is referred to as a of .spectral resolution 198 Advanced Linear Algebra We claim that the 's are the eigenvalues of and . Theorem 2.25 im implies that im im If , then im and so . Hence, and so im im im which implies that andim The converse also holds, for if and if is projection onto along the direct sum of the other eigenspaces, then and since , it follows that Theorem 8.12 A linear operator is diagonalizable if and only if it has a spectral resolution In this case, is the spectrum of and im and ker Exercises 1. Let be the matrix all of whose entries are equal to . Find the minimal polynomial and characteristic polynomial of and the eigenvalues. 2. Prove that the eigenvalues of a matrix do not form a complete set of invariants under similarity. 3. Show that is invertible if and only if is not an eigenvalue of . 4. Let be an matrix over a field that contains all roots of the characteristic polynomial of . Prove that is the product of thedet eigenvalues of , counting multiplicity. 5. Show that if is an eigenvalue of , then is an eigenvalue of , for any polynomial . Also, if , then is an eigenvalue for . 6. An operator is if for some positive . nilpotent Eigenvalues and Eigenvectors 199 a Show that if is nilpotent, then the spectrum of is .) b Find a nonnilpotent operator with spectrum .) 7. Show that if and one of and is invertible, then and so and have the same eigenvalues, counting multiplicty. 8. Halmos() a Find a linear operator that is not idempotent but for which) . b Find a linear operator that is not idempotent but for which) . c Prove that if , then is idempotent.) 9. An is a linear operator for which . If is idempotentinvolution what can you say about ? Construct a one-to-one correspondence between the set of idempotents on and the set of involutions. 10. Let and suppose that but and . Show that if commutes with both and , then for some scalar . 11. Let and let be a -cyclic submodule of with minimal polynomial where is prime of degree . Let restricted to . Show that is the direct sum of -cyclic submodules each of dimension , that is, Hint: For each , consider the set 12. Fix . Show that any complex matrix is similar to a matrix that looks just like a Jordan matrix except that the entries that are equal to are replaced by entries with value , where is any complex number. Thus, any complex matrix is similar to a matrix that is “almost” diagonal. :Hint consider the fact that 13. Show that the Jordan canonical form is not very robust in the sense that a small change in the entries of a matrix may result in a large jump in the entries of the Jordan form . : consider the matrix Hint What happens to the Jordan form of as ? 200 Advanced Linear Algebra 14. Give an example of a complex nonreal matrix all of whose eigenvalues are real. Show that any such matrix is similar to a real matrix. What about the type of the invertible matrices that are used to bring the matrix to Jordan form? 15. Let be the Jordan form of a linear operator . For a given Jordan block of let be the subspace of spanned by the basis vectors of associated with that block. a Show that has a single eigenvalue with geometric multiplicity .) In other words, there is essentially only one eigenvector up to scalar( multiple associated with each Jordan block. Hence, the geometric) multiplicity of for is the number of Jordan blocks for . Show that the algebraic multiplicity is the sum of the dimensions of the Jordan blocks associated with . b Show that the number of Jordan blocks in is the maximum number) of linearly independent eigenvectors of . c What can you say about the Jordan blocks if the algebraic multiplicity) of every eigenvalue is equal to its geometric multiplicity? 16. Assume that the base field is algebraically closed. Then assuming that the eigenvalues of a matrix are known, it is possible to determine the Jordan form of by looking at the rank of various matrix powers. A matrix is nilpotent if for some . The smallest such exponent is called the .index of nilpotence a Let be a single Jordan block of size . Show that) is nilpotent of index . Thus, is the smallest integer for which .rk Now let be a matrix in Jordan form but possessing only one eigenvalue . b Show that is nilpotent. Let be its index of nilpotence. Show) that is the maximum size of the Jordan blocks of and that rk is the number of Jordan blocks in of maximum size. c Show that is equal to times the number of Jordan)rk blocks of maximum size plus the number of Jordan blocks of size one less than the maximum. d Show that the sequence for uniquely)rk determines the number and size of all of the Jordan blocks in , that is, it uniquely determines up to the order of the blocks. e Now let be an arbitrary Jordan matrix. If is an eigenvalue for ) show that the sequence for where is therk first integer for which uniquelyrk rk determines up to the order of the blocks. f Prove that for any matrix with spectrum the sequence) rk for and where is the first integer for which uniquelyrk rk determines the Jordan matrix for up to the order of the blocks. 17. Let . Eigenvalues and Eigenvectors 201 a If all the roots of the characteristic polynomial of lie in prove that) is similar to its transpose . Hint: Let be the matrix with 's on the diagonal that moves up from left to right and 's elsewhere. Let be a Jordan block of the same size as . Show that . b Let . Let be a field containing . Show that if and) are similar over , that is, if where , then and are also similar over , that is, there exists for which . c Show that any matrix is similar to its transpose.) The Trace of a Matrix 18. Let . Verify the following statements. a) A , for .tr tr b) .tr tr tr c) .tr tr d) . Find an example to show thattr tr tr tr tr may not equal . e) The trace is an invariant under similarity. f) If is algebraically closed, then the trace of is the sum of the eigenvalues of . 19. Use the concept of the trace of a matrix, as defined in the previous exercise, to prove that there are no matrices , for which 20. Let be a function with the following properties. For all matrices and , 1 A) 2 ) 3 ) Show that there exists for which tr , for all . Commuting Operators Let be a family of operators on a vector space . Then is a if commuting family every pair of operators commutes, that is, for all . A subspace 202 Advanced Linear Algebra of is if it is -invariant for every . It is often of interest -invariant to know whether a family of linear operators on has a common eigenvector, that is, a single vector that is an eigenvector for every (the corresponding eigenvalues may be different for each operator, however). 21. A pair of linear operators is if simultaneously diagonalizable there is an ordered basis for for which and are both diagonal, that is, is an ordered basis of eigenvectors for both and . Prove that two diagonalizable operators and are simultaneously diagonalizable if and only if they commute, that is, . : If , then the Hint eigenspaces of are invariant under . 22. Let . Prove that if and commute, then every eigenspace of is -invariant. Thus, if is a commuting family, then every eigenspace of any member of is -invariant. 23. Let be a family of operators in with the property that each operator in has a full set of eigenvalues in the base field , that is, the characteristic polynomial splits over . Prove that if is a commuting family, then has a common eigenvector . 24. What do the real matrices and have to do with the issue of common eigenvectors? Geršgorin Disks It is generally impossible to determine precisely the eigenvalues of a given complex operator or matrix , for if , then the characteristic equation has degree and cannot in general be solved. As a result, the approximation of eigenvalues is big business. Here we consider one aspect of this approximation problem, which also has some interesting theoretical consequences. Let and suppose that where . Comparing th rows gives which can also be written in the form If has the property that for all , we have Eigenvalues and Eigenvectors 203 and thus ()8.7 The right-hand side is the sum of the absolute values of all entries in the th row of except the diagonal entry . This sum is the th deleted absolute row sum of . The inequality 8.7 says that, in the complex plane, the () eigenvalue lies in the disk centered at the diagonal entry with radius equal to . This disk GR is called the for the th row of . The union of all of theGeršgorin row disk Geršgorin row disks is called the for .Geršgorin row region Since there is no way to know in general which is the index for which , the best we can say in general is that the eigenvalues of lie in the union of all Geršgorin row disks, that is, in the Geršgorin row region of . Similar definitions can be made for columns and since a matrix has the same eigenvalues as its transpose, we can say that the eigenvalues of lie in the Geršgorin column region of . The of a matrixGeršgorin region is the intersection of the Geršgorin row region and the Geršgorin column region and we can say that all eigenvalues of lie in the Geršgorin region of . In symbols, . 25. Find and sketch the Geršgorin region and the eigenvalues for the matrix 26. A matrix is if for each , diagonally dominant and it is if strict inequality holds. Prove thatstrictly diagonally dominant if is strictly diagonally dominant, then it is invertible. 27. Find a matrix that is diagonally dominant but not invertible. 28. Find a matrix that is invertible but not strictly diagonally dominant. Chapter 9 Real and Complex Inner Product Spaces We now turn to a discussion of real and complex vector spaces that have an additional function defined on them, called an , as described in theinner product following definition. In this chapter, will denote either the real or complex field. Also, the complex conjugate of is denoted by . Definition Let be a vector space over or . An inner product on is a function with the following properties: 1 For all ,)( )Positive definiteness and 2 For )( ) : Conjugate symmetry For :( )Symmetry 3 For all and )( )Linearity in the first coordinate A real or complex vector space , together with an inner product, is called a() real complex inner product space or .() If , then we let and Note that a vector subspace of an inner product space is also an inner product space under the restriction of the inner product of to . 206 Advanced Linear Algebra We will study bilinear forms (also called ) on vector spaces overinner products fields other than or in Chapter 11. Note that property 1) implies that is always real, even if is a complex vector space. If , then properties 2) and 3) imply that the inner product is linear in both coordinates, that is, the inner product is . However, if , thenbilinear This is referred to as in the second coordinate. Specifically,conjugate linearity a function between complex vector spaces is if conjugate linear and for all and . Thus, a complex inner product is linear in its first coordinate and conjugate linear in its second coordinate. This is often described by saying that a complex inner product is . (Sesqui means “one andsesquilinear a half times.”) Example 9.1 1) The vector space is an inner product space under the standard inner product dot product, or , defined by The inner product space is often called -dimensional Euclidean space. 2) The vector space is an inner product space under the standard inner product defined by This inner product space is often called .-dimensional unitary space 3) The vector space of all continuous complex-valued functions on the closed interval is a complex inner product space under the inner product Example 9.2 One of the most important inner product spaces is the vector space of all real (or complex) sequences with the property that Real and Complex Inner Product Spaces 207 under the inner product Such sequences are called . Of course, for this inner productsquare summable to make sense, the sum on the right must converge. To see this, note that if , then and so which implies that . We leave it to the reader to verify that is an inner product space. The following simple result is quite useful. Lemma 9.1 If is an inner product space and for all , then . The next result points out one of the main differences between real and complex inner product spaces and will play a key role in later work. Theorem 9.2 Let be an inner product space and let . 1) for all 2 If is a complex inner product space, then) for all but this does not hold in general for real inner product spaces. Proof. Part 1) follows directly from Lemma 9.1. As for part 2), let , for and . Then Setting gives and setting gives 208 Advanced Linear Algebra These two equations imply that for all and so part 1) implies that . For the last statement, rotation by degrees in the real plane has the property that for all . Norm and Distance If is an inner product space, the , or of is defined by norm length ()9.1 A vector is a if . Here are the basic properties of the norm. unit vector Theorem 9.3 1 and if and only if .) 2 For all and ,) 3 For all ,)( )The Cauchy–Schwarz inequality with equality if and only if one of and is a scalar multiple of the other. 4 For all ,)( )The triangle inequality with equality if and only if one of and is a scalar multiple of the other. 5 For all ,) 6 For all ,) 7 For all ,)( )The parallelogram law Proof. We prove only Cauchy–Schwarz and the triangle inequality. For Cauchy–Schwarz, if either or is zero the result follows, so assume that . Then, for any scalar , Choosing makes the value in the square brackets equal to Real and Complex Inner Product Spaces 209 and so which is equivalent to the Cauchy–Schwarz inequality. Furthermore, equality holds if and only if , that is, if and only if , which is equivalent to and being scalar multiples of one another. To prove the triangle inequality, the Cauchy–Schwarz inequality gives from which the triangle inequality follows. The proof of the statement concerning equality is left to the reader. Any vector space , together with a function that satisfies properties 1), 2) and 4) of Theorem 9.3, is called a and thenormed linear space function is called a . Thus, any inner product space is a normed linear norm space, under the norm given by 9.1 .() It is interesting to observe that the inner product on can be recovered from the norm. Thus, knowing the length of all vectors in is equivalent to knowing all inner products of vectors in . Theorem 9.4 ()The polarization identities 1 If is a real inner product space, then) 2 If is a complex inner product space, then) The norm can be used to define the distance between any two vectors in an inner product space. Definition Let be an inner product space. The between any distance two vectors and in is ()9.2 Here are the basic properties of distance. 210 Advanced Linear Algebra Theorem 9.5 1 and if and only if ) 2)( )Symmetry 3)( )The triangle inequality Any nonempty set , together with a function that satisfies the properties of Theorem 9.5, is called a and the function is calledmetric space a on . Thus, any inner product space is a metric space under the metricmetric ()9.2 . Before continuing, we should make a few remarks about our goals in this and the next chapter. The presence of an inner product, and hence a metric, permits the definition of a topology on , and in particular, convergence of infinite sequences. A sequence of vectors in to if converges lim Some of the more important concepts related to convergence are closedness and closures, completeness and the continuity of linear operators and linear functionals. In the finite-dimensional case, the situation is very straightforward: All subspaces are closed, all inner product spaces are complete and all linear operators and functionals are continuous. However, in the infinite-dimensional case, things are not as simple. Our goals in this chapter and the next are to describe some of the basic properties of inner product spaces—both finite and infinite-dimensional—and then discuss certain special types of operators (normal, unitary and self-adjoint) in the finite-dimensional case only. To achieve the latter goal as rapidly as possible, we will postpone a discussion of convergence-related properties until Chapter 12. This means that we must state some results only for the finite- dimensional case in this chapter. Isometries An isomorphism of vector spaces preserves the vector space operations. The corresponding concept for inner product spaces is the .isometry Definition Let and be inner product spaces and let . Real and Complex Inner Product Spaces 211 1 is an if it preserves the inner product, that is, if) isometry for all . 2 A bijective isometry is called an . When ) isometric isomorphism is an isometric isomorphism, we say that and are isometrically isomorphic. It is clear that an isometry is injective and so it is an isometric isomorphism provided it is surjective. Moreover, if dim dim injectivity implies surjectivity and is an isometry if and only if is an isometric isomorphism. On the other hand, the following simple example shows that this is not the case for infinite-dimensional inner product spaces. Example 9.3 The map defined by is an isometry, but it is clearly not surjective. Since the norm determines the inner product, the following should not come as a surprise. Theorem 9.6 A linear transformation is an isometry if and only if it preserves the norm, that is, if and only if for all . Proof. Clearly, an isometry preserves the norm. The converse follows from the polarization identities. In the real case, we have and so is an isometry. The complex case is similar. Orthogonality The presence of an inner product allows us to define the concept of orthogonality. 212 Advanced Linear Algebra Definition Let be an inner product space. 1 Two vectors are , written , if) orthogonal 2 Two subsets are , written , if ,) orthogonal that is, if for all and . We write in place of . 3 The of a subset is the set) orthogonal complement The following result is easily proved. Theorem 9.7 Let be an inner product space. 1 The orthogonal complement of any subset is a subspace of .) 2 For any subspace of ,) Definition An inner product space is the of orthogonal direct sum subspaces and if In this case, we write More generally, is the of the subspaces , orthogonal direct sum written if and for Theorem 9.8 Let be an inner product space. The following are equivalent. 1) 2 and ) Proof. If , then by definition, . However, if , then where and . Then is orthogonal to both and and so is orthogonal to itself, which implies that and so . Hence, . The converse is clear. Orthogonal and Orthonormal Sets Definition A nonempty set of vectors in an inner product space is said to be an if for all . If, inorthogonal set addition, each vector is a unit vector, then is an . Thus, a orthonormal set Real and Complex Inner Product Spaces 213 set is orthonormal if for all , where is the Kronecker delta function. Of course, given any nonzero vector , we may obtain a unit vector by multiplying by the reciprocal of its norm: This process is referred to as the vector . Thus, it is a simplenormalizing matter to construct an orthonormal set from an orthogonal set of nonzero vectors. Note that if , then and the converse holds if . Orthogonality is stronger than linear independence. Theorem 9.9 Any orthogonal set of nonzero vectors in is linearly independent. Proof. If is an orthogonal set of nonzero vectors and then and so , for all . Hence, is linearly independent. Gram–Schmidt Orthogonalization The Gram–Schmidt process can be used to transform a sequence of vectors into an orthogonal sequence. We begin with the following. Theorem 9.10 Let be an inner product()Gram–Schmidt augmentation space and let be an orthogonal set of vectors in . If , then there is a nonzero for which is orthogonal and In particular, 214 Advanced Linear Algebra where if if Proof. We simply set and force for all , that is, Thus, if , take and if , take The Gram–Schmidt augmentation is traditionally applied to a sequence of linearly independent vectors, but it also applies to any sequence of vectors. Theorem 9.11 The Gram–Schmidt orthogonalization process() Let be a sequence of vectors in an inner product space . Define a sequence by repeated Gram–Schmidt augmentation, that is, where and if if Then is an orthogonal sequence in with the property that for all . Also, if and only if . Proof. The result holds for . Assume it holds for . If , then Writing Real and Complex Inner Product Spaces 215 we have if if Therefore, when and so . Hence, If then Example 9.4 Consider the inner product space of real polynomials, with inner product defined by Applying the Gram–Schmidt process to the sequence gives 3 4 3 3 and so on. The polynomials in this sequence are at least up to multiplicative( constants the .) Legendre polynomials The QR Factorization The Gram–Schmidt process can be used to factor any real or complex matrix into a product of a matrix with orthogonal columns and an upper triangular matrix. Suppose that is an matrix with columns , where . The Gram–Schmidt process applied to these columns gives orthogonal vectors for which 216 Advanced Linear Algebra for all . In particular, where if if In matrix terms, that is, where has orthogonal columns and is upper triangular. We may normalize the nonzero columns of and move the positive constants to . In particular, if for and for , then and so where the columns of are orthogonal and each column is either a unit vector or the zero vector and is upper triangular with positive entries on the main diagonal. Moreover, if the vectors are linearly independent, then the columns of are nonzero. Also, if and is nonsingular, then is unitary/orthogonal. If the columns of are not linearly independent, we can make one final adjustment to this matrix factorization. If a column is zero, then we may replace this column by any vector as long as we replace the th entry in by . Therefore, we can take nonzero columns of , extend to an orthonormal basis for the span of the columns of and replace the zero columns of by the additional members of this orthonormal basis. In this way, is replaced by a unitary/orthogonal matrix and is replaced by an upper triangular matrix that has nonnegative entries on the main diagonal. Real and Complex Inner Product Spaces 217 Theorem 9.12 Let , where or . There exists a matrix with orthonormal columns and an upper triangular matrix with nonnegative real entries on the main diagonal for which Moreover, if , then is unitary/orthogonal. If is nonsingular, then can be chosen to have positive entries on the main diagonal, in which case the factors and are unique. The factorization is called the factorization of the matrix . If is real, then and may be taken to be real. Proof. As to uniqueness, if is nonsingular and then and the right side is upper triangular with nonzero entries on the main diagonal and the left side is unitary. But an upper triangular matrix with positive entries on the main diagonal is unitary if and only if it is the identity and so and . Finally, if is real, then all computations take place in the real field and so and are real. The decomposition has important applications. For example, a system of linear equations can be written in the form and since , we have This is an upper triangular system, which is easily solved by back substitution; that is, starting from the bottom and working up. We mention also that the factorization is associated with an algorithm for approximating the eigenvalues of a matrix, called the . algorithm Specifically, if is an matrix, define a sequence of matrices as follows: 1) Let be the factorization of and let . 2) Once has been defined, let be the factorization of and let . Then is unitarily/orthogonally similar to , since For complex matrices, it can be shown that under certain circumstances, such as when the eigenvalues of have distinct norms, the sequence converges 218 Advanced Linear Algebra (entrywise) to an upper triangular matrix , which therefore has the eigenvalues of on its main diagonal. Results can be obtained in the real case as well. For more details, we refer the reader to [48], page 115. Hilbert and Hamel Bases Definition A in an inner product space is called amaximal orthonormal set Hilbert basis for . Zorn's lemma can be used to show that any nontrivial inner product space has a Hilbert basis. We leave the details to the reader. Some care must be taken not to confuse the concepts of a basis for a vector space and a Hilbert basis for an inner product space. To avoid confusion, a vector space basis, that is, a maximal linearly independent set of vectors, is referred to as a . We will refer to an orthonormal Hamel basis as anHamel basis orthonormal basis. To be perfectly clear, there are maximal linearly independent sets called (Hamel) bases and maximal orthonormal sets (called Hilbert bases). If a maximal linearly independent set (basis) is orthonormal, it is called an orthonormal basis. Moreover, since every orthonormal set is linearly independent, it follows that an orthonormal basis is a Hilbert basis, since it cannot be properly contained in an orthonormal set. For inner product spaces, the two types offinite-dimensional bases are the same. Theorem 9.13 Let be an inner product space. A finite subset of is an orthonormal Hamel basis for if and only if it is() a Hilbert basis for . Proof. We have seen that any orthonormal basis is a Hilbert basis. Conversely, if is a finite maximal orthonormal set and , where is linearly independent, then we may apply part 1) to extend to a strictly larger orthonormal set, in contradiction to the maximality of . Hence, is maximal linearly independent. The following example shows that the previous theorem fails for infinite- dimensional inner product spaces. Example 9.5 Let and let be the set of all vectors of the form where has a in the th coordinate and 's elsewhere. Clearly, is an orthonormal set. Moreover, it is maximal. For if has the property that , then Real and Complex Inner Product Spaces 219 for all and so . Hence, no nonzero vector is orthogonal to . This shows that is a Hilbert basis for the inner product space . On the other hand, the vector space span of is the subspace of all sequences in that have finite support, that is, have only a finite number of nonzero terms and since , we see that is not a Hamelspan basis for the vector space . The Projection Theorem and Best Approximations Orthonormal bases have a great practical advantage over arbitrary bases. From a computational point of view, if is a basis for , then each has the form In general, determining the coordinates requires solving a system of linear equations of size . On the other hand, if is an orthonormal basis for and then the coefficients are quite easily computed: Even if is not a basis (but just an orthonormal set), we can still consider the expansion Theorem 9.14 Let be an orthonormal subset of an inner product space and let . The with respect to of Fourier expansion a vector is Each coefficient is called a of with respect to . Fourier coefficient The vector can be characterized as follows: 1 is the unique vector for which .) 2 is the to from within , that is, is the unique) best approximation vector that is closest to , in the sense that for all . 220 Advanced Linear Algebra 3 holds for all , that is) Bessel's inequality Proof. For part 1), since it follows that . Also, if for , then and and so . For part 2), if , then implies that and so Hence, is smallest if and only if and the smallest value is . We leave proof of Bessel's inequality as an exercise. Theorem 9.15 The If is a finite-dimensional subspace()projection theorem of an inner product space , then In particular, if , then It follows that dim dim dim Proof. We have seen that and so . But and so . The following example shows that the projection theorem may fail if is not finite-dimensional. Indeed, in the infinite-dimensional case, must be a complete subspace, but we postpone a discussion of this case until Chapter 13. Example 9.6 As in Example 9.5, let and let be the subspace of all sequences with finite support, that is, is spanned by the vectors where has a in the th coordinate and 's elsewhere. If , then for all and so . Therefore, . However, The projection theorem has a variety of uses. Real and Complex Inner Product Spaces 221 Theorem 9.16 Let be an inner product space and let be a finite- dimensional subspace of . 1) 2 If and , then) dim Proof. For part 1), it is clear that . On the other hand, if , then the projection theorem implies that where and . Then is orthogonal to both and and so is orthogonal to itself. Hence, and and so . We leave the proof of part 2) as an exercise. Characterizing Orthonormal Bases We can characterize orthonormal bases using Fourier expansions. Theorem 9.17 Let be an orthonormal subset of an inner product space and let . The following are equivalent: 1 is an orthonormal basis for .) 2) 3 Every vector is equal to its Fourier expansion, that is, for all ,) 4 holds for all , that is,) Bessel's identity 5 holds for all , that is,) Parseval's identity where is the standard dot product in . Proof. To see that 1) implies 2), if is nonzero, then is orthonormal and so is not maximal. Conversely, if is not maximal, there is an orthonormal set for which . Then any nonzero is in . Hence, 2) implies 1). We leave the rest of the proof as an exercise. The Riesz Representation Theorem We have been dealing with linear maps for some time. We now have a need for conjugate linear maps. Definition A function on complex vector spaces is conjugate linear if it is additive, 222 Advanced Linear Algebra and for all . A is a bijective conjugate linear map. conjugate isomorphism If , then the inner product function defined by is a linear functional on . Thus, the linear map defined by is conjugate linear. Moreover, since implies , it follows that is injective and therefore a conjugate isomorphism (since is finite- dimensional). Theorem 9.18 The Riesz representation theorem() Let be a finite- dimensional inner product space. 1 The map defined by) is a conjugate isomorphism. In particular, for each , there exists a unique vector for which , that is, for all . We call the for and denote it by . Riesz vector 2 The map defined by) is also a conjugate isomorphism, being the inverse of . We will call this map the .Riesz map Proof. Here is the usual proof that is surjective. If , then , so let us assume that . Then has codimension and so ker for . Letting for , we require that and since this clearly holds for any , it is sufficient to show that it holds for , that is, Thus, and Real and Complex Inner Product Spaces 223 For part 2), we have for all and so Note that if , then , where is the standard basis for . Exercises 1. Prove that if a matrix is unitary, upper triangular and has positive entries on the main diagonal, must be the identity matrix. 2. Use the QR factorization to show that any triangularizable matrix is unitarily (orthogonally) triangularizable. 3. Verify the statement concerning equality in the triangle inequality. 4. Prove the parallelogram law. 5. Prove the Apollonius identity 6. Let be an inner product space with basis . Show that the inner product is uniquely defined by the values , for all . 7. Prove that two vectors and in a real inner product space are orthogonal if and only if 8. Show that an isometry is injective. 9. Use Zorn's lemma to show that any nontrivial inner product space has a Hilbert basis. 10. Prove Bessel's inequality. 11. Prove that an orthonormal set is a Hilbert basis for a finite-dimensional vector space if and only if , for all . 12. Prove that an orthonormal set is a Hilbert basis for a finite-dimensional vector space if and only if Bessel's identity holds for all , that is, if and only if 224 Advanced Linear Algebra for all . 13. Prove that an orthonormal set is a Hilbert basis for a finite-dimensional vector space if and only if Parseval's identity holds for all , that is, if and only if for all . 14. Let and be in . The Cauchy–Schwarz inequality states that Prove that we can do better: 15. Let be a finite-dimensional inner product space. Prove that for any subset of , we have . span 16. Let be the inner product space of all polynomials of degree at most 3,3 under the inner product Apply the Gram–Schmidt process to the basis , thereby computing the first four at least up to aHermite polynomials ( multiplicative constant .) 17. Verify uniqueness in the Riesz representation theorem. 18. Let be a complex inner product space and let be a subspace of . Suppose that is a vector for which for all . Prove that . 19. If and are inner product spaces, consider the function on defined by Is this an inner product on ? 20. A over or is a vector space over or normed vector space () together with a function for which for all and scalars we have a ) b ) c if and only if ) If is a real normed space over and if the norm satisfies the () parallelogram law Real and Complex Inner Product Spaces 225 prove that the polarization identity defines an inner product on . : Evaluate to show Hint that and . Then complete the proof that . 21. Let be a subspace of a finite-dimensional inner product space . Prove that each coset in contains exactly one vector that is orthogonal to . Extensions of Linear Functionals 22. Let be a linear functional on a subspace of a finite-dimensional inner product space . Let . Suppose that is an extension of , that is, . What is the relationship between the Riesz vectors and ? 23. Let be a nonzero linear functional on a subspace of a finite-dimensional inner product space and let . Show that if is an ker extension of , then . Moreover, for each vector there is exactly one scalar for which the linear functional is an extension of . Positive Linear Functionals on A vector in is also called , written nonnegative positive() , if for all . The vector is , written , if is strictly positive nonnegative but not . The set of all strictly positive vectors in is called the in The vector is , writtennonnegative orthant strongly positive , if for all . The set , of all strongly positive vectors in is the in strongly positive orthant Let be a linear functional on a subspace of . Then is nonnegative also called , written , if()positive for all and is , written , if strictly positive for all 24. Prove that a linear functional on is positive if and only if and strictly positive if and only if . If is a subspace of is it true that a linear functional on is nonnegative if and only if ? 226 Advanced Linear Algebra 25. Let be a strictly positive linear functional on a subspace of . Prove that has a strictly positive extension to . Use the fact that if , where all and is a subspace of , then contains a strongly positive vector. 26. If is a real inner product space, then we can define an inner product on its complexification as follows this is the same formula as for the ordinary ( inner product on a complex vector space :) Show that where the norm on the left is induced by the inner product on and the norm on the right is induced by the inner product on . Chapter 10 Structure Theory for Normal Operators Throughout this chapter, all vector spaces are assumed to be finite-dimensional unless otherwise noted. Also, the field is either or . The Adjoint of a Linear Operator The purpose of this chapter is to study the structure of certain special types of linear operators on finite-dimensional real and complex inner product spaces. In order to define these operators, we introduce another type of adjoint (different from the operator adjoint of Chapter 3). Theorem 10.1 Let and be finite-dimensional inner product spaces over and let . Then there is a unique function , defined by the condition for all and . This function is in and is called the adjoint of . Proof. If exists, then it is unique, for if then for all and and so . We seek a linear map for which By way of motivation, the vector , if it exists, looks very much like a linear map sending to . The only problem is that is supposed to be a vector, not a linear map. But the Riesz representation theorem tells us that linear maps can be represented by vectors. 228 Advanced Linear Algebra Specifically, for each , the linear functional defined by has the form where is the Riesz vector for . If is defined by where is the Riesz map, then Finally, since is the composition of the Riesz map and the map and since both of these maps are conjugate linear, their composition is linear. Here are some of the basic properties of the adjoint. Theorem 10.2 Let and be finite-dimensional inner product spaces. For every and , 1) 2) 3 and so) 4 If , then ) 5 If is invertible, then ) 6 If and , then .) Moreover, if and is a subspace of , then 7 is -invariant if and only if is -invariant.) 8 reduces if and only if is both -invariant and -invariant, in) which case Proof. For part 7), let and and write Now, if is -invariant, then for all and so and is -invariant. Conversely, if is -invariant, then for all and so , whence is -invariant. The first statement in part 8) follows from part 7) applied to both and . For the second statement, since is both -invariant and -invariant, if , Structure Theory for Normal Operators 229 then Hence, by definition of adjoint, . Now let us relate the kernel and image of a linear transformation to those of its adjoint. Theorem 10.3 Let , where and are finite-dimensional inner product spaces. 1) ker ker im imand and so surjective injective injective surjective 2) ker ker ker ker and 3) im im im im and 4) Proof. For part 1), ker im and so . The second equation in part 1) follows by replacing ker im by and taking complements. For part 2), it is clear that . For the reverse inclusion, we haveker ker and so . The second equation follows from the first byker ker replacing with . We leave the rest of the proof for the reader. 230 Advanced Linear Algebra The Operator Adjoint and the Hilbert Space Adjoint We should make some remarks about the relationship between the operator adjoint of , as defined in Chapter 3 and the adjoint that we have just defined, which is sometimes called the . In the first place,Hilbert space adjoint if , then and have different domains and ranges: and The two maps are shown in Figure 10.1, along with the conjugate Riesz isomorphisms and . V * W VW x ** RV RW Figure 10.1 The composite map defined by is linear. Moreover, for all and , and so . Hence, the relationship between and is Loosely speaking, the Riesz functions are like “change of variables” functions from linear functionals to vectors, and we can say that does to Riesz vectors what does to the corresponding linear functionals. Put another way (and just as loosely), and are the same, up to conjugate Riesz isomorphism. In Chapter 3, we showed that the matrix of the operator adjoint is the transpose of the matrix of the map . For Hilbert space adjoints, the situation is slightly different (due to the conjugate linearity of the inner product). Suppose that and are ordered orthonormal bases for and , respectively. Then Structure Theory for Normal Operators 231 and so and are conjugate transposes. The conjugate transpose of a matrix is and is called the of .adjoint Theorem 10.4 Let , where and are finite-dimensional inner product spaces. 1 The operator adjoint and the Hilbert space adjoint are related by) where and are the conjugate Riesz isomorphisms on and , respectively. 2 If and are ordered for and , respectively, then) orthonormal bases In words, the matrix of the adjoint is the adjoint conjugate transpose of () the matrix of . Orthogonal Projections In an inner product space, we can single out some special projection operators. Definition A projection of the form is said to be . orthogonal Equivalently, a projection is orthogonal if . ker im Some care must be taken to avoid confusion between orthogonal projections and two projections that are orthogonal to each other, that is, for which . We have seen that an operator is a projection operator if and only if it is idempotent. Here is the analogous characterization of orthogonal projections. Theorem 10.5 Let be a finite-dimensional inner product space. The following are equivalent for an operator on : 1 is an orthogonal projection) 2 is idempotent and self-adjoint) 3 is idempotent and does not expand lengths, that is) for all . 232 Advanced Linear Algebra Proof. Since it follows that if and only if , that is, if and only if is orthogonal. Hence, 1) and 2) are equivalent. To prove that 1) implies 3), let . Then if for and , it follows that Now suppose that 3) holds. Then im ker ker ker and we wish to show that the first sum is orthogonal. If , then im , where and . Hence,ker ker and so the orthogonality of and implies that Hence, and so , which implies that . im im ker ker Orthogonal Resolutions of the Identity We have seen (Theorem 2.25) that resolutions of the identity on correspond to direct sum decompositions of . If, in addition, the projections are orthogonal, then the direct sum is an orthogonal sum. Definition An is a resolution of theorthogonal resolution of the identity identity in which each projection is orthogonal. The following theorem displays a correspondence between orthogonal direct sum decompositions of and orthogonal resolutions of the identity. Theorem 10.6 Let be an inner product space. Orthogonal resolutions of the identity on correspond to orthogonal direct sum decompositions of as follows: 1 If is an orthogonal resolution of the identity, then) im im and is orthogonal projection onto .im Structure Theory for Normal Operators 233 2 Conversely, if) and if is orthogonal projection onto , then is an orthogonal resolution of the identity. Proof. To prove 1), if is an orthogonal resolution of the identity, Theorem 2.25 implies that im im However, since the 's are pairwise orthogonal and self-adjoint, it follows that and so im im For the converse, Theorem 2.25 implies that is a resolution of the identity where is projection onto alongim ker im im Hence, is orthogonal. Unitary Diagonalizability We have seen (Theorem 8.10) that a linear operator on a finite- dimensional vector space is diagonalizable if and only if Of course, each eigenspace has an orthonormal basis , but the union of these bases need not be an basis for .orthonormal Definition A linear operator is when is unitarily diagonalizable ( complex and when is real if there is an)( )orthogonally diagonalizable ordered orthonormal basis of for which the matrix is diagonal, or equivalently, if for all . Here is the counterpart of Theorem 8.10 for inner product spaces. Theorem 10.7 Let be a finite-dimensional inner product space and let . The following are equivalent: 1 is unitarily orthogonally diagonalizable.)( ) 2 has an orthonormal basis that consists entirely of eigenvectors of .) 234 Advanced Linear Algebra 3 has the form) where are the distinct eigenvalues of . For simplicity in exposition, we will tend to use the term unitarily diagonalizable for both cases. Since unitarily diagonalizable operators are so well behaved, it is natural to seek a characterization of such operators. Remarkably, there is a simple one, as we will see next. Normal Operators Operators that commute with their own adjonts are very special. Definition 1 A linear operator on an inner product space is if it commutes) normal with its adjoint: 2 A matrix is if commutes with its adjoint .) normal If is normal and is an ordered orthonormal basis of , then and and so is normal if and only if is normal for some, and hence all, orthonormal bases for . Note that this does not hold for bases that are not orthonormal. Normal operators have some very special properties. Theorem 10.8 Let be normal. 1 The following are also normal:) a , if reduces ) b ) c , if is invertible) d , for any polynomial ) 2 For any , ) and, in particular, Structure Theory for Normal Operators 235 and so ker ker 3 For any integer , ) ker ker 4 The minimal polynomial is a product of distinct prime monic) polynomials. 5) 6 If and are submodules of with relatively prime orders, then .) 7 If and are distinct eigenvalues of , then .) Proof. We leave part 1) for the reader. For part 2), normality implies that We prove part 3) first for the operator , which is , that is, self-adjoint If for , then and so . Continuing in this way gives . Now, if for , then and so . Hence, and so . For part 4 , suppose that) where is monic and prime. Then for any , and since is also normal, part 3) implies that for all . Hence, , which implies that . Thus, the prime factors of appear only to the first power. 236 Advanced Linear Algebra Part 5) follows from part 2): ker ker ker For part 6), if and , then there are polynomials and for which and so Now, annihilates and annihilates . Therefore also annihilates and so Part 7) follows from part 6), since and are relatively prime when . Alternatively, for and , we have and so implies that . The Spectral Theorem for Normal Operators Theorem 10.8 implies that when , the minimal polynomial splits into distinct linear factors and so Theorem 8.11 implies that is diagonalizable, that is, Moreover, since distinct eigenspaces of a normal operator are orthogonal, we have and so is unitarily diagonalizable. The converse of this is also true. If has an orthonormal basis of eigenvectors for , then since and are diagonal, these matrices commute and therefore so do and . Theorem 10.9 ()The spectral theorem for normal operators: complex case Let be a finite-dimensional complex inner product space and let . The following are equivalent: 1 is normal.) 2 is unitarily diagonalizable, that is,) Structure Theory for Normal Operators 237 3 has an ) orthogonal spectral resolution (10.1) where and is orthogonal for all , in which case, is the spectrum of and im and ker Proof. We have seen that 1) and 2) are equivalent. To see that 2) and 3) are equivalent, Theorem 8.12 says that if and only if and in this case, im and ker But for if and only if im ker that is, if and only if each is orthogonal. Hence, the direct sum is an orthogonal sum if and only if each projection is orthogonal. The Real Case If , then has the form where each is an irreducible monic quadratic. Hence, the primary cyclic decomposition of gives where is cyclic with prime quadratic order . Therefore, Theorem 8.8 implies that there is an ordered basis for which Theorem 10.10 A()The spectral theorem for normal operators: real case linear operator on a finite-dimensional real inner product space is normal if and only if 238 Advanced Linear Algebra where is the spectrum of and each is an indecomposable two- dimensional -invariant subspace with an ordered basis for which Proof. We need only show that if has such a decomposition, then is normal. But and so is normal. It follows easily that is normal. Special Types of Normal Operators We now want to introduce some special types of normal operators. Definition Let be an inner product space. 1 is also called in the complex case and)( self-adjoint Hermitian symmetric in the real case if) 2 is also called in the)( skew self-adjoint skew-Hermitian complex case and in the real case ifskew-symmetric ) 3 is in the complex case and in the real case if) unitary orthogonal is invertible and There are also matrix versions of these definitions, obtained simply by replacing the operator by a matrix . Moreover, the operator is self-adjoint if and only if any matrix that represents with respect to an ordered basis isorthonormal self-adjoint. Similar statements hold for the other types of operators in the previous definition. In some sense, square complex matrices are a generalization of complex numbers and the adjoint (conjugate transpose) is a generalization of the complex conjugate. In looking for a better analogy, we could consider just the diagonal matrices, but this is a bit too restrictive. The next logical choice is the set of normal matrices. Indeed, among the complex numbers, there are some special subsets: the real numbers, the positive numbers and the numbers on the unit circle. We will soon see that a complex matrix is self-adjoint if and only if its complex eigenvalues Structure Theory for Normal Operators 239 are real. This would suggest that the analog of the set of real numbers is the set of self-adjoint matrices. Also, we will see that a complex matrix is unitary if and only if its eigenvalues have norm , so numbers on the unit circle seem to correspond to the set of unitary matrices. This leaves open the question of which normal matrices correspond to the positive real numbers. These are the positive definite matrices, which we will discuss later in the chapter. Self-Adjoint Operators Let us consider the basic properties of self-adjoint operators. The quadratic form associated with the linear operator is the function defined by We have seen (Theorem 9.2) that in a inner product space, if andcomplex only if but this does not hold, in general, for real inner product spaces. However, it does hold for symmetric operators on a real inner product space. Theorem 10.11 Let be a finite-dimensional inner product space and let . 1 If and are self-adjoint, then so are the following:) a ) b , if is invertible) c , for any real polynomial ) 2 A complex operator is Hermitian if and only if is real for all) . 3 If is a complex operator or a real symmetric operator, then) 4 The characteristic polynomial of a self-adjoint operator splits over) , that is, all complex roots of are real. Hence, the minimal polynomial of is the product of distinct monic linear factors over . Proof. For part 2), if is Hermitian, then and so is real. Conversely, if , then and so . For part 3), we need only prove that implies when . But if , then 240 Advanced Linear Algebra and so . For part 4), if is Hermitian ( ) and , then and so is real. If is symmetric ( ) , we must be a bit careful, since a nonreal root of is an eigenvalue of . However, matrix techniques not can come to the rescue here. If for any ordered orthonormal basis for , then . Now, is a real symmetric matrix, but can be thought of as a complex Hermitian matrix with real entries. As such, it represents a Hermitian linear operator on the complex space and so, by what we have just shown, all (complex) roots of its characteristic polynomial are real. But the characteristic polynomial of is the same, whether we think of as a real or a complex matrix and so the result follows. Unitary Operators and Isometries We now turn to the basic properties of unitary operators. These are the workhorse operators, in that a unitary operator is precisely a normal operator that maps orthonormal bases to orthonormal bases. Note that is unitary if and only if for all . Theorem 10.12 Let be a finite-dimensional inner product space and let . 1 If and are unitary/orthogonal, then so are the following:) a , for ) b ) c , if is invertible.) 2 is unitary/orthogonal if and only it is an isometric isomorphism.) 3 is unitary/orthogonal if and only if it takes some orthonormal basis to an) orthonormal basis, in which case it takes all orthonormal bases to orthonormal bases. 4 If is unitary/orthogonal, then the eigenvalues of have absolute value .) Structure Theory for Normal Operators 241 Proof. We leave the proof of part 1) to the reader. For part 2), a unitary/orthogonal map is injective and since is finite-dimensional, it is bijective. Moreover, for a bijective linear map , we have is an isometry for all for all is unitary/orthogonal For part 3), suppose that is unitary/orthogonal and that is an orthonormal basis for . Then and so is an orthonormal basis for . Conversely, suppose that and are orthonormal bases for . Then which implies that for all and so is unitary/orthogonal. For part 4), if is unitary and , then and so , which implies that . We also have the following theorem concerning unitary and orthogonal() matrices. Theorem 10.13 Let be an matrix over or . 1 The following are equivalent:) a is unitary/orthogonal.) b The columns of form an orthonormal set in .) c The rows of form an orthonormal set in .) 2 If is unitary, then . If is orthogonal, then .) det det Proof. The matrix is unitary if and only if , which is equivalent to the rows of being orthonormal. Similarly, is unitary if and only if , which is equivalent to the columns of being orthonormal. As for part 2), det det det det from which the result follows. 242 Advanced Linear Algebra Unitary/orthogonal matrices play the role of change of basis matrices when we restrict attention to orthonormal bases. Let us first note that if is an ordered orthonormal basis and then where the right hand side is the standard inner product in and so if and only if . We can now state the analog of Theorem 2.9. Theorem 10.14 If we are given any two of the following: 1 A unitary/orthogonal matrix ,) 2 An ordered orthonormal basis for ,) 3 An ordered orthonormal basis for ,) then the third is uniquely determined by the equation Proof. Let be a basis for . If is an orthonormal basis for , then where is the th column of . Hence, is unitary if and only if is orthonormal. We leave the rest of the proof to the reader. Unitary Similarity We have seen that the change of basis formula for operators is given by where is an invertible matrix. What happens when the bases are orthonormal? Definition 1 Two complex matrices and are also called)( unitarily similar unitarily equivalent) if there exists a unitary matrix for which The equivalence classes associated with unitary similarity are called unitary similarity classes. 2 Similarly, two real matrices and are also called)( orthogonally similar orthogonally equivalent) if there exists an orthogonal matrix for which The equivalence classes associated with orthogonal similarity are called orthogonal similarity classes. Structure Theory for Normal Operators 243 The analog of Theorem 2.19 is the following. Theorem 10.15 Let be an inner product space of dimension . Then two matrices and are unitarily/orthogonally similar if and only if they represent the same linear operator with respect to possibly different () ordered orthonormal bases. In this case, and represent exactly the same set of linear operators in with respect to ordered bases. orthonormal Proof. If and represent , that is, if and for ordered orthonormal bases and , then and according to Theorem 10.14, is unitary/orthogonal. Hence, and are unitarily/orthogonally similar. Now suppose that and are unitarily/orthogonally similar, say where is unitary/orthogonal. Suppose also that represents a linear operator for some ordered orthonormal basis , that is, Theorem 10.14 implies that there is a unique ordered orthonormal basis for for which . Hence and so also represents . By symmetry, we see that and represent the same set of linear operators, under all possible ordered orthonormal bases. We have shown (see the discussion of Schur's theorem) that any complex matrix is unitarily similar to an upper triangular matrix, that is, that is unitarily upper triangularizable. However, upper triangular matrices do not form a set of canonical forms under unitary similarity. Indeed, the subject of canonical forms for unitary similarity is rather complicated and we will not discuss it in this book, but instead refer the reader to the survey article [28]. Reflections The following defines a very special type of unitary operator. 244 Advanced Linear Algebra Definition For a nonzero , the unique operator for which for all is called a or a .reflection Householder transformation It is easy to verify that Moreover, for if and only if for some and so we can uniquely identify by the behavior of the reflection on . If is a reflection and if we extend to an ordered orthonormal basis for , then is the matrix obtained from the identity matrix by replacing the upper left entry by , Thus, a reflection is both unitary and Hermitian, that is, Given two nonzero vectors of equal length, there is precisely one reflection that interchanges these vectors. Theorem 10.16 Let be distinct nonzero vectors of equal length. Then is the unique reflection sending to and to . Proof. If , then and so from which it follows that and . As to uniqueness, suppose is a reflection for which . Since , we have and so which implies that . Reflections can be used to characterize unitary operators. Theorem 10.17 Let be a finite-dimensional inner product space. The following are equivalent for an operator : Structure Theory for Normal Operators 245 1 is unitary/orthogonal) 2 is a product of reflections.) Proof. Since reflections are unitary/orthogonal and the product of unitary/ orthogonal operators is unitary, it follows that 2) implies 1). For the converse, let be unitary. Let be an orthonormal basis for . Then and so if then that is, is the identity on . Suppose that we have found reflections for which is the identity on . Then Moreover, we claim that for , since Hence, if , then and so is the identity on . Thus, for we have and so , as desired. The Structure of Normal Operators The following theorem includes the spectral theorems stated above for real and complex normal operators, along with some further refinements related to self- adjoint and unitary/orthogonal operators. Theorem 10.18 ()The structure theorem for normal operators 1 Let be a finite-dimensional complex inner product)( )Complex case space. a The following are equivalent for :) i is normal) ii is unitarily diagonalizable) iii has an orthogonal spectral resolution) 246 Advanced Linear Algebra b Among the normal operators, the Hermitian operators are precisely) those for which all complex eigenvalues are real. c Among the normal operators, the unitary operators are precisely those) for which all complex eigenvalues have norm . 2 Let be a finite-dimensional real inner product space.)( )Real case a is normal if and only if) where is the spectrum of and each is a two- dimensional indecomposable -invariant subspace with an ordered basis for which b Among the real normal operators, the symmetric operators are those) for which there are no subspaces in the decomposition of part 2a . ) Hence, the following are equivalent for : i is symmetric.) ii is orthogonally diagonalizable.) iii has the orthogonal spectral resolution) c Among the real normal operators, the orthogonal operators are) precisely those for which the eigenvalues are equal to and the matrices described in part 2a have rows and columns of norm )( ) , that is, sin cos cos sin for some . Proof. We have proved part 1a). As to part 1b), it is only necessary to look at a diagonal matrix representing . This matrix has the eigenvalues of on its main diagonal and so it is Hermitian if and only if the eigenvalues of are real. Similarly, is unitary if and only if the eigenvalues of have absolute value equal to . We have proved part 2a). Parts 2b) and 2c) follow by looking at the matrix where . This matrix is symmetric if and only if is diagonal, and is orthogonal if and only if and the matrices have orthonormal rows. Matrix Versions We can formulate matrix versions of the structure theorem for normal operators. Structure Theory for Normal Operators 247 Theorem 10.19 ()The structure theorem for normal matrices 1)( )Complex case a A complex matrix is normal if and only if it is unitarily) diagonalizable, that is, if and only if there is a unitary matrix for which diag b A complex matrix is Hermitian if and only if 1a holds, where all)) eigenvalues are real. c A complex matrix is unitary if and only if 1a holds, where all)) eigenvalues have norm . 2)( )Real case a A real matrix is normal if and only if there is an orthogonal matrix) for which diag b A real matrix is symmetric if and only if it is orthogonally) diagonalizable, that is, if and only if there is an orthogonal matrix for which diag c A real matrix is orthogonal if and only if there is an orthogonal) matrix for which diag sin cos sin cos cos sin cos sin for some . Functional Calculus Let be a normal operator on a finite-dimensional inner product space and let have spectral resolution Since each is idempotent, we have for all . The pairwise orthogonality of the projections implies that More generally, for any polynomial over , Note that a polynomial of degree is uniquely determined by specifying an 248 Advanced Linear Algebra arbitrary set of of its values at the distinct points . This follows from the Lagrange interpolation formula Therefore, we can define a unique polynomial by specifying the values , for . For example, for a given , if is a polynomial for which for , then and so each projection is a polynomial function of . As another example, if is invertible and , then as can easily be verified by direct calculation. Finally, if , then since each is self-adjoint, we have and so is a polynomial in . We can extend this idea further by , for functiondefining any the linear operator by For example, we may define and so on. Notice, however, that since the spectral resolution of is a finite sum, we gain nothing (but convenience) by using functions other than polynomials, for we can always find a polynomial for which for and so . The study of the properties of functions of an operator is referred to as the of .functional calculus According to the spectral theorem, if is complex and is normal, then is a normal operator whose eigenvalues are . Similarly, if is real and is symmetric, then is symmetric, with eigenvalues . Structure Theory for Normal Operators 249 Commutativity The functional calculus can be applied to the study of the commutativity properties of operators. Here are two simple examples. Theorem 10.20 Let be a finite-dimensional complex inner product space. For , we write to denote the fact that and commute. Let and have spectral resolutions Then 1 For any ,) for all 2) , for all 3 If and are injective functions,) then Proof. For 1), if for all , then and the converse follows from the fact that is a polynomial in . Part 2) is similar. For part 3), clearly implies . For the converse, let . Since is injective, the inverse function is well-defined and . Thus, is a function of . Similarly, is a function of . It follows that implies . Theorem 10.21 Let and be normal operators on a finite-dimensional complex inner product space . Then and commute if and only if they have the form where and are polynomials. Proof. If and are polynomials in , then they clearly commute. For the converse, suppose that and let and be the orthogonal spectral resolutions of and . 250 Advanced Linear Algebra Then Theorem 10.20 implies that . Hence, It follows that for any polynomial in two variables, So if we choose with the property that are distinct, then and we can also choose and so that for all and for all . Then and similarly, . Positive Operators One of the most important cases of the functional calculus is . Recall that the quadratic form associated with a linear operator is Definition A self-adjoint linear operator is 1 if for all ) positive 2 if for all .) positive definite Theorem 10.22 A self-adjoint operator on a finite-dimensional inner product space is 1 positive if and only if all of its eigenvalues are nonnegative) 2 positive definite if and only if all of its eigenvalues are positive.) Proof. If and , then Structure Theory for Normal Operators 251 and so . Conversely, if all eigenvalues of are nonnegative, then and since , and so is positive. Part 2) is proved similarly. If is a positive operator, with spectral resolution then we may take the of ,positive square root where is the nonnegative square root of . It is clear that and it is not hard to see that is the only positive operator whose square is . In other words, every positive operator has a unique positive square root. Conversely, if has a positive square root, that is, if , for some positive operator , then is positive. Hence, an operator is positive if and only if it has a positive square root. If is positive, then is self-adjoint and so Conversely, if for some operator , then is positive, since it is clearly self-adjoint and Thus, is positive if and only if it has the form for some operator . (A complex number is nonnegative if and only if has the form for some complex number .) Theorem 10.23 Let . 1 is positive if and only if it has a positive square root.) 2 is positive if and only if it has the form for some operator .) Here is an application of square roots. Theorem 10.24 If and are positive operators and , then is positive. 252 Advanced Linear Algebra Proof. Since is a positive operator, it has a positive square root , which is a polynomial in . A similar statement holds for . Therefore, since and commute, so do and . Hence, Since and are self-adjoint and commute, their product is self-adjoint and so is positive. The Polar Decomposition of an Operator It is well known that any nonzero complex number can be written in the polar form , where is a positive number and is real. We can do the same for any nonzero linear operator on a finite-dimensional complex inner product space. Theorem 10.25 Let be a nonzero linear operator on a finite-dimensional complex inner product space . 1 There exist a positive operator and a unitary operator for which) . Moreover, is unique and if is invertible, then is also unique. 2 Similarly, there exist a positive operator and a unitary operator for) which . Moreover, is unique and if is invertible, then is also unique. Proof. Let us suppose for a moment that . Then and so Also, if , then These equations give us a clue as to how to define and . Let us define to be the unique positive square root of the positive operator . Then ()10.2 Define on byim for all . Equation 10.2 shows that implies that and so () this definition of on is well-defined.im Moreover, is an isometry on , since 10.2 givesim ( ) Structure Theory for Normal Operators 253 Thus, if is an orthonormal basis for , then im is an orthonormal basis for . Finally, weim im may extend both orthonormal bases to orthonormal bases for and then extend the definition of to an isometry on for which . As for the uniqueness, we have seen that must satisfy and since has a unique positive square root, we deduce that is uniquely defined. Finally, if is invertible, then so is since . Hence, is ker ker uniquely determined by . Part 2 can be proved by applying the previous theorem to the map , to get) where is unitary. We leave it as an exercise to show that any unitary operator has the form , where is a self-adjoint operator. This gives the following corollary. Corollary 10.26 Let be a nonzero linear operator on()Polar decomposition a finite-dimensional complex inner product space. Then there is a positive operator and a self-adjoint operator for which has the polar decomposition Moreover, is unique and if is invertible, then is also unique. Normal operators can be characterized using the polar decomposition. Theorem 10.27 Let be a polar decomposition of a nonzero linear operator . Then is normal if and only if . Proof. Since and we see that is normal if and only if 254 Advanced Linear Algebra or equivalently, Now, is a polynomial in and is a polynomial in and so this holds if and only if . Exercises 1. Let . If is surjective, find a formula for the right inverse of in terms of . If is injective, find a formula for a left inverse of in terms of . : Consider and . Hint 2. Let where is a complex vector space and let and Show that and are self-adjoint and that and What can you say about the uniqueness of these representations of and ? 3. Prove that all of the roots of the characteristic polynomial of a skew- Hermitian matrix are pure imaginary. 4. Give an example of a normal operator that is neither self-adjoint nor unitary. 5. Prove that if for all , where is complex, then is normal. 6. Let be a normal operator on a complex finite-dimensional inner product space or a self-adjoint operator on a real finite-dimensional inner product space. a Show that , for some polynomial .) b Show that for any , implies . In other) words, commutes with all operators that commute with . 7. Show that a linear operator on a finite-dimensional complex inner product space is normal if and only if whenever is an invariant subspace under , so is . 8. Let be a finite-dimensional inner product space and let be a normal operator on . a Prove that if is idempotent, then it is also self-adjoint.) b Prove that if is nilpotent, then .) c Prove that if , then is idempotent.) 9. Show that if is a normal operator on a finite-dimensional complex inner product space, then the algebraic multiplicity is equal to the geometric multiplicity for all eigenvalues of . 10. Show that two orthogonal projections and are orthogonal to each other if and only if .im im Structure Theory for Normal Operators 255 11. Let be a normal operator and let be any operator on . If the eigenspaces of are -invariant, show that and commute. 12. Prove that if and are normal operators on a finite-dimensional complex inner product space and if for some operator then . 13. Prove that if two normal complex matrices are similar, then they are unitarily similar, that is, similar via a unitary matrix. 14. If is a unitary operator on a complex inner product space, show that there exists a self-adjoint operator for which . 15. Show that a positive operator has a unique positive square root. 16. Prove that if has a square root, that is, if , for some positive operator , then is positive. 17. Prove that if (that is, is positive) and if is a positive operator that commutes with both and then . 18. Using the factorization, prove the following result, known as the Cholsky decomposition. An invertible linear operator is positive if and only if it has the form where is upper triangularizable. Moreover, can be chosen with positive eigenvalues, in which case the factorization is unique. 19. Does every self-adjoint operator on a finite-dimensional real inner product space have a square root? 20. Let be a linear operator on and let be the eigenvalues of , each one written a number of times equal to its algebraic multiplicity. Show that tr where is the trace. Show also that equality holds if and only if istr normal. 21. If where is a real inner product space, show that the Hilbert space adjoint satisfies . Part II—TopicsChapter 11 Metric Vector Spaces: The Theory of Bilinear Forms In this chapter, we study vector spaces over arbitrary fields that have a bilinear form defined on them. Unless otherwise mentioned, all vector spaces are assumed to be finite- dimensional. The symbol denotes an arbitrary field and denotes a finite field of size . Symmetric, Skew-Symmetric and Alternate Forms We begin with the basic definition. Definition Let be a vector space over . A mapping is called a if it is linear in each coordinate, that is, ifbilinear form and A bilinear form is 1 if) symmetric for all . 2 or if)( )skew-symmetric antisymmetric for all . 260 Advanced Linear Algebra 3 or if)( )alternate alternating for all . A bilinear form that is either symmetric, skew-symmetric, or alternate is referred to as an and a pair , where is a vector spaceinner product and is an inner product on , is called a or metric vector space inner product space. As usual, we will refer to as a metric vector space when the form is understood. 4 A metric vector space with a symmetric form is called an ) orthogonal geometry over . 5 A metric vector space with an alternate form is called a ) symplectic geometry over . The term , from the Greek for “intertwined,” was introduced in 1939symplectic by the famous mathematician Hermann Weyl in his book ,The Classical Groups as a substitute for the term . According to the dictionary, symplecticcomplex means “relating to or being an intergrowth of two different minerals.” An example is , which is marble spotted with green serpentine.ophicalcite Example 11.1 is the four-dimensional real orthogonalMinkowski space 4 geometry with inner product defined by 33 44 for where is the standard basis for . 4 As is traditional, when the inner product is understood, we will use the phrase “let be a metric vector space.” The real inner products discussed in Chapter 9 are inner products in the present sense and have the additional property of being —a notion thatpositive definite does not even make sense if the base field is not ordered. Thus, a real inner product space is an orthogonal geometry. On the other hand, the complex inner products of Chapter 9, being sesquilinear, are not inner products in the present sense. For this reason, we use the term in this chapter, rathermetric vector space than .inner product space If is a vector subspace of a metric vector space , then inherits the metric structure from . With this structure, we refer to as a of . subspace The concepts of being symmetric, skew-symmetric and alternate are not independent. However, their relationship depends on the characteristic of the base field , as do many other properties of metric vector spaces. In fact, the Metric Vector Spaces: The Theory of Bilinear Forms 261 next theorem tells us that we do not need to consider skew-symmetric forms per se, since skew-symmetry is always equivalent to either symmetry or alternateness. Theorem 11.1 Let be a vector space over a field . 1 If , then)char alternate symmetric skew-symmetric 2 If , then)char alternate skew-symmetric Also, the only form that is both alternate and symmetric is the zero form: for all . Proof. First note that for an alternating form over any base field, and so which shows that the form is skew-symmetric. Thus, alternate always implies skew-symmetric. If , then and so the definitions of symmetric and skew-char symmetric are equivalent, which proves 1 . If and the form is)char skew-symmetric, then for any , we have or , which implies that . Hence, the form is alternate. Finally, if the form is alternate and symmetric, then it is also skew-symmetric and so for all , that is, for all . Example 11.2 The standard inner product on , defined by is symmetric, but not alternate, since The Matrix of a Bilinear Form If is an ordered basis for a metric vector space , then a bilinear form is completely determined by the matrix of values This is referred to as the (or the matrix of ) with respect tomatrix of the form the ordered basis . Moreover, any matrix over is the matrix of some bilinear form on . 262 Advanced Linear Algebra Note that if then and It follows that if , then and this uniquely defines the matrix , that is, if for all , then . A matrix is if it is skew-symmetric and has 's on the main diagonal.alternate Thus, we can say that a form is symmetric (skew-symmetric, alternate) if and only if the matrix is symmetric (skew-symmetric, alternate). Now let us see how the matrix of a form behaves with respect to a change of basis. Let be an ordered basis for . Recall from Chapter 2 that the change of basis matrix , whose th column is , satisfies Hence, and so This prompts the following definition. Definition Two matrices are if there exists an congruent invertible matrix for which The equivalence classes under congruence are called .congruence classes Metric Vector Spaces: The Theory of Bilinear Forms 263 Thus, if two matrices represent the same bilinear form on , they must be congruent. Conversely, if represents a bilinear form on and where is invertible, then there is an ordered basis for for which and so Thus, represents the same form with respect to . Theorem 11.2 Let be an ordered basis for an inner product space , with matrix 1 The form can be recovered from the matrix by the formula) 2 If is also an ordered basis for , then) where is the change of basis matrix from to . 3 Two matrices and represent the same bilinear form on a vector space) if and only if they are congruent, in which case they represent the same set of bilinear forms on . In view of the fact that congruent matrices have the same rank, we may define the rank of a bilinear form (or of ) to be the rank of any matrix that represents that form. The Discriminant of a Form If and are congruent matrices, then det det det det and so and differ by a square factor. The of adet det discriminant bilinear form is the set of determinants of all of the matrices that represent the form. Thus, if is an ordered basis for , then det det 264 Advanced Linear Algebra Quadratic Forms There is a close link between symmetric bilinear forms on and quadratic forms on . Definition A on a vector space is a map with thequadratic form following properties: 1 For all ,) 2 The map) is a symmetric bilinear form.() Thus, every quadratic form on defines a symmetric bilinear form on . Conversely, if and if is a symmetric bilinear form on , char then the function is a quadratic form . Moreover, the bilinear form associated with is the original bilinear form: Thus, the maps and are inverses and so there is a one-to-one correspondence between symmetric bilinear forms on and quadratic forms on . Put another way, knowing the quadratic form is equivalent to knowing the corresponding bilinear form. Again assuming that , if is an ordered basis for anchar orthogonal geometry and if the matrix of the symmetric form on is , then for , and so is a homogeneous polynomial of degree 2 in the coordinates . (The term “form” means —hence the term quadratichomogeneous polynomial form.) Metric Vector Spaces: The Theory of Bilinear Forms 265 Orthogonality As we will see, not all metric vector spaces behave as nicely as real inner product spaces and this necessitates the introduction of a new set of terminology to cover various types of behavior. (The base field is the culprit, of course.) The most striking differences stem from the possibility that for a nonzero vector . The following terminology should be familiar. Definition Let be a metric vector space. A vector is to a vector orthogonal , written , if . A vector is to a subset oforthogonal , written , if for all . A subset of is to aorthogonal subset of , written , if for all and . The orthogonal complement of a subset of is the subspace Note that regardless of whether the form is symmetric or alternate and hence( skew-symmetric , orthogonality is a symmetric relation, that is, implies) . Indeed, this is precisely why we restrict attention to these two types of bilinear forms. There are two types of degenerate behaviors that a vector may possess: It may be orthogonal to itself or, worse yet, it may be orthogonal to vector in .every With respect to the former, we have the following terminology. Definition Let be a metric vector space. 1 A nonzero is or if ; otherwise it is)( ) isotropic null nonisotropic. 2 is if it contains at least one isotropic vector. Otherwise, is) isotropic nonisotropic anisotropic or .() 3 is that is, symplectic if all vectors in are)( )totally isotropic isotropic. Note that if is an isotropic vector, then so is for all . This can be expressed by saying that the set of isotropic vectors in is a in . (A cone cone in is a nonempty subset that is closed under scalar multiplication.) With respect to the more severe forms of degeneracy, we have the following terminology. Definition Let be a metric vector space. 266 Advanced Linear Algebra 1 A vector is if . The set of all degenerate) degenerate vectors is called the of and denoted by . Thus,radical rad rad 2 is , or , if .)rad nonsingular nondegenerate 3 is , or , if .)rad singular degenerate 4 is , or , if .)rad totally singular totally degenerate Some of the above terminology is not entirely standard, so care should be exercised in reading the literature. Theorem 11.3 A metric vector space is nonsingular if and only if all representing matrices are nonsingular. A note of caution is in order. If is a subspace of a metric vector space , then rad rad denotes the set of vectors in that are degenerate in , that is, is the radical of , as a metric vector space in its own right. However, denotes the set of all vectors in that are orthogonal to . Thus, rad Note also that rad rad and so if is singular, then so is . Example 11.3 Recall that is the set of all ordered -tuples whose components come from the finite field . (See Example 11.2.) It is easy to see that the subspace of has the property that . Note also that is nonsingular and yet the subspace is singular. totally The following result explains why we restrict attention to symmetric or alternate forms (which includes skew-symmetric forms). Theorem 11.4 Let be a vector space with a bilinear form. The following are equivalent: 1 Orthogonality is a symmetric relation, that is,) 2 The form on is symmetric or alternate, that is, is a metric vector) space. Metric Vector Spaces: The Theory of Bilinear Forms 267 Proof. It is clear that orthogonality is symmetric if the form is symmetric or alternate, since in the latter case, the form is also skew-symmetric. For the converse, assume that orthogonality is symmetric. For convenience, let mean that and let mean that for all . If for all , then is orthogonal and we are done. So let us examine vectors with the property that . We wish to show that is isotropic and (11.1) Note that if the second conclusion holds, then since , it follows that is isotropic. So suppose that . Since , there is a for which and so if and only if Now, But reversing the coordinates in the last expression gives and so the symmetry of orthogonality implies that the last expression is and so we have proven (11.1). Let us assume that is not orthogonal and show that all vectors in are isotropic, whence is symplectic. Since is not orthogonal, there exist for which and so and . Hence, the vectors and are isotropic and for all , Since all vectors for which are isotropic, let . Then and and so and . Now write where , since is isotropic. Since the sum of two orthogonal isotropic vectors is isotropic, it follows that is isotropic if is isotropic. But 268 Advanced Linear Algebra and so , which implies that is isotropic. Thus, is also isotropic and so all vectors in are isotropic. Orthogonal and Symplectic Geometries If a metric vector space is both orthogonal and symplectic, then the form is both symmetric and skew-symmetric and so Therefore, when , is orthogonal and symplectic if and only if char is totally degenerate. However, if , then there are orthogonal symplectic geometries thatchar are not totally degenerate. For example, let be a two- span dimensional vector space and define a form on whose matrix is Since is both symmetric and alternate, so is the form. Linear Functionals The Riesz representation theorem says that every linear functional on a finite- dimensional real or complex inner product space is represented by a Riesz vector , in the sense that for all . A similar result holds for metric vector spaces. nonsingular Let be a metric vector space over . Let and define the inner product map by This is easily seen to be a linear functional and so we can define a linear map by The bilinearity of the form ensures that is linear and the kernel of is ker rad Hence, is injective (and therefore an isomorphism) if and only if is nonsingular. Theorem 11.5 The Riesz representation theorem() Let be a finite- dimensional nonsingular metric vector space. The map defined by Metric Vector Spaces: The Theory of Bilinear Forms 269 is an isomorphism from to . It follows that for each there exists a unique vector for which for all . The requirement that be nonsingular is necessary. As a simple example, if is totally singular, then no nonzero linear functional could possibly be represented by an inner product. The Riesz representation theorem applies to nonsingular metric vector spaces. However, we can also achieve something useful for subspaces of asingular nonsingular metric vector space. The reason is that any linear functional can be extended to a linear functional on , where it has a Riesz vector, that is, Hence, also has this form, where its “Riesz vector” is an element of , but is not necessarily in . Theorem 11.6 The Riesz representation theorem for subspaces() Let be a subspace of a metric vector space . If either or is nonsingular, the linear map defined by is surjective and has kernel . Hence, for any linear functional , there is a not necessarily unique vector for which for all .() Moreover, if is nonsingular, then can be taken from , in which case it is unique. Orthogonal Complements and Orthogonal Direct Sums Definition A metric vector space is the of the orthogonal direct sum subspaces and , written if and . If is a subspace of a real inner product space, the projection theorem says that the orthogonal complement of is a true vector space complement of , that is, 270 Advanced Linear Algebra However, in general metric vector spaces, an orthogonal complement may not be a vector space complement. In fact, Example 11.3 shows that in some cases . In other cases, for example, if is degenerate, then . However, as we will see, the orthogonal complement of is a vector space complement if and only if either the sum is correct, , or the intersection is correct, . Note that the latter is equivalent to the nonsingularity of . Many nice properties of orthogonality in real inner product spaces do carry over to metric vector spaces. Moreover, the next result shows that thenonsingular restriction to nonsingular spaces is not that severe. Theorem 11.7 Let be a metric vector space. Then rad where is nonsingular and is totally singular. rad Proof. If is any vector space complement of , then and so rad rad rad Also, is nonsingular since . rad rad Here are some properties of orthogonality in nonsingular metric vector spaces. In particular, if either or is nonsingular, then the orthogonal complement of always has the expected dimension, dim dim dim even if is not well behaved with respect to its intersection with . Theorem 11.8 Let be a subspace of a finite-dimensional metric vector space . 1) If either or is nonsingular, then dim dim dim Hence, the following are equivalent: a ) b is nonsingular, that is, ) c .) 2 If is nonsingular, then) a ) b )rad rad c is nonsingular if and only if is nonsingular.) Proof. For part 1), the map of Theorem 11.6 is surjective and has kernel . Thus, the rank-plus-nullity theorem implies that Metric Vector Spaces: The Theory of Bilinear Forms 271 dim dim dim However, and so part 1) follows. For part 2), sincedim dim rad rad the nonsingularity of implies the nonsingularity of . Then part 1) implies that dim dim dim and dim dim dim Hence, and . rad rad The previous theorem cannot in general be strengthened. Consider the two- dimensional metric vector space span where If , then . Now, is nonsingular but is singular span span and so 2c) does not hold. Also, and and so 2b)rad rad fails. Finally, and so 2a) fails. Isometries We now turn to a discussion of structure-preserving maps on metric vector spaces. Definition Let and be metric vector spaces. We use the same notation for the bilinear form on each space. A bijective linear map is called an ifisometry for all vectors and in . If an isometry exists from to , we say that and are and write . It is evident that the set of all isometric isometries from to forms a group under composition. If is a nonsingular orthogonal geometry, an isometry of is called an orthogonal transformation. The set of all orthogonal transformations on is a group under composition, known as the of .orthogonal group If is a nonsingular symplectic geometry, an isometry of is called a symplectic transformation. The set of all symplectic transformations onSp is a group under composition, known as the of .symplectic group 272 Advanced Linear Algebra Note that, in contrast to the case of real inner product spaces, we must include the requirement that be bijective since this does not follow automatically if is singular. Here are a few of the basic properties of isometries. Theorem 11.9 Let be a linear transformation between finite- dimensional metric vector spaces and . 1 Let be a basis for . Then is an isometry if and only if ) is bijective and for all . 2 If is orthogonal and , then is an isometry if and only if it is)char bijective and for all . 3 Suppose that is an isometry and) and If , then . Proof. We prove part 3 only. To see that , if and ,) then since , we can write for some and so whence . But since the dimensions are equal, it follows that . Hyperbolic Spaces A special type of two-dimensional metric vector space plays an important role in the structure theory of metric vector spaces. Definition Let be a metric vector space. A is a pair of hyperbolic pair vectors for which Note that if is orthogonal and if is symplectic. In either case, the subspace is called a and any span hyperbolic plane space of the form where each is a hyperbolic plane, is called a . If is hyperbolic space a hyperbolic pair for , then we refer to the basis Metric Vector Spaces: The Theory of Bilinear Forms 273 for as a . In the symplectic case, the usual term is hyperbolic basis ( symplectic basis.) Note that any hyperbolic space is nonsingular. In the orthogonal case, hyperbolic planes can be characterized by their degree of isotropy, so to speak. In the symplectic case, all spaces are totally isotropic by( definition. Indeed, we leave it as an exercise to prove that a two-dimensional) nonsingular orthogonal geometry is a hyperbolic plane if and only if contains exactly two one-dimensional totally isotropic equivalently, totally( degenerate subspaces. Put another way, the cone of isotropic vectors is the) union of two one-dimensional subspaces of . Nonsingular Completions of a Subspace Let be a subspace of a nonsingular metric vector space . If is singular, it is of interest to find a nonsingular subspace of containing .minimal Definition Let be a nonsingular metric vector space and let be a subspace of . A subspace of for which is called an of . A extension nonsingular completion of is an extension of that is minimal in the family of all nonsingular extensions of . Theorem 11.10 Let be a nonsingular finite-dimensional metric vector space over . We assume that when is orthogonal. char 1 Let be a subspace of . If is isotropic and the orthogonal direct sum) span exists, then there is a hyperbolic plane for which span exists. In particular, if is isotropic, then there is a hyperbolic plane containing . 2 Let be a subspace of and let) span where is nonsingular and are linearly independent in rad . Then there is a hyperbolic space with hyperbolic basis for which is a nonsingular proper extension of . If is a basis for rad , then dim dim dim rad 274 Advanced Linear Algebra and we refer to as a of . If is nonsingular, we hyperbolic extension say that is a hyperbolic extension of itself. Proof. For part 1 , the nonsingularity of implies that . Hence,) and so there is an for which . If is symplectic, then all vectors are isotropic and so we can take . If is orthogonal, let . The conditions defining as a hyperbolic pair are since is isotropic() and Since , the first of these equations can be solved for and since char , the second equation can then be solved for . Thus, in either case, there is a vector for which is hyperbolic. Hence, span and since is nonsingular, that is, , we have and so exists. Part 2) is proved by induction on . Note first that all of the vectors are isotropic. If , then exists and so part 1 implies that there is span ) a hyperbolic plane for which exists. span Assume that the result is true for independent sets of size less than . Since span span exists, part 1) implies that there exists a hyperbolic plane for span which span exists. Since are in the radical of , the inductive span hypothesis implies that there is a hyperbolic space with hyperbolic basis for which the orthogonal direct sum exists. Hence, also exists. We can now prove that the hyperbolic extensions of are precisely the minimal nonsingular extensions of . Theorem 11.11 Let be a subspace of a()Nonsingular extension theorem nonsingular finite-dimensional metric vector space . The following are equivalent: 1 is a hyperbolic extension of ) 2 is a minimal nonsingular extension of ) Metric Vector Spaces: The Theory of Bilinear Forms 275 3 is a nonsingular extension of and) dim dim dim rad Thus, any two nonsingular completions of are isometric. Proof. If where is nonsingular, then we may apply Theorem 11.10 to as a subspace of , to obtain a hyperbolic extension of for which Thus, every nonsingular extension of contains a hyperbolic extension of . Moreover, all hyperbolic extensions of have the same dimension: dim dim dim rad and so no hyperbolic extension of is properly contained in another hyperbolic extension of . This proves that 1)–3) are equivalent. The final statement follows from the fact that hyperbolic spaces of the same dimension are isometric. Extending Isometries to Nonsingular Completions Let and be isometric nonsingular metric vector spaces and let rad be a subspace of , with nonsingular completion . If is an isometry, then it is a simple matter to extend to an isometry from onto a nonsingular completion of . To see this, let be a hyperbolic basis for . Since is a basis for rad rad , it follows that is a basis for . Hence, we can hyperbolically extend to get rad where has hyperbolic basis . To extend , simply set for all . Theorem 11.12 Let and be isometric nonsingular metric vector spaces and let be a subspace of , with nonsingular completion . Any isometry can be extended to an isometry from onto a nonsingular completion of . The Witt Theorems: A Preview There are two important theorems that are quite easy to prove in the case of real inner product spaces, but require more work in the case of metric vector spaces in general. Let and be isometric nonsingular metric vector spaces over a field . We assume that if is orthogonal. char 276 Advanced Linear Algebra The says that if is a subspace of , then any isometryWitt extension theorem can be extended to an isometry from to . The Witt cancellation theorem says that if and then We will prove these theorems in both the orthogonal and symplectic cases a bit later in the chapter. For now, we simply want to show that it is easy to prove one Witt theorem using the other. Suppose that the Witt extension theorem holds and assume that and and . Then any isometry can be extended to an isometry from to . According to Theorem 11.9, we have and so . Hence, the Witt cancellation theorem holds. Conversely, suppose that the Witt cancellation theorem holds and let be an isometry. Since can be extended to a nonsingular completion of , we may assume that is nonsingular. Then Since is an isometry, is also nonsingular and we can write Since , Witt's cancellation theorem implies that . If is an isometry, then the map defined by for and is an isometry that extends . Hence Witt's extension theorem holds. The Classification Problem for Metric Vector Spaces The for a class of metric vector spaces such as theclassification problem ( orthogonal or symplectic spaces is the problem of determining when two metric) vector spaces in the class are isometric. The classification problem is considered “solved,” at least in a theoretical sense, by finding a set of canonical forms or a complete set of invariants for matrices under congruence. Metric Vector Spaces: The Theory of Bilinear Forms 277 To see why, suppose that is an isometry and is an ordered basis for . Then is an ordered basis for and Thus, the congruence class of matrices representing is identical to the congruence class of matrices representing . Conversely, suppose that and are metric vector spaces with the same congruence class of representing matrices. Then if is an ordered basis for , there is an ordered basis for for which Hence, the map defined by is an isometry from to . We have shown that two metric vector spaces are isometric if and only if they have the same congruence class of representing matrices. Thus, we can determine whether any two metric vector spaces are isometric by representing each space with a matrix and determining whether these matrices are congruent, using a set of canonical forms or a set of complete invariants. Symplectic Geometry We now turn to a study of the structure of orthogonal and symplectic geometries and their isometries. Since the study of the structure and the structure itself of() symplectic geometries is simpler than that of orthogonal geometries, we begin with the symplectic case. The reader who is interested only in the orthogonal case may omit this section. Throughout this section, let be a nonsingular symplectic geometry. The Classification of Symplectic Geometries Among the simplest types of metric vector spaces are those that possess an orthogonal basis. However, it is easy to see that a symplectic geometry has an orthogonal basis if and only if it is totally degenerate and so no “interesting” symplectic geometries have orthogonal bases. Thus, in searching for an orthogonal decomposition of , we turn to two- dimensional subspaces and this puts us in mind of hyperbolic spaces. Let be the family of all hyperbolic subspaces of , which is nonempty since the zero subspace is singular and so has a nonzero hyperbolic extension. Since is finite-dimensional, has a maximal member . Since is nonsingular, if , then where . But then if is nonzero, there is a hyperbolic extension 278 Advanced Linear Algebra of containing , which contradicts the maximality of . Hence, . This proves the following structure theorem for symplectic geometries. Theorem 11.13 1 A symplectic geometry has an orthogonal basis if and only if it is totally) degenerate. 2 Any nonsingular symplectic geometry is a hyperbolic space, that is,) where each is a hyperbolic plane. Thus, there is a hyperbolic basis for , that is, a basis for which the matrix of the form is In particular, the dimension of is even. 3 Any symplectic geometry has the form) rad where is a hyperbolic space and is a totally degenerate space. rad The rank of the form is and is uniquely determined up todim isometry by its rank and its dimension. Put another way, up to isometry, there is precisely one symplectic geometry of each rank and dimension. Symplectic forms are represented by alternate matrices, that is, skew-symmetric matrices with zero diagonal. Moreover, according to Theorem 11.13, each alternate matrix is congruent to a matrix of the form block Since the rank of is , no two such matrices are congruent. Theorem 11.14 The set of matrices of the form is a set of canonical forms for alternate matrices under congruence. The previous theorems solve the classification problem for symplectic geometries by stating that the rank and dimension of form a complete set of Metric Vector Spaces: The Theory of Bilinear Forms 279 invariants under congruence and that the set of all matrices of the form is a set of canonical forms. Witt's Extension and Cancellation Theorems We now prove the Witt theorems for symplectic geometries. Theorem 11.15 Witt's extension theorem() Let and be isometric nonsingular symplectic geometries over a field . Then any isometry on a subspace of can be extended to an isometry from to . Proof. According to Theorem 11.12, we can extend to a nonsingular completion of , so we may simply assume that and are nonsingular. Hence, and To complete the extension of to , we need only choose a hyperbolic basis for and a hyperbolic basis for and define the extension by setting and . As a corollary to Witt's extension theorem, we have Witt's cancellation theorem. Theorem 11.16 Witt's cancellation theorem() Let and be isometric nonsingular symplectic geometries over a field . If and then The Structure of the Symplectic Group: Symplectic Transvections Let us examine the nature of symplectic transformations (isometries) on a nonsingular symplectic geometry . Recall that for a real vector space, an isometric isomorphism, which corresponds to an isometry in the present context, is the same as an orthogonal map and orthogonal maps are products of reflections (Theorem 10.17). Recall also that a reflection is defined as an operator for which 280 Advanced Linear Algebra for all and that In the present context, we do not dare divide by , since all vectors are isotropic. So here is the next-best thing. Definition Let be a nonsingular symplectic geometry over . Let be nonzero and let . The map defined by is called the determined by and .symplectic transvection Note that if , then and if , then is the identity precisely on the subspace of codimension . In the case of a reflection, is thespan identity precisely on andspan span span However, for a symplectic transvection, is the identity precisely on span span span (for ) but . Here are the basic properties of symplectic transvections. Theorem 11.17 Let be a symplectic transvection on . Then 1 is a symplectic transformation isometry .)( ) 2 if and only if .) 3 If , then . For , if and only if .) 4 .) 5 .) 6 For any symplectic transformation ,) 7 For ,) Note that if is a subspace of and if is a symplectic transvection on , then, by definition, . However, the formula also defines a symplectic transvection on , where ranges over . Moreover, for any , we have and so is the identity on . Metric Vector Spaces: The Theory of Bilinear Forms 281 We now wish to prove that any symplectic transformation on a nonsingular symplectic geometry is the product of symplectic transvections. The proof is not difficult, but it is a bit lengthy, so we break it up into parts. Our first goal is to show that we can get from any hyperbolic pair to any other hyperbolic pair using a product of symplectic transvections. Let us say that two and are if there is ahyperbolic pairs connected product of symplectic transvections that carries to and to and write or . It is clear that connectedness is an equivalence relation on the set of hyperbolic pairs. Theorem 11.18 In a nonsingular symplectic geometry , every pair of hyperbolic pairs are connected. Proof. Note first that if , then and so Taking gives . Therefore, if is hyperbolic, then we can always find a vector for which namely, and are hyperbolic, then . Also, if both since and so . Actually, these statements are still true if . For in this case, there is a nonzero vector for which and . This follows from the fact that there is an for which and and so the Riesz vector is such a vector. Therefore, if is hyperbolic, then and if both and are hyperbolic, then Hence, transitivity gives the same result as in the case . Finally, if and are hyperbolic, then there is a for which and so transitivity shows that . 282 Advanced Linear Algebra We can now show that the symplectic transvections generate the symplectic group. Theorem 11.19 Every symplectic transformation on a nonsingular symplectic geometry is the product of symplectic transvections. Proof. Let be a symplectic transformation on . We proceed by induction on dim . If , then is a hyperbolic plane andspan Theorem 11.18 implies that there is a product of symplectic transvections on for which This proves the result if . Assume that the result holds for all dimensions less than and let . dim Now, where and is a symplectic geometry of dimension less than span that of . As before, there is a product of symplectic transvections on for which and so Note that and so Theorem 11.9 implies that . Since , the inductive hypothesis applied to the symplecticdim dim transformation on implies that there is a product of symplectic transvections on for which . As remarked earlier, is also a product of symplectic transvections on that is the identity on and so and on Thus, on both and on and so is a product of symplectic transvections on . The Structure of Orthogonal Geometries: Orthogonal Bases We have seen that no interesting that is, not totally degenerate symplectic() geometries have orthogonal bases. By contrast, almost all interesting orthogonal geometries have orthogonal bases. To understand why, it is convenient to group the orthogonal geometries into two classes: those that are also symplectic and those that are not. The reason is that all orthogonal geometries have orthogonal bases, as we will see.nonsymplectic However, an orthogonal geometry has an orthogonal basis if andsymplectic Metric Vector Spaces: The Theory of Bilinear Forms 283 only if it is totally degenerate. Furthermore, we have seen that if ,char then all orthogonal symplectic geometries are totally degenerate and so all such geometries have orthogonal bases. But if , then there are orthogonalchar symplectic geometries that are not totally degenerate and therefore do not have orthogonal bases. Thus, if we exclude orthogonal symplectic geometries when , wechar can say that every orthogonal geometry has an orthogonal basis. If a metric vector space has an orthogonal basis, the natural next step is to look for an orthonormal basis. However, if is singular, then there is a nonzero vector and such a vector can never be a linear combination of vectors from an orthonormal basis , since the coefficients in such a linear combination are . However, even if is nonsingular, orthonormal bases do not always exist and the question of how close we can come to such an orthonormal basis depends on the nature of the base field. We will examine this issue in three cases: algebraically closed fields, the field of real numbers and finite fields. We should also mention that even when has an orthogonal basis, the Gram– Schmidt orthogonalization process may not apply to produce such a basis, because even nonsingular orthogonal geometries may have isotropic vectors, and so division by is problematic. For example, consider an orthogonal hyperbolic plane and span assume that . Thus, and are isotropic and . Thechar vector cannot be extended to an orthogonal basis using the Gram–Schmidt process, since is orthogonal if and only if . However, does have an orthogonal basis, namely, . Orthogonal Bases Let be an orthogonal geometry. As we have discussed, if is also symplectic, then has an orthogonal basis if and only if it is totally degenerate. Moreover, when , all orthogonal symplectic geometries are totallychar degenerate and so all orthogonal symplectic geometries have an orthogonal basis. If is orthogonal but not symplectic, then contains a nonisotropic vector , the subspace is nonsingular andspan span where . If is not symplectic, then we may decompose it to get span span span 284 Advanced Linear Algebra This process may be continued until we reach a decomposition span span where is symplectic as well as orthogonal. (This includes the case .) Let . If , then is totally degenerate. Thus, if is a basis for , then thechar union is an orthogonal basis for . If , then char rad , where is hyperbolic and so span span rad where is totally degenerate and the are nonisotropic. Ifrad is a hyperbolic basis for and is an ordered basis for , then the unionrad is an ordered orthogonal basis for . However, we can do better (in some sense). The following lemma says that when , a pair of isotropic basischar vectors, such as , can be replaced by a pair of nonisotropic basis vectors, when coupled with a nonisotropic basis vector, such as . Lemma 11.20 Suppose that . Let be a three-dimensionalchar orthogonal geometry of the form span span where is nonisotropic and is a hyperbolic plane. Then span span span span where each is nonisotropic. Proof. It is straightforward to check that if , then the vectors are linearly independent and mutually orthogonal. Details are left to the reader. Using the previous lemma, we can replace the vectors with the nonisotropic vectors , while retaining orthogonality. Moreover, the replacement process can continue until the isotropic vectors are absorbed, leaving an orthogonal basis of nonisotropic vectors. Metric Vector Spaces: The Theory of Bilinear Forms 285 Let us summarize. Theorem 11.21 Let be an orthogonal geometry. 1 If is also symplectic, then has an orthogonal basis if and only if it is) totally degenerate. When , all orthogonal symplecticchar geometries have an orthogonal basis, but this is not the case when char . 2 If is not symplectic, then has an ordered orthogonal basis) for which and . Hence, has the diagonal form with nonzero entries on the diagonal. rk As a corollary, we get a nice theorem about symmetric matrices. Corollary 11.22 Let be a symmetric matrix and assume that is not alternate if . Then is congruent to a diagonal matrix.char The Classification of Orthogonal Geometries: Canonical Forms We now want to consider the question of improving upon Theorem 11.21. The diagonal matrices of this theorem do not form a set of canonical forms for congruence. In fact, if are nonzero scalars, then the matrix of with respect to the basis is ()11.2 Hence, and are congruent diagonal matrices. Thus, by a simple change of basis, we can multiply any diagonal entry by a nonzero square in . The determination of a set of canonical forms for symmetric nonalternate when( char ) matrices under congruence depends on the properties of the base field. Our plan is to consider three types of base fields: algebraically closed 286 Advanced Linear Algebra fields, the real field and finite fields. Here is a preview of the forthcoming results. 1 When the base field is algebraically closed, there is an ordered basis ) for which If is nonsingular, then is an identity matrix and has an orthonormal basis. 2 Over the real base field, there is an ordered basis for which) Z 3 If is a finite field, there is an ordered basis for which) Z where is unique up to multiplication by a square and if , then char we can take . Now let us turn to the details. Algebraically Closed Fields If is algebraically closed, then for every , the polynomial has a root in , that is, every element of has a square root in . Therefore, we may choose in 11.2 , which leads to the following result. () Metric Vector Spaces: The Theory of Bilinear Forms 287 Theorem 11.23 Let be an orthogonal geometry over an algebraically closed field . Provided that is not symplectic as well when , then char has an ordered orthogonal basis for which and . Hence, has the diagonal form with ones and zeros on the diagonal. In particular, if is nonsingular, then has an orthonormal basis. The matrix version of Theorem 11.23 follows. Theorem 11.24 Let be the set of all symmetric matrices over an algebraically closed field . If , we restrict to the set of all char symmetric matrices with at least one nonzero entry on the main diagonal. 1 Any matrix in is congruent to a unique matrix of the form Z , in) fact, and . rk rk 2 The set of all matrices of the form Z for is a set of canonical) forms for congruence on . 3 The rank of a matrix is a complete invariant for congruence on .) The Real Field If , we can choose , so that all nonzero diagonal elements in ()11.2 will be either , or . Theorem 11.25 Sylvester's law of inertia() Any real orthogonal geometry has an ordered orthogonal basis for which , and . Hence, the matrix has the diagonal form 288 Advanced Linear Algebra Z with ones, negative ones and zeros on the diagonal. Here is the matrix version of Theorem 11.25. Theorem 11.26 Let be the set of all symmetric matrices over the real field . 1 Any matrix in is congruent to a unique matrix of the form Z for) some and . 2 The set of all matrices of the form Z for is a set of) canonical forms for congruence on . 3 Let and let be congruent to . Then is the rank of) and is the of and the triple is the signature inertia of . The pair , or equivalently the pair , is a complete invariant under congruence on . Proof. We need only prove the uniqueness statement in part 1 . Let) and be ordered bases for which the matrices and have the form shown in Theorem 11.25. Since the rank of these matrices must be equal, we have and so . If and , then span On the other hand, if and , then span Hence, if then . It follows that span Metric Vector Spaces: The Theory of Bilinear Forms 289 span span and so that is, . By symmetry, and so . Finally, since , it follows that . Finite Fields To deal with the case of finite fields, we must know something about the distribution of squares in finite fields, as well as the possible values of the scalars . Theorem 11.27 Let be a finite field with elements. 1 If , then every element of is a square.)char 2 If , then exactly half of the nonzero elements of are)char squares, that is, there are nonzero squares in . Moreover, if is any nonsquare in , then all nonsquares have the form , for some . Proof. Write , let be the subgroup of all nonzero elements in and let be the subgroup of all nonzero squares in . The Frobenius map defined by is a surjective group homomorphism, with kernel ker If , then and so is bijective and ,char ker which proves part 1 . If , then and so ,)char ker which proves the first part of part 2 . We leave proof of the last statement to the) reader. Definition A bilinear form on is if for any nonzero there universal exists a vector for which . Theorem 11.28 Let be an orthogonal geometry over a finite field with char and assume that has a nonsingular subspace of dimension at least . Then the bilinear form of is universal. Proof. Theorem 11.21 implies that contains two linearly independent vectors and for which 290 Advanced Linear Algebra Given any , we want to find and for which or If , then , since there are nonzero squares , along with . If , then for the same reasons . It follows that cannot be the empty set and so there exist and for which . Now we can proceed with the business at hand. Theorem 11.29 Let be an orthogonal geometry over a finite field and assume that is not symplectic if . If , then let be a char char fixed nonsquare in . For any nonzero , write where .rk 1 If , then there is an ordered basis for which .)char 2 If , then there is an ordered basis for which equals)char or . Proof. We can dispose of the case quite easily: Referring to 11.2 ,char ( ) since every element of has a square root, we may take . If , then Theorem 11.21 implies that there is an ordered orthogonalchar basis for which and . Hence, has the diagonal form Metric Vector Spaces: The Theory of Bilinear Forms 291 Now consider the nonsingular orthogonal geometry . span According to Theorem 11.28, the form is universal when restricted to . Hence, there exists a for which . Now, for not both , and we may swap and if necessary to ensure that . Hence, is an ordered basis for for which the matrix is diagonal and has a in the upper left entry. We can repeat the process with the subspace . span Continuing in this way, we can find an ordered basis for which for some nonzero . Now, if is a square in , then we can replace by to get a basis for which . If is not a square in , then for some and so replacing by gives a basis for which . Theorem 11.30 Let be the set of all symmetric matrices over a finite field . If , we restrict to the set of all symmetric matrices with char at least one nonzero entry on the main diagonal. 1 If , then any matrix in is congruent to a unique matrix of the)char form and the matrices form a set of canonical forms for under congruence. Also, the rank is a complete invariant. 2 If , let be a fixed nonsquare in . Then any matrix is)char congruent to a unique matrix of the form or . The set is a set of canonical forms for congruence on . Thus, there are exactly two congruence classes for each rank . () The Orthogonal Group Having “settled” the classification question for orthogonal geometries over certain types of fields, let us turn to a discussion of the structure-preserving maps, that is, the isometries. Rotations and Reflections We begin by examining the matrix of an orthogonal transformation. If is an ordered basis for , then for any , and so if , then Hence, is an orthogonal transformation if and only if 292 Advanced Linear Algebra Taking determinants gives det det det Therefore, if is nonsingular, then det Since the determinant is an invariant under similarity, we have the following theorem. Theorem 11.31 Let be an orthogonal transformation on a nonsingular orthogonal geometry . 1 is the same for all ordered bases for and)det det This determinant is called the of and denoted by .determinant det 2 If , then is called a and if , then is)det det rotation called a .reflection 3 The set of rotations is a subgroup of the orthogonal group ) and the determinant map is an epimorphism withdet kernel . Hence, if , then is a normal subgroup char of of index . Symmetries Recall again that for a real inner product space, a reflection is defined as an operator for which for all and that In particular, if and is nonisotropic, then ischar span nonsingular and so span span Then the reflection is well-defined and, in the context of general orthogonal geometries, is called the determined by and we will denote it bysymmetry . We can also write , that is, for all and . span span Metric Vector Spaces: The Theory of Bilinear Forms 293 For real inner product spaces, Theorem 10.16 says that if , then is the unique reflection sending to , that is, . In the present context, we must be careful, since symmetries are defined for nonisotropic vectors only. Here is what we can say. Theorem 11.32 Let be a nonsingular orthogonal geometry over a field , with . If are nonisotropic vectors with the same nonzerochar () “length,” that is, if then there exists a symmetry for which or Proof. Since and are nonisotropic, one of or must also be nonisotropic, for otherwise, since and are orthogonal, their sum would also be isotropic. If is nonisotropic, then and and so . On the other hand, if is nonisotropic, then and and so . Recall that an operator on a real inner product space is unitary if and only if it is a product of reflections. Here is the generalization to nonsingular orthogonal geometries. Theorem 11.33 Let be a nonsingular orthogonal geometry over a field with . A linear transformation on is an orthogonalchar transformation if and only if is the product of symmetries on . Proof. The proof is by induction on . If , then dim span where . Let for . Since is unitary and so . If , then is the identity, which is equal to . On the other hand, if then . In either case, is a product of symmetries. 294 Advanced Linear Algebra Assume now that the theorem is true for dimensions less than and let dim . Let be nonisotropic. Since , Theorem 11.32 implies the existence of a symmetry on for which where . Thus, on . Sinc implies that span e Theorem 11.9 span is -invariant, we may apply the induction hypothesis to on span to get span where and each is a symmetry on . But each can span span be extended to a symmetry on by setting . Assume that is the extension of to , where on . Hence, on and span span on span. If , then on and so , which completes the proof. If , then on since is the identity on and span span on on and so on span. Hence, . The Witt Theorems for Orthogonal Geometries We are now ready to consider the Witt theorems for orthogonal geometries. Theorem 11.34 Witt's cancellation theorem() Let and be isometric nonsingular orthogonal geometries over a field with . Suppose char that and Then Proof. First, we prove that it is sufficient to consider the case . Suppose that the result holds when and that is an isometry. Then Furthermore, . We can therefore apply the theorem to to get as desired. To prove the theorem when , assume that where and are nonsingular and . Let be an isometry. We proceed by induction on .dim Metric Vector Spaces: The Theory of Bilinear Forms 295 Suppose first that and that . Sincedim span Theorem 11.32 implies that there is a symmetry for which where . Hence, is an isometry of for which and Theorem 11.9 implies that . Thus, is the desired isometry. Now suppose the theorem is true for and let . Letdim dim be an isometry. Since is nonsingular, we can choose a nonisotropic vector and write , where is nonsingular. It follows span that span and span Now we may apply the one-dimensional case to deduce that If is an isometry, then But and since , the induction hypothesis dim dim implies that . As we have seen, Witt's extension theorem is a corollary of Witt's cancellation theorem. Theorem 11.35 Witt's extension theorem() Let and be isometric nonsingular orthogonal geometries over a field , with . Suppose char that is a subspace of and is an isometry. Then can be extended to an isometry from to . Maximal Hyperbolic Subspaces of an Orthogonal Geometry We have seen that any orthogonal geometry can be written in the form rad where is nonsingular. Nonsingular spaces are better behaved than singular ones, but they can still possess isotropic vectors. 296 Advanced Linear Algebra We can improve upon the preceding decomposition by noticing that if is isotropic, then Theorem 11.10 implies that can be “captured” in aspan hyperbolic plane . Then we can write span rad where is the orthogonal complement of in and has “one fewer” isotropic vector. In order to generalize this process, we first discuss maximal totally degenerate subspaces. Maximal Totally Degenerate Subspaces Let be a nonsingular orthogonal geometry over a field , with . char Suppose that and are maximal totally degenerate subspaces of . We claim that . For if , then there is a vectordim dim dim dim space isomorphism , which is also an isometry, since and are totally degenerate. Thus, Witt's extension theorem implies the existence of an isometry that extends . In particular, is a totally degenerate space that contains and so , which shows that dim dim . Theorem 11.36 Let be a nonsingular orthogonal geometry over a field , with .char 1 All maximal totally degenerate subspaces of have the same dimension,) which is called the of and is denoted by .Witt index 2 Any totally degenerate subspace of of dimension is maximal.) Maximal Hyperbolic Subspaces We can prove by a similar argument that all maximal hyperbolic subspaces of have the same dimension. Let and be maximal hyperbolic subspaces of and suppose that and span span . We may assume that .dim dim The linear map defined by is clearly an isometry from to . Thus, Witt's extension theorem implies the existence of an isometry that extends . In particular, is a hyperbolic space that contains and so . It follows that dim dim . Metric Vector Spaces: The Theory of Bilinear Forms 297 It is not hard to see that the maximum dimension of a hyperbolic subspace of is , where is the Witt index of . First, the nonsingular extension of a maximal totally degenerate subspace of is a hyperbolic space of dimension and so . On the other hand, there is a totally degenerate subspace contained in any hyperbolic space and so , that is, . Hence and sodim . Theorem 11.37 Let be a nonsingular orthogonal geometry over a field , with .char 1 All maximal hyperbolic subspaces of have dimension .) 2 Any hyperbolic subspace of dimension must be maximal.) 3 The Witt index of a hyperbolic space is .) The Anisotropic Decomposition of an Orthogonal Geometry If is a maximal hyperbolic subspace of , then Since is maximal, is anisotropic, for if were isotropic, then the nonsingular extension of would be a hyperbolic space strictly span larger than . Thus, we arrive at the following decomposition theorem for orthogonal geometries. Theorem 11.38 The anisotropic decomposition of an orthogonal geometry() Let be an orthogonal geometry over , with . Let rad char be a maximal hyperbolic subspace of , where if has no isotropic vectors. Then rad where is anisotropic, is hyperbolic of dimension and is rad totally degenerate. Exercises 1. Let be subspaces of a metric vector space . Show that a ) b ) c ) 2. Let be subspaces of a metric vector space . Show that a ) b ) 3. Prove that the following are equivalent: a is nonsingular) b for all implies ) 298 Advanced Linear Algebra 4. Show that a metric vector space is nonsingular if and only if the matrix of the form is nonsingular, for every ordered basis . 5. Let be a finite-dimensional vector space with a bilinear form . We do not assume that the form is symmetric or alternate. Show that the following are equivalent: a for all ) b for all ) : Consider the singularity of the matrix of the form.Hint 6. Find a diagonal matrix congruent to 7. Prove that the matrices and are congruent over the base field of rational numbers. Find an invertible matrix such that . 8. Let be an orthogonal geometry over a field with . We char wish to construct an orthogonal basis for , starting with any generating set . Justify the following steps, essentially due to Lagrange. We may assume that is not totally degenerate. a If for some , then let . Otherwise, there are indices) for which . Let . b Assume we have found an ordered set of vectors ) that form an orthogonal basis for a subspace of and that none of the 's are isotropic. Then . c For each , let) Then the vectors span . If is totally degenerate, take any basis for and append it to . Otherwise, repeat step a on to ) get another vector and let . Eventually, we arrive at an orthogonal basis for . 9. Prove that orthogonal hyperbolic planes may be characterized as two- dimensional nonsingular orthogonal geometries that have exactly two one- dimensional totally isotropic equivalently: totally degenerate subspaces.() 10. Prove that a two-dimensional nonsingular orthogonal geometry is a hyperbolic plane if and only if its discriminant is . 11. Does Minkowski space contain any isotropic vectors? If so, find them. 12. Is Minkowski space isometric to Euclidean space ? Metric Vector Spaces: The Theory of Bilinear Forms 299 13. If is a symmetric bilinear form on and , show that char is a quadratic form. 14. Let be a vector space over a field , with ordered basis . Let be a polynomial of degree over , that is, homogeneous a polynomial each of whose terms has degree . The defined by -form is the function from to defined as follows. If , then ()We use the same notation for the form and the polynomial. Prove that - forms are the same as quadratic forms. 15. Show that is an isometry on if and only if where is the quadratic form associated with the bilinear form on . Assume that ( char . 16. Show that a quadratic form on satisfies the parallelogram law: 17. Show that if is a nonsingular orthogonal geometry over a field , with char , then any totally isotropic subspace of is also a totally degenerate space. 18. Is it true that ? rad rad 19. Let be a nonsingular symplectic geometry and let be a symplectic transvection. Prove that a ) b For any symplectic transformation ,) c For ,) d For a fixed , the map is an isomorphism from the) additive group of onto the group Sp . 20. Prove that if is any nonsquare in a finite field , then all nonsquares have the form , for some . Hence, the product of any two nonsquares in is a square. 21. Formulate Sylvester's law of inertia in terms of quadratic forms on . 22. Show that a two-dimensional space is a hyperbolic plane if and only if it is nonsingular and contains an isotropic vector. Assume that .char 23. Prove directly that a hyperbolic plane in an orthogonal geometry cannot have an orthogonal basis when .char 24. a Let be a subspace of . Show that the inner product) on the quotient space is well-defined if and only if . rad b If , when is nonsingular?)rad 25. Let , where is a totally degenerate space. 300 Advanced Linear Algebra a Prove that if and only if is nonsingular.)rad b If is nonsingular, prove that .)rad 26. Let . Prove that impliesdim dim rad rad . 27. Let . Prove that a ) rad rad rad b ) rad rad rad c ) rad rad raddim dim dim d is nonsingular if and only if and are both nonsingular.) 28 Because the Riesz. Let be a nonsingular metric vector space. representation theorem is valid in , we can define the adjoint of a linear map exactly as in the case of real inner product spaces. Prove that is an isometry if and only if it is bijective and unitary that is, ( ). 29. If , prove that is an isometry if and only if it ischar bijective and for all . 30. Let be a basis for . Prove that is an isometry if and only if it is bijective and for all . 31. Let be a linear operator on a metric vector space . Let be an ordered basis for and let be the matrix of the form relative to . Prove that is an isometry if and only if 32. Let be a nonsingular orthogonal geometry and let be an isometry. a Show that .)imdim ker dim b Show that . How would you describe)imker ker in words? c If is a symmetry, what is ?) dim ker d Can you characterize symmetries by means of ?)dim ker 33. A linear transformation is called if is nilpotent. unipotent Suppose that is a nonisotropic metric vector space and that is unipotent and isometric. Show that . 34. Let be a hyperbolic space of dimension and let be a hyperbolic subspace of of dimension . Show that for each , there is a hyperbolic subspace of for which . 35. Let . Prove that if is a totally degenerate subspace of anchar orthogonal geometry , then . dim dim 36. Prove that an orthogonal geometry of dimension is a hyperbolic space if and only if is nonsingular, is even and contains a totally degenerate subspace of dimension . 37. Prove that a symplectic transformation has determinant equal to . Chapter 12 Metric Spaces The Definition In Chapter 9, we studied the basic properties of real and complex inner product spaces. Much of what we did does not depend on whether the space in question is finite-dimensional or infinite-dimensional. However, as we discussed in Chapter 9, the presence of an inner product and hence a metric, on a vector space, raises a host of new issues related to convergence. In this chapter, we discuss briefly the concept of a metric space. This will enable us to study the convergence properties of real and complex inner product spaces. A metric space is not an algebraic structure. Rather it is designed to model the abstract properties of distance. Definition A is a pair , where is a nonempty set andmetric space is a real-valued function, called a on , with themetric following properties. The expression is read “the distance from to .” 1 For all ,)( )Positive definiteness and if and only if . 2 For all ,)( )Symmetry 3 For all ,)( )Triangle inequality As is customary, when there is no cause for confusion, we simply say “let be a metric space.” 302 Advanced Linear Algebra Example 12.1 Any nonempty set is a metric space under the discrete metric, defined by if if Example 12.2 1 The set is a metric space, under the metric defined for ) and by This is called the on . We note that is also a metricEuclidean metric space under the metric Of course, and are different metric spaces. 2 The set is a metric space under the ) unitary metric where and are in . Example 12.3 1 The set of all real-valued or complex-valued continuous functions)( ) on is a metric space, under the metric sup We refer to this metric as the .sup metric 2 The set of all real-valued or complex-valued continuous functions)) on is a metric space, under the metric Example 12.4 Many important sequence spaces are metric spaces. We will often use boldface italic letters to denote sequences, as in and . 1 The set of all bounded sequences of real numbers is a metric space) under the metric defined by sup The set of all bounded complex sequences, with the same metric, is also a metric space. As is customary, we will usually denote both of these spaces by . Metric Spaces 303 2 For , let be the set of all sequences of real or complex)) numbers for which We define the of by-norm Then is a metric space, under the metric The fact that is a metric follows from some rather famous results about sequences of real or complex numbers, whose proofs we leave as well-( hinted exercises.) Let and . If and ,Holder's inequality¨ then the product sequence is in and that is, A special case of this with 2 is the () Cauchy–Schwarz inequality Minkowski's inequality For , if then the sum is in and that is, 304 Advanced Linear Algebra If is a metric space under a metric , then any nonempty subset of is also a metric under the restriction of to . The metric space thus obtained is called a of .subspace Open and Closed Sets Definition Let be a metric space. Let and let be a positive real number. 1 The centered at , with radius , is) open ball 2 The centered at , with radius , is) closed ball 3 The centered at , with radius , is) sphere Definition A subset of a metric space is said to be if each point of open is the center of an open ball that is contained completely in . More specifically, is open if for all , there exists an such that . Note that the empty set is open. A set is if itsclosed complement in is open. It is easy to show that an open ball is an open set and a closed ball is a closed set. If , we refer to any open set containing as an open neighborhood of . It is also easy to see that a set is open if and only if it contains an open neighborhood of each of its points. The next example shows that it is possible for a set to be both open and closed, or neither open nor closed. Example 12.5 In the metric space with the usual Euclidean metric, the open balls are just the open intervals and the closed balls are the closed intervals Consider the half-open interval , for . This set is not open, since it contains no open ball centered at and it is not closed, since its complement is not open, since it contains no open ball about . Metric Spaces 305 Observe also that the empty set is both open and closed, as is the entire space . (Although we will not do so, it is possible to show that these are the only two sets that are both open and closed in . It is not our intention to enter into a detailed discussion of open and closed sets, the subject of which belongs to the branch of mathematics known as .topology In order to put these concepts in perspective, however, we have the following result, whose proof is left to the reader. Theorem 12.1 The collection of all open subsets of a metric space has the following properties: 1 , ) 2 If , then ) 3 If is any collection of open sets, then .) These three properties form the basis for an axiom system that is designed to generalize notions such as convergence and continuity and leads to the following definition. Definition Let be a nonempty set. A collection of subsets of is called a topology for if it has the following properties: 1) 2 If then ) 3 If is any collection of sets in , then .) We refer to subsets in as and the pair as a open sets topological space. According to Theorem 12.1, the open sets as we defined them earlier in a() metric space form a topology for , called the topology by the induced metric. Topological spaces are the most general setting in which we can define concepts such as convergence and continuity, which is why these concepts are called topological concepts. However, since the topologies with which we will be dealing are induced by a metric, we will generally phrase the definitions of the topological properties that we will need directly in terms of the metric. Convergence in a Metric Space Convergence of sequences in a metric space is defined as follows. Definition A sequence in a metric space to , written converges , if lim Equivalently, if for any , there exists an such that 306 Advanced Linear Algebra or equivalently, In this case, is called the of the sequence . limit If is a metric space and is a subset of , by a , we mean a sequence in sequence whose terms all lie in . We next characterize closed sets and therefore also open sets, using convergence. Theorem 12.2 Let be a metric space. A subset is closed if and only if whenever is a sequence in and , then . In loose terms, a subset is closed if it is closed under the taking of sequential limits. Proof. Suppose that is closed and let , where for all . Suppose that . Then since and is open, there exists an for which . But this implies that which contradicts the fact that . Hence, . Conversely, suppose that is closed under the taking of limits. We show that is open. Let and suppose to the contrary that no open ball about is contained in . Consider the open balls , for all . Since none of these balls is contained in , for each , there is an . It is clear that and so . But cannot be in both and . This contradiction implies that is open. Thus, is closed. The Closure of a Set Definition Let be any subset of a metric space . The of , denoted closure by , is the smallest closed set containing .cl We should hasten to add that, since the entire space is closed and since the intersection of any collection of closed sets is closed exercise , the closure of() any set does exist and is the intersection of all closed sets containing . The following definition will allow us to characterize the closure in another way. Definition Let be a nonempty subset of a metric space . An element is said to be a , or , of if every open balllimit point accumulation point centered at meets at a point other than itself. Let us denote the set of all limit points of by . Here are some key facts concerning limit points and closures. Metric Spaces 307 Theorem 12.3 Let be a nonempty subset of a metric space . 1 if and only if there is a sequence in for which for) all and . 2 is closed if and only if . In words, is closed if and only if it) contains all of its limit points. 3 .)cl 4 if and only if there is a sequence in for which .)cl Proof. For part 1 , assume first that . For each , there exists a point) such that . Thus, we have and so . For the converse, suppose that , where . If is any ball centered at , then there is some such that implies . Hence, for any ball centered at , there is a point such that . Thus, is a limit point of . As for part 2 , if is closed, then by part 1 , any is the limit of a)) sequence in and so must be in . Hence, . Conversely, if , then is closed. For if is any sequence in and , then there are two possibilities. First, we might have for some , in which case . Second, we might have for all , in which case implies that . In either case, and so is closed under the taking of limits, which implies that is closed. For part 3 , let . Clearly, . To show that is closed, we) show that it contains all of its limit points. So let . Hence, there is a sequence for which and . Of course, each is either in , or is a limit point of . We must show that , that is, that is either in or is a limit point of . Suppose for the purposes of contradiction that and . Then there is a ball for which . However, since , there must be an . Since cannot be in , it must be a limit point of . Referring to Figure 12.1, if , then consider the ball . This ball is completely contained in and must contain an element of , since its center is a limit point of . But then , a contradiction. Hence, or . In either case, and so is closed. Thus, is closed and contains and so . On the other hand, cl cl cl and so . 308 Advanced Linear Algebra Figure 12.1 For part 4 , if , then there are two possibilities. If , then the)cl constant sequence , with for all , is a sequence in that converges to . If , then and so there is a sequence in for which and . In either case, there is a sequence in converging to . Conversely, if there is a sequence in for which , then either for some , in which case , or else for all , incl which case . cl Dense Subsets The following concept is meant to convey the idea of a subset being “arbitrarily close” to every point in . Definition A subset of a metric space is in if . A dense cl metric space is said to be if it contains a dense subset.separable countable Thus, a subset of is dense if every open ball about any point contains at least one point of . Certainly, any metric space contains a dense subset, namely, the space itself. However, as the next examples show, not every metric space contains a countable dense subset. Example 12.6 1 The real line is separable, since the rational numbers form a countable) dense subset. Similarly, is separable, since the set is countable and dense. 2 The complex plane is separable, as is for all .) 3 A discrete metric space is separable if and only if it is countable. We leave) proof of this as an exercise. Metric Spaces 309 Example 12.7 The space is not separable. Recall that is the set of all bounded sequences of real numbers or complex numbers with metric() sup To see that this space is not separable, consider the set of all binary sequences or for all This set is in one-to-one correspondence with the set of all subsets of and so is uncountable. It has cardinality 2 . Now, each sequence in is( certainly bounded and so lies in . Moreover, if , then the two sequences must differ in at least one position and so . In other words, we have a subset of that is uncountable and for which the distance between any two distinct elements is . This implies that the balls in the uncountable collection are mutually disjoint. Hence, no countable set can meet every ball, which implies that no countable set can be dense in . Example 12.8 The metric spaces are separable, for . The set of all sequences of the form for all , where the 's are rational, is a countable set. Let us show that it is dense in . Any satisfies Hence, for any , there exists an such that Since the rational numbers are dense in , we can find rational numbers for which for all . Hence, if , then which shows that there is an element of arbitrarily close to any element of . Thus, is dense in and so is separable. 310 Advanced Linear Algebra Continuity Continuity plays a central role in the study of linear operators on infinite- dimensional inner product spaces. Definition Let be a function from the metric space to the metric space . We say that is if for any , continuous at there exists a such that or, equivalently, ()See Figure 12.2. A function is if it is continuous at everycontinuous . Figure 12.2 We can use the notion of convergence to characterize continuity for functions between metric spaces. Theorem 12.4 A function is continuous if and only if whenever is a sequence in that converges to , then the sequence converges to , in short, Proof. Suppose first that is continuous at and let . Then, given , the continuity of implies the existence of a such that Since , there exists an such that for and so Thus, . Conversely, suppose that implies . Suppose, for the purposes of contradiction, that is not continuous at . Then there exists an Metric Spaces 311 such that for all , Thus, for all , and so we may construct a sequence by choosing each term with the property that , but Hence, , but does not converge to . This contradiction implies that must be continuous at . The next theorem says that the distance function is a continuous function in both variables. Theorem 12.5 Let be a metric space. If and , then . Proof. We leave it as an exercise to show that But the right side tends to as and so . Completeness The reader who has studied analysis will recognize the following definitions. Definition A sequence in a metric space is a if for Cauchy sequence any , there exists an for which We leave it to the reader to show that any convergent sequence is a Cauchy sequence. When the converse holds, the space is said to be .complete Definition Let be a metric space. 1 is said to be if every Cauchy sequence in converges in .) complete 2 A subspace of is if it is complete as a metric space. Thus, ) complete is complete if every Cauchy sequence in converges to an element in . Before considering examples, we prove a very useful result about completeness of subspaces. 312 Advanced Linear Algebra Theorem 12.6 Let be a metric space. 1 Any complete subspace of is closed.) 2 If is complete, then a subspace of is complete if and only if it is) closed. Proof. To prove 1 , assume that is a complete subspace of . Let be a) sequence in for which . Then is a Cauchy sequence in and since is complete, must converge to an element of . Since limits of sequences are unique, we have . Hence, is closed. To prove part 2 , first assume that is complete. Then part 1 shows that is)) closed. Conversely, suppose that is closed and let be a Cauchy sequence in . Since is also a Cauchy sequence in the complete space , it must converge to some . But since is closed, we have . Hence, is complete. Now let us consider some examples of complete and incomplete metric spaces.() Example 12.9 It is well known that the metric space is complete. However, a ( proof of this fact would lead us outside the scope of this book. Similarly, the complex numbers are complete. Example 12.10 The Euclidean space and the unitary space are complete. Let us prove this for . Suppose that is a Cauchy sequence in , where Thus, as and so, for each coordinate position , which shows that the sequence of th coordinates is a Cauchy 2 sequence in . Since is complete, we must have as If , then as and so . This proves that is complete. Metric Spaces 313 Example 12.11 The metric space of all real-valued or complex- ( valued continuous functions on , with metric) sup is complete. To see this, we first observe that the limit with respect to is the uniform limit on , that is if and only if for any , there is an for which for all Now let be a Cauchy sequence in . Thus, for any , there is an for which for all 12.1() This implies that, for each , the sequence is a Cauchy sequence of real or complex numbers and so it converges. We can therefore define a() function on by lim Letting in 12.1 , we get () for all Thus, converges to uniformly. It is well known that the uniform limit of continuous functions is continuous and so . Thus, and so is complete. Example 12.12 The metric space of all real-valued or complex- ( valued continuous functions on , with metric) is not complete. For convenience, we take and leave the general case for the reader. Consider the sequence of functions whose graphs are shown in Figure 12.3. The definition of should be clear from the graph.( 314 Advanced Linear Algebra Figure 12.3 We leave it to the reader to show that the sequence is Cauchy, but does not converge in . The sequence converges to a function that is not ( continuous.) Example 12.13 The metric space is complete. To see this, suppose that is a Cauchy sequence in , where 2 Then, for each coordinate position , we have sup as 12.2() Hence, for each , the sequence of th coordinates is a Cauchy sequence in or . Since or is complete, we have() () as for each coordinate position . We want to show that and that . Letting in 12.2 gives ) sup as 12.3 () and so, for some , for all and so for all But since , it is a bounded sequence and therefore so is . That is, . Since 12.3 implies that , we see that is) complete. Metric Spaces 315 Example 12.14 The metric space is complete. To prove this, let be a Cauchy sequence in , where 2 Then, for each coordinate position , which shows that the sequence of th coordinates is a Cauchy sequence in or . Since or is complete, we have() () as We want to show that and that . To this end, observe that for any , there is an for which for all . Now we let , to get for all . Letting , we get, for any , which implies that and so and in addition, . As we will see in the next chapter, the property of completeness plays a major role in the theory of inner product spaces. Inner product spaces for which the induced metric space is complete are called .Hilbert spaces Isometries A function between two metric spaces that preserves distance is called an isometry. Here is the formal definition. Definition Let and be metric spaces. A function is called an ifisometry 316 Advanced Linear Algebra for all . If is a bijective isometry from to , we say that and are and write . isometric Theorem 12.7 Let be an isometry. Then 1 is injective) 2 is continuous) 3 is also an isometry and hence also continuous.) Proof. To prove 1 , we observe that) To prove 2 , let in . Then) as and so , which proves that is continuous. Finally, we have and so is an isometry. The Completion of a Metric Space While not all metric spaces are complete, any metric space can be embedded in a complete metric space. To be more specific, we have the following important theorem. Theorem 12.8 Let be any metric space. Then there is a complete metric space and an isometry for which is dense in . The metric space is called a of . Moreover,completion is unique, up to bijective isometry. Proof. The proof is a bit lengthy, so we divide it into various parts. We can simplify the notation considerably by thinking of sequences in as functions , where . Cauchy Sequences in The basic idea is to let the elements of be equivalence classes of Cauchy sequences in . So let denote the set of all Cauchy sequences in . If CS CS , then, intuitively speaking, the terms get closer together as and so do the terms . Therefore, it seems reasonable that should approach a finite limit as . Indeed, since as it follows that is a Cauchy sequence of real numbers, which implies that Metric Spaces 317 lim ()12.4 (That is, the limit exists and is finite. Equivalence Classes of Cauchy Sequences in We would like to define a metric on the set by CS lim However, it is possible that lim for distinct sequences and , so this does not define a metric. Thus, we are led to define an equivalence relation on byCS lim Let be the set of all equivalence classes of Cauchy sequences andCS define, for , CS lim ( )12.5 where and . To see that is well-defined, suppose that and . Then since and , we have as . Thus, and lim lim which shows that is well-defined. To see that is a metric, we verify the triangle inequality, leaving the rest to the reader. If and are Cauchy sequences, then Taking limits gives lim lim lim 318 Advanced Linear Algebra and so Embedding in For each , consider the constant Cauchy sequence , where for all . The map defined by is an isometry, since lim Moreover, is dense in . This follows from the fact that we can approximate any Cauchy sequence in by a constant sequence. In particular, let . Since is a Cauchy sequence, for any , there exists an such that Now, for the constant sequence we have lim and so is dense in . Is Complete Suppose that 3 is a Cauchy sequence in . We wish to find a Cauchy sequence in for which lim as Since and since is dense in , there is a constant sequence for which Metric Spaces 319 We can think of as a constant approximation to , with error at most . Let be the sequence of these constant approximations: This is a Cauchy sequence in . Intuitively speaking, since the 's get closer to each other as , so do the constant approximations. In particular, we have as . To see that converges to , observe that lim lim Now, since is a Cauchy sequence, for any , there is an such that In particular, lim and so which implies that , as desired. Uniqueness Finally, we must show that if and are both completions of , then . Note that we have bijective isometries and Hence, the map is a bijective isometry from onto , where is dense in . See Figure 12.4. 320 Advanced Linear Algebra Figure 12.4 Our goal is to show that can be extended to a bijective isometry from to . Let . Then there is a sequence in for which . Since is a Cauchy sequence in , is a Cauchy sequence in and since is complete, we have for some . Let us define . To see that is well-defined, suppose that and , where both sequences lie in . Then as and so and converge to the same element of , which implies that does not depend on the choice of sequence in converging to . Thus, is well-defined. Moreover, if , then the constant sequence converges to and so lim , which shows that is an extension of . To see that is an isometry, suppose that and . Then and and since is continuous, we have lim lim Thus, we need only show that is surjective. Note first that im im im. Thus, if is closed, we can deduce from the fact that is dense in that . So, suppose that is a im sequence in and . Then is a Cauchy sequence andim therefore so is . Thus, . But is continuous and so , which implies that and so . Hence, isim surjective and . Metric Spaces 321 Exercises 1. Prove the generalized triangle inequality 3 2. a Use the triangle inequality to prove that) b Prove that) 3. Let be the subspace of all binary sequences sequences of 's and ( 's . Describe the metric on .) 4. Let be the set of all binary -tuples. Define a function by letting be the number of positions in which and differ. For example, . Prove that is a metric. It( is called the and plays an important role inHamming distance function the theory of error-correcting codes. 5. Let . a If show that ) b Find a sequence that converges to but is not an element of any for) . 6. a Show that if , then for all .) b Find a sequence that is in for , but is not in .) 7. Show that a subset of a metric space is open if and only if contains an open neighborhood of each of its points. 8. Show that the intersection of any collection of closed sets in a metric space is closed. 9. Let be a metric space. The of a nonempty subset diameter is sup A set is if . bounded a Prove that is bounded if and only if there is some and ) for which . b Prove that if and only if consists of a single point.) c Prove that implies .) d If and are bounded, show that is also bounded.) 10. Let be a metric space. Let be the function defined by 322 Advanced Linear Algebra a Show that is a metric space and that is bounded under this) metric, even if it is not bounded under the metric . b Show that the metric spaces and have the same open) sets. 11. If and are subsets of a metric space , we define the distance between and by inf a Is it true that if and only if ? Is a metric?) b Show that if and only if .)cl 12. Prove that is a limit point of if and only if every neighborhood of meets in a point other than itself. 13. Prove that is a limit point of if and only if every open ball contains infinitely many points of . 14. Prove that limits are unique, that is, , implies that . 15. Let be a subset of a metric space . Prove that if and only if cl there exists a sequence in that converges to . 16. Prove that the closure has the following properties: a )cl b )cl cl c )cl cl cl d )cl cl cl Can the last part be strengthened to equality? 17. a Prove that the closed ball is always a closed subset.) b Find an example of a metric space in which the closure of an open ball) is not equal to the closed ball . 18. Provide the details to show that is separable. 19. Prove that is separable. 20. Prove that a discrete metric space is separable if and only if it is countable. 21. Prove that the metric space of all bounded functions on , with metric sup is not separable. 22. Show that a function is continuous if and only if the inverse image of any open set is open, that is, if and only if is open in whenever is an open set in . 23. Repeat the previous exercise, replacing the word open by the word closed. 24. Give an example to show that if is a continuous function and is an open set in , it need not be the case that is open in . Metric Spaces 323 25. Show that any convergent sequence is a Cauchy sequence. 26. If in a metric space , show that any subsequence of also converges to . 27. Suppose that is a Cauchy sequence in a metric space and that some subsequence of converges. Prove that converges to the same limit as the subsequence. 28. Prove that if is a Cauchy sequence, then the set is bounded. What about the converse? Is a bounded sequence necessarily a Cauchy sequence? 29. Let and be Cauchy sequences in a metric space . Prove that the sequence converges. 30. Show that the space of all convergent sequences of real numbers or complex numbers is complete as a subspace of .) 31. Let denote the metric space of all polynomials over , with metric sup Is complete? 32. Let be the subspace of all sequences with finite support that is, ( with a finite number of nonzero terms . Is complete?) 33. Prove that the metric space of all integers, with metric , is complete. 34. Show that the subspace of the metric space under the sup metric () consisting of all functions for which is complete. 35. If and is complete, show that is also complete. 36. Show that the metric spaces and , under the sup metric, are isometric. 37. Prove Ho¨lder's inequality as follows: a Show that ) b Let and be positive real numbers and consider the rectangle in) with corners , , and , with area . Argue geometrically that is, draw a picture to show that() and so c Now let and . Apply the results of) part b to) 324 Advanced Linear Algebra and then sum on to deduce Ho¨lder's inequality. 38. Prove Minkowski's inequality as follows: a Prove it for first.) b Assume . Show that) c Sum this from to and apply Ho¨lder's inequality to each sum on) the right, to get Divide both sides of this by the last factor on the right and let to deduce Minkowski's inequality. 39. Prove that is a metric space. Chapter 13 Hilbert Spaces Now that we have the necessary background on the topological properties of metric spaces, we can resume our study of inner product spaces without qualification as to dimension. As in Chapter 9, we restrict attention to real and complex inner product spaces. Hence will denote either or . A Brief Review Let us begin by reviewing some of the results from Chapter 9. Recall that an inner product space over is a vector space , together with an inner product . If , then the inner product is bilinear and if , the inner product is sesquilinear. An inner product induces a norm on , defined by We recall in particular the following properties of the norm. Theorem 13.1 1 For all ,)( )The Cauchy-Schwarz inequality with equality if and only if for some . 2 For all ,)( )The triangle inequality with equality if and only if for some . 3)( )The parallelogram law We have seen that the inner product can be recovered from the norm, as follows. 326 Advanced Linear Algebra Theorem 13.2 1 If is a real inner product space, then) 2 If is a complex inner product space, then) The inner product also induces a metric on defined by Thus, any inner product space is a metric space. Definition Let and be inner product spaces and let . 1 is an if it preserves the inner product, that is, if) isometry for all . 2 A bijective isometry is called an . When ) isometric isomorphism is an isometric isomorphism, we say that and are isometrically isomorphic. It is easy to see that an isometry is always injective but need not be surjective, even if . Theorem 13.3 A linear transformation is an isometry if and only if it preserves the norm, that is, if and only if for all . The following result points out one of the main differences between real and complex inner product spaces. Theorem 13.4 Let be an inner product space and let . 1 If for all , then .) 2 If is a complex inner product space and for all) , then . 3 Part 2 does not hold in general for real inner product spaces.)) Hilbert Spaces Since an inner product space is a metric space, all that we learned about metric spaces applies to inner product spaces. In particular, if is a sequence of Hilbert Spaces 327 vectors in an inner product space , then if and only if as The fact that the inner product is continuous as a function of either of its coordinates is extremely useful. Theorem 13.5 Let be an inner product space. Then 1) 2) Complete inner product spaces play an especially important role in both theory and practice. Definition An inner product space that is complete under the metric induced by the inner product is said to be a .Hilbert space Example 13.1 One of the most important examples of a Hilbert space is the space . Recall that the inner product is defined by (In the real case, the conjugate is unnecessary. The metric induced by this inner product is 2 which agrees with the definition of the metric space given in Chapter 12. In other words, the metric in Chapter 12 is induced by this inner product. As we saw in Chapter 12, this inner product space is complete and so it is a Hilbert space. In fact, it is the prototype of all Hilbert spaces, introduced by David( Hilbert in 1912, even before the axiomatic definition of Hilbert space was given by John von Neumann in 1927. The previous example raises the question whether the other metric spaces ), with distance given by 13.1() are complete inner product spaces. The fact is that they are not even inner product spaces! More specifically, there is no inner product whose induced metric is given by 13.1 . To see this, observe that, according to Theorem 13.1,() 328 Advanced Linear Algebra any norm that comes from an inner product must satisfy the parallelogram law But the norm in 13.1 does not satisfy this law. To see this, take() and . Then and Thus, the left side of the parallelogram law is and the right side is , 2 which equals if and only if . Just as any metric space has a completion, so does any inner product space. Theorem 13.6 Let be an inner product space. Then there exists a Hilbert space and an isometry for which is dense in . Moreover, is unique up to isometric isomorphism. Proof. We know that the metric space , where is induced by the inner product, has a unique completion , which consists of equivalence classes of Cauchy sequences in . If and , then we set and lim It is easy to see that since and are Cauchy sequences, so are and . In addition, these definitions are well-defined, that is, they are independent of the choice of representative from each equivalence class. For instance, if , then lim and so (The Cauchy sequence is bounded. Hence, lim lim We leave it to the reader to show that is an inner product space under these operations. Hilbert Spaces 329 Moreover, the inner product on induces the metric , since lim lim Hence, the metric space isometry is an isometry of inner product spaces, since Thus, is a complete inner product space and is a dense subspace of that is isometrically isomorphic to . We leave the issue of uniqueness to the reader. The next result concerns subspaces of inner product spaces. Theorem 13.7 1 Any complete subspace of an inner product space is closed.) 2 A subspace of a Hilbert space is a Hilbert space if and only if it is closed.) 3 Any finite-dimensional subspace of an inner product space is closed and) complete. Proof. Parts 1 and 2 follow from Theorem 12.6. Let us prove that a finite-)) dimensional subspace of an inner product space is closed. Suppose that is a sequence in , and . Let be an orthonormal Hamel basis for . The Fourier expansion in has the property that but Thus, if we write and , the sequence , which is in , converges to a vector that is orthogonal to . But this is impossible, because implies that This proves that is closed. To see that any finite-dimensional subspace of an inner product space is complete, let us embed as an inner product space in its own right in its () completion . Then or rather an isometric copy of is a finite-dimensional () 330 Advanced Linear Algebra subspace of a complete inner product space and as such it is closed. However, is dense in and so , which shows that is complete. Infinite Series Since an inner product space allows both addition of vectors and convergence of sequences, we can define the concept of infinite sums, or infinite series. Definition Let be an inner product space. The of theth partial sum sequence in is If the sequence of partial sums converges to a vector , that is, if as then we say that the series to and write converges We can also define absolute convergence. Definition A series is said to be if the series absolutely convergent converges. The key relationship between convergence and absolute convergence is given in the next theorem. Note that completeness is required to guarantee that absolute convergence implies convergence. Theorem 13.8 Let be an inner product space. Then is complete if and only if absolute convergence of a series implies convergence. Proof. Suppose that is complete and that . Then the sequence of partial sums is a Cauchy sequence, for if , we have Hence, the sequence converges, that is, the series converges. Conversely, suppose that absolute convergence implies convergence and let be a Cauchy sequence in . We wish to show that this sequence converges. Since is a Cauchy sequence, for each , there exists an Hilbert Spaces 331 with the property that Clearly, we can choose , in which case and so Thus, according to hypothesis, the series converges. But this is a telescoping series, whose th partial sum is and so the subsequence converges. Since any Cauchy sequence that has a convergent subsequence must itself converge, the sequence converges and so is complete. An Approximation Problem Suppose that is an inner product space and that is a subset of . It is of considerable interest to be able to find, for any , a vector in that is closest to in the metric induced by the inner product, should such a vector exist. This is the for .approximation problem Suppose that and let inf Then there is a sequence for which as shown in Figure 13.1. 332 Advanced Linear Algebra Figure 13.1 Let us see what we can learn about this sequence. First, if we let , then according to the parallelogram law, or ()13.2 Now, if the set is , that is, if convex for all ()in words, contains the line segment between any two of its points , then and so Thus, 13.2 gives() as . Hence, if is convex, then the sequence is a Cauchy sequence and therefore so is . If we also require that be complete, then the Cauchy sequence converges to a vector and by the continuity of the norm, we must have . Let us summarize and add a remark about uniqueness. Theorem 13.9 Let be an inner product space and let be a complete convex subset of . Then for any , there exists a unique for which inf The vector is called the to in . best approximation Hilbert Spaces 333 Proof. Only the uniqueness remains to be established. Suppose that Then, by the parallelogram law, 2 and so . Since any subspace of an inner product space is convex, Theorem 13.9 applies to complete subspaces. However, in this case, we can say more. Theorem 13.10 Let be an inner product space and let be a complete subspace of . Then for any , the best approximation to in is the unique vector for which . Proof. Suppose that , where . Then for any , we have and so Hence is the best approximation to in . Now we need only show that , where is the best approximation to in . For any , a little computation reminiscent of completing the square gives 2 Now, this is smallest when 334 Advanced Linear Algebra in which case Replacing by gives But is the best approximation to in and since we must have Hence, or equivalently, Hence, . According to Theorem 13.9, if is a complete subspace of an inner product space , then for any , we may write where and . Hence, and since , we also have . This is the projection theorem for arbitrary inner product spaces. Theorem 13.11 The projection theorem() If is a complete subspace of an inner product space , then In particular, if is a closed subspace of a Hilbert space , then Theorem 13.12 Let , and be subspaces of an inner product space . 1 If then .) 2 If then .) Proof. If , then by definition of orthogonal direct sum. On the other hand, if , then , for some and . Hence, Hilbert Spaces 335 and so , implying that . Thus, . Part 2 follows from part ) 1.) Let us denote the closure of the span of a set of vectors by .cspan Theorem 13.13 Let be a Hilbert space. 1 If is a subset of , then) cspan 2 If is a subspace of , then) cl 3 If is a closed subspace of , then) Proof. We leave it as an exercise to show that . Hence cspan cspan cspan cspan But since is closed, we also have and so by Theorem 13.12, . The rest follows easily from partcspan 1.) In the exercises, we provide an example of a closed subspace of an inner product space for which . Hence, we cannot drop the requirement that be a Hilbert space in Theorem 13.13. Corollary 13.14 If is a of a Hilbert space , then is dense in subset span if and only if . Proof. As in the previous proof, cspan and so if and only if . cspan Hilbert Bases We recall the following definition from Chapter 9. Definition A maximal orthonormal set in a Hilbert space is called a Hilbert basis for . Zorn's lemma can be used to show that any nontrivial Hilbert space has a Hilbert basis. Again, we should mention that the concepts of Hilbert basis and Hamel basis a maximal linearly independent set are quite different. We will show() 336 Advanced Linear Algebra later in this chapter that any two Hilbert bases for a Hilbert space have the same cardinality. Since an orthonormal set is maximal if and only if , Corollary 13.14 gives the following characterization of Hilbert bases. Theorem 13.15 Let be an orthonormal subset of a Hilbert space . The following are equivalent: 1 is a Hilbert basis) 2) 3 is a of , that is, .)cspantotal subset Part 3 of this theorem says that a subset of a Hilbert space is a Hilbert basis if) and only if it is a total orthonormal set. Fourier Expansions We now want to take a closer look at best approximations. Our goal is to find an explicit expression for the best approximation to any vector from within a closed subspace of a Hilbert space . We will find it convenient to consider three cases, depending on whether has finite, countably infinite, or uncountable dimension. The Finite-Dimensional Case Suppose that is an orthonormal set in a Hilbert space . Recall that the Fourier expansion of any , with respect to , is given by where is the Fourier coefficient of with respect to . Observe that and so span . Thus, according to Theorem 13.9, the Fourier expansion is the best approximation to in . Moreover, since span , we have and so with equality if and only if , which happens if and only if . span Let us summarize. Hilbert Spaces 337 Theorem 13.16 Let be a finite orthonormal set in a Hilbert space . For any , the Fourier expansion of is the best approximation to in . We also have span Bessel's inequality or equivalently, ()13.3 with equality if and only if . span The Countably Infinite-Dimensional Case In the countably infinite case, we will be dealing with infinite sums and so questions of convergence will arise. Thus, we begin with the following. Theorem 13.17 Let be a countably infinite orthonormal set in a Hilbert space . The series ()13.4 converges in if and only if the series ()13.5 converges in . If these series converge, then they converge unconditionally (that is, any series formed by rearranging the order of the terms also converges . Finally, if the series 13.4 converges, then)( ) Proof. Denote the partial sums of the first series by and the partial sums of the second series by . Then for Hence is a Cauchy sequence in if and only if is a Cauchy sequence in . Since both and are complete, converges if and only if converges. If the series 13.5 converges, then it converges absolutely and hence() unconditionally. A real series converges unconditionally if and only if it( 338 Advanced Linear Algebra converges absolutely. But if 13.5 converges unconditionally, then so does () ()13.4 . The last part of the theorem follows from the continuity of the norm. Now let be a countably infinite orthonormal set in . The Fourier expansion of a vector is defined to be the sum ()13.6 To see that this sum converges, observe that for any , 13.3 gives () and so which shows that the series on the left converges. Hence, according to Theorem 13.17, the Fourier expansion 13.6 converges unconditionally.() Moreover, since the inner product is continuous, and so . Hence, is the best approximation span cspan to in . Finally, since , we again have cspan and so with equality if and only if , which happens if and only if . cspan Thus, the following analog of Theorem 13.16 holds. Theorem 13.18 Let be a countably infinite orthonormal set in a Hilbert space . For any , the Fourier expansion of converges unconditionally and is the best approximation to in . cspan We also have Bessel's inequality Hilbert Spaces 339 or equivalently, with equality if and only if . cspan The Arbitrary Case To discuss the case of an arbitrary orthonormal set , let us first define and discuss the concept of the sum of an arbitrary number of terms. (This is a bit of a digression, since we could proceed without all of the coming details but they are interesting. Definition Let be an arbitrary family of vectors in an inner product space . The sum is said to to a vector and we writeconverge ()13.7 if for any , there exists a finite set for which finite For those readers familiar with the language of convergence of nets, the set of all finite subsets of is a under inclusion for everydirected set ( there is a containing and and the function) is a net in . Convergence of 13.7 is convergence of this net. In any case, we ) will refer to the preceding definition as the of convergence.net definition It is not hard to verify the following basic properties of net convergence for arbitrary sums. Theorem 13.19 Let be an arbitrary family of vectors in an inner product space . If and then 340 Advanced Linear Algebra 1)( )Linearity for any 2)( )Continuity and The next result gives a useful “Cauchy-type” description of convergence. Theorem 13.20 Let be an arbitrary family of vectors in an inner product space . 1 If the sum) converges, then for any , there exists a finite set such that finite 2 If is a Hilbert space, then the converse of 1 also holds.)) Proof. For part 1 , given , let , finite, be such that) finite 2 If , finite, then 22 As for part 2 , for each , let be a finite set for which) finite and let Hilbert Spaces 341 Then is a Cauchy sequence, since Since is assumed complete, we have . Now, given , there exists an such that 2 Setting gives for finite, max and so converges to . The following theorem tells us that convergence of an arbitrary sum implies that only countably many terms can be nonzero so, in some sense, there is no such thing as a nontrivial sum.uncountable Theorem 13.21 Let be an arbitrary family of vectors in an inner product space . If the sum converges, then at most a countable number of terms can be nonzero. Proof. According to Theorem 13.20, for each , we can let , finite, be such that finite Let . Then is countable and for all for all 342 Advanced Linear Algebra Here is the analog of Theorem 13.17. Theorem 13.22 Let be an arbitrary orthonormal family of vectors in a Hilbert space . The two series and converge or diverge together. If these series converge, then Proof. The first series converges if and only if for every , there exists a finite set such that finite or equivalently, finite and this is precisely what it means for the second series to converge. We leave proof of the remaining statement to the reader. The following is a useful characterization of arbitrary sums of nonnegative real terms. Theorem 13.23 Let be a collection of nonnegative real numbers. Then sup finite ()13.8 provided that either of the preceding expressions is finite. Proof. Suppose that sup finite Then, for any , there exists a finite set such that Hilbert Spaces 343 Hence, if is a finite set for which , then since , and so which shows that converges to . Finally, if the sum on the left of 13.8 () converges, then the supremum on the right is finite and so 13.8 holds.() The reader may have noticed that we have two definitions of convergence for countably infinite series: the net version and the traditional version involving the limit of partial sums. Let us write and for the net version and the partial sum version, respectively. Here is the relationship between these two definitions. Theorem 13.24 Let be a Hilbert space. If , then the following are equivalent: 1 converges net version to )( ) 2 converges unconditionally to ) Proof. Assume that 1 holds. Suppose that is any permutation of . Given) any , there is a finite set for which finite Let us denote the set of integers by and choose a positive integer such that . Then for we have and so 2 holds.) 344 Advanced Linear Algebra Next, assume that 2 holds, but that the series in 1 does not converge. Then)) there exists an such that for any finite subset , there exists a finite subset with for which From this, we deduce the existence of a countably infinite sequence of mutually disjoint finite subsets of with the property that max min and Now we choose any permutation with the following properties 1) 2 if , then) 2 The intention in property 2 is that for each , takes a set of consecutive) integers to the integers in . For any such permutation , we have which shows that the sequence of partial sums of the series is not Cauchy and so this series does not converge. This contradicts 2 and) shows that 2 implies at least that 1 converges. But if 1 converges to ,)) ) then since 1 implies 2 and since unconditional limits are unique, we have)) . Hence, 2 implies 1 .)) Now we can return to the discussion of Fourier expansions. Let be an arbitrary orthonormal set in a Hilbert space . Given any , we may apply Theorem 13.16 to all finite subsets of , to deduce Hilbert Spaces 345 that sup finite and so Theorem 13.23 tells us that the sum converges. Hence, according to Theorem 13.22, the Fourier expansion of also converges and Note that, according to Theorem 13.21, is a countably infinite sum of terms of the form and so is in . cspan The continuity of infinite sums with respect to the inner product Theorem( 13.19 implies that) and so span cspan . Hence, Theorem 3.9 tells us that is the best approximation to in . Finally, since , we again cspan have and so with equality if and only if , which happens if and only if . cspan Thus, we arrive at the most general form of a key theorem about Hilbert spaces. Theorem 13.25 Let be an orthonormal family of vectors in a Hilbert space . For any , the Fourier expansion of converges in and is the unique best approximation to in . cspan Moreover, we have Bessel's inequality 346 Advanced Linear Algebra or equivalently, with equality if and only if . cspan A Characterization of Hilbert Bases Recall from Theorem 13.15 that an orthonormal set in a Hilbert space is a Hilbert basis if and only if cspan Theorem 13.25, then leads to the following characterization of Hilbert bases. Theorem 13.26 Let be an orthonormal family in a Hilbert space . The following are equivalent: 1 is a Hilbert basis a maximal orthonormal set)( ) 2) 3 is total that is, )( cspan ) 4 for all ) 5 Equality holds in Bessel's inequality for all , that is,) for all 6) Parseval's identity holds for all , that is, Proof. Parts 1 , 2 and 3 are equivalent by Theorem 13.15. Part 4 implies part)) ) ) 3 , since and 3 implies 4 since the unique best approximation of)cspan ) ) any is itself and so . Parts 3 and 5 are equivalent by cspan ) ) Theorem 13.25. Parseval's identity follows from part 4 using Theorem 13.19.) Finally, Parseval's identity for implies that equality holds in Bessel's inequality. Hilbert Dimension We now wish to show that all Hilbert bases for a Hilbert space have the same cardinality and so we can define the Hilbert dimension of to be that cardinality. Hilbert Spaces 347 Theorem 13.27 All Hilbert bases for a Hilbert space have the same cardinality. This cardinality is called the of , which weHilbert dimension denote by .hdim Proof. If has a finite Hilbert basis, then that set is also a Hamel basis and so all finite Hilbert bases have size . Suppose next that dim and are infinite Hilbert bases for . Then for each , we have where is the countable set . Moreover, since no can be orthogonal to every , we have . Thus, since each is countable, we have By symmetry, we also have and so the Schro¨der–Bernstein theorem implies that . Theorem 13.28 Two Hilbert spaces are isometrically isomorphic if and only if they have the same Hilbert dimension. Proof. Suppose that . Let be ahdim hdim Hilbert basis for and a Hilbert basis for . We may define a map as follows: We leave it as an exercise to verify that is a bijective isometry. The converse is also left as an exercise. A Characterization of Hilbert Spaces We have seen that any vector space is isomorphic to a vector space of all functions from to that have finite support. There is a corresponding result for Hilbert spaces. Let be any nonempty set and let The functions in are referred to as . We can square summable functions also define a real version of this set by replacing by . We define an inner product on by The proof that is a Hilbert space is quite similar to the proof that 348 Advanced Linear Algebra is a Hilbert space and the details are left to the reader. If we define by if if then the collection is a Hilbert basis for , of cardinality . To see this, observe that and so is orthonormal. Moreover, if , then for only a countable number of , say . If we define by then and for all , which implies that . cspan This shows that and so is a total orthonormal set, that is, a cspan Hilbert basis for . Now let be a Hilbert space, with Hilbert basis . We define a map as follows. Since is a Hilbert basis, any has the form Since the series on the right converges, Theorem 13.22 implies that the series converges. Hence, another application of Theorem 13.22 implies that the following series converges: It follows from Theorem 13.19 that is linear and it is not hard to see that it is also bijective. Notice that and so takes the Hilbert basis for to the Hilbert basis for . Hilbert Spaces 349 Notice also that and so is an isometric isomorphism. We have proved the following theorem. Theorem 13.29 If is a Hilbert space of Hilbert dimension and if is any set of cardinality , then is isometrically isomorphic to . The Riesz Representation Theorem We conclude our discussion of Hilbert spaces by discussing the Riesz representation theorem. As it happens, not all linear functionals on a Hilbert space have the form “take the inner product with ,” as in the finite- dimensional case. To see this, observe that if , then the function is certainly a linear functional on . However, it has a special property. In particular, the Cauchy–Schwarz inequality gives, for all , or, for all , Noticing that equality holds if , we have sup This prompts us to make the following definition, which we do for linear transformations between Hilbert spaces this covers the case of linear( functionals .) Definition Let be a linear transformation from to . Then is said to be ifbounded sup If the supremum on the left is finite, we denote it by and call it the of norm . 350 Advanced Linear Algebra Of course, if is a bounded linear functional on , then sup The set of all bounded linear functionals on a Hilbert space is called the continuous dual space conjugate space, or , of and denoted by . Note that this differs from the algebraic dual of , which is the set of all linear functionals on . In the finite-dimensional case, however, since all linear functionals are bounded exercise , the two concepts agree. Unfortunately,() ( there is no universal agreement on the notation for the algebraic dual versus the continuous dual. Since we will discuss only the continuous dual in this section, no confusion should arise. The following theorem gives some simple reformulations of the definition of norm. Theorem 13.30 Let be a bounded linear transformation. 1) sup 2) sup 3 for all ) inf The following theorem explains the importance of bounded linear transformations. Theorem 13.31 Let be a linear transformation. The following are equivalent: 1 is bounded) 2 is continuous at any point ) 3 is continuous.) Proof. Suppose that is bounded. Then as . Hence, is continuous at . Thus, 1 implies 2 . If 2 holds, then )) ) for any , we have as , since is continuous at and as . Hence, is continuous at any and 3 holds. Finally, suppose that 3 holds. Thus, )) is continuous at and so there exists a such that Hilbert Spaces 351 In particular, and so Thus, is bounded. Now we can state and prove the Riesz representation theorem. Theorem 13.32 The Riesz representation theorem() Let be a Hilbert space. For any bounded linear functional on , there is a unique such that for all . Moreover, . Proof. If , we may take , so let us assume that . Hence, ker and since is continuous, is closed. Thus Now, the first isomorphism theorem, applied to the linear functional , implies that as vector spaces . In addition, Theorem 3.5 implies that () and so . In particular, .dim For any , we have Since , all we need do is find for whichdim for then for all , showing that for as well. But if , then has this property, as can be easily checked. The fact that has already been established. 352 Advanced Linear Algebra Exercises 1. Prove that the sup metric on the metric space of continuous functions on does not come from an inner product. Hint: let and a a and consider the parallelogram law. 2. Prove that any Cauchy sequence that has a convergent subsequence must itself converge. 3. Let be an inner product space and let and be subsets of . Show that a ) b is a closed subspace of ) c )cspan 4. Let be an inner product space and . Under what conditions is ? 5. Prove that a subspace of a Hilbert space is closed if and only if . 6. Let be the subspace of consisting of all sequences of real numbers with the property that each sequence has only a finite number of nonzero terms. Thus, is an inner product space. Let be the subspace of consisting of all sequences in with the property that . Show that is closed, but that . Hint: For the latter, show that by considering the sequences , where the term is in the th coordinate position. 7. Let be an orthonormal set in . If converges, show that 8. Prove that if an infinite series converges absolutely in a Hilbert space , then it also converges in the sense of the “net” definition given in this section. 9. Let be a collection of nonnegative real numbers. If the sum on the left below converges, show that sup finite 10. Find a countably infinite sum of real numbers that converges in the sense of partial sums, but not in the sense of nets. 11. Prove that if a Hilbert space has infinite Hilbert dimension, then no Hilbert basis for is a Hamel basis. 12. Prove that is a Hilbert space for any nonempty set . Hilbert Spaces 353 13. Prove that any linear transformation between finite-dimensional Hilbert spaces is bounded. 14. Prove that if , then is a closed subspace of . ker 15. Prove that a Hilbert space is separable if and only if .hdim 16. Can a Hilbert space have countably infinite Hamel dimension? 17. What is the Hamel dimension of ? 18. Let and be bounded linear operators on . Verify the following: a ) b ) c ) 19. Use the Riesz representation theorem to show that for any Hilbert space . Chapter 14 Tensor Products In the preceding chapters, we have seen several ways to construct new vector spaces from old ones. Two of the most important such constructions are the direct sum and the vector space of all linear transformations from to . In this chapter, we consider another very important construction, known as the .tensor product Universality We begin by describing a general type of that will help motivate theuniversality definition of tensor product. Our description is strongly related to the formal notion of a in category theory, but we will be somewhat lessuniversal pair formal to avoid the need to formally define categorical concepts. Accordingly, the terminology that we shall introduce is not standard, but does not contradict any standard terminology. Referring to Figure 14.1, consider a set and two functions and , with domain . A g S X f Figure 14.1 Suppose that there exists a function for which this diagram commutes, that is, This is sometimes expressed by saying that can be . Whatfactored through does this say about the relationship between the functions and ? 356 Advanced Linear Algebra Let us think of the “information” about contained in a function as the way in which elements of using from . The distinguishes labels relationship above implies that and this can be phrased by saying that whatever ability has to distinguish elements of is also possessed by . Put another way, except for labeling differences, any information about that is contained in is also contained in . If happens to be injective, then the difference between and is the only values of the labels. That is, the two functions have the same information about . However, in general, is not required to be injective and so may contain more information than . Now consider a family of sets and a family Assume that and . If the diagram in Figure 14.1 commutes for all , then the information contained in every function in is also contained in . Moreover, since , the function cannot contain more information than is contained in the entire family and so we conclude that contains exactly the same information as is contained in the entire family . In this sense, is among all functions in . universal In this way, a single function , or more precisely, a single pair , can capture a mathematical concept as described by a family of functions. Some examples from linear algebra are basis for a vector space, quotient space, direct sum and bilinearity (as we will see). Let us make a formal definition. Definition Referring to Figure 14.2, let be a set and let be a family of sets. Let be a family of functions, all of which have domain and range a member of . Let be a family of functions with domain and range in . We assume that has the following structure: 1 contains the identity function for each member of .) Tensor Products 357 2 is closed under composition of functions, which is an associative) operation. 3 For any and , the composition is defined and belongs to) . A S3f3 S2 f2 S1 f1 3 2 1 Figure 14.2 We refer to as the and its members as measuring family measuring functions. A pair , where and has the for universal property the family , or is a for , if for every as measured by universal pair in , there is a unique in for which the diagram in Figure 14.1 commutes, that is, for which or equivalently, any can be . The unique measuring factored through function is called the for . mediating morphism Note the requirement that the mediating morphism be unique. Universal pairs are essentially unique, as the following describes. Theorem 14.1 Let and be universal pairs for . Then there is a bijective measuring function for which . In fact, the mediating morphism of with respect to and the mediating morphism of with respect to are isomorphisms. Proof. With reference to Figure 14.3, there are mediating morphisms and for which Hence, However, referring to the third diagram in Figure 14.3, both and the identity map are mediating morphisms for and so the uniqueness 358 Advanced Linear Algebra of mediating morphisms implies that . Similarly and so and are inverses of one another, making the desired bijection. A g S T f A g S T f A f S S f Figure 14.3 Examples of Universality Now let us look at some examples of the universal property. Let denoteVect the family of all vector spaces over the base field . We use the term ( family informally to represent what in set theory is formally referred to as a class. A class is a “collection” that is too large to be considered a set. For example, Vect ) is a class. Example 14.1 Let be a nonempty set and let()Bases 1) Vect 2) set functions from to members of 3) linear transformations If is a vector space with basis , then the pair , where is the inclusion map , is universal for . To see this, note that the condition that can be factored through , is equivalent to the statement that for each basis vector . But this uniquely defines a linear transformation . In fact, the universality of the pair is the statement that a linear precisely transformation is uniquely determined by assigning its values arbitrarily on a basis , the function doing the arbitrary assignment in this context. Note also that Theorem 14.1 implies that if is also universal for , then there is a bijective mediating morphism from to , that is, and are isomorphic. Example 14.2 Let be a vector()Quotient spaces and canonical projections space and let be a subspace of . Let 1) Vect Tensor Products 359 2) linear maps with domain , whose kernels contain 3) linear transformations Theorem 3.4 says precisely that the pair , where is the canonical projection map, has the universal property for as measured by . Example 14.3 Let and be vector spaces over . Let()Direct sums 1) Vect 2) ordered pairs of linear transformations 3) linear transformations Here we have a slight variation on the definition of universal pair: In this case, is a family of of functions. For and , we setpairs Then the pair , where and are called the , has the universal property for . Tocanonical injections see this, observe that for any pair in , the condition is equivalent to or and But these conditions define a unique linear transformation . Thus, bases, quotient spaces and direct sums are all examples of universal pairs and it should be clear from these examples that the notion of universal property is, well, universal. In fact, it happens that the most useful definition of tensor product is through a universal property, which we now explore. Bilinear Maps The universality that defines tensor products rests on the notion of a bilinear map. Definition Let , and be vector spaces over . Let be the cartesian product of and . A set function as sets 360 Advanced Linear Algebra is if it is linear in both variables separately, that is, ifbilinear and The set of all bilinear functions from to is denoted by hom . A bilinear function with values in the base field is called a on . bilinear form Note that bilinearity can also be expressed in matrix language as follows: If and then is bilinear if where . It is important to emphasize that, in the definition of bilinear function, is the , not the direct product of vector spaces. In othercartesian product of sets words, we do not consider any algebraic structure on when defining bilinear functions, so expressions like and are meaningless. In fact, if is a vector space, there are two classes of functions from to : the linear maps , where is the direct product of vector spaces, and the bilinear maps , where ishom just the cartesian product of sets. We leave it as an exercise to show that these two classes have only the zero map in common. In other words, the only map that is both linear and bilinear is the zero map. We made a thorough study of bilinear forms on a finite-dimensional vector space in Chapter 11 although this material is not assumed here . However, () bilinearity is far more important and far-reaching than its application to metric vector spaces, as the following examples show. Indeed, both multiplication and evaluation are bilinear. Example 14.4 If is an algebra, the product map()Multiplication is bilinear defined by Tensor Products 361 is bilinear, that is, multiplication is linear in each position. Example 14.5 If and are vector spaces, then the()Evaluation is bilinear evaluation map defined by is bilinear. In particular, the evaluation map defined by is a bilinear form on . Example 14.6 If and are vector spaces, and and , then the product map defined by is bilinear. Dually, if and , then the map defined by is bilinear. It is precisely the tensor product that will allow us to generalize the previous example. In particular, if and , then we would like to consider a “product” map defined by ? The tensor product is just the thing to replace the question mark, because it has the desired bilinearity property, as we will see. In fact, the tensor product is bilinear and nothing else, so it is what we need!exactly Tensor Products Let and be vector spaces. Our guide for the definition of the tensor product will be the desire to have a universal property for bilinear functions, as measured by linearity. Referring to Figure 14.4, we want to define a vector space and a bilinear map so that any bilinear map with domain can be factored through . Intuitively speaking, is the most “general” or “universal” bilinear map with domain : It is bilinear and nothing more. 362 Advanced Linear Algebra W f bilinear linear TVU t bilinear Figure 14.4 Definition Let be the cartesian product of two vector spaces over . Let Vect . Let hom be the family of all bilinear maps from to any vector space . The measuring family is the family of all linear transformations. A pair is if it is universal for universal for bilinearity , that is, if for every bilinear map , there is a unique linear transformation for which The map is called the for . mediating morphism We can now define the tensor product via this universal property. Definition Let and be vector spaces over a field . Any universal pair for bilinearity is called a of and . Thetensor product vector space is denoted by and sometimes referred to by itself as the tensor product. The map is called the and the elements of tensor map are called .tensors It is customary to use the symbol to denote the image of any ordered pair under the tensor map, that is, for any and . A tensor of the form is said to be decomposable, that is, the decomposable tensors are the images under the tensor map. Since universal pairs are unique up to isomorphism, we may refer to “the” tensor product of vector spaces. Note also that the tensor product is not a product in the sense of a binary operation on a set. In fact, even when , the tensor product is not in , but rather in . Tensor Products 363 As we will see, there are other, more constructive ways to define the tensor product. Since we have adopted the universal pair definition, the other ways to define tensor product are, for us, constructions rather than definitions. Let us examine some of these constructions. Construction I: Intuitive but Not Coordinate Free The universal property for bilinearity captures the essence of bilinearity and the tensor map is the most “general” bilinear function on . To see how this universality can be achieved in a constructive manner, let be a basis for and let be a basis for . Then a bilinear map on is uniquely determined by assigning arbitrary values to the “basis” pairs and extending by bilinearity, that is, if and , then Now, the tensor map , being the most general bilinear map, must do this and nothing more. To achieve this goal, we define the tensor map on the pairs in such a way that the images , and then extenddo not interact by bilinearity. In particular, for each ordered pair , we invent a new formal symbol, written , and define to be the vector space with basis The tensor map is defined by setting and extending by bilinearity. Thus, To see that the pair is the tensor product of and , if is bilinear, the universality condition is equivalent to which does indeed uniquely define a map . Hence, haslinear the universal property for bilinearity and so we can write and refer to as the tensor map. Note that while the set is a basis for (by definition), the set of decomposable tensors spans , but is not linearly independent. This does cause some initial confusion during the learning process. For example, one cannot define a linear map on by assigning values arbitrarily to the decomposable tensors, nor is it always easy to tell when a tensor is 364 Advanced Linear Algebra equal to . We will consider the latter issue in some detail a bit later in the chapter. The fact that is a basis for gives the following. Theorem 14.2 For finite-dimensional vector spaces and , dim dim dim Construction II: Coordinate Free The previous construction of the tensor product is reasonably intuitive, but has the disadvantage of not being coordinate free. The following approach does not require the choice of a basis. Let be the vector space over with basis . Let be the subspace of generated by all vectors of the form ()14.1 and ()14.2 where and and are in the appropriate spaces. Note that these vectors are precisely what we must “identify” as the zero vector in order to enforce bilinearity. Put another way, these vectors are if the ordered pairs are replaced by tensors according to our previous construction. Accordingly, the quotient space is also sometimes taken as the definition of the tensor product of and . (Strictly speaking, we should not be using the symbol until we have shown that this is the tensor product.) The elements of have the form However, since and , we can absorb the scalar in either coordinate, that is, and so the elements of can be written simply as It is customary to denote the coset by , and so any element of Tensor Products 365 has the form as in the previous construction. The tensor map is defined by This map is bilinear, since and similarly for the second coordinate. We next prove that the pair is universal for bilinearity when is defined as a quotient space . Theorem 14.3 Let and be vector spaces. The pair is the tensor product of and . Proof. Consider the diagram in Figure 14.5. Here is the vector space with basis . F VU j W f VU VU t Figure 14.5 Since we have The universal property of vector spaces described in Example 14.1 implies that 366 Advanced Linear Algebra there is a unique linear transformation for which Note that sends the vectors (14.1) and (14.2) that generate to the zero vector and so . For example, ker and similarly for the second coordinate. Hence, Theorem 3.4 the universal( property described in Example 14.2) implies that there exists a unique linear transformation for which Hence, As to uniqueness, if , then and since the cosets generate , we conclude that . Thus, is the mediating morphism and is universal for bilinearity. Let us take a moment to compare the two previous constructions. Let and be bases for and , respectively. Let be the tensor product as constructed using these two bases and let be the tensor product construction using quotient spaces. Since both of these pairs are universal for bilinearity, Theorem 14.1 implies that the mediating morphism for with respect to , that is, the map defined by is a vector space isomorphism. Therefore, the basis of is sent to the set , which is therefore a basis for . In other words, given any two bases and for and , respectively, the tensors form a basis for , regardless of which construction of the tensor product we use. Therefore, we are free to think of either as a formal symbol belonging to a basis for or as the coset belonging to a basis for . Tensor Products 367 Bilinearity on Equals Linearity on The universal property for bilinearity says that to each functionbilinear , there corresponds a unique function ,linear called the mediating morphism for . Thus, we can define the mediating morphism map hom by setting . In other words, is the unique linear map for which Observe that is itself linear, since if , then hom and so is the mediating morphism for , that is, Also, is surjective, since if is any linear map, then is bilinear and has mediating morphism , that is, . Finally, is injective, for if , then . We have established the following result. Theorem 14.4 Let , and be vector spaces over . Then the mediating morphism map , where is the unique hom linear map satisfying , is an isomorphism and so hom When Is a Tensor Product Zero? Armed with the universal property of bilinearity, we can now discuss some of the basic properties of tensor products. Let us first consider the question of when a tensor is zero. The bilinearity of the tensor product gives and so . Similarly, . Now suppose that where we may assume that none of the vectors and are . Let be a bilinear map and let be its mediating morphism, that is, . Then 368 Advanced Linear Algebra The key point is that this holds for bilinear function . Inany particular, let and and define by which is easily seen to be bilinear. Then the previous display becomes If, for example, the vectors are linearly independent, we can take to be a dual vector to get and since this holds for all linear functionals , it follows that . We have proved the following useful result. Theorem 14.5 If are linearly independent vectors in and are arbitrary vectors in , then for all In particular, if and only if or . Coordinate Matrices and Rank If is a basis for and is a basis for , then any vector has a unique expression as a sum where only a finite number of the coefficients are nonzero. In fact, for a fixed , we may reindex the bases so that where none of the rows or columns of the matrix consists only of 's. The matrix is called a of with respect to the coordinate matrix bases and . Note that a coordinate matrix is determined only up to the order of its rows and columns. We could remove this ambiguity by considering ordered bases, Tensor Products 369 but this is not necessary for our discussion and adds a complication, since the bases may be infinite. Suppose that and are also bases for and , respectively, and that where is a coordinate matrix of with respect to these bases. We claim that the coordinate matrices and have the same rank, which can then be defined as the of the tensor .rank Each is a finite linear combination of basis vectors in , perhaps involving some of and perhaps involving other vectors in . We can further reindex so that each is a linear combination of the vectors , where and set span Next, extend to a basis for . (Since we no longer need the rest of the basis , we have commandeered the symbols , for simplicity. Hence ) for where is invertible of size . Now repeat this process on the second coordinate. Reindex the basis so that the subspace span contains and extend to a basis for . Then for where is invertible of size . Next, write by setting for or . Thus, the matrix comes from by adding rows of 's to the bottom and then columns of 's. In particular, and have the same rank. 370 Advanced Linear Algebra The expression for in terms of the basis vectors and can also be extended using coefficients to where the matrix has the same rank as . Now at last, we can compute. First, bilinearity gives and so Thus and so . Since and are invertible, we deduce that rk rk rk rk as desired. Moreover, in block matrix terms, we can write block block and and if we write block block and then implies that Tensor Products 371 We shall soon have use for the following special case. If ()14.3 then and so for and for where if and , then The Rank of a Decomposable Tensor Recall that a tensor of the form is said to be decomposable. If is a basis for and is a basis for , then any decomposable vector has the form Hence, the rank of a decomposable vector is , since the rank of a matrix whose th entry is is . Characterizing Vectors in a Tensor Product There are several useful representations of the tensors in . Theorem 14.6 Let be a basis for and let be a basis for . By an “essentially unique” sum, we mean unique up to order and presence of zero terms. 1 Every has an essentially unique expression as a finite sum of) the form where and the tensors are distinct. 372 Advanced Linear Algebra 2 Every has an essentially unique expression as a finite sum of) the form where and the 's are distinct. 3 Every has an essentially unique expression as a finite sum of) the form where and the 's are distinct. 4 Every nonzero has an expression of the form) where the 's are distinct, the 's are distinct and the sets and are linearly independent. As to uniqueness, is the rank of and so it is unique. Also, the equation where the 's are distinct, the 's are distinct and and are linearly independent, holds if and only if there exist invertible matrices and for which and and for . Proof. Part 1) merely expresses the fact that is a basis for . From part 2), we write Uniqueness follows from Theorem 14.5. Part 3) is proved similarly. As to part 4), we start with the expression from part 2): where we may assume that none of the 's are . If the set is linearly independent, we are done. If not, then we may suppose (after reindexing if Tensor Products 373 necessary) that Then But the vectors are linearly independent. This reduction can be repeated until the second coordinates are linearly independent. Moreover, the identity matrix is a coordinate matrix for and so rk rk . As to uniqueness, one direction was proved earlier; see ()14.3 and the other direction is left to the reader. The proof of Theorem 14.6 shows that if and where and , then if the multiset is not linearly independent, we can rewrite in the form where is linearly independent. Then we can do the same for the second coordinate to arrive so at the representation rk where the multisets and are linearly independent sets. Therefore, rk and so the rank of is the integer for which can besmallest written as a sum of decomposable tensors. This is often taken as the definition of the rank of a tensor. However, we caution the reader that there is another meaning to the word rank when applied to a tensor, namely, it is the number of indices required to write the tensor. Thus, a scalar has rank , a vector has rank , the tensor above has rank and a tensor of the form 374 Advanced Linear Algebra has rank . Defining Linear Transformations on a Tensor Product One of the simplest and most useful ways to define a linear transformation on the tensor product is through the universal property, for this property says precisely that a bilinear function on gives rise to a unique (and well-defined) linear transformation on . The proof of the following theorem illustrates this well. Theorem 14.7 Let and be vector spaces. There is a unique linear transformation defined by where Moreover, is an embedding and is an isomorphism if and are finite- dimensional. Thus, the tensor product of linear functionals is via this ( embedding a linear functional on tensor products.) Proof. Informally, for fixed and , the function is bilinear in and and so there is a unique linear map taking to . The function is bilinear in and since and so there is a unique linear map taking to . More formally, for fixed and , the map defined by is bilinear and so the universal property of tensor products implies that there exists a unique for which Next, the map defined by Tensor Products 375 is bilinear since, for example, which shows that is linear in its first coordinate. Hence, the universal property implies that there exists a unique linear map for which To see that is an injection, if is nonzero, then we may write in the form where the are nonzero and is linearly independent. If , then for any and , we have Hence, for each nonzero , the linear functional is the zero map and so the linear independence of implies that for all . Since is arbitrary, it follows that for all and so . Finally, in the finite-dimensional case, the map is a bijection since dim dim Combining the isomorphisms of Theorem 14.4 and Theorem 14.7, we have, for finite-dimensional vector spaces and , hom The Tensor Product of Linear Transformations We wish to generalize Theorem 14.7 to arbitrary linear transformations. Let and . While the product does not make sense, the product does and is bilinear in and , that is, thetensor following function is bilinear: 376 Advanced Linear Algebra The same argument that we used in the proof of Theorem 14.7 will work here. Namely, the map from to is bilinear in and and so there is a unique linear map for which The function defined by is bilinear, since and similarly for the second coordinate. Hence, there is a unique linear transformation satisfying that is, To see that is injective, if is nonzero, then we may write where the are nonzero and the set is linearly independent. If , then for all and we have Since , it follows that for some and so we may choose a such that for some . Moreover, we may assume, by reindexing if Tensor Products 377 necessary, that the set is a maximal linearly independent subset of . Hence, for each , we have and so Thus, the linear independence of implies that for each , for all and so But this contradicts the fact that the set is linearly independent. Hence, it cannot happen that for and so is injective. The embedding of into means that each can be thought of as the linear transformation from to , defined by In fact, the notation is often used to denote both the tensor product of vectors linear transformations and the linear map , and we will do this as() well. In summary, we can say that the tensor product of linear transformations is (up to isomorphism) a linear transformation on tensor products. 378 Advanced Linear Algebra Theorem 14.8 There is a unique linear transformation defined by where Moreover, is an embedding and is an isomorphism if all vector spaces are finite-dimensional. Thus, the tensor product of linear transformations is ()via this embedding a linear transformation on tensor products. Let us note a few special cases of the previous theorem. Corollary 14.9 Let us use the symbol to denote the fact that there is an embedding of into that is an isomorphism if and are finite- dimensional. 1 Taking gives) where for . 2 Taking and gives) where 3 Taking and noting that and gives) ()letting where 4 Taking and gives letting )( ) where Tensor Products 379 Change of Base Field The tensor product provides a convenient way to extend the base field of a vector space that is more general than the complexification of a real vector space, discussed earlier in the book. We refer to a vector space over a field as an and write .-space Actually, there are several approaches to “upgrading” the base field of a vector space. For instance, suppose that is an extension field of , that is, . If is a basis for , then every has the form where . We can define a -space simply by taking all formal linear combinations of the form where . Note that the dimension of as a -space is the same as the dimension of as an -space. Also, is an -space just restrict the scalars ( to and as such, the inclusion map sending to ) is an -monomorphism. The approach described in the previous paragraph uses an arbitrarily chosen basis for and is therefore not coordinate free. However, we can give a coordinate-free approach using tensor products as follows. Since is a vector space over , we can form the tensor product It is customary to include the subscript on to denote the fact that the tensor product is taken with respect to the base field . (All relevant maps are -bilinear and -linear.) However, since is not a -space, the only tensor product of and that makes sense is the -tensor product and so we will drop the subscript . The tensor product is an -space by definition of tensor product, but we can make it into a -space as follows. For , the temptation is to “absorb” the scalar into the first coordinate, but we must be certain that this is well-defined, that is, But for a fixed , the map is bilinear and so the universal property of tensor products implies that there is a unique linear map , which we define to be scalar multiplication by . 380 Advanced Linear Algebra To be absolutely clear, we have two distinct vector spaces: the -space defined by the tensor product and the -space with scalar multiplication by elements of defined as absorption into the first coordinate. The spaces and are identical as sets and as abelian groups. It is only the “permission to multiply by” that is different. Accordingly, we can recover from simply by restricting scalar multiplication to scalars from . Thus, we can speak of “ -linear” maps from into , with the expected meaning, that is, for all scalars . If the dimension of as a vector space over is , then dim dim dim As to the dimension of , it is not hard to see that if is a basis for , then is a basis for . Hence dim dim The map defined by is easily seen to be injective and -linear and so contains an isomorphic copy of . We can also think of as mapping into , in which case is called the of . -extension map This map has a universal property of its own, as described in the next theorem. Theorem 14.10 The -linear -extension map has the universal property for the family of all -linear maps from into a -space, as measured by -linear maps. Specifically, for any -linear map , where is a -space, there exists a unique -linear map for which the diagram in Figure 14.6 commutes, that is, for which Proof. If such a -linear map is to exist, then it must satisfy, for any , This shows that if exists, it is uniquely determined by . As usual, when searching for a linear map on a tensor product such as , we look for a bilinear map. The map defined by Tensor Products 381 is bilinear and so there exists a unique -linear map for which It is easy to see that is also -linear, since if , then VFKVF Y f Figure 14.6 Theorem 14.10 is the key to describing how to extend an -linear map to a - linear map. Figure 14.7 shows an -linear map between -spaces and . It also shows the -extensions for both spaces, where and are -spaces. V W W KV V KW Figure 14.7 If there is a unique -linear map that makes the diagram in Figure 14.7 commute, then this would be the obvious choice for the extension of the - linear map to a -linear map. Consider the -linear map into the -space . Theorem 14.10 implies that there is a unique -linear map for which that is, Now, satisfies 382 Advanced Linear Algebra and so . Theorem 14.11 Let and be -spaces, with -extension maps and , respectively. See Figure 14.7. Then for any -linear map , the() map is the unique -linear map that makes the diagram in Figure 14.7 commute, that is, for which Multilinear Maps and Iterated Tensor Products The tensor product operation can easily be extended to more than two vector spaces. We begin with the extension of the concept of bilinearity. Definition If and are vector spaces over , a function is said to be if it is linear in each coordinatemultilinear separately, that is, if for all . A multilinear function of variables is also referred to as an . The set of all -linear functions as defined above will be-linear function denoted by . A multilinear function from tohom the base field is called a or .multilinear form -form Example 14.7 1 If is an algebra, then the product map defined by) is -linear. 2 The determinant function is an -linear form on the columns)det of the matrices in . The tensor product is defined via its universal property. Definition As pictured in Figure 14.8, let be the cartesian product of vector spaces over . A pair is universal for multilinearity if for every multilinear map , there is a unique linear transformation for which Tensor Products 383 The map is called the for . If is universal for mediating morphism multilinearity, then is called the of and denoted by tensor product . The map is called the .tensor map W f V1Vn tV1Vn Figure 14.8 As we have seen, the tensor product is unique up to isomorphism. The basis construction and coordinate-free construction given earlier for the tensor product of two vector spaces carry over to the multilinear case. In particular, let be a basis for for . For each ordered -tuple , construct a new formal symbol and define to be the vector space with basis The tensor map is defined by setting and extending by multilinearity. This uniquely defines a multilinear map that is universal for multilinear functions from . Indeed, if is multilinear, the condition is equivalent to which uniquely defines a linear map . Hence, has the universal property for multilinearity. Alternatively, we may take the coordinate-free quotient space approach as follows. Definition Let be vector spaces over and let be the vector space with basis . Let be the subspace of generated by all vectors of the form 384 Advanced Linear Algebra for , and for . The quotient space is the tensor product of and the tensor map is the map As before, we denote the coset by and so any element of is a sum of decomposable tensors, that is, where the vector space operations are linear in each variable. Here are some of the basic properties of multiple tensor products. Proof is left to the reader. Theorem 14.12 The tensor product has the following properties. Note that all vector spaces are over the same field . 1 There exists an isomorphism)( )Associativity for which In particular, 2 Let be any permutation of the indices . Then)( )Commutativity there is an isomorphism for which 3 There is an isomorphism for which) and similarly, there is an isomorphism for which Hence, . The analog of Theorem 14.4 is the following. Tensor Products 385 Theorem 14.13 Let and be vector spaces over . Then the mediating morphism map hom defined by the fact that is the unique mediating morphism for is an isomorphism. Thus, hom Moreover, if all vector spaces are finite-dimensional, then dim hom dim dim Theorem 14.8 and its corollary can also be extended. Theorem 14.14 The linear transformation defined by is an embedding and is an isomorphism if all vector spaces are finite- dimensional. Thus, the tensor product of linear transformations is ()via this embedding a linear transformation on tensor products. Two important special cases of this are where and where Tensor Spaces Let be a finite-dimensional vector space. For nonnegative integers and , the tensor product 386 Advanced Linear Algebra factors factors is called the space of , where is the tensors of type contravariant type and is the . If , then , the base field. Here covariant type we use the notation for the -fold tensor product of with itself. We will also write for the -fold cartesian product of with itself. Since , we have hom which is the space of all multilinear functionals on factors factors In fact, tensors of type are often defined as multilinear functionals in this way. Note that dim dim Also, the associativity and commutativity of tensor products allows us to write at least up to isomorphism. Tensors of type are called contravariant tensors factors and tensors of type are called covariant tensors factors Tensors with both contravariant and covariant indices are called .mixed tensors In general, a tensor can be interpreted in a variety of ways as a multilinear map on a cartesian product, or a linear map on a tensor product. Indeed, the interpretation we mentioned above that is sometimes used as the definition is only one possibility. We simply need to decide how many of the contravariant factors and how many of the covariant factors should be “active participants” and how many should be “passive participants.” Tensor Products 387 More specifically, consider a tensor of type , written where and . Here we are choosing the first vectors and the first linear functionals as active participants. This determines the number of arguments of the map. In fact, we define a map from the cartesian product factors factors to the tensor product factors factors of the remaining factors by In words, the first group of (active) vectors interacts with the first group of arguments to produce the scalar . The first group of (active) functionals interacts with the second group of arguments to produce the scalar . The remaining (passive) vectors and functionals are just “copied” to the image tensor. It is easy to see that this map is multilinear and so there is a unique linear map from the tensor product factors factors to the tensor product factors factors defined by Moreover, the map defined by 388 Advanced Linear Algebra is an isomorphism, since if is the zero map then for all and , which implies that As usual, we denote the map by Theorem 11.15 For and , When and , we get as before. Let us look at some special cases. For we have where When , we get for and , where and for and , where Finally, when , we get a multilinear form Consider also a tensor of type . When we get a multilinear functional defined by Tensor Products 389 This is just a bilinear form on . Contraction Covariant and contravariant factors can be “combined” in the following way. Consider the map defined by This is easily seen to be multilinear and so there is a unique linear map defined by This is called the in the contravariant index and covariant indexcontraction . Of course, contraction in other indices (one contravariant and one covariant) can be defined similarly. Example 14.8 Let and consider the tensor space , which isdim isomorphic to via the map For a “decomposable” linear operator of the form as defined above with and , we have , which has codimension .ker ker Hence, if , then ker where is the eigenspace of associated with the eigenvalue . In particular, if , then and so is an eigenvector for the nonzero eigenvalue . Hence, and so the trace of is 390 Advanced Linear Algebra tr where is the contraction map. The Tensor Algebra of Consider the contravariant tensor spaces For we take . The external direct sum of these tensor spaces is a vector space with the property that This is an example of a , where are the elements of graded algebra grade . The graded algebra is called the over . We willtensor algebra ( formally define graded structures a bit later in the chapter.) Since factors there is no need to look separately at . Special Multilinear Maps The following definitions describe some special types of multilinear maps. Definition 1 A multilinear map is if interchanging any two) symmetric coordinate positions changes nothing, that is, if for any . 2 A multilinear map is or if) antisymmetric skew-symmetric interchanging any two coordinate positions introduces a factor of , that is, if for . Tensor Products 391 3 A multilinear map is or if) alternate alternating for some As in the case of bilinear forms, we have some relationships between these concepts. In particular, if , thenchar alternate symmetric skew-symmetric and if , thenchar alternate skew-symmetric A few remarks about permutations are in order. A of the setpermutation is a bijective function . We denote the group under ( composition of all such permutations by . This is the on ) symmetric group symbols. A of length is a permutation of the form , whichcycle sends to for and also sends to . All other elements of are left fixed. Every permutation is the product (composition) of disjoint cycles. A is a cycle of length . Every cycle and therefore everytransposition ( permutation is the product of transpositions. In general, a permutation can be) expressed as a product of transpositions in many ways. However, no matter how one represents a given permutation as such a product, the number of transpositions is either always even or always odd. Therefore, we can define the parity of a permutation to be the parity of the number of transpositions in any decomposition of as a product of transpositions. The of a sign permutation is defined by sg has even parity has odd parity If sg , then is an and if sg , then is an even permutation odd permutation. The sign of is often written . With these facts in mind, it is apparent that is symmetric if and only if 1) for all permutations and that is skew-symmetric if and only if 1) for all permutations . A word of caution is in order with respect to the notation above, which is very convenient albeit somewhat prone to confusion. It is intended that a permutation permutes the coordinate positions in , not the indices (despite appearances). Suppose, for example, that and that is a basis for . 392 Advanced Linear Algebra If , then applied to gives and not , since permutes the two coordinate positions in . Graded Algebras We need to pause for a few definitions that are useful in discussing tensor algebras. An algebra over is said to be a if as a vector graded algebra space over , can be written in the form for subspaces of , and where multiplication behaves nicely, that is, The elements of are said to be . If is written homogeneous of degree for , , then is called the of of homogeneous component degree . The ring of polynomials provides a prime example of a graded algebra, since where is the subspace of consisting of all scalar multiples of . More generally, the ring of polynomials in several variables is a graded algebra, since it is the direct sum of the subspaces of homogeneous polynomials of degree . A polynomial is if each( homogeneous of degree term has degree . For example, is homogeneous of degree .) The Symmetric and Antisymmetric Tensor Algebras Our discussion of symmetric and antisymmetric tensors will benefit by a discussion of a few definitions and setting a bit of notation at the outset. Let denote the vector space of all homogeneous polynomials of degree (together with the zero polynomial) in the independent variables . As is sometimes done in this context, we denote the product in by , for example, writing as . The algebra of all polynomials in is denoted by . Tensor Products 393 We will also need the counterpart of in which multiplication acts anticommutatively, that is, . Definition Let be a sequence of independent variables. For , let be the vector space over with basis consisting of all words of length over that are in ascending order. Let , which we identify with by identifying with . Define a product on the direct sum as follows. First, the product of monomials and is defined as follows: 1 If has a repeated factor then .) 2 Otherwise, reorder in ascending order, say , via the) permutation and set Extend the product by distributivity to . The resulting product makes into a noncommutative algebra over . This product is () called the or on .wedge product exterior product For example, by definition of wedge product, Let be a basis for . It will be convenient to group the decomposable basis tensors according to their index multiset. Specifically, for each multiset with , let be the set of all tensors where is a permutation of . For example, if , then If has the form where , then let be the subset of whose elements appear 394 Advanced Linear Algebra in the sum for . For example, if then Let denote the sum of the terms of associated with . For example, Thus, can be written in the form where the sum is over a collection of multisets with . Note also that since . Finally, let be the unique member of for which . Now we can get to the business at hand. Symmetric and Antisymmetric Tensors Let be the symmetric group on . For each , the multilinear map defined by determines a unique linear operator on for which For example, if and , then Let be a basis for . Since is a bijection of the basis it follows that is an isomorphism of . Note also that is a permutation of each , that is, the sets are invariant under . Definition Let be a finite-dimensional vector space. Tensor Products 395 1 A tensor is if) symmetric for all permutations . The set of all symmetric tensors for all is a subspace of , called the of degree symmetric tensor space over . 2 A tensor is if) antisymmetric The set of all antisymmetric tensors for all is a subspace of , called the or antisymmetric tensor space exterior product space of degree over . We can develop the theory of symmetric and antisymmetric tensors in tandem. Accordingly, let us write (anti)symmetric to denote a tensor that is either symmetric or antisymmetrtic. Since for any , there is a permutation taking to , an (anti)symmetric tensor must have and so Since is a permutation of , it follows that is symmetric if and only if for all and this holds if and only if the coefficients of are equal, say for all . Hence, the symmetric tensors are precisely the tensors of the form The tensor is antisymmetric if and only if (14.4) In this case, the coefficients of differ only by sign. Before examining this more closely, we observe that must be a set. For if has an element of multiplicity greater than , we can split into two disjoint parts: 396 Advanced Linear Algebra where are the tensors that have in positions and : position position Then fixes each element of and sends the elements of to other elements of . Hence, applying to the corresponding decomposition of : gives and so , whence . Thus, is a set. Now, since for any , equation (14.4) implies that which holds if and only if , or equivalently, for all and . Choosing , where , as standard-bearer, if denotes the permutation for which , then Thus, is antisymmetric if and only if it has the form where and the sum is over a family of . sets In summary, the symmetric tensors are Tensor Products 397 where is a multiset and the antisymmetric tensors are where is a set. We can simplify these expressions considerably by representing the inside sums more succinctly. In the symmetric case, define a surjective linear map by and extending by linearity. Since takes every member of to the same monomial , where , we have In the antisymmetric case, define a surjective linear map by and extending by linearity. Since we have 398 Advanced Linear Algebra Thus, in both cases, where with and or depending on whether is symmetric or antisymmetric. However, in either case, the monomials are linearly independent for distinct multisets/sets . Therefore, if then for all multisets/sets . Hence, if char , then and so . This shows that the restricted maps and are isomorphisms. Theorem 14.16 Let be a finite-dimensional vector space over a field with char . 1 The symmetric tensor space is isomorphic to the algebra) of homogeneous polynomials, via the isomorphism 2 For , the antisymmetric tensor space is isomorphic to the) algebra of anticommutative homogeneous polynomials of degree , via the isomorphism The direct sum is called the of and the direct sumsymmetric tensor algebra is called the or the of . Theseantisymmetric tensor algebra exterior algebra vector spaces are graded algebras, where the product is defined using the vector space isomorphisms described in the previous theorem to move the products of and to and , respectively. Thus, restricting the domains of the maps gives a nice description of the symmetric and antisymmetric tensor algebras, when . However,char there are many important fields, such as finite fields, that have nonzero characteristic. We can proceed in a different, albeit somewhat less appealing, Tensor Products 399 manner that holds regardless of the characteristic of the base field. Namely, rather than restricting the domain of in order to get an isomorphism, we can factor out by the kernel of . Consider a tensor Since sends elements of different groups to different monomials in or , it follows that if and ker only if for all , that is, if and only if In the symmetric case, is constant on and so if and only if ker In the antisymmetric case, where and so ker if and only if In both cases, we solve for and substitute into . In the symmetric case, and so In the antisymmetric case, and so Since , it follows that and therefore , is in the span of tensors of the form in the symmetric case and in the antisymmetric case, where and . Hence, in the symmetric case, ker and since , it follows that . In the antisymmetric ker case, 400 Advanced Linear Algebra ker and since , it follows that . ker We now have quotient-space characterizations of the symmetric and antisymmetric tensor spaces that do not place any restriction on the characteristic of the base field. Theorem 14.17 Let be a finite-dimensional vector space over a field . 1 The surjective linear map defined by) has kernel and so The vector space is also referred to as the symmetric tensor space of degree of . 2 The surjective linear map defined by) has kernel and so The vector space is also referred to as the antisymmetric tensor space exterior product space or of degree of . The isomorphic exterior spaces and are usually denoted by and the isomorphic exterior algebras and are usually denoted by . Theorem 14.18 Let be a vector space of dimension . Tensor Products 401 1 The dimension of the symmetric tensor space is equal to the) number of monomials of degree in the variables and this is dim 2 The dimension of the exterior tensor space is equal to the number of) words of length in ascending order over the alphabet and this is dim Proof. For part 1), the dimension is equal to the number of multisets of size taken from an underlying set of size . Such multisets correspond bijectively to the solutions, in nonnegative integers, of the equation where is the multiplicity of in the multiset. To count the number of solutions, invent two symbols and . Then any solution to the previous equation can be described by a sequence of 's and 's consisting of 's followed by one , followed by 's and another , and so on. For example, if and , the solution corresponds to the sequence Thus, the solutions correspond bijectively to sequences consisting of 's and 's. To count the number of such sequences, note that such a sequence can be formed by considering “blanks” and selecting of these blanks for the 's. This can be done in ways. The Universal Property We defined tensor products through a universal property, which as we have seen is a powerful technique for determining the properties of tensor products. It is easy to show that the symmetric tensor spaces are universal for symmetric multilinear maps and the antisymmetric tensor spaces are universal for antisymmetric multilinear maps. Theorem 14.19 Let be a finite-dimensional vector space with basis . 1 The pair , where is the) multilinear map defined by 402 Advanced Linear Algebra is universal for symmetric -linear maps with domain ; that is, for any symmetric -linear map where is a vector space, there is a unique linear map for which 2 The pair , where is the) multilinear map defined by is universal for antisymmetric -linear maps with domain ; that is, for any antisymmetric -linear map where is a vector space, there is a unique linear map for which Proof. For part 1), the property does indeed uniquely define a linear transformation , provided that it is well- defined. However, if and only if the multisets and are the same, which implies that , since is symmetric. For part 2), since is antisymmetric, it is completely determined by the fact that it is alternate and by its values on the basis of ascending words . Accordingly, the condition uniquely defines a linear transformation . The Symmetrization Map When , we can define a linear map , calledchar the , bysymmetrization map Since , we have Tensor Products 403 and so is symmetric. The reason for the factor is that if is a symmetric tensor, then and so that is, the symmetrization map fixes all symmetric tensors. It follows that for any tensor , Thus, is idempotent and is therefore the projection map of onto im . The Determinant The universal property for antisymmetric multilinear maps has the following corollary. Corollary 14.20 Let be a vector space of dimension over a field . Let be an ordered basis for . Then there is at most one antisymmetric -linear form for which Proof. According to the universal property for antisymmetric -linear forms, for every antisymmetric -linear form satisfying , there is a unique linear map for which But has dimension and so there is only one linear map with . Therefore, if and are two such forms, then , from which it follows that We now wish to construct an antisymmetric form , which is unique by the previous theorem. Let be a basis for . For any , write for the th coordinate of the coordinate matrix . Thus, For clarity, and since we will not change the basis, let us write for . 404 Advanced Linear Algebra Consider the map defined by Then is multilinear since () and similarly for any coordinate position. To see that is alternating, and therefore antisymmetric since , char suppose for instance that . For any permutation , let Then for and and Hence, . Also, since , if the sets and intersect, then they are identical. Thus, the distinct sets form a partition of . It follows that pairs But and since , the sum of the two terms involving the pair is . Hence, . A similar argument holds for any coordinate pair. Tensor Products 405 Finally, Thus, the map is the unique antisymmetric -linear form on for which . Under the ordered basis , we can view as the space of coordinate vectors and view as the space of matrices, via the isomorphism where all coordinate matrices are with respect to . With this viewpoint, becomes an antisymmetric -form on the columns of a matrix given by This is called the of the matrix .determinant Properties of the Determinant Let us explore some of the properties of the determinant function. Theorem 14.21 If , then Proof. We have as desired. 406 Advanced Linear Algebra Theorem 14.22 If , then Proof. Consider the map defined by We can consider as a function on the columns of and think of it as a composition Each step in this map is multilinear and so is multilinear. It is also clear that is antisymmetric and so is a scalar multiple of the determinant function, say . Then Setting gives and so as desired. Theorem 14.23 A matrix is invertible if and only if . Proof. If is invertible, then and so which shows that and . Conversely, any matrix is equivalent to a diagonal matrix where and are invertible and is diagonal with 's and 's on the main diagonal. Hence, and so if , then , which happens if and only if , whence is invertible. Exercises 1. Show that if is a linear map and is bilinear, then is bilinear. 2. Show that the only map that is both linear and -linear for is the () zero map. 3. Find an example of a bilinear map whose image im is not a subspace of . Tensor Products 407 4. Let be a basis for and let be a basis for . Show that the set is a basis for by showing that it is linearly independent and spans. 5. Prove that the following property of a pair with bilinear characterizes the tensor product up to isomorphism, and thus could have been used as the definition of tensor product: For a pair with bilinear if is a basis for and is a basis for , then is a basis for . 6. Prove that . 7. Let and be nonempty sets. Use the universal property of tensor products to prove that . 8. Let and . Assuming that , show that if and only if and , for . 9. Let be a basis for and be a basis for . Show that any function can be extended to a linear function . Deduce that the function can be extended in a unique way to a bilinear map . Show that all bilinear maps are obtained in this way. 10. Let be subspaces of . Show that 11. Let and be subspaces of vector spaces and , respectively. Show that 12. Let and be subspaces of and , respectively. Show that 13. Find an example of two vector spaces and and a nonzero vector that has at least two distinct not including order of the terms() representations of the form where the 's are linearly independent and so are the 's. 14. Let denote the identity operator on a vector space . Prove that . 408 Advanced Linear Algebra 15. Suppose that and . Prove that 16. Connect the two approaches to extending the base field of an -space to at least in the finite-dimensional case by showing that() . 17. Prove that in a tensor product for which not all vectors dim have the form for some . : Suppose that are Hint linearly independent and consider . 18. Prove that for the block matrix block we have . 19. Let . Prove that if either or is invertible, then the matrices are invertible except for a finite number of 's. The Tensor Product of Matrices 20. Let be the matrix of a linear operator with respect to the ordered basis . Let be the matrix of a linear operator with respect to the ordered basis . Consider the ordered basis ordered lexicographically; that is if or and . Show that the matrix of with respect to is block This matrix is called the , or tensor product Kronecker product direct product of the matrix with the matrix . 21. Show that the tensor product is not, in general, commutative. 22. Show that the tensor product is bilinear in both and . 23. Show that if and only if or . 24. Show that a ) b when )( ) 25. Show that if , then as row vectors . () 26. Suppose that and are matrices of the given sizes. Prove that Discuss the case . Tensor Products 409 27. Prove that if and are nonsingular, then so is and 28. Prove that .tr tr tr 29. Suppose that is algebraically closed. Prove that if has eigenvalues and has eigenvalues , both lists including multiplicity, then has eigenvalues , again counting multiplicity. 30. Prove that .det det det Chapter 15 Positive Solutions to Linear Systems: Convexity and Separation It is of interest to determine conditions that guarantee the existence of positive solutions to homogeneous systems of linear equations where . Definition Let . 1 is , written , if) nonnegative for all ()The term is also used for this property. The set of all nonnegativepositive vectors in is the in nonnegative orthant 2 is , written , if is nonnegative but not , that is, if) strictly positive for all and for some The set of all strictly positive vectors in is the strictly positive orthant in 3 is , written , if) strongly positive for all The set of all strongly positive vectors in is the strongly positive orthant in We are interested in conditions under which the system has strictly positive or strongly positive solutions. Since the strictly and strongly positive orthants in are not subspaces of , it is difficult to use strictly linear methods in studying this issue: we must also use geometric methods, in particular, methods of convexity. 412 Advanced Linear Algebra Let us pause briefly to consider an important application of strictly positive solutions to a system . If is a strictly positive solution to this system, then so is the vector which is a , that is, and . Moreover,probability distribution if is a strongly positive solution, then has the property that each probability is positive. Now, the product is the expected value of the columns of with respect to the probability distribution . Hence, has a strictly (strongly) positive solution if and only if there is a strictly (strongly) positive probability distribution for which the columns of have expected value . If each column of represents the possible payoffs from a game of chance, where each row is a different possible outcome of the game, then the game is fair when the expected value of the columns is . Thus, has a strictly (strongly) positive solution if and only if the game with payoffs and probabilities is fair. As another (related) example, in discrete option pricing models of mathematical finance, the absence of arbitrage opportunities in the model is equivalent to the fact that a certain vector describing the gains in a portfolio does not intersect the strictly positive orthant in . As we will see in this chapter, this is equivalent to the existence of a strongly positive solution to a homogeneous system of equations. This solution, when normalized to a probability distribution, is called a .martingale measure Of course, the equation has a strictly positive solution if and only if ker contains a strictly positive vector, that is, if and only if ker RowSpace meets the strictly positive orthant in . Thus, we wish to characterize the subspaces of for which meets the strictly positive orthant in , in symbols, for these are precisely the row spaces of the matrices for which has a strictly positive solution. A similar statement holds for strongly positive solutions. Looking at the real plane , we can divine the answer with a picture. A one- dimensional subspace of has the property that its orthogonal complement meets the strictly positive orthant quadrant in if and only if is the -() axis, the -axis or a line with negative slope. For the case of the strongly Positive Solutions to Linear Systems: Convexity and Separation 413 positive orthant, must have negative slope. Our task is to generalize this to . This will lead us to the following results, which are quite intuitive in and : (15.1) and (15.2) Let us translate these statements into the language of the matrix equation . If , then and so we haveRowSpace ker ker RowSpace and ker RowSpace Now, RowSpace and RowSpace and so these statements become has a strongly positive solution and has a strictly positive solution We can rephrase these results in the form of a , thattheorem of the alternative is, a theorem that says that exactly one of two conditions holds. Theorem 15.1 Let . 1 Exactly one of the following holds:) a for some strongly positive .) b for some .) 2 Exactly one of the following holds:) a for some strictly positive .) b for some .) Before proving Theorem 15.1, we require some background. Convex, Closed and Compact Sets We shall need the following concepts. 414 Advanced Linear Algebra Definition 1 Let . Any linear combination of the form) where and is called a of convex combination the vectors . 2 A subset is if whenever , then the line segment) convex between and also lies in , in symbols, 3 A subset is if whenever is a convergent sequence of) closed elements of , then the limit is also in . 4 A subset is if it is both closed and bounded.) compact 5 A subset is a if implies that for all .) cone We will also have need of the following facts from analysis. 1 A continuous function that is defined on a compact set in takes on) maximum and minimum values at some points within the set . 2 A subset of is compact if and only if every sequence in has a) subsequence that converges to a point in . Theorem 15.2 Let and be subsets of . Define 1 If and are convex, then so is ) 2 If is compact and is closed, then is closed.) Proof. For 1), let and be in . The line segment between these two points is for and so is convex. For part 2), let be a convergent sequence in . Suppose that . We must show that . Since is a sequence in the compact set , it has a convergent subsequence whose limit lies in . Since and we can conclude that . Since is closed, it follows that and so . Convex Hulls We will also have use for the notion of the smallest convex set containing a given set. Positive Solutions to Linear Systems: Convexity and Separation 415 Definition The of a set of vectors in is theconvex hull smallest convex set in that contains . We will denote the convex hull of by . Here is a characterization of convex hulls. Theorem 15.3 Let be a set of vectors in . Then the convex hull is the set of all convex combinations of vectors in , that is, Proof. Clearly, if is a convex set that contains , then also contains . Hence . To prove the reverse inclusion, we need only show that is convex, since then implies that . So let be in . If and then But this is also a convex combination of the vectors in , because max max and Thus, . Theorem 15.4 The convex hull of a set of vectors finite in is a compact set. Proof. The set is closed and bounded in and therefore compact. Define a function as follows: If , then To see that is continuous, let and let . Given max , if then 416 Advanced Linear Algebra and so Finally, since , it follows that is compact. Linear and Affine Hyperplanes We next discuss hyperplanes in . A in is an - linear hyperplane dimensional subspace of . As such, it is the solution set of a linear equation of the form or where is nonzero and . Geometrically speaking, this is the set of all vectors in that are perpendicular (normal) to the vector . An , or just , in is a linear hyperplane that hasaffine hyperplane hyperplane been translated by a vector. Thus, it is the solution set to an equation of the form or equivalently, where . We denote this hyperplane by Note that the hyperplane contains the point , which is the point of closest to the origin, since Cauchy's inequality gives and so for all . Moreover, we leave it as an Positive Solutions to Linear Systems: Convexity and Separation 417 exercise to show that any hyperplane has the form for an appropriate vector . A hyperplane defines two closed half-spaces and two disjoint open half-spaces It is clear that and that the sets , and form a partition of . If and , we let and write to denote the fact that for all . Definition Two subsets and of are by a hyperplane strictly separated if lies in one open half-space determined by and lies in the other open half-space; in symbols, one of the following holds: 1) 2) Note that 1) holds for and if and only if 2) holds for and , and so we need only consider one of the conditions to demonstrate that two sets and are strictly separated. Specifically, if 1) fails for all and , then thenot condition also fails for all and and so 2) also fails for all and , whence and are not strictly separated. Definition Two subsets and of are by a hyperplane strongly separated if there is an for which one of the following holds: 1) 2) 418 Advanced Linear Algebra As before, we need only consider one of the conditions to show that two sets are not strongly separated. Note also that if for , then and are stongly separated by the hyperplane Separation Now that we have the preliminaries out of the way, we can get down to some theorems. The first is a well-known that is the basis forseparation theorem many other separation theorems. It says that if a closed convex set does not contain a vector , then can be separated from . strongly Theorem 15.5 Let be a closed convex subset of . 1 contains a vector of minimum norm, that is, there is a unique) unique vector for which for all . 2 If , then lies in the closed half-space) that is, where is the unique vector of minimum norm in the closed convex set Hence, and are strongly separated by the hyperplane Proof. For part 1), if then this is the unique vector of minimum norm, so we may assume that . It follows that no two distinct elements of can be negative scalar multiples of each other. For if and were in , where then taking gives which is false. Positive Solutions to Linear Systems: Convexity and Separation 419 We first show that contains a vector of minimum norm. Recall that the Euclidean norm (distance) is a continuous function. Although need not be compact, if we choose a real number such that the closed ball intersects , then that intersection is both closed and bounded and so is compact. The norm function therefore achieves its minimum on , say at the point . It is clear that if for some , then , in contradiction to the minimality of . Hence, is a vector of minimum norm in . We establish uniqueness first for closed line segments in . If where , then is smallest when for and for . Assume that and are not scalar multiples of each other and suppose that in have minimum norm . If then since and are also not scalar multiples of each other, the Cauchy-Schwarz inequality is strict and so which contradicts the minimality of . Thus, has a unique point of minimum norm. Finally, if also has minimum norm, then and are points of minimum norm in the line segment and so . Hence, has a unique element of minimum norm. For part 2), suppose the result is true when . Then implies that and so if has smallest norm, then Therefore, and so and are strongly separated by the hyperplane 420 Advanced Linear Algebra Thus, we need only prove part 2) for , that is, we need only prove that If there is a nonzero for which then and for some . Then for the open line segment with , we have Let denote the final expression above, which is a quadratic in . It is easy to see that has its minimum at the interior point of the line segment corresponding to and so , which is a contradiction. The next result brings us closer to our goal by replacing a single vector with a subspace disjoint from . However, we must also require that be bounded, and therefore compact. Theorem 15.6 Let be a compact convex subset of and let be a subspace of such that . Then there exists a nonzero such that for all . Hence, the hyperplane strongly separates and . Proof. Theorem 15.2 implies that the set is closed and convex. Furthermore, implies that and so Theorem 15.5 implies that can be strongly separated from the origin. Hence, there is a nonzero such that Positive Solutions to Linear Systems: Convexity and Separation 421 for all and . But if for some , then we can replace by an appropriate scalar multiple of in order to make the left side of this inequality negative, which is impossible. Hence, for all , that is, and We can now prove (15.1) and (15.2). Theorem 15.7 Let be a subspace of . 1 if and only if ) 2 if and only if ) Proof. In both cases, one direction is easy. It is clear that there cannot exist vectors and that are orthogonal. Hence, and cannot both be nonempty and so implies . Also, and cannot both be nonempty and so implies that . For the converse in part 1), to prove that a good candidate for an element of would be a normal to a hyperplane that separates from a subset of . Note that our separation theorems do not allow us to separate from , because is not compact. So consider instead the convex hull of the standard basis vectors in : which is compact. Moreover, implies that and so Theorem 15.6 implies that there is a nonzero vector such that for all Taking gives and so , which is therefore nonempty. To prove part 2 , again we note that there cannot exist orthogonal vectors) and and so and cannot both be nonempty. Thus, implies that . 422 Advanced Linear Algebra To finish the proof of part 2), we must prove that Let be a basis for . Then if and only if for all . In matrix terms, if has rows , then if and only if , that is, Now, contains a strictly positive vector if and only if this equation holds, where for all and for some . Moreover, we may assume without loss of generality that , or equivalently, that is in the convex hull of the row space of . Hence, Thus, we wish to prove that or equivalently, Now we have something to separate. Since is closed and convex, Theorem 15.5 implies that there is a nonzero vector for which Consider the vector The th coordinate of is and so is strongly positive. Hence, , which is therefore nonempty. Inhomogeneous Systems We now turn our attention to inhomogeneous systems The following lemma is required. Positive Solutions to Linear Systems: Convexity and Separation 423 Lemma 15.8 Let . Then the set is a closed, convex cone. Proof. We leave it as an exercise to prove that is a convex cone and omit the proof that is closed. Theorem 15.9 Let and let be()Farkas's lemma nonzero. Then exactly one of the following holds: 1 There is a strictly positive solution to the system .) 2 There is a vector for which and .) Proof. Suppose first that 1) holds. If 2) also holds, then However, and imply that . This contradiction implies that 2) cannot hold. Assume now that 1) fails to hold. By Lemma 15.8, the set is closed and convex. The fact that 1) fails to hold is equivalent to . Hence, there is a hyperplane that strongly separates and . All we require is that and be strictly separated, that is, for some and , for all Since , it follows that and so . Also, the first inequality is equivalent to , that is, for all . We claim that this implies that cannot have any positive coordinates and thus . For if the th coordinate is positive, then taking for we get which does not hold for large . Thus, 2) holds. In the exercises, we ask the reader to show that the previous result cannot be improved by replacing in statement 2) with . Exercises 1. Show that any hyperplane has the form for an appropriate vector . 424 Advanced Linear Algebra 2. If is an matrix prove that the set is a convex cone in . 3. If and are strictly separated subsets of and if is finite, prove that and are strongly separated as well. 4. Let be a vector space over a field with . Show that a char subset of is closed under the taking of convex combinations of any two of its points if and only if is closed under the taking of arbitrary convex combinations, that is, for all , 5. Explain why an -dimensional subspace of is the solution set of a linear equation of the form . 6 Show that and that , and are pairwise disjoint and 7. A function is if it has the form for affine , where . Prove that if is convex, then so is . 8. Find a cone in that is not convex. Prove that a subset of is a convex cone if and only if implies that for all . 9. Prove that the convex hull of a set in is bounded, without using the fact that it is compact. 10. Suppose that a vector has two distinct representations as convex combinations of the vectors . Prove that the vectors are linearly dependent. 11. Suppose that is a nonempty convex subset of and that is a hyperplane disjoint from . Prove that lies in one of the open half-spaces determined by . 12. Prove that the conclusion of Theorem 15.6 may fail if we assume only that is closed and convex. 13. Find two nonempty convex subsets of that are strictly separated but not strongly separated. 14. Prove that and are strongly separated by if and only if for all and for all where and and where is the closed unit ball. Positive Solutions to Linear Systems: Convexity and Separation 425 15. Show that Farkas's lemma cannot be improved by replacing in statement 2 with . : A nice counterexample exists for) Hint . Chapter 16 Affine Geometry In this chapter, we will study the geometry of a finite-dimensional vector space , along with its structure-preserving maps. Throughout this chapter, all vector spaces are assumed to be finite-dimensional. Affine Geometry The cosets of a quotient space have a special geometric name. Definition Let be a subspace of a vector space . The coset is called a in with and . We also refer toflat base flat representative as a of . The set of all flats in is called the translate affine geometry dimension of . The of is defined to be . dim dim While a flat may have many flat representatives, it only has one base since implies that and so , whence . Definition The of a flat is . A flat of dimension isdimension dim called a . A -flat is a , a -flat is a and a -flat is a . A flat -flat point line plane of dimension is called a .dim hyperplane Definition Two flats and are said to be if parallel or . This is denoted by . We will denote subspaces of by the letters and flats in by . Here are some of the basic intersection properties of flats. 428 Advanced Linear Algebra Theorem 16.1 Let and be subspaces of and let and be flats in . 1 The following are equivalent:) a some translate of is in : for some ) b some translate of is in : for some ) c ) 2 The following are equivalent:) a and are translates: for some ) b and are translates: for some ) c ) 3) 4) 5 If then , or ) 6 if and only if some translation of one of these flats is contained in) the other. Proof. If 1a) holds, then and so 1b) holds. If 1b) holds, then and so and so 1c) holds. If 1c) holds, then and so 1a) holds. Part 2) is proved in a similar manner. For part 3), implies that for some and so if then and so , which implies that . Conversely, if then part 1) implies that . Part 4) follows similarly. We leave proof of 5) and 6) to the reader. Affine Combinations Let be a nonempty subset of . It is well known that 1) is a subspace of if and only if is closed under linear combinations, or equivalently, is closed under linear combinations of any two vectors in . 2) The smallest subspace of containing is the set of all linear combinations of elements of . In different language, the of linear hull is equal to the of .linear span We wish to establish the corresponding properties of affine subspaces of , beginning with the counterpart of a linear combination. Definition Let be a vector space and let . A linear combination where and is called an of the vectors . affine combination Let us refer to a nonempty subset of as if is closed under affine closed any affine combination of vectors in and if is closedtwo-affine closed Affine Geometry 429 under affine combinations of any two vectors in . These are not standard terms. The containing two distinct vectors is the setline of all affine combinations of and . Thus, a subset of is two-affine closed if and only if contains the line through any two of its points. Theorem 16.2 Let be a vector space over a field with . Then a char subset of is affine closed if and only if it is two-affine closed. Proof. The theorem is proved by induction on the number of terms in an affine combination. The case holds by assumption. Assume the result true for affine combinations with fewer than terms and consider the affine combination where . There are two cases to consider. If either of and is not equal to , say , write and if , then since 2, we may write char 33 In either case, the inductive hypothesis applies to the expression inside the square brackets and then to . The requirement is necessary, for if , then the subsetchar of is two-affine closed but not affine closed. We can now characterize flats. Theorem 16.3 A nonempty subset of a vector space is a flat if and only if is affine closed. Moreover, if , then is a flat if and only if ischar two-affine closed. Proof. Let be a flat and let , where . If , then and so is affine closed. Conversely, suppose that is affine closed, let and let . If and then 430 Advanced Linear Algebra for . Since the sum of the coefficients of , and in the last expression is , it follows that and so . Thus, is a subspace of and is a flat. The rest follows from Theorem 16.2. Affine Hulls The following definition is the analog of the subspace spanned by a collection of vectors. Definition Let be a nonempty set of vectors in . 1 The of , denoted by , is the smallest flat containing) affhullaffine hull . 2 The of , denoted by , is the set of all affine) affspanaffine span combinations of vectors in . Theorem 16.4 Let be a nonempty subset of . Then affhull affspan span or equivalently, for a subspace of , affspan span Also, dim dim affspan span Proof. Theorem 16.3 implies that and so it isaffspan affhull sufficient to show that is a flat, or equivalently, that for any affspan , the set is a subspace of . To this end, let Then any two elements of have the form and , where and 2 are in . But if , then Affine Geometry 431 2 2 which is in , since the last sum is an affine sum. Hence, is a subspace of . We leave the rest of the proof to the reader. The Lattice of Flats The intersection of subspaces is a subspace, although it may be trivial. For flats, if the intersection is not empty, then it is also a flat. However, since the intersection of flats may be empty, the set does not form a lattice under intersection. However, we can easily fix this. Theorem 16.5 Let be a vector space. The set of all flats in , together with the empty set, is a complete lattice in which meet is intersection. In particular: 1 is closed under arbitrary intersection. In fact, if) has nonempty intersection, then for some . In other words, the base of the intersection is the intersection of the bases. 2 The join of the family is the intersection of all) flats containing the members of . Also, affhull 3 If and are flats in , then) If , then Proof. For part 1), if then for all and so 432 Advanced Linear Algebra We leave proof of part 2) to the reader. For part 3), since , it follows that for some subspace of . Thus, . Also, implies that and similarly , whence and so if , then . Hence, . On the other hand, and and so . Thus, . If , then we may take the flat representatives for and to be any element , in which case part 1) gives and since , we also have . We can now describe the dimension of the join of two flats. Theorem 16.6 Let and be flats in . 1 If , then) dim dim dim dim dim 2 If , then) dim dim Proof. We have seen that if , then and so dim dim On the other hand, if , then and since , we getdim Affine Geometry 433 dim dim Finally, we have dim dim dim dim and Theorem 16.5 implies that dim dim Affine Independence We now discuss the affine counterpart of linear independence. Theorem 16.7 Let be a nonempty set of vectors in . The following are equivalent: 1 For all , the set) is linearly independent. 2 For all ,) affhull 3 For any vectors ,) for all 4 For affine combinations of vectors in ,) for all 5 When is finite, ) dim affhull A set of vectors satisfying any any hence all of these conditions is said to be () affinely independent. Proof. If 1) holds but there is an affine combination equal to , where for all , then 434 Advanced Linear Algebra Since is nonzero for some , this contradicts 1). Hence, 1) implies 2). Suppose that 2) holds and where . If some , say , is nonzero then affhull which contradicts 2) and so for all . Hence, 2) implies 3). If 3) holds and the affine combinations satisfy then and since , it follows that for all . Hence, 4) holds. Thus, it is clear that 3) and 4) are equivalent. If 3) holds and for , then and so 3) implies that for all . Finally, suppose that . Since dim dim affhull it follows that 5) holds if and only if , which has size , is linearly independent. Affinely independent sets enjoy some of the basic properties of linearly independent sets. For example, a nonempty subset of an affinely independent set is affinely independent. Also, any nonempty set contains an affinely independent set. Since the affine hull of an affinely independent set is not the affhull affine hull of any proper subset of , we deduce that is a minimal affine spanning set of its affine hull. Affine Geometry 435 Affine Bases and Barycentric Coordinates We have seen that a set is affinely independent if and only if the set is linearly independent. We have also seen that for a subsapce of , affspan span Therefore, if by analogy, we define a subset of a flat to be an affine basis for if is affinely independent and , then is an affspan affine basis for if and only if is a basis for . Theorem 16.8 Let be a flat of dimension . Let be an ordered basis for and let be an ordered affine basis for . Then every has a unique expression as an affine combination The coefficients are called the of with respect to barycentric coordinates the ordered affine basis . For example, in , a plane is a flat of the form where is an ordered basis for a two-dimensional subspace of . Then are barycentric coordinates for the plane, that is, any has the form where . Affine Transformations Now let us discuss some properties of maps that preserve affine structure. Definition A function that preserves affine combinations, that is, for which is called an or , or .affine transformation affine map affinity() We should mention that some authors require that be bijective in order to be an affine map. The following theorem is the analog of Theorem 16.2. 436 Advanced Linear Algebra Theorem 16.9 If , then a function is an affinechar transformation if and only if it preserves affine combinations of every pair of its points, that is, if and only if Thus, if , then a map is an affine transformation if and only if itchar sends the line through and to the line through and . It is clear that linear transformations are affine transformations. So are the following maps. Definition Let . The affine map defined by for all , is called by . translation It is not hard to see that any composition , where , is affine. Conversely, any affine map must have this form. Theorem 16.10 A function is an affine transformation if and only if it is a linear operator followed by a translation, where and . Proof. We leave proof that is an affine transformation to the reader. Let be an affine map and suppose that . Then . Moreover, letting , we have and so is linear. Corollary 16.11 1 The composition of two affine transformations is an affine transformation.) 2 An affine transformation is bijective if and only if is bijective.) 3 The set of all bijective affine transformations on is a group under)aff composition of maps, called the of .affine group Let us make a few group-theoretic remarks about . The set of allaff trans translations of is a subgroup of . We can define a function aff aff by Affine Geometry 437 It is not hard to see that is a well-defined group homomorphism from aff onto , with kernel . Hence, is a normal subgroup of trans trans aff and aff trans Projective Geometry If , the join affine hull of any two distinct points in is a line. Ondim () the other hand, it is not the case that the intersection of any two lines is a point, since the lines may be parallel. Thus, there is a certain asymmetry between the concepts of points and lines in . This asymmetry can be removed by constructing the . Our plan here is to very briefly describe oneprojective plane possible construction of projective geometries of all dimensions. By way of motivation, let us consider Figure 16.1. Figure 16.1 Note that is a hyperplane in a 3-dimensional vector space and that . Now, the set of all flats of that lie in is an affine geometry of dimension . According to our definition of affine geometry, must be a( vector space in order to define . However, we hereby extend the definition of affine geometry to include the collection of all flats contained in a flat of . Figure 16.1 shows a one-dimensional flat and its linear span , as well as a zero-dimensional flat and its span . Note that, for any flat in , we have dim dim Note also that if and are any two distinct lines in , the corresponding 438 Advanced Linear Algebra planes and have the property that their intersection is a line through the origin, . We are now ready to define projectiveeven if the lines are parallel geometries. Let be a vector space of any dimension and let be a hyperplane in not containing the origin. To each flat in , we associate the subspace of generated by . Thus, the linear span function maps affine subspaces of to subspaces of . The span function is not surjective: Its image is the set of all subspaces that are contained in the base subspacenot of the flat . The linear span function is one-to-one and its inverse is intersection with , for any subspace not contained in . The affine geometry is, as we have remarked, somewhat incomplete. In the case , every pair of points determines a line but not every pairdim of lines determines a point. Now, since the linear span function is injective, we can identify with its image , which is the set of all subspaces of not contained in the base subspace . This view of allows us to “complete” by including the base subspace . In the three-dimensional case of Figure 16.1, the base plane, in effect, adds a projective line at infinity. With this inclusion, every pair of lines intersects, parallel lines intersecting at a point on the line at infinity. This two-dimensional projective geometry is called the .projective plane Definition Let be a vector space. The set of all subspaces of is called the of . The ofprojective geometry projective dimensionpdim is defined as pdim dim The of is defined to be . Aprojective dimension pdim dim subspace of projective dimension is called a and a subspace projective point of projective dimension is called a . projective line Thus, referring to Figure 16.1, a projective point is a line through the origin and, provided that it is not contained in the base plane , it meets in an affine point. Similarly, a projective line is a plane through the origin and, provided that it is not , it will meet in an affine line. In short, span span affine point line through the origin projective point affine line plane through the origin projective line The linear span function has the following properties. Affine Geometry 439 Theorem 16.12 The linear span function from the affine geometry to the projective geometry defined by satisfies the following properties: 1 The linear span function is injective, with inverse given by) for all subspaces not contained in the base subspace of . 2 The image of the span function is the set of all subspaces of that are not) contained in the base subspace of . 3 if and only if ) 4 If are flats in with nonempty intersection, then) span span 5 For any collection of flats in ,) span span 6 The linear span function preserves dimension, in the sense that) pdim span dim 7 if and only if one of and is contained in the) other. Proof. To prove part 1 , let be a flat in . Then and so) , which implies that . Note also that and for some , and . This implies that , which implies that either or . But implies and so , which implies that . In other words, Since the reverse inclusion is clear, we have This establishes 1 .) To prove 2 , let be a subspace of that is not contained in . We wish to) show that is in the image of the linear span function. Note first that since and , we have and sodim dim dim dim dim dim dim 440 Advanced Linear Algebra Now let . Then for some Thus, for some . Hence, the flat lies in and dim dim dim which implies that lies in and hasspan the same dimension as . In other words, span We leave proof of the remaining parts of the theorem as exercises. Exercises 1. Show that if , then the set is a subspace of . 2. Prove that if is nonempty then affhull 3. Prove that the set in is closed under the formation of lines, but not affine hulls. 4. Prove that a flat contains the origin if and only if it is a subspace. 5. Prove that a flat is a subspace if and only if for some we have for some . 6. Show that the join of a collection of flats in is the intersection of all flats that contain all flats in . 7. Is the collection of all flats in a lattice under set inclusion? If not, how can you “fix” this? 8. Suppose that and . Prove that if dim dim and , then . 9. Suppose that and are disjoint hyperplanes in . Show that . 10. (The parallel postulate) Let be a flat in and . Show that there is exactly one flat containing , parallel to and having the same dimension as . 11. a Find an example to show that the join of two flats may not be) the set of all lines connecting all points in the union of these flats. b Show that if and are flats with , then is the) union of all lines where and . 12. Show that if and , then dim max dim dim Affine Geometry 441 13. Let . Prove the following:dim a The join of any two distinct points is a line.) b The intersection of any two nonparallel lines is a point.) 14. Let . Prove the following:dim a The join of any two distinct points is a line.) b The intersection of any two nonparallel planes is a line.) c The join of any two lines whose intersection is a point is a plane.) d The intersection of two coplanar nonparallel lines is a point.) e The join of any two distinct parallel lines is a plane.) f The join of a line and a point not on that line is a plane.) g The intersection of a plane and a line not on that plane is a point.) 15. Prove that is a surjective affine transformation if and only if for some and . 16. Verify the group-theoretic remarks about the group homomorphism aff trans aff and the subgroup of . Chapter 17 Singular Values and the Moore–Penrose Inverse Singular Values Let and be finite-dimensional inner product spaces over or and let . The spectral theorem applied to can be of considerable help in understanding the relationship between and its adjoint . This relationship is shown in Figure 17.1. Note that and can be decomposed into direct sums and in such a manner that and act symmetrically in the sense that and Also, both and are zero on and , respectively. We begin by noting that is a positive Hermitian operator. Hence, if rk rk , then has an ordered orthonormal basis of eigenvectors for , where the corresponding eigenvalues can be arranged so that The set is an ordered orthonormal basis for ker ker and so is an ordered orthonormal basis for . ker im 444 Advanced Linear Algebra (uk)=skvk (vk)=skuk u1 v1 ur+1 vr+1 un vm im( *) ker()ker( *) im() ur vr ONB of eigenvectors for * ONB of eigenvectors for * Figure 17.1 For , the positive numbers are called the singular values of . If we set for , then for . We can achieve some “symmetry” here between and by setting for each , giving and The vectors are orthonormal, since if , then Hence, is an orthonormal basis for , which can be im ker extended to an orthonormal basis for , the extension being an orthonormal basis for . Moreover, sinceker the vectors are eigenvectors for with the same eigenvalues as for . This completes the picture in Figure 17.1. 445 Theorem 17.1 Let and be finite-dimensional inner product spaces over or and let have rank . Then there are ordered orthonormal bases and for and , respectively, for which ONB for ONB for im ker and ONB for ONB for im ker Moreover, for , where are called the of , defined by singular values for . The vectors are called the for and right singular vectors the vectors are called the for . left singular vectors The matrix version of the previous discussion leads to the well-known singular- value decomposition of a matrix. Let and let and be the orthonormal bases from and , respectively, in Theorem 17.1, for the operator . Then diag A change of orthonormal bases from the standard bases to and gives where and are unitary/orthogonal. This is the singular- value decomposition of . As to uniqueness, if , where and are unitary and is diagonal, with diagonal entries , then and since , it follows that the 's are eigenvalues of diag , that is, they are the squares of the singular values along with a sufficient number of 's. Hence, is uniquely determined by , up to the order of the diagonal elements. Singular Values and the Moore–Penrose Inverse446 Advanced Linear Algebra We state without proof the following uniqueness facts and refer the reader to [48] for details. If and if the eigenvalues are distinct, then is uniquely determined up to multiplication on the right by a diagonal matrix of the form with . If , then is never uniquely diag determined. If , then for any given there is a unique . Thus, we see that, in general, the singular-value decomposition is not unique. The Moore–Penrose Generalized Inverse Singular values lead to a generalization of the inverse of an operator that applies to all linear transformations. The setup is the same as in Figure 17.1. Referring to that figure, we are prompted to define a linear transformation by for for since then and Hence, if , then . The transformation is called the Moore–Penrose generalized inverse Moore–Penrose pseudoinverse or of . We abbreviate this as MP inverse. Note that the composition is the identity on the largest possible subspace of on which any composition of the form could be the identity, namely, the orthogonal complement of the kernel of . A similar statement holds for the composition . Hence, is as “close” to an inverse for as is possible. We have said that if is invertible, then . More is true: If is injective, then and so is a left inverse for . Also, if is surjective, then is a right inverse for . Hence the MP inverse generalizes the one- sided inverses as well. Here is a characterization of the MP inverse. Theorem 17.2 Let . The MP inverse of is completely characterized by the following four properties: 1) 2) 3 is Hermitian) 4 is Hermitian) 447 Proof. We leave it to the reader to show that does indeed satisfy conditions 1)–4) and prove only the uniqueness. Suppose that and satisfy 1)–4) when substituted for . Then and which shows that . The MP inverse can also be defined for matrices. In particular, if , then the matrix operator has an MP inverse . Since this is a linear transformation from to , it is just multiplication by a matrix . This matrix is the for and is denoted by . MP inverse Since and , the matrix version of Theorem 17.2 implies that is completely characterized by the four conditions 1) 2) 3 is Hermitian) 4 is Hermitian) Moreover, if is the singular-value decomposition of , then Singular Values and the Moore–Penrose Inverse448 Advanced Linear Algebra where is obtained from by replacing all nonzero entries by their multiplicative inverses. This follows from the characterization above and also from the fact that for , and for , Least Squares Approximation Let us now discuss the most important use of the MP inverse. Consider the system of linear equations where . As usual, or . This system has a solution () if and only if . If the system has no solution, then it is of considerable im practical importance to be able to solve the system where is the unique vector in that is closest to , as measured by the im unitary or Euclidean distance. This problem is called the () linear least squares problem. Any solution to the system is called a least squares solution to the system . Put another way, a least squares solution to is a vector for which is minimized. Suppose that and are least squares solutions to . Then and so . We will write for . Thus, if is a particular least ker ( ) squares solution, then the set of all least squares solutions is . ker Among all solutions, the most interesting is the solution of minimum norm. Note that if there is a least squares solution that lies in , then for anyker ker , we have and so will be the unique least squares solution of minimum norm. Before proceeding, we recall Theorem 9.14 that if is a subspace of a finite-() dimensional inner product space , then the best approximation to a vector from within is the unique vector for which . Now we can see how the MP inverse comes into play. 449 Theorem 17.3 Let . Among the least squares solutions to the system there is a unique solution of minimum norm, given by , where is the MP inverse of . Proof. A vector is a least squares solution if and only if . Using the characterization of the best approximation , we see that is a solution to if and only if im Since this is equivalent toim ker or This system of equations is called the for . Itsnormal equations solutions are precisely the least squares solutions to the system . To see that is a least squares solution, recall that, in the notation of Figure 17.1, and so and since is a basis for , we conclude that satisfies the normal equations. Finally, since , we deduce by the preceding ker remarks that is the unique least squares solution of minimum norm. Exercises 1. Let . Show that the singular values of are the same as those of . 2. Find the singular values and the singular value decomposition of the matrix Find . 3. Find the singular values and the singular value decomposition of the matrix Singular Values and the Moore–Penrose Inverse450 Advanced Linear Algebra Find . : Is it better to work with or ? Hint 4. Let be a column matrix over . Find a singular-value decomposition of . 5. Let and let be the square matrix block Show that, counting multiplicity, the nonzero eigenvalues of are precisely the singular values of together with their negatives. : Let Hint be a singular-value decomposition of and try factoring into a product where is unitary. Do not read the following second hint unless you get stuck. : Verify the block factorizationSecond Hint What are the eigenvalues of the middle factor on the right? Try ( and . ) 6. Use the results of the previous exercise to show that a matrix , its adjoint , its transpose and its conjugate all have the same singular values. Show also that if and are unitary, then and have the same singular values. 7. Let be nonsingular. Show that the following procedure produces a singular-value decomposition of . a Write where and the 's are)diag positive and the columns of form an orthonormal basis of eigenvectors for . We never said that this was a practical procedure. () b Let where the square roots are nonnegative.)diag Also let and U . 8. If is an matrix, then the of is Frobenius norm Show that is the sum of the squares of the singular values of . Chapter 18 An Introduction to Algebras Motivation We have spent considerable time studying the structure of a linear operator on a finite-dimensional vector space over a field . In our studies, we defined the -module and used the decomposition theorems for modules over a principal ideal domain to dissect this module. We concentrated on an individual operator , rather than the entire vector space . In fact, we have made relatively little use of the fact that is an algebra under composition. In this chapter, we give a brief introduction to the theory of algebras, of which is the most general, in the sense of Theorem 18.2 below. Associative Algebras An algebra is a combination of a ring and a vector space, with an axiom that links the ring product with scalar multiplication. Definition associative An over a field , or an , is a() algebra -algebra nonempty set , together with three operations, called denoted by addition ( )( ) (, denoted by juxtaposition and alsomultiplication scalar multiplication denoted by juxtaposition , for which the following properties hold:) 1 is a vector space over under addition and scalar multiplication.) 2 is a ring with identity under addition and multiplication.) 3 If and , then) An algebra is if it is finite-dimensional as a vector space. Anfinite-dimensional algebra is if is a commutative ring. An element iscommutative invertible if there is for which . Our definition requires that have a multiplicative identity. Such algebras are called . Algebras without unit are also of great importance, butunital algebras 452 Advanced Linear Algebra we will not study them here. Also, in this chapter, we will assume that all algebras are associative. Nonassociative algebras, such as Lie algebras and Jordan algebras, are important as well. The Center of an Algebra Definition The of an -algebra is the setcenter for all of all elements of that commute with every element of . The center of an algebra is never trivial since it contains a copy of : Definition An -algebra is if its center is as small as possible, that central is, if From a Vector Space to an Algebra If is a vector space over a field and if is a basis for , then it is natural to wonder whether we can form an -algebra simply by defining a product for the basis elements and then using the distributive laws to extend the product to . In particular, we choose a set of constants with the property that for each pair , only finitely many of the are nonzero. Then we set and make multiplication bilinear, that is, and for . It is easy to see that this does define a nonunital associative algebra provided that An Introduction to Algebras 453 for all and that is commutative if and only if for all . The constants are called the for the structure constants algebra . To get a unital algebra, we can take for a given , the structure constants to be in which case is the multiplicative identity. An alternative is to adjoin a new ( element to the basis and define its structure constants in this way.) Examples The following examples will make it clear why algebras are important. Example 18.1 If are fields, then is a vector space over . This vector space structure, along with the ring structure of , is an algebra over . Example 18.2 The ring of polynomials is an algebra over . Example 18.3 The ring of all matrices over a field is an algebra over , where scalar multiplication is defined by Example 18.4 The set of all linear operators on a vector space over a field is an -algebra, where addition is addition of functions, multiplication is composition of functions and scalar multiplication is given by The identity map is the multiplicative identity and the zero map is the additive identity. This algebra is also denoted by ,End since the linear operators on are also called endomorphisms of . Example 18.5 If is a group and is a field, then we can form a vector space over by taking all formal -linear combinations of elements of and treating as a basis for . This vector space can be made into an -algebra where the structure constants are determined by the group product, that is, if , then . The group identity is the algebra identity since and so and similarly, . The resulting associative algebra is called the over . group algebra Specifically, the elements of have the form 454 Advanced Linear Algebra where and . If then we can include additional terms with coefficients and reindex if necessary so that we may assume that and for all . Then the sum in is given by Also, the product is given by and the scalar product is The Usual Suspects Algebras have substructures and structure-preserving maps, as do groups, rings and other algebraic structures. Subalgebras Definition Let be an -algebra. A of is a subset of that is subalgebra a subring of with the same identity as and a subspace of . () The intersection of subalgebras is a subalgebra and so the family of all subalgebras of is a complete lattice, where meet is intersection and the join of a family of subalgebras is the intersection of all subalgebras of that contain the members of . The by a nonempty subset of an algebra is thesubalgebra generated smallest subalgebra of that contains and is easily seen to be the set of all linear combinations of finite products of elements of , that is, the subspace spanned by the products of finite subsets of elements of : alg Alternatively, is the set of all polynomials in the variables in . In alg particular, the algebra generated by a single element is the set of all polynomials in over . An Introduction to Algebras 455 Ideals and Quotients In defining the notion of an ideal of an algebra , we must consider the fact that may be noncommutative. Definition two-sided A of an associative algebra is a nonempty() ideal subset of that is closed under addition and subtraction, that is, and also left and right multiplication by elements of , that is, The by a nonempty subset of is the smallest idealideal generated containing and is equal to ideal Definition An algebra is if simple 1 The product in is not trivial, that is, for at least one pair of) elements 2 has no proper nonzero ideals.) Definition If is an ideal in , then the is the quotient quotient algebra ring/quotient space with operations where and . These operations make an -algebra. Homomorphisms Definition If and are -algebras a map is an algebra homomorphism if it is a ring homomorphism as well as a linear transformation, that is, and for . 456 Advanced Linear Algebra The usual terms monomorphism, epimorphism, isomorphism, embedding, endomorphism and automorphism apply to algebras with the analogous meaning as for vector spaces and modules. Example 18.6 Let be an -dimensional vector space over . Fix an ordered basis for . Consider the map defined by where is the matrix representation of with respect to the ordered basis . This map is a vector space isomorphism and since it is also an algebra isomorphism. Another View of Algebras If is an algebra over , then contains a copy of . Specifically, we define a function by for all , where is the multiplicative identity. The elements are in the center of , since for any , and Thus, . To see that is a ring homomorphism, we have Moreover, if and , then and so provided that in , we have . Thus, is an embedding. Theorem 18.1 1 If is an associative algebra over and if in , then the map) defined by is an embedding of the field into the center of the ring . Thus, can be embedded as a subring of . An Introduction to Algebras 457 2 Conversely, if is a ring with identity and if is a field, then ) is an -algebra with scalar multiplication defined by the product in . One interesting consequence of this theorem is that a ring whose center does not contain a field is not an algebra over field . This happens, for example,any with the ring . The Regular Representation of an Algebra An algebra homomorphism is called a of the representation algebra in . A representation is if it is injective, that is, if faithful is an embedding. In this case, is isomorphic to a subalgebra of . Actually, the endomorphism algebras are the most general algebras possible, in the sense that any algebra has a faithful representation in some endomorphism algebra. Theorem 18.2 Any associative -algebra is isomorphic to a subalgebra of the endomorphism algebra . In fact, if is the left multiplication map defined by then the map is an algebra embedding, called the left regular representation of . When , we can select an ordered basis for and representdim the elements of by matrices. This gives an embedding of into the matrix algebra , called the of left regular matrix representation with respect to the ordered basis . Example 18.7 Let be a finite cyclic group. Let be an ordered basis for the group algebra . The multiplication map that is multiplcation by is a shifting of with wraparound and so the matrix () representation of is the matrix whose columns are obtained from the identity matrix by shifting columns to the right with wrap around . For example, () These matrices are called .circulant matrices 458 Advanced Linear Algebra Since the endomorphism algebras are of obvious importance, let us examine them a bit more closely. Theorem 18.3 Let be a vector space over a field . 1 The algebra has center) and so is central. 2 The set of all elements of that have finite rank is an ideal of) and is contained in all other ideals of . 3 is simple if and only if is finite-dimensional.) Proof. We leave the proof of parts 1) and 3) as exercises. For part 2), we leave it to the reader to show that is an ideal of . Let be a nonzero ideal of . Let have rank . Then there is a basis (a disjoint union) and a nonzero for which is a finite set, and for all . Thus, is a linear combination over of endomorphisms defined by Hence, we need only show that . If is nonzero, then there is an for which . If is defined by and is defined by then and so . Annihilators and Minimal Polynomials If is an -algebra an , then it may happen that satisfies a nonzero polynomial . This always happens, in particular, if is finite- dimensional, since in this case the powers must be linearly dependent and so there is a nonzero polynomial in that is equal to . Definition Let be an -algebra. An element is if there is a algebraic nonzero polynomial for which . If is algebraic, the An Introduction to Algebras 459 monic polynomial of smallest degree that is satisfied by is called the minimal polynomial of . If is algebraic over , then the subalgebra generated by over is deg deg and this is isomorphic to the quotient algebra where is the ideal generated by the minimal polynomial of . We leave the details of this as an exercise. The minimal polynomial can be used to tell when an element is invertible. Theorem 18.4 1 The minimal polynomial of generates the of ,) annihilator that is, the ideal ann of all polynomials that annihilate . 2 The element is invertible if and only if has nonzero constant) term. Proof. We prove only the second statement. If is invertible but then . Multiplying by gives , which contradicts the minimality of . Conversely, ifdeg where , then and so and so 460 Advanced Linear Algebra Theorem 18.5 If is a finite-dimensional -algebra, then every element of is algebraic. There are infinite-dimensional algebras in which all elements are algebraic. Proof. The first statement has been proved. To prove the second, let us consider the complex field as a -algebra. The set of algebraic elements of is a field, known as the field of . These are the complex numbersalgebraic numbers that are roots of some nonzero polynomial with rational or integral() coefficients. To see this, if , then the subalgebra is finite-dimensional. Also, is a field. To prove this, first note that since is a field, the minimal polynomial of any nonzero is irreducible, for if , then and so one of and is , which implies that or . Since is irreducible, it has nonzero constant term and so the inverse of is a polynomial in , that is, . Of course, is closed under addition and multiplication and so is a subfield of . Thus, is an algebra over . By similar reasoning, if , then the minimal polynomial of over is irreducible and so . Since is the set of all polynomials in the “variables” and , it is closed under addition and multiplication as well. Hence, is a finite- dimensional algebra over , as well as a subfield of . Now, dim dim dim and so is finite-dimensional over . Hence, the elements of are algebraic over , that is, . But contains and and so is a field. We claim that is not finite-dimensional over . This follows from the fact that for every prime , the polynomial is irreducible over by ( Eisenstein's criterion . Hence, if is a complex root of , then has) minimal polynomial over and so the dimension of over is . Hence, cannot be finite-dimensional. The Spectrum of an Element Let be an algebra. A nonzero element is a if left zero divisor for some and a if for some . In the right zero divisor exercises, we ask the reader to show that an element is a left zeroalgebraic divisor if and only if it is a right zero divisor. Theorem 18.6 Let be a algebra. An algebraic element is invertible if and only if it is not a zero divisor. Proof. If is invertible and , then multiplying by gives . Conversely, suppose that is not invertible but implies . Then An Introduction to Algebras 461 for some nonzero polynomial and so , which implies that , a contradiction to the minimality of . We have seen that the eigenvalues of a linear operator on a finite-dimensional vector space are the roots of the minimal polynomial of , or equivalently, the scalars for which is not invertible. By analogy, we can define the eigenvalues of an element of an algebra . Theorem 18.7 Let be an algebra and let be algebraic. An element is a root of the minimal polynomial if and only if is not invertible in . Proof. If is not invertible, then and since is satisfied by , it follows that Hence, . Alternatively, if is not invertible, then there is a nonzero such that , that is, . Hence, for any polynomial we have . Setting gives . Conversely, if , then and so , which shows that is a zero divisor and therefore not invertible. Definition Let A be an -algebra and let be algebraic. The roots of the minimal polynomial of are called the of . The set of alleigenvalues eigenvalues of Spec is called the of .spectrum Note that is invertible if and only if . Spec Theorem 18.8 The Let be an algebra over an()spectral mapping theorem algebraically closed field . Let and let . Then Spec Spec Spec Proof. We leave it as an exercise to show that . For Spec Spec the reverse inclusion, let and suppose that Spec Then 462 Advanced Linear Algebra and since the left-hand side is not invertible, neither is one of the factors , whence . ButSpec and so . Hence, . Spec Spec Spec Theorem 18.9 Let be an algebra over an algebraically closed field . If , then Spec Spec Proof. If , then is invertible and a simple computation Spec gives and so is invertible and . If , then is Spec Spec invertible. We leave it as an exercise to show that this implies that is also invertible and so . Thus, and by symmetry, Spec Spec Spec equality must hold. Division Algebras Some important associative algebras have the property that all nonzero elements are invertible and yet is not a field since it is not commutative. Definition An associative algebra over a field is a if division algebra every nonzero element has a multiplicative inverse. Our goal in this section is to classify all finite-dimensional division algebras over the real field , over any algebraically closed field and over any finite field. The classification of finite-dimensional division algebras over the rational field is quite complicated and we will not treat it here. The Quaternions Perhaps the most famous noncommutative division algebra is the following. Define a real vector space with basis To make into an -algebra, define the product of basis vectors as follows: 1 for all ) 2) 3) 4) An Introduction to Algebras 463 Note that 3 can be stated as follows: The product of two consecutive elements) is the next element with wraparound . Also, 4 says that for() ) . This product is extended to all of by distributivity. We leave it to the reader to verify that is a division algebra, called Hamilton's quaternions, after their discoverer William Rowan Hamilton 1805-1865 .() (Readers familiar with group theory will recognize the quaternion group . The quaternions have applications in geometry, computer) science and physics. Finite-Dimensional Division Algebras over an Algebraically Closed Field It happens that there are no interesting finite-dimensional division algebras over an algebraically closed field. Theorem 18.10 If is a finite-dimensional division algebra over an algebraically closed field then . Proof. Let have minimal polynomial . Since a division algebra has no zero divisors, must be irreducible over and so must be linear. Hence, and so . Finite-Dimensional Division Algebras over a Finite Field The finite-dimensional division algebras over a finite field are also easily described: they are all commutative and so are finite fields. The proof, however, is a bit more challenging. To understand the proof, we need two facts: the class equation and some information about the complex roots of unity. So let us briefly describe what we need. The Class Equation Those who have studied group theory have no doubt encountered the famous class equation. Let be a finite group. Each can be thought of as a permutation of defined by for all . The set of all conjugates of is denoted by and so This set is also called a in . Now, the following areconjugacy class equivalent: 464 Advanced Linear Algebra where is the of . But if and only if and are in the samecentralizer coset of . Thus, there is a one-to-one correspondence between the conjugates of and the cosets of . Hence, Since the distinct conjugacy classes form a partition of (because conjugacy is an equivalence relation), we have where is a set consisting of exactly one element from each conjugacy class . Note that a conjugacy class has size if and only if for all , that is, for all and these are precisely the elements in the center of . Hence, the previous equation can be written in the form where is a set consisting of exactly one element from each conjugacy class of size greater than . This is the for .class equation The Complex Roots of Unity If is a positive integer, then the complex th are the complex roots of unity solutions to the equation The set of complex th roots of unity is a cyclic group of order . To see this, note first that is an abelian group since implies that and . Also, since has no multiple roots, has order . Now, in any finite abelian group , if is the maximum order of all elements of , then for all . Thus, if no element of has order , then and every satisfies the equation , which has fewer than solutions. This contradiction implies that some element of must have order and so is cyclic. An Introduction to Algebras 465 The elements of that generate , that is, the elements of order are called the th roots of unity. We denote the set of primitive th roots ofprimitive unity by . Hence, if , then has size , where is the Euler phi function. The value is defined to ( be the number of positive integers less than or equal to and relatively prime to .) The th is defined by cyclotomic polynomial Thus, deg Since every th root of unity is a primitive th root of unity for some and since every primitive th root of unity for is also an th root of unity, we deduce that where the union is a disjoint one. It follows that Finally, we show that is monic and has integer coefficients by induction on . It is clear from the definition that is monic. Since , the result is true for . If is a prime, then all nonidentity th roots of unity are primitive and so 2 and the result holds for . Assume the result holds for all proper divisors of . Then By the induction hypothesis, has integer coefficients and it follows that must also have integer coefficients. Wedderburn's Theorem Now we can prove Wedderburn's theorem. 466 Advanced Linear Algebra Theorem 18.11 If is a finite division algebra, then()Wedderburn's theorem is a field. Proof. We must show that is commutative. Let be the multiplicative group of all nonzero elements of . The class equation is where the sum is taken over one representative from each conjugacy class of size greater than . If we assume for the purposes of contradiction that is not commutative, that is, that , then the sum on the far right is not an empty sum and so for some . The sets and are subalgebras of and, in fact, is a commutative division algebra; that is, a field. Let . Since , we may view and as vector spaces over and so and for integers . The class equation now gives and since , it follows that . If is the th cyclotomic polynomial, then divides . But also divides each summand on the far right above, since its roots are not roots of . It follows that . On the other hand, and since implies that , we have a contradiction. Hence and is commutative, that is, is a field. Finite-Dimensional Real Division Algebras We now consider the finite-dimensional division algebras over the real field . In 1877, Frobenius proved that there are only three such division algebras. Theorem 18.12 If is a finite-dimensional division algbera()Frobenius, 1877 over , then or Proof. Note first that the minimal polynomial of any is either linear, in which case or irreducible quadratic with . Completing the square gives An Introduction to Algebras 467 Hence, any has the form where and either or . Hence, but . Thus, every element of is the sum of an element of and an element of the set that is, as sets: Also, . To see that is a subspace of , let . We wish to show that . If for some , then . So assume that and are linearly independent. Then and are nonzero and so also nonreal. Now, and cannot both be real, since then and would be real. We have seen that and where , at least one of or is nonzero and . Then and so Collecting the real part on one side gives Now, if we knew that and were linearly independent over we could conclude that and so and which shows that and are in . To see that is linearly independent, it is equivalent to show that is linearly independent. But if 468 Advanced Linear Algebra for , then and since , it follows that and so or . But since and since are linearly independent. Thus, is a subspace of and We now look at , which is a real vector space. If , then and we are done, so assume otherwise. If is nonzero, then where . Hence, satisfies . If then and we are done. If not, then is a proper subspace of . In the quaternion field, there is an element for which . So we seek a with this property. To this end, define a bilinear form on by Then it is easy to see that this form is a real inner product on positive ( definite, symmetric and bilinear . Hence, if is a proper subspace of , then) where denotes the orthogonal direct sum. If is nonzero, then for and so if , then and Now, is a subspace of and so Setting , we have and and so and we can write An Introduction to Algebras 469 Now, if , then there is a for which and The third equation is and so whence , which is false. Hence, and This completes the proof. Exercises 1. Prove that the subalgebra generated by a nonempty subset of an algebra is the subspace spanned by the products of finite subsets of elements of : alg 2. Verify that the group algebra is indeed an associative algebra over . 3. Show that the kernel of an algebra homomorphism is an ideal. 4. Let be a finite-dimensional algebra over and let be a subalgebra. Show that if is invertible, then . 5. If is an algebra and is nonempty, define the of centralizer to be the set of elements of that commute with all elements of . Prove that is a subalgebra of . 6. Show that is not an algebra over any field. 7. Let be the algebra generated over by a single algebraic element . Show that is isomorphic to the quotient algebra , where is the ideal generated by . What can you say about ? What is the dimension of ? What happens if is not algebraic? 8. Let be a finite group. For of the form let . Prove that is an algebra homomorphism, where is an algebra over itself. 9. Prove the of algebras: A homomorphismfirst isomorphism theorem of -algebras induces an isomorphism ker im defined by . ker 10. Prove that the quaternion field is an -algebra and a field. : For Hint 470 Advanced Linear Algebra () consider 11. Describe the left regular representation of the quaternions using the ordered basis . 12. Let be the group of permutations bijective functions of the ordered set () , under composition. Verify the following statements. Each defines a linear isomorphism on the vector space with basis over a field . This defines an algebra homomorphism with the property that . What does the matrix representation of a look like? Is the representation faithful? 13. Show that the center of the algebra is 14. Show that is simple if and only if . dim 15. Prove that for , the matrix algebras are central and simple. 16. An element is if there is a for which , in left-invertible which case is called a of . Similarly, is left inverse right- invertible if there is a for which , in which case is called a right inverse one-sided inverses of . Left and right inverses are called and an ordinary inverse is called a . Let betwo-sided inverse algebraic over . a Prove that for some if and only if for some .) Does necessarily equal ? b Prove that if has a one-sided inverse , then is a two-sided inverse.) Does this hold if is not algebraic? : Consider the algebra Hint . c Let be algebraic. Show that is invertible if and only if ) and are invertible, in which case is also invertible. Chapter 19 The Umbral Calculus In this chapter, we give a brief introduction to an area called the umbral calculus. This is a linear-algebraic theory used to study certain types of polynomial functions that play an important role in applied mathematics. We give only a brief introduction to the subject, emphasizing the algebraic aspects rather than the applications. For more on the umbral calculus, may we suggest The Umbral Calculus, by Roman 1984 ? One bit of notation: The are defined bylower factorial numbers Formal Power Series We begin with a few remarks concerning formal power series. Let denote the algebra of formal power series in the variable , with complex coefficients. Thus, is the set of all formal sums of the form ()19.1 where the complex numbers . Addition and multiplication are purely () formal: and The of is the smallest exponent of that appears with a nonzeroorder coefficient. The order of the zero series is defined to be . Note that a series 472 Advanced Linear Algebra has a multiplicative inverse, denoted by , if and only if . We leave it to the reader to show that and min If is a sequence in with as , then for any series we may substitute for to get the series which is well-defined since the coefficient of each power of is a finite sum. In particular, if , then and so the composition is well-defined. It is easy to see that . If , then has a compositional inverse, denoted by and satisfying A series with is called a . delta series The sequence of powers of a delta series forms a for , in the pseudobasis sense that for any , there exists a unique sequence of constants for which Finally, we note that the formal derivative of the series 19.1 is given by() The operator is a derivation, that is, The Umbral Calculus 473 The Umbral Algebra Let denote the algebra of polynomials in a single variable over the complex field. One of the starting points of the umbral calculus is the fact that any formal power series in can play three different roles: as a formal power series, as a linear functional on and as a linear operator on . Let us first explore the connection between formal power series and linear functionals. Let denote the vector space of all linear functionals on . Note that is the algebraic dual space of , as defined in Chapter 2. It will be convenient to denote the action of on by ()This is the “bra-ket” notation of Paul Dirac. The vector space operations on then take the form and Note also that since any linear functional on is uniquely determined by its values on a basis for the functional is uniquely determined by the values for . Now, any formal series in can be written in the form ! and we can use this to define a linear functional by setting for . In other words, the linear functional is defined by ! where the expression on the left is just a formal power series. Note in particular that ! where is the Kronecker delta function. This implies that and so is the functional “ th derivative at .” Also, is evaluation at . 474 Advanced Linear Algebra As it happens, any linear functional on has the form . To see this, we simply note that if ! then for all and so as linear functionals, . Thus, we can define a map by . Theorem 19.1 The map defined by is a vector space isomorphism from onto . Proof. To see that is injective, note that for all Moreover, the map is surjective, since for any , the linear functional has the property that . Finally, ! !! From now on, we shall identify the vector space with the vector space , using the isomorphism . Thus, we think of linear functionals on simply as formal power series. The advantage of this approach is that is more than just a vector space—it is an algebra. Hence, we have automatically defined a multiplication of linear functionals, namely, the product of formal power series. The algebra , when thought of as both the algebra of formal power series and the algebra of linear functionals on , is called the . umbral algebra Let us consider an example. Example 19.1 For , the is defined by evaluation functional The Umbral Calculus 475 In particular, and so the formal power series representation for this functional is !! which is the exponential series. If is evaluation at , then and so the product of evaluation at and evaluation at is evaluation at . When we are thinking of a delta series as a linear functional, we refer to it as a . Similarly, an invertible series is referred to as andelta functional invertible functional. Here are some simple consequences of the development so far. Theorem 19.2 1 For any ,) 2 For any ,) 3 For any ,) 4) deg 5 If for all , then) where the sum on the right is a finite one. 6 If for all , then) for all 7 If for all , then)deg for all 476 Advanced Linear Algebra Proof. We prove only part 3 . Let) !! and Then ! and applying both sides of this as linear functionals to gives) The result now follows from the fact that part 1 implies and) . We can now present our first “umbral” result. Theorem 19.3 For any and , Proof. By linearity, we need only establish this for . But if ! then ! ! Let us consider a few examples of important linear functionals and their power series representations. Example 19.2 1 We have already encountered the , satisfying) evaluation functional The Umbral Calculus 477 2 The is the delta functional ,) forward difference functional satisfying 3 The is the delta functional e , satisfying) Abel functional e 4 The invertible functional satisfies) as can be seen by setting and expanding the expression . 5 To determine the linear functional satisfying) we observe that ! The inverse of this functional is associated with the Bernoulli polynomials, which play a very important role in mathematics and its applications. In fact, the numbers are known as the .Bernoulli numbers Formal Power Series as Linear Operators We now turn to the connection between formal power series and linear operators on . Let us denote the th derivative operator on by . Thus, We can then extend this to formal series in , ! 19.2() 478 Advanced Linear Algebra by defining the linear operator by !! the latter sum being a finite one. Note in particular that ()19.3 With this definition, we see that each formal power series plays three roles in the umbral calculus, namely, as a formal power series, as a linear functional and as a linear operator. The two notations and will make it clear whether we are thinking of as a functional or as an operator. It is important to note that in if and only if as linear functionals, which holds if and only if as linear operators. It is also worth noting that and so we may write without ambiguity. In addition, for all and . When we are thinking of a delta series as an operator, we call it a delta operator. The following theorem describes the key relationship between linear functionals and linear operators of the form . Theorem 19.4 If , then for all polynomials . Proof. If has the form 19.2 , then by 19.3 , () () () 19.4 By linearity, this holds for replaced by any polynomial . Hence, applying this to the product gives Equation 19.4 shows that applying the linear functional is equivalent to() ( applying the operator and then following by evaluation at . The Umbral Calculus 479 Here are the operator versions of the functionals in Example 19.2. Example 19.3 1 The operator satisfies) ! and so for all . Thus is a . translation operator 2 The is the delta operator , where) forward difference operator ) 3 The is the delta operator e , where) Abel operator e 4 The invertible operator satisfies) 5 The operator is easily seen to satisfy)) We have seen that all linear functionals on have the form , for . However, not all linear operators on have this form. To see this, observe that deg deg but the linear operator defined by does not have this property. Let us characterize the linear operators of the form . First, we need a lemma. Lemma 19.5 If is a linear operator on and for some delta series , then . deg deg Proof. For any , deg deg deg and so 480 Advanced Linear Algebra deg deg Since we have the basis for an induction. When deg we get . Assume that the result is true for . Thendeg deg deg Theorem 19.6 The following are equivalent for a linear operator . 1 has the form , that is, there exists an for which , as) linear operators. 2 commutes with the derivative operator, that is, .) 3 commutes with any delta operator , that is, .) 4 commutes with any translation operator, that is, .) Proof. It is clear that 1 implies 2 . For the converse, let)) ! Then Now, since commutes with , we have and since this holds for all and we get . We leave the rest of the proof as an exercise. Sheffer Sequences We can now define the principal object of study in the umbral calculus. When referring to a sequence in , we shall always assume that deg for all . Theorem 19.7 Let be a delta series, let be an invertible series and consider the geometric sequence in . Then there is a unique sequence in satisfying the orthogonality conditions The Umbral Calculus 481 (19.5) for all . Proof. The uniqueness follows from Theorem 19.2. For the existence, if we set and where , then 19.5 is () Taking we get For we have and using the fact that we can solve this for . By successively taking we can solve the resulting equations for the coefficients of the sequence . Definition The sequence in 19.5 is called the for the () Sheffer sequence ordered pair . We shorten this by saying that is Sheffer for . Two special types of Sheffer sequences deserve explicit mention. Definition The Sheffer sequence for a pair of the form is called the associated sequence for . The Sheffer sequence for a pair of the form is called the for .Appell sequence 482 Advanced Linear Algebra Note that the sequence is Sheffer for if and only if which is equivalent to which, in turn, is equivalent to saying that the sequence is the associated sequence for . Theorem 19.8 The sequence is Sheffer for if and only if the sequence is the associated sequence for . Before considering examples, we wish to describe several characterizations of Sheffer sequences. First, we require a key result. Theorem 19.9 The expansion theorems() Let be Sheffer for . 1 For any ,) ! 2 For any ,) ! Proof. Part 1 follows from Theorem 19.2, since) !! ! Part 2 follows in a similar way from Theorem 19.2.) We can now begin our characterization of Sheffer sequences, starting with the generating function. The idea of a generating function is quite simple. If is a sequence of polynomials, we may define a formal power series of the form ! This is referred to as the for the sequence()exponential generating function . The term exponential refers to the presence of ! in this series. When( this is not present, we have an ordinary generating function. Since the series is a formal one, knowing is equivalent in theory, if not always in practice () The Umbral Calculus 483 to knowing the polynomials . Moreover, a knowledge of the generating function of a sequence of polynomials can often lead to a deeper understanding of the sequence itself, that might not be otherwise easily accessible. For this reason, generating functions are studied quite extensively. For the proofs of the following characterizations, we refer the reader to Roman 1984 . Theorem 19.10 Generating function() 1 The sequence is the associated sequence for a delta series if and) only if ! where is the compositional inverse of . 2 The sequence is Sheffer for if and only if) ! The sum on the right is called the of .generating function Proof. Part 1 is a special case of part 2 . For part 2 , the expression above is)) ) equivalent to ! which is equivalent to ! But if is Sheffer for , then this is just the expansion theorem for . Conversely, this expression implies that ! and so , which says that is Sheffer for . We can now give a representation for Sheffer sequences. Theorem 19.11 Conjugate representation() 484 Advanced Linear Algebra 1 A sequence is the associated sequence for if and only if) 2 A sequence is Sheffer for if and only if) Proof. We need only prove part 2 . We know that is Sheffer for) if and only if ! But this is equivalent to ! Expanding the exponential on the left gives ! Replacing by gives the result. Sheffer sequences can also be characterized by means of linear operators. Theorem 19.12 Operator characterization ) 1 A sequence is the associated sequence for if and only if) a ) b for ) 2 A sequence is Sheffer for for some invertible series if) and only if for all . Proof. For part 1 , if is associated with , then) and The Umbral Calculus 485 and since this holds for all we get 1b . Conversely, if 1a and 1b hold, )) ) then and so is the associated sequence for . As for part 2 , if is Sheffer for , then) and so , as desired. Conversely, suppose that and let be the associated sequence for . Let be the invertible linear operator on defined by Then and so Lemma 19.5 implies that for some invertible series . Then and so is Sheffer for . We next give a formula for the action of a linear operator on a Sheffer sequence. 486 Advanced Linear Algebra Theorem 19.13 Let be a Sheffer sequence for and let be associated with . Then for any we have Proof. By the expansion theorem ! we have ! ! which is the desired formula. Theorem 19.14 1 A sequence is the associated sequence for a)( )The binomial identity delta series if and only if it is of , that is, if and only if it binomial type satisfies the identity for all . 2 A sequence is Sheffer for for)( )The Sheffer identity some invertible if and only if for all , where is the associated sequence for . Proof. To prove part 1 , if is an associated sequence, then taking) in Theorem 19.13 gives the binomial identity. Conversely, suppose that the sequence is of binomial type. We will use the operator characterization to show that is an associated sequence. Taking we have for , and so . Also, The Umbral Calculus 487 and so . Assuming that for we have and so . Thus, . Next, define a linear functional by Since and we deduce that is a delta series. Now, the binomial identity gives and so and since this holds for all , we get . Thus, is the associated sequence for . For part 2 , if is a Sheffer sequence, then taking in Theorem) 19.13 gives the Sheffer identity. Conversely, suppose that the Sheffer identity holds, where is the associated sequence for . It suffices to show that for some invertible . Define a linear operator by Then and by the Sheffer identity, and the two are equal by part 1 . Hence, commutes with and is therefore) of the form , as desired. 488 Advanced Linear Algebra Examples of Sheffer Sequences We can now give some examples of Sheffer sequences. While it is often a relatively straightforward matter to verify that a given sequence is Sheffer for a given pair , it is quite another matter to find the Sheffer sequence for a given pair. The umbral calculus provides two formulas for this purpose, one of which is direct, but requires the usually very difficult computation of the series . The other is a recurrence relation that expresses each in terms of previous terms in the Sheffer sequence. Unfortunately, space does not permit us to discuss these formulas in detail. However, we will discuss the recurrence formula for associated sequences later in this chapter. Example 19.4 The sequence is the associated sequence for the delta series . The generating function for this sequence is and the binomial identity is the well-known binomial formula Example 19.5 The lower factorial polynomials form the associated sequence for the forward difference functional discussed in Example 19.2. To see this, we simply compute, using Theorem 19.12. Since is defined to be , we have . Also, The generating function for the lower factorial polynomials is log The Umbral Calculus 489 which can be rewritten in the more familiar form Of course, this is a formal identity, so there is no need to make any restrictions on . The binomial identity in this case is which can also be written in the form This is known as the .Vandermonde convolution formula Example 19.6 The Abel polynomials form the associated sequence for the Abel functional e also discussed in Example 19.2. We leave verification of this to the reader. The generating function for the Abel polynomials is Taking the formal derivative of this with respect to gives which, for , gives a formula for the compositional inverse of the series , Example 19.7 The famous form the AppellHermite polynomials sequence for the invertible functional 490 Advanced Linear Algebra We ask the reader to show that is the Appell sequence for if and only if . Using this fact, we get The generating function for the Hermite polynomials is and the Sheffer identity is We should remark that the Hermite polynomials, as defined in the literature, often differ from our definition by a multiplicative constant. Example 19.8 The well-known and important Laguerre polynomials of order form the Sheffer sequence for the pair It is possible to show although we will not do so here that The generating function of the Laguerre polynomials is As with the Hermite polynomials, some definitions of the Laguerre polynomials differ by a multiplicative constant. We presume that the few examples we have given here indicate that the umbral calculus applies to a significant range of important polynomial sequences. In Roman 1984 , we discuss approximately 30 different sequences of polynomials that are or are closely related to Sheffer sequences.() Umbral Operators and Umbral Shifts We have now established the basic framework of the umbral calculus. As we have seen, the umbral algebra plays three roles: as the algebra of formal power series in a single variable, as the algebra of all linear functionals on and as the The Umbral Calculus 491 algebra of all linear operators on that commute with the derivative operator. Moreover, since is an algebra, we can consider geometric sequences in , where and . We have seen by example that the orthogonality conditions define important families of polynomial sequences. While the machinery that we have developed so far does unify a number of topics from the classical study of polynomial sequences for example, special( cases of the expansion theorem include Taylor's expansion, the Euler– MacLaurin formula and Boole's summation formula , it does not provide much) new insight into their study. Our plan now is to take a brief look at some of the deeper results in the umbral calculus, which center on the interplay between operators on and their adjoints, which are operators on the umbral algebra . We begin by defining two important operators on associated with each Sheffer sequence. Definition Let be Sheffer for . The linear operator defined by is called the for the pair , or for the sequenceSheffer operator . If is the associated sequence for , the Sheffer operator is called the for , or for .umbral operator Definition Let be Sheffer for . The linear operator defined by is called the for the pair , or for the sequence . IfSheffer shift is the associated sequence for , the Sheffer operator is called the for , or for .umbral shift 492 Advanced Linear Algebra It is clear that each Sheffer sequence uniquely determines a Sheffer operator and vice versa. Hence, knowing the Sheffer operator of a sequence is equivalent to knowing the sequence. Continuous Operators on the Umbral Algebra It is clearly desirable that a linear operator on the umbral algebra pass under infinite sums, that is, that ()19.6 whenever the sum on the left is defined, which is precisely when as . Not all operators on have this property, which leads to the following definition. Definition A linear operator on the umbral algebra is if it continuous satisfies .19.6) The term continuous can be justified by defining a topology on . However, since no additional topological concepts will be needed, we will not do so here. Note that in order for 19.6 to make sense, we must have . It() turns out that this condition is also sufficient. Theorem 19.15 A linear operator on is continuous if and only if ()19.7 Proof. The necessity is clear. Suppose that 19.7 holds and that .() For any , we have ()19.8 Since ()19.7 implies that we may choose large enough that and for The Umbral Calculus 493 Hence, 19.8 gives() which implies the desired result. Operator Adjoints If is a linear operator on , then its operator adjoint is an () operator on defined by In the symbolism of the umbral calculus, this is (We have reduced the number of parentheses used to aid clarity. Let us recall the basic properties of the adjoint from Chapter 3. Theorem 19.16 For , 1) 2 for any ) 3) 4 for any invertible ) Thus, the map that sends to its adjoint is a linear transformation from to . Moreover, since implies that for all and , which in turn implies that , we deduce that is injective. The next theorem describes the range of . Theorem 19.17 A linear operator is the adjoint of a linear operator if and only if is continuous. Proof. First, suppose that for some and let . If , then for all we have and so it is only necessary to take large enough that for deg all , whence 494 Advanced Linear Algebra for all and so . Thus, and is continuous. For the converse, assume that is continuous. If did have the form , then and since we are prompted to bydefine This makes sense since as and so the sum on the right is a finite sum. Then which implies that for all . Finally, since and are both continuous, we have . Umbral Operators and Automorphisms of the Umbral Algebra Figure 19.1 shows the map , which is an isomorphism from the vector space onto the space of all continuous linear operators on . We are interested in determining the images under this isomorphism of the set of umbral operators and the set of umbral shifts, as pictured in Figure 19.1. The Umbral Calculus 495 Figure 19.1 Let us begin with umbral operators. Suppose that is the umbral operator for the associated sequence , with delta series . Then for all and . Hence, and the continuity of implies that More generally, for any , ()19.9 In words, is composition by . From 19.9 , we deduce that is a vector space isomorphism and that() Hence, is an automorphism of the umbral algebra . It is a pleasant fact that this characterizes umbral operators. The first step in the proof of this is the following, whose proof is left as an exercise. Theorem 19.18 If is an automorphism of the umbral algebra, then preserves order, that is, . In particular, is continuous. Theorem 19.19 A linear operator on is an umbral operator if and only if its adjoint is an automorphism of the umbral algebra . Moreover, if is an umbral operator, then 496 Advanced Linear Algebra for all . In particular, . Proof. We have already shown that the adjoint of is an automorphism satisfying 19.9 . For the converse, suppose that is an automorphism of .() Since is surjective, there is a unique series for which . Moreover, Theorem 19.18 implies that is a delta series. Thus, which shows that is the associated sequence for and hence that is an umbral operator. Theorem 19.19 allows us to fill in one of the boxes on the right side of Figure 19.1. Let us see how we might use Theorem 19.19 to advantage in the study of associated sequences. We have seen that the isomorphism maps the set of umbral operators on onto the set of automorphisms of . But is a group aut aut under composition. So if and are umbral operators, then since is an automorphism of , it follows that the composition is an umbral operator. In fact, since we deduce that . Also, since we have . Thus, the set of umbral operators is a group under composition with and Let us see how this plays out with respect to associated sequences. If the The Umbral Calculus 497 associated sequence for is then and so is the umbral operator for the associated sequence This sequence, denoted by ()19.10 is called the of with . The umbral operatorumbral composition is the umbral operator for the associated sequence where and so Let us summarize. Theorem 19.20 1 The set of umbral operators on is a group under composition, with) and 2 The set of associated sequences forms a group under umbral composition) In particular, the umbral composition is the associated sequence for the composition , that is, The identity is the sequence and the inverse of is the associated sequence for the compositional inverse . 498 Advanced Linear Algebra 3 Let and . Then as operators,) 4 Let and . Then) Proof. We prove 3 as follows. For any and ,) which gives the desired result. Part 4 follows immediately from part 3 since )) is composition by . Sheffer Operators If is Sheffer for , then the linear operator defined by is called a . Sheffer operators are closely related to umbralSheffer operator operators, since if is associated with , then and so It follows that the Sheffer operators form a group with composition and inverse From this, we deduce that the umbral composition of Sheffer sequences is a Sheffer sequence. In particular, if is Sheffer for and is Sheffer for , then The Umbral Calculus 499 is Sheffer for . Umbral Shifts and Derivations of the Umbral Algebra We have seen that an operator on is an umbral operator if and only if its adjoint is an automorphism of . Now suppose that is the umbral shift for the associated sequence , associated with the delta series . Then 1) and so ()19.11 This implies that ()19.12 and further, by continuity, that ()19.13 Let us pause for a definition. Definition Let be an algebra. A linear operator on is a if derivation b for all . Thus, we have shown that the adjoint of an umbral shift is a derivation of the umbral algebra . Moreover, the expansion theorem and 19.11 show that () is surjective. This characterizes umbral shifts. First we need a preliminary result on surjective derivations. 500 Advanced Linear Algebra Theorem 19.21 Let be a surjective derivation on the umbral algebra . Then c for any and f , if . Inconstant particular, is continuous. Proof. We begin by noting that and so for all constants . Since is surjective, there must exist an for which Writing , we have which implies that . Finally, if , then , where and so Theorem 19.22 A linear operator on is an umbral shift if and only if its adjoint is a surjective derivation of the umbral algebra . Moreover, if is an umbral shift, then is derivation with respect to , that is, for all . In particular, . Proof. We have already seen that is derivation with respect to . For the converse, suppose that is a surjective derivation. Theorem 19.21 implies that there is a delta functional such that . If is the associated sequence for , then 1) Hence, , that is, is the umbral shift for . We have seen that the fact that the set of all automorphisms on is a group under composition shows that the set of all associated sequences is a group under umbral composition. The set of all surjective derivations on does not form a group. However, we do have the chain rule for derivations The Umbral Calculus 501 Theorem 19.23 The chain rule() Let and be surjective derivations on . Then Proof. This follows from and so continuity implies the result. The chain rule leads to the following umbral result. Theorem 19.24 If and are umbral shifts, then Proof. Taking adjoints in the chain rule gives We leave it as an exercise to show that . Now, by taking in Theorem 19.24 and observing that and so is multiplication by , we get Applying this to the associated sequence for gives the following important recurrence relation for . Theorem 19.25 The recurrence formula() Let be the associated sequence for . Then 1) 2) Proof. The first part is proved. As to the second, using Theorem 19.20 we have Example 19.9 The recurrence relation can be used to find the associated sequence for the forward difference functional . Since , the recurrence relation is 1) 502 Advanced Linear Algebra Using the fact that , we have 3 and so on, leading easily to the lower factorial polynomials Example 19.10 Consider the delta functional log Since is the forward difference functional, Theorem 19.20 implies that the associated sequence for is the inverse, under umbral composition, of the lower factorial polynomials. Thus, if we write then The coefficients in this equation are known as the Stirling numbers of the second kind and have great combinatorial significance. In fact, is the number of partitions of a set of size into blocks. The polynomials are called the .exponential polynomials The recurrence relation for the exponential polynomials is Equating coefficients of on both sides of this gives the well-known formula for the Stirling numbers Many other properties of the Stirling numbers can be derived by umbral means. Now we have the analog of part 3 of Theorem 19.20.) Theorem 19.26 Let be an umbral shift. Then The Umbral Calculus 503 Proof. We have from which the result follows. If , then is multiplication by and is the derivative with respect to and so the previous result becomes as operators on . The right side of this is called the of Pincherle derivative the operator . See [104]. () Sheffer Shifts Recall that the linear map where is Sheffer for is called a Sheffer shift. If is associated with , then and so and so From Theorem 19.26, the recurrence formula and the chain rule, we have We have proved the following. 504 Advanced Linear Algebra Theorem 19.27 Let be a Sheffer shift. Then 1) 2) The Transfer Formulas We conclude with a pair of formulas for the computation of associated sequences. Theorem 19.28 The ()transfer formulas Let be the associated sequence for . Then 1) 2) Proof. First we show that 1 and 2 are equivalent. Write . Then)) To prove 1 , we verify the operation conditions for an associated sequence for) the sequence . First, when the fourth equality above gives If , then , and so in general, we have as required. For the second required condition, Thus, is the associated sequence for . The Umbral Calculus 505 A Final Remark Unfortunately, space does not permit a detailed discussion of examples of Sheffer sequences nor the application of the umbral calculus to various classical problems. In [105], one can find a discussion of the following polynomial sequences: The lower factorial polynomials and Stirling numbers The exponential polynomials and Dobinski's formula The Gould polynomials The central factorial polynomials The Abel polynomials The Mittag-Leffler polynomials The Bessel polynomials The Bell polynomials The Hermite polynomials The Bernoulli polynomials and the Euler–MacLaurin expansion The Euler polynomials The Laguerre polynomials The Bernoulli polynomials of the second kind The Poisson–Charlier polynomials The actuarial polynomials The Meixner polynomials of the first and second kinds The Pidduck polynomials The Narumi polynomials The Boole polynomials The Peters polynomials The squared Hermite polynomials The Stirling polynomials The Mahler polynomials The Mott polynomials and more. In [105], we also find a discussion of how the umbral calculus can be used to approach the following types of problems: The connection constants problem Duplication formulas The Lagrange inversion formula Cross sequences Steffensen sequences Operational formulas Inverse relations Sheffer sequence solutions to recurrence relations Binomial convolution 506 Advanced Linear Algebra Finally, it is possible to generalize the classical umbral calculus that we have described in this chapter to provide a context for studying polynomial sequences such as those of the names Gegenbauer, Chebyshev and Jacobi. Also, there is a q-version of the umbral calculus that involves the alsoq-binomial coefficients ( known as the Gaussian coefficients) in place of the binomial coefficients. There is also a logarithmic version of the umbral calculus, which studies the and sequences ofharmonic logarithms logarithmic type. For more on these topics, please see [103], [106] and [107]. Exercises 1. Prove that , for any . 2. Prove that min , for any . 3. Show that any delta series has a compositional inverse. 4. Show that for any delta series , the sequence is a pseudobasis. 5. Prove that is a derivation. 6. Show that is a delta functional if and only if and . 7. Show that is invertible if and only if . 8. Show that for any a , and . 9. Show that e a for any polynomial . 10. Show that in if and only if as linear functionals, which holds if and only if as linear operators. 11. Prove that if is Sheffer for , then . Hint: Apply the functionals to both sides. 12. Verify that the Abel polynomials form the associated sequence for the Abel functional. 13. Show that a sequence is the Appell sequence for if and only if . 14. If is a delta series, show that the adjoint of the umbral operator is a vector space isomorphism of . 15. Prove that if is an automorphism of the umbral algebra, then preserves order, that is, . In particular, is continuous. 16. Show that an umbral operator maps associated sequences to associated sequences. 17. Let and be associated sequences. Define a linear operator by . Show that is an umbral operator. 18. Prove that if and are surjective derivations on , then . References General References [1] Jacobson, N., , second edition, W.H. Freeman, 1985.Basic Algebra I [2] Snapper, E. and Troyer, R., , Dover Publications,Metric Affine Geometry 1971. General Linear Algebra [3] Akivis, M., Goldberg, V., An Introduction To Linear Algebra and Tensors, Dover, 1977. [4] Blyth, T., Robertson, E., , Springer, 2002.Further Linear Algebra [5] Brualdi, R., Friedland, S., Klee, V., Combinatorial and Graph- Theoretical Problems in Linear Algebra, Springer, 1993. [6] Curtis, M., Place, P., , Springer, 1990.Abstract Linear Algebra [7] Fuhrmann, P., , Springer, 1996.A Polynomial Approach to Linear Algebra [8] Gel'fand, I. M., , Dover, 1989.Lectures On Linear Algebra [9] Greub, W., , Springer, 1995.Linear Algebra [10] Halmos, P. R., , Mathematical AssociationLinear Algebra Problem Book of America, 1995. [11] Halmos, P. R., , Springer, 1974.Finite-Dimensional Vector Spaces [12] Hamilton, A. G., , Cambridge University Press, 1990.Linear Algebra [13] Jacobson, N., , Springer,Lectures in Abstract Algebra II: Linear Algebra 1953. [14] Jänich, K., , Springer, 1994.Linear Algebra [15] Kaplansky, I., , Dover,Linear Algebra and Geometry: A Second Course 2003. [16] Kaye, R., Wilson, R., , Oxford University Press, 1998.Linear Algebra [17] Kostrikin, A. and Manin, Y., , Gordon andLinear Algebra and Geometry Breach Science Publishers, 1997. [18] Lax, P., , John Wiley, 1996.Linear Algebra [19] Lewis, J. G., Proceedings of the 5th SIAM Conference On Applied Linear Algebra, SIAM, 1994. 508 Advanced Linear Algebra [20] Marcus, M., Minc, H., , Dover, 1988.Introduction to Linear Algebra [21] Mirsky, L., , Dover, 1990.An Introduction to Linear Algebra [22] Nef, W., , Dover, 1988.Linear Algebra [23] Nering, E. D., , John Wiley, 1976.Linear Algebra and Matrix Theory [24] Pettofrezzo, A., , Dover, 1978.Matrices and Transformations [25] Porter, G., Hill, D., Interactive Linear Algebra: A Laboratory Course Using Mathcad, Springer, 1996. [26] Schneider, H., Barker, G., , Dover, 1989.Matrices and Linear Algebra [27] Schwartz, J., , Dover, 2001.Introduction to Matrices and Vectors [28] Shapiro, H., A survey of canonical forms and invariants for unitary similarity, 147:101–167 1991 .Linear Algebra and Its Applications () [29] Shilov, G., , Dover, 1977.Linear Algebra [30] Wilkinson, J., , Oxford UniversityThe Algebraic Eigenvalue Problem Press, 1988. Matrix Theory [31] Antosik, P., Swartz, C., , Springer, 1985.Matrix Methods in Analysis [32] Bapat, R., Raghavan, T., ,Nonnegative Matrices and Applications Cambridge University Press, 1997. [33] Barnett, S., , Oxford University Press, 1990.Matrices [34] Bellman, R., , SIAM, 1997.Introduction to Matrix Analysis [35] Berman, A., Plemmons, R., Non-negative Matrices in the Mathematical Sciences, SIAM, 1994. [36] Bhatia, R., , Springer, 1996.Matrix Analysis [37] Bowers, J., , Oxford University Press,Matrices and Quadratic Forms 2000. [38] Boyd, S., El Ghaoui, L., Feron, E.; Balakrishnan, V., Linear Matrix Inequalities in System and Control Theory, SIAM, 1994. [39] Chatelin, F., , John Wiley, 1993.Eigenvalues of Matrices [40] Ghaoui, L., ,Advances in Linear Matrix Inequality Methods in Control SIAM, 1999. [41] Coleman, T., Van Loan, C., , SIAM,Handbook for Matrix Computations 1988. [42] Duff, I., Erisman, A., Reid, J., ,Direct Methods for Sparse Matrices Oxford University Press, 1989. [43] Eves, H., , Dover, 1966.Elementary Matrix Theory [44] Franklin, J., , Dover, 2000.Matrix Theory [45] Gantmacher, F.R., , American Mathematical Society,Matrix Theory I 2000. [46] Gantmacher, F.R., , American Mathematical Society,Matrix Theory II 2000. [47] Gohberg, I., Lancaster, P., Rodman, L., Invariant Subspaces of Matrices with Applications, John Wiley, 1986. [48] Horn, R. and Johnson, C., , Cambridge University Press,Matrix Analysis 1985. References 509 [49] Horn, R. and Johnson, C., , CambridgeTopics in Matrix Analysis University Press, 1991. [50] Jennings, A., McKeown, J. J., , John Wiley, 1992.Matrix Computation [51] Joshi, A. W., , John Wiley, 1995.Matrices and Tensors in Physics [52] Laub, A., , SIAM, 2004.Matrix Analysis for Scientists and Engineers [53] Lütkepohl, H., , John Wiley, 1996.Handbook of Matrices [54] Marcus, M., Minc, H., A Survey of Matrix Theory and Matrix Inequalities, Dover, 1964. [55] Meyer, C., , SIAM, 2000.Matrix Analysis and Applied Linear Algebra [56] Muir, T., , Dover, 2003.A Treatise on the Theory of Determinants [57] Perlis, S., , Dover, 1991.Theory of Matrices [58] Serre, D., , Springer, 2002.Matrices: Theory and Applications [59] Stewart, G., , SIAM, 1998.Matrix Algorithms [60] Stewart, G., , SIAM, 2001.Matrix Algorithms Volume II: Eigensystems [61] Watkins, D., , John Wiley, 1991.Fundamentals of Matrix Computations Multilinear Algebra [62] Marcus, M., , MarcelFinite Dimensional Multilinear Algebra, Part I Dekker, 1971. [63] Marcus, M., , MarcelFinite Dimensional Multilinear Algebra, Part II Dekker, 1975. [64] Merris, R., , Gordon & Breach, 1997.Multilinear Algebra [65] Northcott, D. G., , Cambridge University Press, 1984.Multilinear Algebra Applied and Numerical Linear Algebra [66] Anderson, E., , SIAM, 1995.LAPACK User's Guide [67] Axelsson, O., , Cambridge University Press,Iterative Solution Methods 1994. [68] Bai, Z., Templates for the Solution of Algebraic Eigenvalue Problems: A Practical Guide, SIAM, 2000. [69] Banchoff, T., Wermer, J., , Springer,Linear Algebra Through Geometry 1992. [70] Blackford, L., , SIAM, 1997.ScaLAPACK User's Guide [71] Ciarlet, P. G., Introduction to Numerical Linear Algebra and Optimization, Cambridge University Press, 1989. [72] Campbell, S., Meyer, C., Generalized Inverses of Linear Transformations, Dover, 1991. [73] Datta, B., Johnson, C., Kaashoek, M., Plemmons, R., Sontag, E., Linear Algebra in Signals, Systems and Control, SIAM, 1988. [74] Demmel, J., , SIAM, 1997.Applied Numerical Linear Algebra [75] Dongarra, J., Bunch, J. R., Moler, C. B., Stewart, G. W., Linpack Users' Guide, SIAM, 1979. [76] Dongarra, J., Numerical Linear Algebra for High-Performance Computers, SIAM, 1998. 510 Advanced Linear Algebra [77] Dongarra, J., Templates for the Solution of Linear Systems: Building Blocks For Iterative Methods, SIAM, 1993. [78] Faddeeva, V. N., , Dover,Computational Methods of Linear Algebra [79] Frazier, M., ,An Introduction to Wavelets Through Linear Algebra Springer, 1999. [80] George, A., Gilbert, J., Liu, J., Graph Theory and Sparse Matrix Computation, Springer, 1993. [81] Golub, G., Van Dooren, P., Numerical Linear Algebra, Digital Signal Processing and Parallel Algorithms, Springer, 1991. [82] Granville S., , 2nd edition,Computational Methods of Linear Algebra John Wiley, 2005. [83] Greenbaum, A., , SIAM,Iterative Methods for Solving Linear Systems 1997. [84] Gustafson, K., Rao, D., Numerical Range: The Field of Values of Linear Operators and Matrices, Springer, 1996. [85] Hackbusch, W., ,Iterative Solution of Large Sparse Systems of Equations Springer, 1993. [86] Jacob, B., , Springer, 1995.Linear Functions and Matrix Theory [87] Kuijper, M., , Birkhäuser,First-Order Representations of Linear Systems 1994. [88] Meyer, C., Plemmons, R., Linear Algebra, Markov Chains, and Queueing Models, Springer, 1993. [89] Neumaier, A., , CambridgeInterval Methods for Systems of Equations University Press, 1991. [90] Nevanlinna, O., ,Convergence of Iterations for Linear Equations Birkhäuser, 1993. [91] Olshevsky, V., Fast Algorithms for Structured Matrices: Theory and Applications, SIAM, 2003. [92] Plemmon, R.J., Gallivan, K.A., Sameh, A.H., Parallel Algorithms for Matrix Computations, SIAM, 1990. [93] Rao, K. N., , John Wiley,Linear Algebra and Group Theory for Physicists 1996. [94] Reichel, L., Ruttan, A., Varga, R., , Walter deNumerical Linear Algebra Gruyter, 1993. [95] Saad, Y., , SIAM, 2003.Iterative Methods for Sparse Linear Systems [96] Scharlau, W., , Springer, 1985.Quadratic and Hermitian Forms [97] Snapper, E., Troyer, R., , Dover,Metric Affine Geometry [98] Spedicato, E., Computer Algorithms for Solving Linear Algebraic Equations, Springer, 1991. [99] Trefethen, L., Bau, D., , SIAM, 1997.Numerical Linear Algebra [100] Van Dooren, P., Wyman, B., ,Linear Algebra for Control Theory Springer, 1994. [101] Vorst, H., , CambridgeIterative Krylov Methods for Large Linear Systems University Press, 2003. [102] Young, D., , Dover, 2003.Iterative Solution of Large Linear Systems References 511 The Umbral Calculus [103] Loeb, D. and Rota, G.-C., Formal Power Series of Logarithmic Type, Advances in Mathematics, Vol. 75, No. 1, May 1989 1–118.() [104] Pincherle, S. \"Operatori lineari e coefficienti di fattoriali.\" Alti Accad. Naz. Lincei, Rend. Cl. Fis. Mat. Nat. 6 18, 417–519, 1933.() [105] Roman, S., , Pure and Applied Mathematics vol.The Umbral Calculus 111, Academic Press, 1984. [106] Roman, S., The logarithmic binomial formula, American Mathematical Monthly 99 1992 641–648.() [107] Roman, S., The harmonic logarithms and the binomial formula, Journal of Combinatorial Theory, series A, 63 1992 143–163.() Index of Symbols : the companion matrix of : characteristic polynomial of crk : column rank of cs : column space of diag : a block diagonal matrix with 's on the block diagonal ElemDiv : the multiset of elementary divisors InvFact: the multiset of invariant factors of : Jordan block : minimal polynomial of null: the nullity of : canonical projection modulo : Riesz vector for rk: the rank of rrk : row rank of rs : row space of : projection onto along : the multiplication by operator supp : the support of a function : the -vector space/ -module where : the complexification of : assignment, for example, means that stands for : subspace or submodule : proper subspace or proper submodule : subspace/ideal spanned by : submodule spanned by an embedding that is an isomorphism when all is finite-dimensional. : similarity of matrices or operators, associate in a ring. : cartesian product : orthogonal direct sum : external direct product : external direct sum : internal direct sum : means 514 : wedge product : tensor product : -fold tensor product : -fold cartesion product : and are relatively prime : affine combination Index of Symbols Index Abel functional, 477, 489 Abel operator, 479 Abel polynomials, 489 abelian, 17 absolutely convergent, 330 accumulation point, 306 adjoint, 227, 231 affine basis, 435 affine closed, 428 affine combination, 428 affine geometry, 427 affine group, 436 affine hull, 430 affine hyperplane, 416 affine map, 435 affine span, 430 affine subspace, 57 affine transformation, 435 affine, 424 affinely independent, 433 affinity, 435 algebra homomorphism, 455 algebra, 31, 451 algebraic, 100, 458 algebraic closure, 30 algebraic dual space, 94 algebraic multiplicity, 189 algebraic numbers, 460 algebraically closed, 30 algebraically reflexive, 101 algorithm, 217 almost upper triangular, 194 along, 73 alternate, 260, 262, 391 alternating, 260, 391 ancestor, 14 anisotropic, 265 annihilator, 102, 115, 459 antisymmetric, 259, 390, 395 antisymmetric tensor algebra, 398 antisymmetric tensor space, 395, 400 antisymmetry, 10 Apollonius identity, 223 Appell sequence, 481 approximation problem, 331 as measured by, 357 ascending chain condition, 26, 133 associate classes, 27 associated sequence, 481 associates, 26 automorphism, 60 barycentric coordinates, 435 base ring, 110 base, 427 basis, 47, 116 Bernoulli numbers, 477 Bernstein theorem, 13 Bessel's identity, 221 Bessel's inequality, 220, 337, 338, 345 best approximation, 219, 332 bijection, 6 bijective, 6 bilinear form, 259, 360 bilinear, 206, 360 binomial identity, 486 binomial type, 486 block diagonal matrix, 3 block matrix, 3 blocks, 7 bottom, 10 bounded, 321, 349 canonical form, 8 canonical injections, 359 canonical map, 100 canonical projection, 89 Cantor's theorem, 13 516 Index cardinal number, 13 cardinality, 12, 13 cartesian product, 14 Cauchy sequence, 311 Cauchy–Schwarz inequality, 208, 303, 325 Cayley-Hamilton theorem, 170 center, 452 central, 452 centralizer, 464, 469 chain, 11 chain rule, 501 change of basis matrix, 65 change of basis operator, 65 change of coordinates operator, 65 characteristic, 30 characteristic equation, 186 characteristic polynomial, 170 characteristic value, 185 characteristic vector, 186 Cholsky decomposition, 255 circulant matrices, 457 class equation, 464 classification problem, 276 closed ball, 304 closed half-spaces, 417 closed interval, 143 closed, 304, 414 closure, 306 codimension, 93 coefficients, 36 column equivalent, 9 column rank, 52 column space, 52 common eigenvector, 202 commutative, 17, 19, 451 commutativity, 15, 35, 384 commuting family, 201 compact, 414 companion matrix, 173 complement, 42, 120 complemented, 120 complete, 40, 311 complete invariant, 8 complete system of invariants, 8 completion, 316 complex operator, 59 complex vector space, 36 complexification, 53, 54, 82 complexification map, 54 composition, 472 cone, 265, 414 congruence classes, 262 congruence relation, 88 congruent modulo, 21, 87 congruent, 9, 262 conjugacy class, 463 conjugate isomorphism, 222 conjugate linear, 206, 221 conjugate linearity, 206 conjugate representation, 483 conjugate space, 350 conjugate symmetry, 205 connected, 281 continuity, 340 continuous, 310, 492 continuous dual space, 350 continuum, 16 contraction, 389 contravariant tensors, 386 contravariant type, 386 converge, 339, 210, 305, 330 convex combination, 414 convex, 332, 414 convex hull, 415 coordinate map, 51 coordinate matrix, 52, 368 correspondence theorem, 90, 118 coset, 22, 87, 118 coset representative, 22, 87 countable, 13 countably infinite, 13 covariant tensors, 386 covariant type, 386 cycle, 391 cyclic basis, 166 cyclic decomposition, 149, 168 cyclic group generated by, 18 cyclic group of order, 18 cyclic submodule, 113 cyclotomic polynomial, 465 decomposable, 362 Index 517 degenerate, 266 degree, 5 deleted absolute row sum, 203 delta functional, 475 delta operator, 478 delta series, 472 dense, 308 derivation, 499 descendants, 13 determinant, 292, 405 diagonal, 4 diagonalizable, 196 diagonally dominant, 203 diameter, 321 dimension, 50, 427 direct product, 41, 408 direct sum, 41, 73, 119 direct summand, 42, 120 discrete metric, 302 discriminant, 263 distance, 209, 322 divides, 5, 26 division algebra, 462 division algorithm, 5 domain, 6 dot product, 206 double, 100 dual basis, 96 dual space, 59, 100 eigenspace, 186 eigenvalue, 185, 186, 461 eigenvector, 186 elementary divisor basis, 169 elementary divisor form, 176 elementary divisor version, 177 elementary divisors, 155, 167, 168 elementary divisors and dimensions, 168 elementary matrix, 3 elementary symmetric functions, 189 embedding, 59, 117 endomorphism, 59, 117 epimorphism, 59, 117 equivalence class, 7 equivalence relation, 7 equivalent, 9, 69 essentially unique, 45 Euclidean metric, 302 Euclidean space, 206 evaluation at, 96, 100 evaluation functional, 474, 476 even permutation, 391 even weight subspace, 38 exponential polynomials, 502 exponential, 482 extension by, 103 extension, 6, 273 exterior algebra, 398 exterior product, 393 exterior product space, 395, 400 external direct sum, 40, 41, 119 factored through, 355, 357 factorization, 217 faithful, 457 Farkas's lemma, 423 field of quotients, 24 field, 19, 29 finite support, 41 finite, 1, 12, 18 finite-dimensional, 50, 451 finitely generated, 113 first isomorphism theorem, 92, 118, 469 flat representative, 427 flat, 427 form, 299, 382 forward difference functional, 477 forward difference operator, 479 Fourier coefficient, 219 Fourier expansion, 219, 338, 345 free, 116 Frobenius norm, 450, 466 functional calculus, 248 functional, 94 Gaussian coefficients, 57, 506 generating function, 482, 483 geometric multiplicity, 189 Geršgorin region, 203 Geršgorin row disk, 203 Geršgorin row region, 203 graded algebra, 392 518 Index Gram-Schmidt augmentation, 213 Gram-Schmidt orthogonalization process, 214 greatest common divisor, 5 greatest lower bound, 11 group algebra, 453 group, 17 Hamel basis, 218 Hamming distance function, 321 Hermite polynomials, 224, 489 Hermitian, 238 Hilbert basis theorem, 136 Hilbert basis, 218, 335 Hilbert dimension, 347 Hilbert space adjoint, 230 Hilbert space, 315, 327 Hölder's inequality, 303 homogeneous, 392 homomorphism, 59, 117 Householder transformation, 244 hyperbolic basis, 273 hyperbolic extension, 274 hyperbolic pair, 272 hyperbolic plane, 272 hyperbolic space, 272 hyperplane, 416, 427 ideal generated by, 21, 455 ideal, 20, 455 idempotent, 74, 125 identity, 17 image, 6, 61 imaginary part, 54 indecomposable, 158 index of nilpotence, 200 induced, 305 inertia, 288 infinite, 13 infinite-dimensional, 50 injection, 6, 117 inner product, 205, 260 inner product space, 205, 260 integral domain, 23 invariant, 8, 73, 83, 165 invariant factor basis, 179 invariant factor decomposition, 157 invariant factor form, 178 invariant factor version, 179 invariant factors, 157, 167, 168 invariant factor decomposition theorem, 157 invariant ideals, 157 invariant under, 73 inverses, 17 invertible functional, 475 involution, 199 irreducible, 5, 26, 83 isometric isomorphism, 211, 326 isometric, 271, 316 isometrically isomorphic, 211, 326 isometry, 211, 271, 315, 326 isomorphic, 59, 62, 117 isotropic, 265 join, 40 Jordan basis, 191 Jordan block, 191 Jordan canonical form, 191 kernel, 61 Kronecker delta function, 96 Kronecker product, 408 Lagrange interpolation formula, 248 Laguerre polynomials, 490 largest, 10 lattice, 39, 40 leading coefficient, 5 leading entry, 3 least, 10 least squares solution, 448 least upper bound, 11 left inverse, 122, 470 left regular matrix representation, 457 left regular representation, 457 left singular vectors, 445 left zero divisor, 460 left-invertible, 470 Legendre polynomials, 215 length, 208 limit, 306 limit point, 306 line, 427, 429 Index 519 linear code, 38 linear combination, 36, 112 linear function, 382 linear functional, 59, 94 linear hyperplane, 416 linear least squares, 448 linear operator, 59 linear transformation, 59 linearity, 340 linearity in the first coordinate, 205 linearly dependent, 45, 114 linearly independent, 45, 114 linearly ordered set, 11 lower bound, 11 lower factorial numbers, 471 lower factorial polynomials, 488 lower triangular, 4 main diagonal, 2 matrix, 64 matrix of, 66 matrix of the form, 261 maximal element, 10 maximal ideal, 23 maximal orthonormal set, 218 maximum, 10 measuring family, 357 measuring functions, 357 mediating morphism map, 367 mediating morphism, 357, 362, 383 meet, 40 metric, 210, 301 metric space, 210, 301 metric vector space, 260 mimimum, 10 minimal element, 11 minimal polynomial, 165, 166, 459 Minkowski space, 260 Minkowski's inequality, 37, 303 mixed tensors, 386 modular law, 56 module, 109, 133, 167 modulo, 22, 87, 118 monic, 5 monomorphism, 59, 117 Moore-Penrose generalized inverse, 446 Moore-Penrose pseudoinverse, 446 MP inverse, 447 multilinear, 382 multilinear form, 382 multiplicity, 1 multiset, 1 natural map, 100 natural projection, 89 natural topology, 80, 82 negative, 17 net definition, 339 nilpotent, 198, 200 Noetherian, 133 nondegenerate, 266 nonderogatory, 171 nonisotropic, 265 nonnegative orthant, 225, 411 nonnegative, 225, 411 nonsingular, 266 nonsingular completion, 273 nonsingular extension theorem, 274 nontrivial, 36 norm, 208, 209, 303, 349 normal equations, 449 normal, 234 normalizing, 213 normed linear space, 209, 224 null, 265 nullity, 61 odd permutation, 391 one-sided inverses, 122, 470 one-to-one, 6 onto, 6 open ball, 304 open half-spaces, 417 open neighborhood, 304 open rectangles, 79 open sets, 305 operator adjoint, 104 operator characterization, 484 order, 18, 101, 139, 471 order ideals, 115 ordered basis, 51 order-reversing, 102 520 Index orthogonal complement, 212, 265 orthogonal direct sum, 212, 269 orthogonal geometry, 260 orthogonal group, 271 orthogonal resolution of the identity, 232 orthogonal set, 212 orthogonal similarity classes, 242 orthogonal spectral resolution, 237 orthogonal transformation, 271 orthogonal, 75, 212, 231, 238, 265 orthogonality conditions, 480 orthogonally diagonalizable, 233 orthogonally equivalent, 242 orthogonally similar, 242 orthonormal basis, 218 orthonormal set, 212 parallel, 427 parallelogram law, 208, 325 parity, 391 Parseval's identity, 221, 346 partial order, 10 partially ordered set, 10 partition, 7 permutation, 391 Pincherle derivative, 503 plane, 427 point, 427 polar decomposition, 253 polarization identities, 209 posets, 10 positive definite, 205, 250, 301 positive square root, 251 power of the continuum, 16 power set, 13 primary, 147 primary cyclic decomposition theorem, 153, 168 primary decomposition theorem, 147 primary decomposition, 147, 168 prime subfield, 97 prime, 26 primitive, 465 principal ideal domain, 24 principal ideal, 24 product, 15 projection modulo, 89 projection theorem, 220, 334 projection, 73 projective dimension, 438 projective geometry, 438 projective line, 438 projective plane, 438 projective point, 438 proper subspace, 37 properly divides, 27 pseudobasis, 472 pure in, 161 q-binomial coefficients, 506 quadratic form, 239, 264 quaternions, 463 quotient algebra, 455 quotient field, 24 quotient module, 118 quotient ring, 22 quotient space, 87, 89 radical, 266 range, 6 rank, 53, 61, 129, 369 rank plus nullity theorem, 63 rational canonical form, 176–179 real operator, 59 real part, 54 real vector space, 36 real version, 53 recurrence formula, 501 reduce, 169 reduced row echelon form, 3, 4 reflection, 244, 292 reflexivity, 7, 10 relatively prime, 5, 27 representation, 457 resolution of the identity, 76 restriction, 6 retract, 122 retraction map, 122 Riesz map, 222 Riesz representation theorem, 222, 268, 351 Riesz vector, 222 right inverse, 122, 470 right singular vectors, 445 Index 521 right zero divisor, 460 right-invertible, 470 ring, 18 ring homomorphism, 19 ring with identity, 19 roots of unity, 464 rotation, 292 row equivalent, 4 row rank, 52 row space, 52 scalar multiplication, 31, 35, 451 scalars, 2, 35, 109 Schröder, 13 Schur's theorem, 192, 195 second isomorphism theorem, 93, 119 self-adjoint, 238 separable, 308 sesquilinear, 206 Sheffer for, 481 Sheffer identity, 486 Sheffer operator, 491, 498 Sheffer sequence, 481 Sheffer shift, 491 sign, 391 signature, 288 similar, 9, 70, 71 similarity classes, 70, 71 simple, 138, 455 simultaneously diagonalizable, 202 singular, 266 singular values, 444, 445 singular-value decomposition, 445 skew self-adjoint, 238 skew-Hermitian, 238 skew-symmetric, 2, 238, 259, 390 smallest, 10 span, 45, 112 spectral mapping theorem, 187, 461 spectral theorem for normal operators, 236, 237 spectral resolution, 197 spectrum, 186, 461 sphere, 304 split, 5 square summable, 207 square summable functions, 347 standard basis, 47, 62, 131 standard inner product, 206 standard topology, 79 standard vector, 47 Stirling numbers of the second kind, 502 strictly diagonally dominant, 203 strictly positive orthant, 411 strictly positive, 225, 411 strictly separated, 417 strongly positive orthant, 225, 411 strongly positive, 56, 225, 411 strongly separated, 417 structure constants, 453 structure theorem for normal matrices, 247 structure theorem for normal operators, 245 subalgebra, 454 subfield, 57 subgroup, 18 submatrix, 2 submodule, 111 subring, 19 subspace spanned, 44 subspace, 37, 260, 304 sup metric, 302 support, 6, 41 surjection, 6 surjective, 6 Sylvester's law of inertia, 287 symmetric, 2, 238, 259, 390, 395 symmetric group, 391 symmetric tensor algebra, 398 symmetric tensor space, 395, 400 symmetrization map, 402 symplectic basis, 273 symplectic geometry, 260 symplectic group, 271 symplectic transformation, 271 symplectic transvection, 280 tensor algebra, 390 tensor map, 362, 383 tensor product, 362, 383, 408 tensors of type, 386 tensors, 362 theorem of the alternative, 413 third isomorphism theorem, 94, 119 522 Index top, 10 topological space, 305 topological vector space, 79 topology, 305 torsion element, 115 torsion module, 115 torsion-free, 115 total subset, 336 totally degenerate, 266 totally isotropic, 265 totally ordered set, 11 totally singular, 266 trace, 188 transfer formulas, 504 transitivity, 7, 10 translate, 427 translation operator, 479 translation, 436 transpose, 2 transposition, 391 triangle inequality, 208, 210, 301, 325 trivial, 36 two-affine closed, 428 two-sided inverse, 122, 470 umbral algebra, 474 umbral composition, 497 umbral operator, 491 umbral shift, 491 uncountable, 13 underlying set, 1 unipotent, 300 unique factorization domain, 28 unit vector, 208 unit, 26 unital algebras, 451 unitarily diagonalizable, 233 unitarily equivalent, 242 unitarily similar, 242 unitarily upper triangularizable, 196 unitary, 238 unitary metric, 302 unitary similarity classes, 242 unitary space, 206 universal, 289 universal for bilinearity, 362 universal for multilinearity, 382 universal pair, 357 universal property, 357 upper bound, 11 upper triangular, 4 upper triangularizable, 192 Vandermonde convolution formula, 489 Vector Space, 167 vector space, 35 vectors, 35 Wedderburn's Theorem, 465, 466 wedge product, 393 weight, 38 well ordering, 12 Well-ordering principle, 12 with respect to the bases, 66 Witt index, 296 Witt's cancellation theorem, 279, 294 Witt's extension theorem, 279, 295 zero divisor, 23 zero element, 17 zero subspace, 37 Zorn's lemma, 12 Graduate Texts in Mathematics (continued from page ii) 76 IITAKA. Algebraic Geometry. 77 HECKE. Lectures on the Theory of Algebraic Numbers. 78 BURRIS/SANKAPPANAVAR. A Course in Universal Algebra. 79 WALTERS. An Introduction to Ergodic Theory. 80 ROBINSON. A Course in the Theory of Groups. 2nd ed. 81 FORSTER. Lectures on Riemann Surfaces. 82 BOTT/TU. Differential Forms in Algebraic Topology. 83 WASHINGTON. Introduction to Cyclotomic Fields. 2nd ed. 84 IRELAND/ROSEN. A Classical Introduction to Modern Number Theory. 2nd ed. 85 EDWARDS. Fourier Series. Vol. II. 2nd ed. 86 VAN LINT. Introduction to Coding Theory. 2nd ed. 87 BROWN. Cohomology of Groups. 88 PIERCE. Associative Algebras. 89 LANG. Introduction to Algebraic and Abelian Functions. 2nd ed. 90 BRØNDSTED. An Introduction to Convex Polytopes. 91 BEARDON. On the Geometry of Discrete Groups. 92 DIESTEL. Sequences and Series in Banach Spaces. 93 DUBROVIN/FOMENKO/NOVIKOV. Modern Geometry—Methods and Applications. Part I. 2nd ed. 94 WARNER. Foundations of Differentiable Manifolds and Lie Groups. 95 SHIRYAEV. Probability. 2nd ed. 96 CONWAY. A Course in Functional Analysis. 2nd ed. 97 KOBLITZ. Introduction to Elliptic Curves and Modular Forms. 2nd ed. 98 BRÖCKER/TOM DIECK. Representations of Compact Lie Groups. 99 GROVE/BENSON. Finite Reﬂection Groups. 2nd ed. 100 BERG/CHRISTENSEN/RESSEL. Harmonic Analysis on Semigroups: Theory of Positive Deﬁnite and Related Functions. 101 EDWARDS. Galois Theory. 102 VARADARAJAN. Lie Groups, Lie Algebras and Their Representations. 103 LANG. Complex Analysis. 3rd ed. 104 DUBROVIN/FOMENKO/NOVIKOV. Modern Geometry—Methods and Applications. Part II. 105 LANG. SL2(R). 106 SILVERMAN. The Arithmetic of Elliptic Curves. 107 OLVER. Applications of Lie Groups to Differential Equations. 2nd ed. 108 RANGE. Holomorphic Functions and Integral Representations in Several Complex Variables. 109 LEHTO. Univalent Functions and Teichmüller Spaces. 110 LANG. Algebraic Number Theory. 111 HUSEMÖLLER. Elliptic Curves. 2nd ed. 112 LANG. Elliptic Functions. 113 KARATZAS/SHREVE. Brownian Motion and Stochastic Calculus. 2nd ed. 114 KOBLITZ. A Course in Number Theory and Cryptography. 2nd ed. 115 BERGER/GOSTIAUX. Differential Geometry: Manifolds, Curves, and Surfaces. 116 KELLEY/SRINIVASAN. Measure and Integral. Vol. I. 117 J.-P. SERRE. Algebraic Groups and Class Fields. 118 PEDERSEN.Analysis Now. 119 ROTMAN. An Introduction to Algebraic Topology. 120 ZIEMER. Weakly Differentiable Functions: Sobolev Spaces and Functions of Bounded Variation. 121 LANG. Cyclotomic Fields I and II. Combined 2nd ed. 122 REMMERT. Theory of Complex Functions. Readings in Mathematics 123 EBBINGHAUS/HERMES et al. Numbers. Readings in Mathematics 124 DUBROVIN/FOMENKO/NOVIKOV. Modern Geometry—Methods and Applications Part III. 125 BERENSTEIN/GAY. Complex Variables: An Introduction. 126 BOREL. Linear Algebraic Groups. 2nd ed. 127 MASSEY. A Basic Course in Algebraic Topology. 128 RAUCH. Partial Differential Equations. 129 FULTON/HARRIS. Representation Theory: A First Course. Readings in Mathematics 130 DODSON/POSTON. Tensor Geometry. 131 LAM. A First Course in Noncommutative Rings. 2nd ed. 132 BEARDON. Iteration of Rational Functions. 133 HARRIS. Algebraic Geometry: A First Course. 134 ROMAN. Coding and Information Theory. 135 ROMAN. Advanced Linear Algebra. 3rd ed. 136 ADKINS/WEINTRAUB. Algebra: An Approach via Module Theory. 137 AXLER/BOURDON/RAMEY. Harmonic Function Theory. 2nd ed. 138 COHEN. A Course in Computational Algebraic Number Theory. 139 BREDON. Topology and Geometry. 140 AUBIN. Optima and Equilibria. An Introduction to Nonlinear Analysis. 141 BECKER/WEISPFENNING/KREDEL. Gröbner Bases. A Computational Approach to Commutative Algebra. 142 LANG. Real and Functional Analysis. 3rd ed. 143 DOOB. Measure Theory. 144 DENNIS/FARB. Noncommutative Algebra. 145 VICK. Homology Theory. An Introduction to Algebraic Topology. 2nd ed. 146 BRIDGES. Computability: A Mathematical Sketchbook. 147 ROSENBERG. Algebraic K-Theory and Its Applications. 148 ROTMAN. An Introduction to the Theory of Groups. 4th ed. 149 RATCLIFFE. Foundations of Hyperbolic Manifolds. 2nd ed. 150 EISENBUD. Commutative Algebra with a View Toward Algebraic Geometry. 151 SILVERMAN. Advanced Topics in the Arithmetic of Elliptic Curves. 152 ZIEGLER. Lectures on Polytopes. 153 FULTON. Algebraic Topology: A First Course. 154 BROWN/PEARCY. An Introduction to Analysis. 155 KASSEL. Quantum Groups. 156 KECHRIS. Classical Descriptive Set Theory. 157 MALLIAVIN. Integration and Probability. 158 ROMAN. Field Theory. 159 CONWAY. Functions of One Complex Variable II. 160 LANG. Differential and Riemannian Manifolds. 161 BORWEIN/ERDÉLYI. Polynomials and Polynomial Inequalities. 162 ALPERIN/BELL. Groups and Representations. 163 DIXON/MORTIMER. Permutation Groups. 164 NATHANSON. Additive Number Theory: The Classical Bases. 165 NATHANSON. Additive Number Theory: Inverse Problems and the Geometry of Sumsets. 166 SHARPE. Differential Geometry: Cartan’s Generalization of Klein’s Erlangen Program. 167 MORANDI. Field and Galois Theory. 168 EWALD. Combinatorial Convexity and Algebraic Geometry. 169 BHATIA. Matrix Analysis. 170 BREDON. Sheaf Theory. 2nd ed. 171 PETERSEN. Riemannian Geometry. 2nd ed. 172 REMMERT. Classical Topics in Complex Function Theory. 173 DIESTEL. Graph Theory. 2nd ed. 174 BRIDGES. Foundations of Real and Abstract Analysis. 175 LICKORISH. An Introduction to Knot Theory. 176 LEE. Riemannian Manifolds. 177 NEWMAN. Analytic Number Theory. 178 CLARKE/LEDYAEV/STERN/WOLENSKI. Nonsmooth Analysis and Control Theory. 179 DOUGLAS. Banach Algebra Techniques in Operator Theory. 2nd ed. 180 SRIVASTAVA. A Course on Borel Sets. 181 KRESS. Numerical Analysis. 182 WALTER. Ordinary Differential Equations. 183 MEGGINSON. An Introduction to Banach Space Theory. 184 BOLLOBAS. Modern Graph Theory. 185 COX/LITTLE/O’SHEA. Using Algebraic Geometry. 2nd ed. 186 RAMAKRISHNAN/VALENZA. Fourier Analysis on Number Fields. 187 HARRIS/MORRISON. Moduli of Curves. 188 GOLDBLATT. Lectures on the Hyperreals: An Introduction to Nonstandard Analysis. 189 LAM. Lectures on Modules and Rings. 190 ESMONDE/MURTY. Problems in Algebraic Number Theory. 2nd ed. 191 LANG. Fundamentals of Differential Geometry. 192 HIRSCH/LACOMBE. Elements of Functional Analysis. 193 COHEN. Advanced Topics in Computational Number Theory. 194 ENGEL/NAGEL. One-Parameter Semigroups for Linear Evolution Equations. 195 NATHANSON. Elementary Methods in Number Theory. 196 OSBORNE. Basic Homological Algebra. 197 EISENBUD/HARRIS. The Geometry of Schemes. 198 ROBERT. A Course in p-adic Analysis. 199 HEDENMALM/KORENBLUM/ZHU. Theory of Bergman Spaces. 200 BAO/CHERN/SHEN. An Introduction to Riemann–Finsler Geometry. 201 HINDRY/SILVERMAN. Diophantine Geometry: An Introduction. 202 LEE. Introduction to Topological Manifolds. 203 SAGAN. The Symmetric Group: Representations, Combinatorial Algorithms, and Symmetric Functions. 204 ESCOFIER. Galois Theory. 205 FÉLIX/HALPERIN/THOMAS. Rational Homotopy Theory. 3rd ed. 206 MURTY. Problems in Analytic Number Theory. Readings in Mathematics 207 GODSIL/ROYLE. Algebraic Graph Theory. 208 CHENEY. Analysis for Applied Mathematics. 209 ARVESON. A Short Course on Spectral Theory. 210 ROSEN. Number Theory in Function Fields. 211 LANG. Algebra. Revised 3rd ed. 212 MATOUŠEK. Lectures on Discrete Geometry. 213 FRITZSCHE/GRAUERT. From Holomorphic Functions to Complex Manifolds. 214 JOST. Partial Differential Equations. 2nd ed. 215 GOLDSCHMIDT. Algebraic Functions and Projective Curves. 216 D. SERRE. Matrices: Theory and Applications. 217 MARKER. Model Theory: An Introduction. 218 LEE. Introduction to Smooth Manifolds. 219 MACLACHLAN/REID. The Arithmetic of Hyperbolic 3-Manifolds. 220 NESTRUEV. Smooth Manifolds and Observables. 221 GRÜNBAUM. Convex Polytopes. 2nd ed. 222 HALL. Lie Groups, Lie Algebras, and Representations: An Elementary Introduction. 223 VRETBLAD. Fourier Analysis and Its Applications. 224 WALSCHAP. Metric Structures in Differential Geometry. 225 BUMP. Lie Groups. 226 ZHU. Spaces of Holomorphic Functions in the Unit Ball. 227 MILLER/STURMFELS. Combinatorial Commutative Algebra. 228 DIAMOND/SHURMAN. A First Course in Modular Forms. 229 EISENBUD. The Geometry of Syzygies. 230 STROOCK. An Introduction to Markov Processes. 231 BJÖRNER/BRENTI. Combinatorics of Coxeter Groups. 232 EVEREST/WARD. An Introduction to Number Theory. 233 ALBIAC/KALTON. Topics in Banach Space Theory. 234 JORGENSON. Analysis and Probability. 235 SEPANSKI. Compact Lie Groups. 236 GARNETT. Bounded Analytic Functions. 237 MARTÍNEZ-AVENDAÑO/ROSENTHAL.An Introduction to Operators on the Hardy-Hilbert Space. 238 AIGNER, A Course in Enumeration. 239 COHEN, Number Theory, Vol. I. 240 COHEN, Number Theory, Vol. II. 241 SILVERMAN. The Arithmetic of Dynamical Systems. 242 GRILLET. Abstract Algebra. 2nd ed. 243 GEOGHEGAN. Topological Methods in Group Theory. 244 BONDY/MURTY. Graph Theory. 245 GILMAN/KRA/RODRIGUEZ. Complex Analysis. 246 KANIUTH. A Course in Commutative Banach Algebras.","libVersion":"0.2.3","langs":""}