{"path":"_assets/tensor-program.pdf","text":"Tensor Programs I: Wide Feedforward or Recurrent Neural Networks of Any Architecture are Gaussian Processes Greg Yang ∗ Microsoft Research AI gregyang@microsoft.com Abstract Wide neural networks with random weights and biases are Gaussian processes, as originally observed by Neal (1995) and more recently by Lee et al. (2018) and Matthews et al. (2018) for deep fully-connected networks, as well as by Novak et al. (2019) and Garriga-Alonso et al. (2019) for deep convolutional networks. We show that this Neural Network-Gaussian Process correspondence surprisingly extends to all modern feedforward or recurrent neural networks composed of multi- layer perceptron, RNNs (e.g. LSTMs, GRUs), (nD or graph) convolution, pooling, skip connection, attention, batch normalization, and/or layer normalization. More generally, we introduce a language for expressing neural network computations, and our result encompasses all such expressible neural networks. This work serves as a tutorial on the tensor programs technique formulated in Yang (2019) and elucidates the Gaussian Process results obtained there. We provide open-source im- plementations of the Gaussian Process kernels of simple RNN, GRU, transformer, and batchnorm+ReLU network at github.com/thegregyang/GP4A. 1 Introduction Motivated to understand the Bayesian prior in neural networks (NNs), Neal [41] theoretically showed that inﬁnitely wide, shallow neural networks with random weights and biases are Gaussian processes (GPs). He empirically explored this phenomenon over deep networks as well, but this was not proven rigorously until recently [37, 40, 43, 18], with concrete progress made over the intervening years [56, 34, 22, 13]. This neural network-Gaussian process correspondence (NN-GP correspondence) has not only allowed one to transform the implicit prior of NNs into explicit priors that can be understood analytically [46, 49, 63, 59, 65], but has also created new state-of-the-art kernels by converting from deep neural networks [37, 43]. Yet, so far the focus has dwelled entirely on multilayer perceptrons (MLPs) or simple convolutional neural networks (CNNs). As new architectures are created with blistering speed, a question starts to emerge and reverberate: Do all inﬁnitely wide, randomly initialized neural networks correspond to Gaussian processes? Even if the answer is yes, at the current rate where each new architecture warrants its own NN-GP correspondence paper, theory will never catch up to practice. On a more basic level, what does this question even mean for recurrent neural networks? Our Contributions In this paper, we formulate the notion of a Gaussian process with variable- dimensional output (see Deﬁnition 2.1), and show that feedforward and recurrent neural networks of standard architectures converge to Gaussian processes in this sense as their widths or number ∗Please see https://arxiv.org/abs/1910.12478 for the full version of this paper. 33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.arXiv:1910.12478v3 [cs.NE] 8 May 2021 of channels go to inﬁnity, when their weights and biases are randomized. By standard architecture we mean any architecture that is some composition of multilayer perceptrons (MLPs), recurrent neural networks (RNNs) (e.g., Long-Short Term Memory (LSTM) [26] or Gated Recurrent Unit (GRU) [10]), skip connections [24, 27], convolutions [16, 17, 47, 35, 36] or graph convolutions [8, 25, 15, 38, 14, 31], pooling [35, 36], batch normalization (batchnorm) [28], layer normalization [1] and/or attention [2, 55]. Even more broadly, we design a new language, NETSOR , for expressing neural network computations, and show the GP convergence for all such expressible networks. By demonstrating that NETSOR can implement any network of standard architectures, we obtain the aforementioned results as a corollary. The results for RNNs, batchnorm, layernorm, attention, and their combination with other layers are new. We open-source reference implementations 2 for the GP kernels of simple RNN, GRU, transformer, and feedforward batchnorm network; see Fig. 3 for an illustration. Relation of This Paper with [60] This paper serves several purposes. 1) Introduce the reader to the tensor programs technique formulated in [60], using the Neural Network-Gaussian Process Correspondence as motivation. 2) Promote a redesigned set of notations for tensor programs that hopefully makes the understanding and the application of this technique easier. 3) Prove a more general version of the Gaussian Process results ﬁrst presented in [60]. 4) Provide example calculations and reference implementations2 of the GP kernels for several architectures like the vanilla RNN, GRU, batchnorm network, and transformers. We assume the reader has not read [60] and seek to explain all results in elementary terms. However, we will provide commentary in footnotes throughout the paper on differences from [60]. Regarding 1), this paper will be the ﬁrst in a series to explain the tensor programs technique, each covering a more powerful type of tensor programs, and each motivated by speciﬁc theorems that can be proved or calculations made possible by these new tensor programs. In particular, here we will only talk about tensor programs without matrix transposes. Regarding 3), the results presented here will supersede all results in [60] concerning Gaussian Processes, with one caveat that here we will not cover architectures using both a weight W and its transpose W ⊤ in its forward pass (but this result will come for free in a later paper in this series). 2 Gaussian Process with Variable-Dimensional Output We ﬁrst clarify the notion of a Gaussian process with variable dimension output. Deﬁnition 2.1 (Gaussian Process). We say a random function f : X → Rm (with ﬁxed dimen- sional output) is a Gaussian process if for any ﬁnite subset {x1, . . . , xk} ⊆ X, the random vector (f (x1), . . . , f (xk)) ∈ Rm×k is distributed as a km-dimensional Gaussian. If f has variable dimen- sional output (e.g. f is an RNN), such as when f (x) ∈ Rl(x) for some length function l : X → N 3, then we say f is a Gaussian process if for any ﬁnite subset {x1, . . . , xk} ⊆ X, the random vector (f (x1), . . . , f (xk)) is distributed as a ( ∑i l(xi))-dimensional Gaussian. To illustrate a GP with variable-dimensional output, consider a simple RNN that runs on two input sequences given by the GloVe embeddings [44] 4 of the words of the two sentences sentence 1 (7 words): “The brown fox jumps over the dog.” sentence 2 (9 words): “The quick brown fox jumps over the lazy dog.” (⋆) A pseudocode is given in Program 2 in Section 4 (ignore the type annotations like G(n), H(n), A(n) for now). The RNN emits a single scalar after reading each token (in Program 2, this is v⊤s ia/√n, where s ia is the RNN state after reading the ith token of the ath sentence, and v is the readout layer); this number takes into account all of the word embeddings read so far. Thus, it will output a total of 7 scalars after reading sentence 1, and a total of 9 scalars after reading sentence 2. To say that this RNN is a GP would imply that all 7 + 9 = 16 scalars are jointly Gaussian-distributed (corresponding to a 16 × 16 kernel), over the randomness of the weights and biases imbued during initialization. This 2github.com/thegregyang/GP4A 3i.e. f : ∏ x∈X Rl(x) is a dependent function 4The embedding associates each word to a real vector of 100 dimensions such that semantically similar words are mapped to closer vectors 2 is indeed the empirical phenomenon with a width-1000 RNN, and Fig. 2(E) visualizes the the joint distribution of the last scalars output by the RNN at the end of each sentence. It clearly exhibits a Gaussian nature, and perfectly ﬁts the theoretically predicted Gaussian distribution (dashed ovals), which we shall describe in Corollary 5.5. 3 Recap: GP Behavior of a Multilayer Perceptron (MLP) Before explaining our main results, we ﬁrst review the argument from prior works [37, 40, 43] for the GP convergence of a wide MLP with randomly initialized weights and biases, and we also demonstrate why such an argument is inadequate for RNNs. Consider an MLP with widths {nl}l, weight matrices {W l ∈ Rnl×nl−1}l, and biases {bl ∈ Rnl }l, where l ranges among the layer numbers of the MLP. Its computation is given recursively as h 1(x) = W 1x + b 1 and h l(x) = W lϕ(h l−1(x)) + bl for l ≥ 2. (1) At initialization time, suppose W l αβ ∼ N (0, σ2 w/nl−1) for each α ∈ [nl], β ∈ [nl−1], and bl α ∼ N (0, σ2 b ). Consider two inputs x, x ′. Conditioned on h l−1(x) and h l−1(x′), iid for each α, (h l(x)α, h l(x′)α) is distributed as N ( 0, σ2 w nl−1 ( ∥ϕ(h l−1(x))∥ 2 ϕ(h l−1(x)) · ϕ(h l−1(x′)) ϕ(h l−1(x)) · ϕ(h l−1(x′)) ∥ϕ(h l−1(x′))∥ 2 ) + σ2 b . ) If (h l−1(x)α, h l−1(x ′)α) is distributed as N (0, Σl−1), iid for each α, then by a law of large number argument, the covariance matrix above converges to a deterministic limit Σl def = σ2 w E (z,z′)∼N (0,Σl−1) ( ϕ(z) 2 ϕ(z)ϕ(z′) ϕ(z)ϕ(z′) ϕ(z′) 2 ) + σ2 b as the width nl−1 → ∞, making (h l(x)α, h l(x′)α) Gaussian distributed as N (0, Σl). Iteratively applying this argument for each l yields the result for a deep MLP. A similar logic works for feedforward CNNs. Unfortunately, this argument breaks down if the weights {W l}l are tied, i.e. all W l are equal to a common matrix W , as in the case of an RNN. In this case, when we condition on the preactivations h l−1(x), h l−1(x′) of the previous layer, W is no longer conditionally an iid random Gaussian matrix, and all subsequent reasoning breaks down. We can repair this situation for RNNs in an ad hoc way via the Gaussian conditioning technique (Lemma G.7), but we prefer to set our sights wider, and deal with all standard architectures, and more, in one fell swoop. To this end, we develop a framework based on our new NETSOR language. 4 NETSOR : Language for Expressing Neural Network Computation To show that networks of all standard architectures converge to GPs, we ﬁrst show that they can be expressed by the following very general NETSOR language (see Programs 1 and 2 for examples)5, and then show that any computation expressed this way exhibits GP behavior when its dimensions are large. Deﬁnition 4.1. 6 NETSOR programs are straight-line programs, where each variable follows one of three types, G, H, or A (such variables are called G-vars, H-vars, and A-vars), and after input variables, new variables can be introduced by one of the rules MatMul, LinComb, Nonlin to be discussed shortly. G and H are vector types and A is a matrix type; intuitively, G-vars should be thought of as vectors that are asymptotically Gaussian, H-vars are images of G-vars by coordinatewise nonlinearities, and A-vars are random matrices with iid Gaussian entries. Each type is annotated by dimensionality information: • If x is a (vector) variable of type G (or H) and has dimension n, we write x : G(n) (or x : H(n)). 5NETSOR is a speciﬁc kind of tensor program; for other variants, see Appendix E. 6We keep the deﬁnition here informal in terms of programming language convention to be accessible to the general machine learning audience. For those with PL background, see Appendix J. 3 NETSOR program 1 MLP Computation on Network Input x Input: W 1x : G(n 1) ▷ layer 1 embedding of input Input: b1 : G(n1) ▷ layer 1 bias Input: W 2 : A(n2, n 1) ▷ layer 2 weights Input: b2 : G(n2) ▷ layer 2 bias Input: v : G(n2) ▷ readout layer weights 1: h1 := W 1x + b1 : G(n1) ▷ layer 1 preactivation; LinComb 2: x1 := ϕ(h 1) : H(n1) ▷ layer 1 activation; Nonlin 3: ˜h 2 := W 2x1 : G(n2) ▷ MatMul 4: h 2 := ˜h 2 + b 2 : G(n2) ▷ layer 2 preactivation; LinComb 5: x2 := ϕ(h 2) : H(n2) ▷ layer 2 activation; Nonlin Output: v⊤x2/ √n2 • If A is a (matrix) variable of type A and has size n1 × n2, we write A : A(n1, n2). G is a subtype of H, so that x : G(n) implies x : H(n). A NETSOR program consists of the following three parts. Input A set of input G- or A-vars. Body New variables can be introduced and assigned via the following rules (with intuition in italics) MatMul if A : A(n1, n2) and x : H(n2), we can form a G-var via matrix-vector product: Ax : G(n1), “random iid matrix times a vector is roughly a Gaussian vector.” 7 LinComb Suppose x1, . . . , xk : G(n) are G-vars with the same dimension and a1, . . . ak ∈ R are constants. Then we can form their linear combination as a G-var: n∑ i=1 aixi : G(n), “linear combination of Gaussian vectors is Gaussian.” Nonlin If x1, . . . , xk : G(n) are G-vars with the same dimension n and ϕ : Rk → R, then ϕ(x1, . . . , xk) : H(n), “image of Gaussian vector is not always Gaussian” where ϕ acts coordinatewise. Output For the purpose of this paper 8, the output of a NETSOR program can be any tuple of scalars, (v1⊤y1/√n1, . . . , vk⊤yk/ √nk), where v1 : G(n1); . . . ; vk : G(nk) are some input G-vars not used elsewhere (and possibly with duplicates vi = vj), and y1 : H(n1); . . . ; yk : H(nk) are some H-vars (possibly with duplicates yi = yj). Examples Program 1 gives an example of a NETSOR program representing an MLP computation. Note that we account for the input x through its embedding W 1x, not x itself. This is because 1) our theorems concern the case where all input G-vars are random; in the context of expressing neural network computation, x is a deterministic input, while W 1x is a Gaussian vector when W 1 has iid Gaussian entries; 2) x has a ﬁxed dimension, while we intend all dimensions (like n1, n 2) in the NETSOR program to tend to inﬁnity, as we’ll describe shortly. Similarly, Program 2 expresses in NETSOR the computation of a simple RNN on two separate input sequences; computation on more input sequences follows the same pattern. Note how weight-sharing is easily expressed in NETSOR because we can re-use A-vars arbitrarily. Appendix A shows more examples of standard architectures in NETSOR and NETSOR+ . More generally, we can allow the nonlinearities in Nonlin to depend on parameters; this will be necessary to express layernorm and attention (see Appendix A). We capture this idea in a new rule: 7Beware: in a later paper (and in [60], tensor program general case), we will introduce matrix transpose as a valid operation, and in that case, Ax can be very far from a Gaussian, and this intuition is no longer correct. Thus, this intuition is more subtle than it might seem at face value. 8In general, the output of a tensor program need not be deﬁned, as most of the time we are concerned with how the H-vars produced over the course of the program interact with each other. 4 NETSOR program 2 Simple RNN Computation on Two Input Sequences // Embeddings of two inputs sequences Input: U x11, . . . , U xt1 : G(n) Input: U x12, . . . , U xr2 : G(n) // Weight and bias Input: W : A(n, n) Input: b : G(n) // Readout weights Input: v : G(n) // Computation on sequence 1 h 11 := U x11 + b : G(n) s 11 := ϕ(h 11) : H(n) ˜h 21 := W s 11 : G(n) h 21 := ˜h 21 + U x21 + b : G(n) s 21 := ϕ(h 21) : H(n) ... ˜h t1 := W s t−1,1 : G(n) h t1 := ˜h t1 + U xt1 + b : G(n) s t1 := ϕ(h t1) : H(n) // Computation on sequence 2 h 12 := U x 12 + b : G(n) s 12 := ϕ(h 12) : H(n) ˜h 22 := W s 12 : G(n) h 22 := ˜h 22 + U x 22 + b : G(n) s 22 := ϕ(h 22) : H(n) ... ˜h r2 := W s r−1,2 : G(n) h r2 := ˜h r2 + U xr2 + b : G(n) s r2 := ϕ(h r2) : H(n) Output: (v⊤s 11/ √n, . . . , v⊤s t1/ √n, v⊤s 12/√n, . . . , v⊤s r2/ √n) Nonlin + Suppose x1, . . . , xk : G(n) are G-vars with the same dimension n and θ1, . . . , θt ∈ R possibly depend on G-vars already deﬁned. If ϕ(−; −) : Rk × Rt → R, then ϕ(x1, . . . , xk; θ1, . . . , θt) : H(n), where ϕ acts coordinatewise. Deﬁnition 4.2. NETSOR+ programs are NETSOR programs allowing Nonlin + rules. NETSOR and NETSOR+ specify different kinds of tensor programs; in Appendix E we discuss other kinds that are semantically equivalent. In a future paper, we shall study the effect of allowing matrix transposes as an operation on A-vars. Remark 4.3. In this paper, in Nonlin +, we will only instantiate θj with continuous functions of “empirical moments” of the form n−1 ∑n i=1 ψ(y1, . . . , yr) for some set of G-vars {yi}i. A key consequence of our scaling limit result is that these “empirical moments” converge almost surely to a deterministic limit under very general conditions (Theorems 5.4 and C.4), so that ϕ(−; Θ) is, under suitable smoothness conditions (Deﬁnition C.1), approximately a ﬁxed nonlinearity when n is large. Thus, we should intuitively treat Nonlin + as Nonlin but with the nonlinearity determined automatically by the NETSOR program itself. Nonlin + expands the expressible computation quite broadly, but to keep the main text lean and focused on the key ideas behind tensor programs, we relegate a more thorough discussion of Nonlin + in the appendix (see Appendices C, D and I). 5 Computing the GP Kernel from a NETSOR Encoding of a Neural Network For readers who wish to be convinced that NETSOR (or NETSOR+ ) can express standard architectures, see Appendix A. In this section, we show that any architecture expressible in NETSOR and satisﬁes some mild conditions will exhibit Gaussian Process behavior in the large width limit. In this section, we make the following simplifying assumption on the dimensions of the program and the randomness of the variables. Assumption 5.1. Fix a NETSOR program. For simplicity, assume all dimensions in the program are equal to n. Suppose for each A-var W : A(n, n), we sample Wαβ ∼ N (0, σ2 W /n) for some σ2 W > 0, and for each α ∈ [n], we sample, i.i.d., {xα : x is input G-var} ∼ N (µ in, Σin) for some mean µ in and (possibly singular) covariance Σin over input G-vars. 5 The constraint on the dimensions can be removed easily; see Appendix F. This sampling induces randomness in all variables created in the program, and we shall characterize this randomness shortly. We ﬁrst review some notation that will be used immediately. Notation In this paper, a kernel Σ on a set X is a symmetric function Σ : X × X → R such that m∑ i=1 m∑ j=1 cicjΣ(xi, xj) ≥ 0 holds for any m ∈ N, x1, . . . , xm ∈ X, and c1, . . . , cm ∈ R. Given a kernel Σ on a set of G-vars, we will both treat it as matrix and as a function, depending on the context. Function Notation As a function, Σ(g, g′) is the value of Σ on the pair of G-vars (g, g′). If G = {g1, . . . , gk} is a set of G-vars, then we also denote by Σ(g, G) the row vector {Σ(g, g1), . . . , Σ(g, gk)}. Likewise Σ(G, g) is the column vector with the same values. If G ′ = {g1′, . . . , gl′} is another set of G-vars (possible with overlap with G), then Σ(G, G ′) is the matrix {Σ(gi, gj ′) : i ∈ [k], j ∈ [l]}. Restriction Notation We also use the “restriction” notation Σ|G to denote the square matrix Σ(G, G) in a more concise way. Matrix Notation When an association of indices to G-vars is clear from context, we will also write Σij for the corresponding value of Σ on the pair of ith and jth G-vars. Juxtaposition implies matrix multiplication, e.g. ΣΩ means matrix product if Ω is a matrix of appropriate size. Indices Notation We will both use superscripts and subscripts for indices. We will never multiply in subscript or superscript, so juxtaposition of indices like W ib αβ is the same as W i,b α,β. H-vars as Both Symbols and Vectors An H-var will be considered both as a symbol (like in Σ(g, g′) above) as well as the corresponding length n vector (like in Theorem 5.4 below), depending on the context. Deﬁnition 5.2. In the setting of Assumption 5.1, we extend µ in and Σin to µ and Σ that resp. take a single and a pair of G-vars and both output to R. Intuitively, µ speciﬁes the mean coordinate of a G-var, and Σ speciﬁes the coordinatewise covariance of a pair of G-vars; this is formalized in Theorem 5.4 below. Index all the G-vars in the program as g1, . . . , gM (including input G-vars), in the order of appearance in the program. For any pair of G-vars g, g′ (among g1, . . . , gM ), we deﬁne recursively µ(g) =    µ in(g) if g is input ∑ i aiµ(yi) if g = ∑ i aiyi, introduced by LinComb 0 otherwise , Σ(g, g′) =  ||||| ||||| Σin(g, g′) if g, g′ are inputs ∑ i aiΣ(yi, g′) if g = ∑ i aiyi, introduced by LinComb ∑ i aiΣ(g, yi) if g′ = ∑ i aiyi, introduced by LinComb σ2 W EZ ϕ(Z) ¯ϕ(Z) if g = W h, g′ = W h ′, introduced by MatMul w/ same A-var W 0 otherwise (2) where • yi are G-vars for all i • (h : H(n)) was introduced by the Nonlin with h := ϕ(g1, . . . , gM ), h ′ was introduced by Nonlin with h ′ := ¯ϕ(g1, . . . , gM ) (where WLOG we have padded the input slots of ϕ and ¯ϕ to account for all G-vars) • Z ∼ N (µ, Σ) is a random Gaussian vector with 1 entry for each G-var in the program. Note that since ϕ and ¯ϕ only depends on entries of Z corresponding to previous G-vars, the expectation EZ ϕ(Z) ¯ϕ(Z) only depends on entries of µ and Σ already deﬁned, so there is no circular logic in this recursive deﬁnition of µ and Σ. See Appendix B.1.1 for a simple, worked-out example of how to recursively compute µ and Σ for Program 1. For our main theorems, we isolate a very general class of nonlinearities that we are concerned with. Deﬁnition 5.3. We say a function ϕ : Rk → R is controlled if |ϕ(x)| is bounded by a function of the form eC∥x∥2−ϵ+c with C, c, ϵ > 0 6 𝑔1 𝑔2 𝑔3 𝑔𝑀𝑛→∞ 𝜓 𝜓 𝜓 𝜓 𝜓 𝜓 ( Average 1 𝑛 ෍ 1 𝑛 ( ( ( ( ( ) ) ) ) ) ) 𝔼 𝑍 ~ 𝑁 𝜇,Σ 𝜓 ( )𝑍𝑔1 𝑍𝑔2 𝑍𝑔3 𝑍𝑔𝑀𝑎. 𝑠. Figure 1: An illustration of the NETSOR Master Theorem Theorem 5.4. Controlled functions can explode faster than exponential but are still L 1 and L 2-integrable against Gaussian measures. Additionally, there is no constraint on the smoothness of ϕ here. Thus this deﬁnition captures almost all functions we would care about in practice. The metric structure of the ﬁnal layer representations of inputs under a deep neural network often reveals semantical information about the inputs. This structure is reﬂected in the inner products between pairs of such representations, e.g. s t1⊤s r2/n for s t1 and s r2 in Program 2. The following Master Theorem allows one to compute such inner products, and much more, for a wide network at initialization time (take ψ below to be ψ(z1, . . . , zM ) def = zM −1zM ). Theorem 5.4 (NETSOR Master Theorem). 9 Fix any NETSOR program satisfying Assumption 5.1 and with all nonlinearities controlled. If g1, . . . , gM are all of the G-vars in the entire program, including all input G-vars, then for any controlled ψ : RM → R, as n → ∞, 1 n n∑ α=1 ψ(g1 α, . . . , gM α ) a.s. −−→ E Z∼N (µ,Σ) ψ(Z) = E Z∼N (µ,Σ) ψ(Z g1, . . . , Z gM ), where a.s. −−→ means almost sure convergence, Z = (Z g1, . . . , Z gM ) ∈ RM , and µ = {µ(gi)} M i=1 ∈ RM and Σ = {Σ(gi, gj)} M i,j=1 ∈ RM ×M are given in Eq. (2). See Fig. 1 for an illustration. Intuitively, Theorem 5.4 says, for each α, (g1 α, . . . , gM α ) ≈ N (µ, Σ) in the large n limit, and each α-slice appears to be “iid” from the point of view of the empirical average by any controlled function ψ. The proof of Theorem 5.4 is given in Appendix H. Combining Theorem 5.4 with Proposition G.4, we can straightforwardly calculate the output distri- bution of a NETSOR program. Corollary 5.5 (Computing the GP Kernel). Adopt the same assumptions and notations as in Theorem 5.4. If the program outputs (v⊤x1/ √n, . . . , v⊤xk/√n), where • v : G(n), vα ∼ N (0, σ2 v), is an input G-var not used elsewhere in the program and is sampled independently from all other G-vars, and • x i was introduced as xi := ϕi(g1, . . . , gM ) then the output vector converges in distribution to N (0, K) where Kij = σ2 v E Z∼N (µ,Σ) ϕi(Z)ϕj(Z), with µ, Σ deﬁned in Eq. (2). (3) Intuitively, this corollary follows from the fact that, for any ﬁnite n, the output vector is some Gaussian N (0, ˜K) conditioned on x1, . . . , xk, and the covariance ˜K converges to a deterministic covariance K, causing the output vector to converge in distribution to N (0, K) as well. The case 9Difference with [60, Thm 4.3]: We have gotten rid of the “rank convergence” assumption by showing that it comes for free. See CoreSet and Lemma H.4 in Appendix H. 7ThebrownfoxjumpsoverthedogThequickbrownfoxjumpsoverthelazydog sent1 sent2 The brown fox jumps over the dog The quick brown fox jumps over the lazy dogsent2 sent1 (A) GloVe correlations 0.0 0.2 0.4 0.6 0.8 1.0ThebrownfoxjumpsoverthedogThequickbrownfoxjumpsoverthelazydog sent1 sent2(B) randRNN correlations (theory) 0.0 0.2 0.4 0.6 0.8 1.0ThebrownfoxjumpsoverthedogThequickbrownfoxjumpsoverthelazydog sent1 sent2(C) randRNN covariances (theory) 0.00 0.05 0.10 0.15 0.20 0.25 0.30ThebrownfoxjumpsoverthedogThequickbrownfoxjumpsoverthelazydog sent1 sent2(D) randRNN covariance std (empirical) 0.006 0.008 0.010 0.012 0.014 0.016 0.018 0.020 2 1 0 1 2 randRNN(\"...lazy dog\") 2.0 1.5 1.0 0.5 0.0 0.5 1.0 1.5 2.0randRNN(\"...the dog\") (E) randRNN: Theory vs Empirics theory Figure 2: Inﬁnite-width theory is highly predictive for simple RNN (Program 2) with 1000 neurons and erf activation. We pass two sentences (“The brown fox jumps over the dog” and “The quick brown fox jumps over the lazy dog”) by their word GloVe embeddings into randomly initialized simple RNNs. (A) Cosine distances between each pair of word GloVe embeddings. (B) Correlation matrix of the limiting Gaussian that Program 2 output vector converges to. Each row/column corresponds to an embedding of of the sentence up to that word. (C) Covariance matrix of the same. See Appendix B.2 for algorithm to compute this covariance. (D) Entrywise standard deviation of empirical covariance around (C), as measured from 100 random simple RNNs. Note the deviations are at least an order of magnitude smaller than the limiting values (C), for 1000 neurons. (E) Visualizing the joint distribution of the ﬁnal outputs of the RNN at the end of each sentence, i.e. (v⊤s t1/ √n, v⊤s r2/ √n) in Program 2. We sampled 100,000 simple RNNs and plotted the 2d histogram as a heatmap. Simultaneously, we plot the limiting Gaussian level curves predicted by our theory, which ﬁt the simulations perfectly. when we have multiple distinct vi (allowed by Deﬁnition 4.1) can be obtained easily as well (see Proposition G.4). Following Corollary 5.5 and its extensions below, the convergence of standard architectures to Gaussian Processes becomes obvious: Express the marginal of the distribution on every ﬁnite set of inputs as a NETSOR (or NETSOR+ ) program, and then apply Corollary 5.5. We summarize the result below. Corollary 5.6. If its nonlinearities are controlled (Deﬁnition 5.3), then a (possibly recurrent) neural network of standard architecture converges to a Gaussian process in ﬁnite-dimensional distribu- tion 10 in the sense of Deﬁnition 2.1 as its widths go to inﬁnity and each of its weights W and biases b are randomized as Wαβ ∼ N (0, σ2 W /n), bα ∼ N (µb, σ2 b ) for a collection of sampling hyperparameters {σW }W , {µb, σb}b. If its nonlinearities are more generally parametrized and are parameter-controlled (Deﬁnition C.1), such as in the case of attention models or where layernorm is involved, then the same result holds as long as Assumption C.3 also holds. An Empirical Demonstration Despite being about inﬁnite width, our theory is highly predictive for ﬁnite-width networks, as shown in Fig. 2. As in Section 2, we randomly initialize a simple RNN (Program 2) with 1000 neurons and erf activation (we choose erf instead of tanh because it simpliﬁes kernel calculations; see Appendix B.2 for the derivation of the algorithm to compute the kernel). We pass the two sentences in (⋆) to the random RNN by their GloVe embeddings. After processing each token, the RNN outputs a scalar, as before, and over the two input sequences, the RNN outputs 7 + 9 = 16 scalars in total. Our result Corollary 5.5 implies that, as the width of the RNN grows to inﬁnity, these 16 scalars are distributed jointly as a Gaussian. Fig. 2(E) illustrates this is indeed the case for the marginal on 2 scalars, as discussed in Section 2. We also compare our theoretically derived, inﬁnite-width covariance of the 16 scalars (Fig. 2(C)) with the empirical ﬁnite-width covariance obtained from multiple random initializations. We ﬁnd that the empirical covariance, as predicted, concentrates around the theoretical, and the entrywise standard deviation is typically at least an order of magnitude lower than the values themselves (Fig. 2(D)) (with width 1000 RNNs). The random RNN is clearly doing nontrivial context embedding, as seen by comparing the correlation matrix of the 16 scalars Fig. 2(B) (context-sensitive) with the matrix of cosine distances (i.e. correlations) between the GloVe embeddings Fig. 2(A) (context-insensitive). A tell-tale sign is the entry corresponding to (“lazy”, “dog”): even though as words, they are not semantically similar 10Stronger convergence results, such as convergence in distribution with respect to some topology on functions on Rd, would be available if one can show additionally the tightness of the random neural networks under this topology. However, here we are content with convergence of ﬁnite-dimensional marginals of the stochastic processes. 8ThebrownfoxjumpsoverthedogThequickbrownfoxjumpsoverthelazydog sent1 sent2 The brown fox jumps over the dog The quick brown fox jumps over the lazy dogsent2 sent1 (A) RNN correlations (theory) 0.0 0.2 0.4 0.6 0.8 1.0ThebrownfoxjumpsoverthedogThequickbrownfoxjumpsoverthelazydog sent1 sent2(B) GRU correlations (theory) 0.80 0.85 0.90 0.95 1.00ThebrownfoxjumpsoverthedogThequickbrownfoxjumpsoverthelazydog sent1 sent2(C) transformer correlation (theory) 0.5 0.6 0.7 0.8 0.9 1.0 0 20 40 60 CIFAR10 sample id (D) BN+relu fc net correlations (theory) 0.2 0.4 0.6 0.8 1.0 4 6 8 10 12 log2width 12 10 8 6 4 22log2(|KsimKth|F/|Kth|F) (E) relative deviation from theory model BN GRU transformer simpleRNN log2width Figure 3: Inﬁnite-width GP kernels (more precisely, their correlation matrices) for which we provide reference implementations, and the deviation of ﬁnite-width simulations from the corresponding inﬁnite-width limits. (A) – (C) The correlation matrices of the GP kernels for the simple RNN (same as in Fig. 2; see Program 2 for the architecture and Appendix B.2 for derivation), GRU (Program 5; Appendix B.5), and transformer (Program 10; Appendix D.3), with input sequences given by the GloVe embeddings of (⋆). (D) The correlation matrix of the GP kernel for a feedforward, fully-connected network with batchnorm+ReLU (batchnorm followed by ReLU) as nonlinearity (see Appendix B.3 for derivation). The inputs are the ﬁrst 64 CIFAR10 images, split into two batches of 32 each. The red lines indicate the batch split. (E) For each architecture above, we independently randomly initialize 100 networks for each width among [25, 26, . . . , 2 13]. We calculate the empirical kernel of the network outputs and plot its (relative) Frobenius distance to the inﬁnite-width kernel. This Frobenius distance drops like 1/√width as one would expect from a central limit intuition. See our code 2 for Python implementations of these kernels and the code for generating this ﬁgure. (so that the entry in Fig. 2(A) is small), the random RNN understands that the two sentences resp. up to “lazy” and “dog” have been very similar (so that the entry in Fig. 2(B) is large). Given the precision of our theoretical predictions, we expect analyses of the equations derived here will lead to many nontrivial insights about recurrent (and other) neural network behavior in practice, which we leave for future work. Examples and Extensions: A Brief Guide to the Appendix Appendix B contains a plethora of worked-out examples of the kernel computation for different architectures, starting from the known case of MLP to the new results of RNN (as shown in Fig. 2), GRU, batchnorm, and others. At this point, we recommend the reader to follow along some of those examples to solidify the understanding of Theorem 5.4. A Master Theorem for NETSOR+ can be similarly proved. This is stated in Appendix C and can be proved easily given the proof of Theorem 5.4; see Appendix I. Appendix D works out examples of kernel computations for layernorm and transformer, which can only be expressed through NETSOR+ . Fig. 3 illustrates the kernels of simple RNN, GRU, transformer, and a batchnorm+ReLU network, and conﬁrms that the ﬁnite width simulations tend to the inﬁnite-width, theoretical kernels. We also discuss different variants of NETSOR and NETSOR+ in Appendix E which trade off syntacti- cal simplicity with ease of use, but are semantically equivalent to NETSOR or NETSOR+ . Appendix F discusses the case when the dimensions of a program need not be equal. With the appropriate setup, a Master Theorem in this case can be proved similarly (Theorem F.4). 6 Related Works NN-GP Correspondence Many works have observed the neural network-Gaussian process corre- spondence (NN-GP correspondence) for subsets of standard architectures [56, 34, 22, 13, 37, 40, 43]. Others have exploited this NN-GP correspondence implicitly or explicitly to build new models [11, 33, 12, 57, 58, 7, 54, 32, 4, 6, 18, 43]. In particular, by directly converting NN to GP using this correspondence, Lee et al. [37] constructed the state-of-the-art (SOTA) permutation-invariant GP on MNIST, and Novak et al. [43] was until recently SOTA on CIFAR10 for any GP with untrainable kernel. Additionally, the NN-GP correspondence has led to new understanding of neural network training and generalization [42, 53, 61]. In this paper, we generalized the NN-GP correspondence to standard architectures and very general nonlinearities (controlled functions; see Deﬁnition 5.3). In contrast, Matthews et al. [40] requires ϕ to be linearly bounded in norm; Daniely et al. [13] requires ϕ be twice-differentiable with |ϕ|, |ϕ′|, |ϕ′′| 9 all bounded, or that ϕ = ReLU; and a sufﬁcient condition given in Novak et al. [43] is that ϕ′ exists and is bounded by exp(O(x2−ϵ)), though it is unclear how the more general set of 3 conditions given there (in their section E.4) compares with ours. Signal Propagation in Neural Networks A long line of work starting with Glorot and Bengio [20] and He et al. [23] studies the effect of initialization in deep neural networks [46, 50, 63, 62, 21, 9, 64, 45], for example, what is the best initialization scheme to avoid gradient vanishing? These works apply the same calculations of covariances as we do for calculating Σ here, though in a much more restricted set of architectures, and they are typically more concerned with the dynamics of such covariances with depth. Reservoir Computing In reservoir computing [30, 39, 51], sequence processing is typically done by a randomly initialized recurrent neural network. A sequence of inputs is fed step by step into the network, and a ﬁnal readout layer transforms the random RNN’s state into an output. The only trainable parameters are the readout layer, but not the random RNN itself. Thus, in the inﬁnite-width limit, reservoir computing corresponds exactly to GP inference with the RNN kernel computed in Appendix B.2. 7 Conclusion We formulated the notion of Gaussian process with variable-dimensional outputs and showed that randomly initialized, wide feedforward and recurrent neural networks of standard architectures converge in distribution to Gaussian processes in such a sense. This signiﬁcantly generalizes prior work on the NN-GP correspondence. We did so by introducing NETSOR , a language for expressing computation common in deep learning, including neural networks of standard architecture, along with a theorem (Theorem 5.4) characterizing the behavior of a NETSOR program as its tensors are randomized and their dimensions tend to inﬁnity; many examples and extensions are exhibited in the appendix. Finally, we empirically veriﬁed our theory for simple RNN, GRU, transformer, and batchnorm (Fig. 3) and open-sourced implementations of the corresponding inﬁnite-width limit kernels at github.com/thegregyang/GP4A. In the next paper in this series, we will introduce a more powerful version of tensor program that allows matrix transposes, and use this tool to compute Neural Tangent Kernel [29] for any architecture. 10 Acknowlegements I’m very thankful for my buddy Hadi Salman who is always ready to help and who donated a lot of time helping me write the detailed examples for MLP and RNN kernels. I’d also like to thank Mimee Xu who read the ﬁrst versions of this paper and provided valuable feedback. Finally, allow me to express my appreciation for myriads of friends and collaborators who have helped me improve this paper in one way or another: Sam Schoenholz, Yihe Dong, Judy Shen, Alessandro Sordoni, Huishuai Zhang, George Phillip, Vinay Rao, Sebastien Bubeck, Zeyuan Allen-Zhu, Kyle Aitkens, Chunyuan Li, Alex Polozov, Ilya Razenshteyn, Jianfeng Gao, Pengchuan Zhang, Jascha Sohl-Dickstein, Jeffrey Pennington, and others. References [1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer Normalization. arXiv:1607.06450 [cs, stat], July 2016. URL http://arxiv.org/abs/1607.06450. 00329 arXiv: 1607.06450. [2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural Machine Translation by Jointly Learning to Align and Translate. arXiv:1409.0473 [cs, stat], September 2014. URL http://arxiv.org/abs/1409.0473. arXiv: 1409.0473. [3] Mohsen Bayati and Andrea Montanari. The dynamics of message passing on dense graphs, with applications to compressed sensing. IEEE Transactions on Information Theory, 57(2): 764–785, February 2011. ISSN 0018-9448, 1557-9654. doi: 10.1109/TIT.2010.2094817. URL http://arxiv.org/abs/1001.3448. arXiv: 1001.3448. [4] Kenneth Blomqvist, Samuel Kaski, and Markus Heinonen. Deep convolutional Gaussian processes. arXiv preprint arXiv:1810.03052, 2018. [5] Erwin Bolthausen. An iterative construction of solutions of the TAP equations for the Sherrington-Kirkpatrick model. arXiv:1201.2891 [cond-mat, physics:math-ph], January 2012. URL http://arxiv.org/abs/1201.2891. arXiv: 1201.2891. [6] Anastasia Borovykh. A gaussian process perspective on convolutional neural networks. arXiv preprint arXiv:1810.10798, 2018. [7] John Bradshaw, Alexander G de G Matthews, and Zoubin Ghahramani. Adversarial examples, uncertainty, and transfer testing robustness in gaussian process hybrid deep networks. arXiv preprint arXiv:1707.02476, 2017. [8] Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral Networks and Locally Connected Networks on Graphs. arXiv:1312.6203 [cs], December 2013. [9] Minmin Chen, Jeffrey Pennington, and Samuel Schoenholz. Dynamical Isometry and a Mean Field Theory of RNNs: Gating Enables Signal Propagation in Recurrent Neural Networks. In Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 873–882, Stockholmsmässan, Stockholm Sweden, July 2018. PMLR. URL http://proceedings.mlr.press/v80/chen18i.html. [10] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning Phrase Representations using RNN Encoder- Decoder for Statistical Machine Translation. arXiv:1406.1078 [cs, stat], June 2014. URL http://arxiv.org/abs/1406.1078. arXiv: 1406.1078. [11] Youngmin Cho and Lawrence K. Saul. Kernel methods for deep learning. In Advances in neural information processing systems, pages 342–350, 2009. URL http://papers.nips. cc/paper/3628-kernel-methods-for-deep-learning. [12] Andreas Damianou and Neil Lawrence. Deep gaussian processes. In Artiﬁcial Intelligence and Statistics, pages 207–215, 2013. 11 [13] Amit Daniely, Roy Frostig, and Yoram Singer. Toward Deeper Understanding of Neural Networks: The Power of Initialization and a Dual View on Expressivity. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems 29, pages 2253–2261. Curran Associates, Inc., 2016. [14] Michaël Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering. arXiv:1606.09375 [cs, stat], June 2016. [15] David K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel, Alan Aspuru-Guzik, and Ryan P Adams. Convolutional Networks on Graphs for Learning Molecular Fingerprints. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 2224–2232. Curran Associates, Inc., 2015. [16] Kunihiko Fukushima. Cognitron: A self-organizing multilayered neural network. Biological cybernetics, 20(3-4):121–136, 1975. [17] Kunihiko Fukushima and Sei Miyake. Neocognitron: A self-organizing neural network model for a mechanism of visual pattern recognition. In Competition and cooperation in neural nets, pages 267–285. Springer, 1982. [18] Adrià Garriga-Alonso, Laurence Aitchison, and Carl Edward Rasmussen. Deep Convolutional Networks as shallow Gaussian Processes. arXiv:1808.05587 [cs, stat], August 2018. URL http://arxiv.org/abs/1808.05587. arXiv: 1808.05587. [19] Alan Genz, Frank Bretz, Tetsuhisa Miwa, Xuefei Mi, Friedrich Leisch, Fabian Scheipl, and Torsten Hothorn. mvtnorm: Multivariate Normal and t Distributions, 2019. URL https: //CRAN.R-project.org/package=mvtnorm. R package version 1.0-11. [20] Xavier Glorot and Yoshua Bengio. Understanding the difﬁculty of training deep feedforward neural networks. In Yee Whye Teh and Mike Titterington, editors, Proceedings of the Thirteenth International Conference on Artiﬁcial Intelligence and Statistics, volume 9 of Proceedings of Machine Learning Research, pages 249–256, Chia Laguna Resort, Sardinia, Italy, May 2010. PMLR. URL http://proceedings.mlr.press/v9/glorot10a.html. 02641. [21] Boris Hanin and David Rolnick. How to Start Training: The Effect of Initialization and Architecture. arXiv:1803.01719 [cs, stat], March 2018. URL http://arxiv.org/abs/1803. 01719. arXiv: 1803.01719. [22] Tamir Hazan and Tommi Jaakkola. Steps Toward Deep Kernel Methods from Inﬁnite Neural Net- works. arXiv:1508.05133 [cs], August 2015. URL http://arxiv.org/abs/1508.05133. arXiv: 1508.05133. [23] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectiﬁers: Surpassing human-level performance on imagenet classiﬁcation. In Pro- ceedings of the IEEE international conference on computer vision, pages 1026– 1034, 2015. URL http://www.cv-foundation.org/openaccess/content_iccv_2015/ html/He_Delving_Deep_into_ICCV_2015_paper.html. [24] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image Recognition. pages 770–778, 2016. URL https://www.cv-foundation.org/ openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_ paper.html. [25] Mikael Henaff, Joan Bruna, and Yann LeCun. Deep Convolutional Networks on Graph- Structured Data. arXiv:1506.05163 [cs], June 2015. [26] Sepp Hochreiter and Jürgen Schmidhuber. Long Short-Term Memory. Neural Comput., 9 (8):1735–1780, November 1997. ISSN 0899-7667. doi: 10.1162/neco.1997.9.8.1735. URL http://dx.doi.org/10.1162/neco.1997.9.8.1735. [27] Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Weinberger. Densely Connected Convolutional Networks. arXiv:1608.06993 [cs], August 2016. URL http://arxiv.org/ abs/1608.06993. arXiv: 1608.06993. 12 [28] Sergey Ioffe and Christian Szegedy. Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. In PMLR, pages 448–456, June 2015. URL http: //proceedings.mlr.press/v37/ioffe15.html. [29] Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural Tangent Kernel: Convergence and Generalization in Neural Networks. arXiv:1806.07572 [cs, math, stat], June 2018. URL http://arxiv.org/abs/1806.07572. 00000 arXiv: 1806.07572. [30] Herbert Jaeger. Echo state network. Scholarpedia, 2(9):2330, September 2007. ISSN 1941-6016. doi: 10.4249/scholarpedia.2330. [31] Thomas N. Kipf and Max Welling. Semi-Supervised Classiﬁcation with Graph Convolutional Networks. arXiv:1609.02907 [cs, stat], September 2016. [32] Vinayak Kumar, Vaibhav Singh, PK Srijith, and Andreas Damianou. Deep Gaussian Processes with Convolutional Kernels. arXiv preprint arXiv:1806.01655, 2018. [33] Neil D Lawrence and Andrew J Moore. Hierarchical Gaussian process latent variable models. In Proceedings of the 24th international conference on Machine learning, pages 481–488. ACM, 2007. [34] Nicolas Le Roux and Yoshua Bengio. Continuous neural networks. In Artiﬁcial Intelligence and Statistics, pages 404–411, 2007. [35] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. [36] Yann LeCun, Patrick Haffner, Léon Bottou, and Yoshua Bengio. Object recognition with gradient-based learning. In Shape, contour and grouping in computer vision, pages 319–345. Springer, 1999. [37] Jaehoon Lee, Yasaman Bahri, Roman Novak, Sam Schoenholz, Jeffrey Pennington, and Jascha Sohl-dickstein. Deep Neural Networks as Gaussian Processes. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=B1EA-M-0Z. [38] Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated Graph Sequence Neural Networks. arXiv:1511.05493 [cs, stat], November 2015. [39] Wolfgang Maass, Thomas Natschläger, and Henry Markram. Real-Time Computing Without Stable States: A New Framework for Neural Computation Based on Perturbations. Neural Computation, 14(11):2531–2560, November 2002. ISSN 0899-7667, 1530-888X. doi: 10.1162/ 089976602760407955. [40] Alexander G. de G. Matthews, Jiri Hron, Mark Rowland, Richard E. Turner, and Zoubin Ghahramani. Gaussian Process Behaviour in Wide Deep Neural Networks. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum? id=H1-nGgWC-. [41] Radford M Neal. BAYESIAN LEARNING FOR NEURAL NETWORKS. PhD Thesis, University of Toronto, 1995. [42] Roman Novak, Yasaman Bahri, Daniel A. Abolaﬁa, Jeffrey Pennington, and Jascha Sohl- Dickstein. Sensitivity and Generalization in Neural Networks: an Empirical Study. In Inter- national Conference on Learning Representations, 2018. URL https://openreview.net/ forum?id=HJC2SzZCW. [43] Roman Novak, Lechao Xiao, Jaehoon Lee, Yasaman Bahri, Daniel A Abolaﬁa, Jeffrey Penning- ton, and Jascha Sohl-Dickstein. Bayesian Deep Convolutional Networks with Many Channels are Gaussian Processes. arXiv preprint arXiv:1810.05148, 2018. [44] Jeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global Vectors for Word Representation. pages 1532–1543. Association for Computational Linguistics, 2014. doi: 10.3115/v1/D14-1162. URL http://aclweb.org/anthology/D14-1162. 04219. 13 [45] Jeffrey Pennington, Samuel Schoenholz, and Surya Ganguli. Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30, pages 4788–4798. Curran Associates, Inc., 2017. 00004. [46] Ben Poole, Subhaneil Lahiri, Maithreyi Raghu, Jascha Sohl-Dickstein, and Surya Ganguli. Exponential expressivity in deep neural networks through transient chaos. In Advances In Neural Information Processing Systems, pages 3360–3368, 2016. 00047. [47] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning internal represen- tations by error propagation. Technical report, California Univ San Diego La Jolla Inst for Cognitive Science, 1985. [48] Nico Schlömer. QuadPy: Numerical integration (quadrature, cubature) in Python, 2016–. URL https://github.com/nschloe/quadpy. [Online; accessed <today>]. [49] Samuel S. Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. Deep Infor- mation Propagation. arXiv:1611.01232 [cs, stat], November 2016. URL http://arxiv.org/ abs/1611.01232. arXiv: 1611.01232. [50] Samuel S. Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. Deep Infor- mation Propagation. 2017. URL https://openreview.net/pdf?id=H1W1UN9gg. [51] Benjamin Schrauwen. An overview of reservoir computing: Theory, applications and imple- mentations. page 12, 2007. [52] Elias M. Stein and Rami Shakarchi. Functional analysis: introduction to further topics in analysis. Princeton University Press, 2011. [53] Guillermo Valle-Pérez, Chico Q. Camargo, and Ard A. Louis. Deep learning generalizes because the parameter-function map is biased towards simple functions. arXiv:1805.08522 [cs, stat], May 2018. [54] Mark van der Wilk, Carl Edward Rasmussen, and James Hensman. Convolutional Gaussian Processes. In Advances in Neural Information Processing Systems 30, pages 2849–2858, 2017. [55] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \\Lukasz Kaiser, and Illia Polosukhin. Attention is All You Need. In Advances in Neural Information Processing Systems, pages 5998–6008, 2017. [56] Christopher K I Williams. Computing with Inﬁnite Networks. In Advances in neural information processing systems, page 7, 1997. [57] Andrew G Wilson, Zhiting Hu, Ruslan R Salakhutdinov, and Eric P Xing. Stochastic Variational Deep Kernel Learning. In Advances in Neural Information Processing Systems, pages 2586– 2594, 2016. [58] Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, and Eric P Xing. Deep kernel learning. In Artiﬁcial Intelligence and Statistics, pages 370–378, 2016. [59] Lechao Xiao, Yasaman Bahri, Jascha Sohl-Dickstein, Samuel Schoenholz, and Jeffrey Pen- nington. Dynamical Isometry and a Mean Field Theory of CNNs: How to Train 10,000- Layer Vanilla Convolutional Neural Networks. In Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Re- search, pages 5393–5402, Stockholmsmässan, Stockholm Sweden, July 2018. PMLR. URL http://proceedings.mlr.press/v80/xiao18a.html. [60] Greg Yang. Scaling Limits of Wide Neural Networks with Weight Sharing: Gaussian Process Behavior, Gradient Independence, and Neural Tangent Kernel Derivation. arXiv:1902.04760 [cond-mat, physics:math-ph, stat], February 2019. [61] Greg Yang and Hadi Salman. A Fine-Grained Spectral Perspective on Neural Networks. July 2019. 14 [62] Greg Yang and Sam S. Schoenholz. Deep Mean Field Theory: Layerwise Variance and Width Variation as Methods to Control Gradient Explosion. February 2018. URL https: //openreview.net/forum?id=rJGY8GbR-. [63] Greg Yang and Samuel S. Schoenholz. Mean Field Residual Network: On the Edge of Chaos. In Advances in neural information processing systems, 2017. [64] Greg Yang, Jeffrey Pennington, Vinay Rao, Jascha Sohl-Dickstein, and Samuel S. Schoenholz. A Mean Field Theory of Batch Normalization. September 2018. URL https://openreview. net/forum?id=SyMDXnCcF7. [65] Greg Yang, Jeffrey Pennington, Vinay Rao, Jascha Sohl-Dickstein, and Samuel S. Schoenholz. A Mean Field Theory of Batch Normalization. arXiv:1902.08129 [cond-mat], February 2019. URL http://arxiv.org/abs/1902.08129. arXiv: 1902.08129. 15 NETSOR program 3 Batchnorm (with Fully Connected Layers) over Multiple Batches // For ϕ : R → R, h ∈ RB, // let ˜ϕ(h) = ϕ((h − ν(h))/σ(h)), // where ν(h) = 1 B ∑B i=1 hi, // σ(h) = √ 1 B ∑B i=1(hi − ν(h))2. // Let ˜ϕi, i ∈ [B], be the ith coordinate of ˜ϕ. // Embeddings of k batches of inputs // with ath batch having size Ba Input: {W 1xia : G(n)}a∈[k],i∈[Ba] Input: W 2, . . . , W L : A(n, n) // Readout layer Input: v : G(n) for a ∈ [k] and i ∈ [Ba] do x1ia := ˜ϕi(W 1x1a, . . . , W 1xBaa) : H(n) end for for l = 2, . . . , L do for a ∈ [k] and i ∈ [Ba] do h lia := W lxl−1,ia : G(n) end for for a ∈ [k] and i ∈ [Ba] do xlia := ˜ϕi(h l1a, . . . , hlBaa) : H(n) end for end for Output: {v⊤xLia/ √n}a∈[k],i∈[Ba] A Writing Standard Architectures in NETSOR In this section, we showcase example programs for batchnorm, skip connection, convolution, pooling, GRU/LSTM, and (scaled) attention. In most cases, we demonstrate the computation on a single input batch, image, or sequence. Generalization to multiple inputs is obvious and follows the pattern of Program 2. It should also be apparent that any composition of these gadgets can be expressed in NETSOR . Additionally, observe that all nonlinearities used in these programs are controlled (Deﬁnition 5.3) or parameter-controlled (Deﬁnition C.1), so that Theorem 5.4 or Theorem C.4 applies. Also we remark that the GP convergence results for batchnorm, GRU/LSTM (and RNNs in general), and attention are new. Batchnorm, Fully-Connected Let ˜ϕ : RB → RB, ˜ϕ(h) def = ϕ ( h − ν(h) σ(h) ) , where ν(h) def = 1 B B∑ i=1 hi, σ(h) def = v u u √ 1 B B∑ i=1(hi − ν(h))2, (4) be batchnorm followed by coordinatewise nonlinearity ϕ, where h ∈ RB should be interpreted as a single neuron across a batch, and ν and σ are the batch mean and standard deviations. Here, B should be thought of as ﬁxed while n → ∞. Then, given a batch of G-vars y1, . . . , yB : G(n) (for example, they could be the preactivations after applying a linear layer), we can express the application of batchnorm via Nonlin as x1 := ˜ϕ1(y1, . . . , yB), . . . , xB := ˜ϕB(y1, . . . , yB), (5) producing B H-vars x1, . . . , xB : H(n). Program 3 expresses more generally the computation of a batchnorm network on multiple batches and over multiple layers. Here, each “for-loop” is just shorthand for the corresponding unrolled program (since Deﬁnition 4.1 doesn’t allow for-loops). Skip Connection If x : H(n) is previous layer activation, and we have weights W : A(n, n) and bias b : G(n), then we can express skip connection as ˜h := W x MatMul h := ˜h + b LinComb ¯x := x + ϕ(h) Nonlin (Graph) Convolution Consider a convolution network with L layers, with layer l having nl channels, with kernel positions kerl (for example, for a 3 × 5 kernel, kerl = [3] × [5]), and with feature map pixel positions pos l (for example, for a 128 × 256 feature map, pos l = [128] × [256]). 16 NETSOR program 4 2-Layer Convolutional Network with Global Average Pooling Input: {W 1 j xi+j : G(n1)}j∈ker1,i∈pos1 s.t. i+j∈pos0 Input: {W 2 j : A(n2, n 1)}j∈ker2 // Readout weights Input: v : G(n2) // Layer 1 Convolution for i ∈ pos1 do // Directly use input embeddings // LinComb // Sum is over all j ∈ ker1 such that // there is i ′ ∈ pos 0 with i ′ = j + i h 1 i := ∑ j W 1 j xi+j : G(n1) x1 i := ϕ(h 1 i ) : H(n1) end for // Layer 2 Convolution for j ∈ ker2, i ∈ pos 1 s.t. i + j ∈ pos2 do // Convolution Weights Multiplication // MatMul h 2 i;j := W 1 j x1 i+j : G(n2) end for for i ∈ pos 2 do // Sum is over all j ∈ ker2 such that // there is i ′ ∈ pos 1 with i′ = j + i h 2 i := ∑ j h 2 i;j : G(n2) end for // Nonlinearity & Global Average Pooling ¯x2 := 1 |pos2| ∑ i∈pos2 ϕ(h 2 i ) : H(n2) Output: v⊤ ¯x2/√n2 Then we can describe the weights of a stride-1 convolution as {W l iαβ}l∈[L],i∈kerl,α∈[nl],β∈[nl−1], so that for each i ∈ kerl, W l i ∈ Rnl×nl−1 is a dense matrix. Further, suppose x is an image input to the network with pixel coordinates pos 0 and n0 channels, {xiα}i∈pos0,α∈[n0], so that xi is a vector of dimension n0. Then the application of the convolution W 1 on x is given by h 1 i := (W 1 ∗ x)i = ∑ j W 1 j xi+j ∈ Rn 1 , h 1 iα = (W 1 ∗ x)iα = ∑ jβ W 1 jαβxi+j,β ∈ R, where the sums are over j ∈ ker1, i + j ∈ pos 0, and β ∈ [n0]. For higher layers we have similar formulas, with the only difference that we treat W 1 j xi+j as input G-vars but treat W l+1 j x l i+j, l ≥ 2, as a MatMul operation. Thus, our framework can express CNN computations by expressing the individual matrix multiplications Wjxi+j and reusing the matrices Wj. Program 4 shows a simple example program, and Program 7 shows a full-ﬂedged example over multiple inputs. Again, each “for-loop” is just shorthand for the corresponding unrolled program. Higher stride and general graph convolution can be expressed likewise. Pooling We continue the notational scheme of the exposition on convolutions above. Given the feature maps of layer l, {xiα}i∈posl,α∈[nl], global average pooling produces a single vector ¯x = {¯xα}α∈[nl] given by ¯x := 1 |posl| ∑ i∈posl xi, using LinComb. See Program 4 for an example in combination with convolutional layers. We can similarly express max pooling. Suppose pos l = [2k] × [2k]. Then max pooling with, for example, 2 × 2 kernel and stride of 2 would produce {ˆxjα}j∈[k]×[k],α∈[nl], with ˆxjα := max({x2j+i,α}i∈{0,1}×{0,1},2j+i∈posl ), using Nonlin. GRU and LSTM We present a program expressing GRU computation in Program 5; the program for LSTM is similar and is omitted. The overall pattern is similar to the program for simple RNN (Program 2), but with a crucial subtlety regarding the gating mechanism. In Program 5, ˜zs, ˜rs, ˜h s are G-vars, but the gates σ(˜zs), σ(˜rs) (where σ is sigmoid) and the candidate update ϕ(˜h s) are not G-vars. As we can only apply Nonlin to G-vars, this requires us to unroll the deﬁnition of h s to be a function of only G-vars. However, Appendix E presents expanded, but semantically equivalent versions of NETSOR which allow a more succinct representation of the same computation; see Program 11. Finally, Program 8 presents a full-ﬂedged program with multiple input sequences. 17 NETSOR program 5 GRU, with Gating Function σ and Activation Function ϕ // Embeddings of input sequence Input: Uzx1, . . . , UzxT : G(n) Input: Urx1, . . . , UrxT : G(n) Input: Uhx 1, . . . , UhxT : G(n) // Parameters Input: Wz, Wr, Wh : A(n, n) Input: bz, br, bh : G(n) // Initial GRU state Input: h 0 : G(n) // Readout layer Input: v : G(n) // Time step 1 h 1 z := Wzh 0 : G(n) ˜z1 := h 1 z + Uzx1 + bz : G(n) h 1 r := Wrh0 : G(n) ˜r1 := h 1 r + Urx1 + br : G(n) // σ is gating function, typically sigmoid; applying Nonlin ˆh 0 := h 0 ⊙ σ(˜r1) : H(n) h 1 h := Whˆh 0 : G(n) ˜h 1 := h 1 h + Uhx1 + bh : G(n) // Apply Nonlin // ϕ is activation function, typically tanh h 1 := (1 − σ(˜z1)) ⊙ h 0 + σ(˜z1) ⊙ ϕ(˜h 1) : H(n) // Time step 2 h 2 z := Wzh 1 : G(n) ˜z2 := h 2 z + Uzx2 + bz : G(n) h 2 r := Wrh 1 : G(n) ˜r2 := h 2 r + Urx2 + br : G(n) // Morally, ˆh 1 = σ(˜r1) ⊙ h 1, but we need to unroll h 1 to apply Nonlin // This can be expressed with more brevity using NETSOR◦ ; see Remark E.5 ˆh 1 := σ(˜r1) ⊙ ((1 − σ(˜z1)) ⊙ h 0 + σ(˜z1) ⊙ ϕ(˜h 1)) : H(n) h 2 h := Whˆh 1 : G(n) ˜h 2 := h 2 h + Uhx2 + bh : G(n) // Unrolling h 2 to a coordinatewise function of G-vars h 2 := (1 − σ(˜z2)) ⊙ (1 − σ(˜z1)) ⊙ h 0 + (1 − σ(˜z2)) ⊙ σ(˜z1) ⊙ ϕ(˜h 1) + σ(˜z2) ⊙ ϕ(˜h 2) : H(n) // Time step 3 ... // Time step T // Deﬁne ˜zT , ˜rT , ˜h T just like above ... // Unrolling h T to a coordinatewise function of G-vars h T := h 0 ⊙ ⊙T i=1(1 − σ(˜zi)) + ∑T j=1 ϕ(˜hj) ⊙ σ(˜zj) ⊙ ⊙T l=j+1(1 − σ(˜zl)) : H(n) Output: (v⊤h 1/√n, . . . , v⊤h T /√n) Layernorm Layernorm requires the extended rule Nonlin + to express. Recall for x ∈ Rn (thought of as the vector of activations with one entry per neuron in a layer; contrast with batchnorm), Layernorm(x) = x−ν(x) σ(x) where ν(x) = 1 n ∑ α xα and σ(x) = √ 1 n ∑n α=1(xα − ν(x))2. As we will see, ν(x) and σ(x) will both converge a.s. to a deterministic limit. Thus Layernorm(x) is just a linear combination of x with the constant-1s vector (considered as a input G-var), with (roughly 18 deterministic) coefﬁcients µ(x) and σ(x). This is expressible using the Nonlin + rule: Layernorm(x) := ψ(x; ν(x), σ(x)), where ψ(x; θ1, θ2) def = x − θ1 θ2 . Similarly, if layernorm is followed up by a nonlinearity ϕ, then we can express ϕ(Layernorm(x)) := ψ(x; ν(x), σ(x)), where ψ(x; θ1, θ2) def = ϕ ( x − θ1 θ2 ) . If layernorm is preceded by a nonlinearity, then likewise we can write Layernorm(ϕ(x)) := ψ(x; ν(ϕ(x)), σ(ϕ(x))), where ψ(x; θ1, θ2) def = ϕ(x) − θ1 θ2 . Scaled Attention Scaled attention requires the extended rule Nonlin + to express. Consider the following version of scaled attention: Given a query vector q ∈ Rn, keys ki ∈ Rn for each i ∈ [r], and corresponding values vi ∈ Rm, i ∈ [r], we can deﬁne the following scaled attention Attention(q, {ki}i, {vi}i) def = r∑ i=1 aivi, ai def = SoftMax(q⊤k1/n, . . . , q⊤kr/n)i. If q, ki are given as H-vars in a NETSOR program, then Theorem 5.4 will show that q⊤ki/n converges almost surely to a deterministic limit, so that each ai converges likewise. If each vi = ψ(gi) for some G-var gi and ﬁxed nonlinearity ψ, then attention can be expressed as follows using Nonlin +: Attention(q, {ki}i, {vi}i) = ϕ(g1, . . . , gr; a1, . . . , ar) where ϕ(x1, . . . , xr; θ1, . . . , θr) def = θ1ψ(x1) + · · · + θrψ(xr). More complicated variants, such as allowing ψ to take multiple G-vars as inputs, is likewise express- ible. When used as part of a decoder, a mask needs to be placed on the pre-softmax values so that no attention is paid to tokens in the future. This is aptly called masked attention and is given by the following formula: for j ∈ [r], MaskedAttentionj(q, {ki} r i=1, {vi} r i=1) = r∑ i=1 a j i vi, where a j i = SoftMax(q⊤k1/n, . . . , q⊤kj/n, −∞, . . . , −∞)i. It can obviously be expressed in NETSOR in the same fashion as (non-masked) attention. Note that Vaswani et al. [55] scales the pre-softmax values by 1/ √n instead of 1/n: ai = SoftMax(q⊤k1/√n, . . . , q⊤kr/√n)i. This is useful if qαki α has mean zero (averaged over α) for each i, so that q⊤ki/√n becomes roughly Gaussian, whereas q⊤ki/n converges to 0. However, when the zero-mean condition doesn’t hold, q⊤ki/ √n would only blow up to inﬁnity. B Example GP Kernel Computation with NETSOR In this section, we show how to compute the GP kernel of different architecture, following the recursive construction of Theorem 5.4. First, we review the V-transform of a nonlinearity. Deﬁnition B.1. Given a multivariate nonlinearity Φ : RB → RB, its V-transform VΦ is a function taking B × B positive semideﬁnite matrices to B × B positive semideﬁnite matrices, and is given by the following formula VΦ(K) def = E z∼N (0,K) Φ(z)Φ(z) ⊤. When ϕ : R → R, we take Vϕ to be V-transform of the RB → RB function that applies ϕ to each coordinate. 19 We collect below some of the common V-transforms. Here we describe the V-transforms using the function notation of kernels, but we shall freely switch between the function notation and the matrix notation in what follows. Fact B.2 ([11]). For any kernel K, VReLU(K)(x, x ′) = 1 2π ( √1 − c2 + (π − arccos c)c) √K(x, x)K(x′, x′) VReLU′(K)(x, x ′) = 1 2π (π − arccos c) where c = K(x, x ′)/ √K(x, x)K(x′, x′). Fact B.3 ([41]). For any kernel K, Verf (K)(x, x ′) = 2 π arcsin K(x, x ′) √(K(x, x) + 0.5)(K(x′, x′) + 0.5) Verf ′(K)(x, x ′) = 4 π√(1 + 2K(x, x))(1 + 2K(x′, x′)) − 4K(x, x′)2 . Fact B.4. Let ϕ(x) = exp(x/σ) for some σ > 0. For any kernel K, Vϕ(K)(x, x ′) = exp ( K(x, x) + 2K(x, x ′) + K(x′, x ′) 2σ2 ) . In Appendix B.3, we will also discuss the V-transform of batchnorm (which has been derived in [65]). B.1 MLP The kernel computation of a multilayer perceptron is by now well-known [46, 50, 37]. In this section, we work out an example of how to recover the usual kernel computation via tensor programs. B.1.1 MLP with Single Input (Program 1) The aim here is to illustrate step-by-step applications of Eq. (2) for Program 1, but note that in practice, it is often more convenient to ﬁnd some bulk recursive or compositional structure of Σ, and to leverage that structure for computing Σ (see Appendix B.2 for an example). For simplicity, assume the nonlinearities ϕ are ReLUs and the widths n1 = n2 = n to satisfy Assumption 5.1. By Corollary 5.5, we know that the output of the MLP, vT x2/ √n, is distributed as a Gaussian with mean 0 and variance of σ2 Ez ϕ2(z), where z ∼ N (µ(h 2), Σ(h 2, h 2)), and h 2 is the layer 2 preactivation (also a G-var) in Program 1. Therefore, all we need is to compute µ(h 2) and Σ(h 2, h 2) using (2), which requires the calculation of µ and Σ for possibly all the other G-vars in Program 1 due to the recursive nature of (2). In this example, we shall compute all entries of µ and Σ explicitly as a demonstration. We do so for each G-var in the order of appearance in the NETSOR program. Explicitly, the ordering is G = (W 1x, b 1, b 2, v, h1, ˜h 2, h 2). The input G-vars are W 1x, b 1, b 2, and v, and the sole input A-var is W 2. Setup In the fashion of typical Glorot initializations, we shall sample the parameters W 1, W 2, b 1, b 2 of the network as W 1 αβ ∼ N (0, 1/ dim x), W 2 αβ ∼ N (0, 1/n), vα ∼ N (0, 1), b 1 α ∼ N (0, 1), b 2 α ∼ N (0, 1). This corresponds, in the context of Assumption 5.1, to setting σW 2 = 1, µ in = 0 (in particular, µ(W 1x) = µ in(W 1x) = 0 due to the sampling of W 1), and Σin as follows: 20 Σin(W 1x, W 1x) = 1 Σin(b1, b 1) = 1 Σin(b2, b 2) = 1 Σin(b2, b 1) = 0 Σin(v, v) = 1 Σin(bi, W 1x) = 0 for i ∈ {1, 2} Σin(bi, v) = 0 for i ∈ {1, 2} Σin(v, W 1x) = 0 Calculating µ and Σ Now we will show a detailed calculation of µ and Σ for all of the G-vars G appearing here. By the input G-var case of Eq. (2), Σ(W 1x, W 1x) = Σ in(W 1x, W 1x) = 1 Σ(b 1, b 1) = Σin(b 1, b 1) = 1 Σ(b 2, b 2) = Σin(b 2, b 2) = 1 Σ(b 1, b 2) = Σin(b 2, b 1) = 0 Σ(v, v) = Σin(v, v) = 1 Σ(W 1x, b i) = Σ(b i, W 1x) = Σ in(bi, W 1x) = 0 for i ∈ {1, 2} Σ(v, b i) = Σ(bi, v) = Σ in(bi, v) = 0 for i ∈ {1, 2} Σ(W 1x, v) = Σ(v, W 1x) = Σ in(v, W 1x) = 0 Next, we extend µ and Σ to h 1, introduced via LinComb by h 1 := W 1x + b1. Note that h 1 is a G-var of the form g = ∑ i aiyi where a1 = a2 = 1, y1 = W 1x, and y2 = b1. Therefore, by the LinComb case of Eq. (2), µ(h 1) = µ(W 1x) + µ(b1) = 0 Σ(h 1, W 1x) = Σ(W 1x, W 1x) + Σ(b1, W 1x) = 1 + 0 = 1 Σ(h 1, b 1) = Σ(W 1x, b 1) + Σ(b 1, b 1) = 0 + 1 = 1 Σ(h 1, b 2) = Σ(W 1x, b 2) + Σ(b 1, b 2) = 0 + 0 = 0 Σ(h 1, v) = Σ(W 1x, v) + Σ(b1, v) = 0 + 0 = 0 Σ(h 1, h 1) = Σ(h 1, W 1x) + Σ(h 1, b 1) = 1 + 1 = 2. So h 1 is correlated with W 1x and b1 in obvious ways, and is independent from b2 and v. Next, we extend µ and Σ to, introduced via MatMul by ˜h 2 := W 2x1. Note that ˜h 2 is a G-var of the form g = W h where W = W 2 is an A-var, and h = x1 is an H-var introduced by x1 := ReLU(h 1). Therefore, by the “otherwise” case of Eq. (2), µ(˜h 2) = 0 Σ(˜h 2, W 1x) = 0 Σ(˜h 2, b 1) = 0 Σ(˜h 2, b 2) = 0 Σ(˜h 2, v) = 0 Σ(˜h 2, h 1) = 0 and by the MatMul case of Eq. (2) (setting ϕ and ¯ϕ there to both be ReLU), Σ(˜h 2, ˜h 2) = σ2 W 2 E z ϕ(z) ¯ϕ(z) = E z ReLU(z) 2 = 1 where z ∼ N (µ(h 1), Σ(h 1, h 1)) = N (0, 2). 21 NETSOR program 6 MLP Computation on a Set of Inputs // Embeddings of inputs Input: W 1x1, . . . , W 1xB : G(n) // Biases across L layers Input: b1, . . . , bL : G(n) // Weights from layer 2 on Input: W 2, . . . , W L : A(n, n) // Readout weights Input: v : G(n) for i = 1, . . . , B do h 1i := W 1xi + b1 : G(n) x1i := ϕ(h 1i) : H(n) for l = 2, . . . , L do ˜h li := W lxl−1,i : G(n) h li := ˜h li + b l : G(n) xli := ϕ(h li) : H(n) end for end for Output: (v⊤xL1/ √n, . . . , v⊤xLB/ √n) Thus, ˜h 2 can be thought of as “independent” from all other G-vars. Finally, we extend µ and Σ to h 2, introduced via LinComb by h 2 := ˜h2 + b2. Note that h 2 is a G-var of the form g = ∑ i aiyi where a1 = a2 = 1, y1 = ˜h 2, and y2 = b2. Then by the LinComb case of Eq. (2), µ(h 2) = µ(˜h 2) + µ(b2) = 0 Σ(h 2, W 1x) = Σ(˜h 2, W 1x) + Σ(b 2, W 1x) = 0 + 0 = 0 Σ(h 2, b 1) = Σ(˜h 2, b 1) + Σ(b2, b 1) = 0 + 0 = 0 Σ(h 2, b 2) = Σ(˜h 2, b 2) + Σ(b2, b 2) = 0 + 1 = 1 Σ(h 2, v) = Σ(˜h 2, v) + Σ(b 2, v) = 0 + 0 = 0 Σ(h 2, h 1) = Σ(˜h 2, h 1) + Σ(b2, h 1) = 0 Σ(h 2, ˜h 2) = Σ(˜h 2, ˜h 2) + Σ(b2, ˜h 2) = 1 + 0 = 1 Σ(h 2, h 2) = Σ(˜h 2, h 2) + Σ(b2, h 2) = Σ(h 2, ˜h 2) + Σ(h 2, b 2) = 1 + 1 = 2. Note that h 2 turns out to be “independent” from h 1, i.e. Σ(h 2, h 1) = 0, just as one might expect from the mean ﬁeld or the NNGP literature. Distribution of the Program Output We are now done with calculating µ(g) and Σ(g, g′) for all g, g′ ∈ G. Recall the output of the program is v⊤x2/ √n, where x2 was introduced via Nonlin by x2 := ReLU(h 2). According to Corollary 5.5, the output variance is then given by σ2 v E z ϕ2(z) = E z ReLU(z) 2 = 1, where z ∼ N (µ(h 2), Σ(h 2, h 2)) = N (0, 2). B.1.2 MLP with Multiple Inputs (Program 6) Now suppose we have an L-layer MLP with B inputs x1, . . . , xB. Program 6 expresses its computa- tion (again, the “for” loops are shorthands for the unrolled series of assignments). Here we will avoid computing out all values of Σ but only those that affect the inﬁnite-width GP. By Corollary 5.5, the output of Program 6 is distributed as Kij def = Cov ( v⊤xLi √n , v⊤xLj √n ) = σ2 v E Z ϕ(Z h Li)ϕ(Z hLj ) (6) where σ2 v is the coordinatewise variance of v, Z ∼ N (µ, Σ), and Z h Li is the component of Z corresponding to h Li (likewise for Z h Lj ). Therefore, we need to compute µ and Σ for the G-vars {h L1, . . . , hLB}. 22 Setup Suppose the inputs xi have dimension m. If we sample the neural network parameters in the usual Glorot fashion, W 1 αβ ∼ N (0, σ2 w/m), W l αβ ∼ N (0, σ2 w/n), ∀2 ≤ l ≤ L, vα ∼ N (0, σ2 v), b l α ∼ N (0, σ2 b ), ∀l ∈ [L], (7) then we have Σin deﬁned by Σin(W 1x i, W 1xj) = σ2 wxi⊤xj/m, Σin(b l, b l) = σ2 b , Σin(v, v) = σ2 v for all i, j ∈ [B] and l ∈ [L], and Σin(g, g′) = 0 for any other pairs of input G-vars g, g′. On the other hand, µ in(g) = 0 for all input G-vars g. Computing µ From Eq. (2), it is clear that µ in = 0 implies µ = 0. Computing Σ Again, our goal is to compute Σ restricted to the G-vars {h L1, . . . , hLB}. Lemma B.5. For any l = 2, . . . , L and any i, j ∈ [B], Σ(h li, h lj) = σ2 w E z1,z2 ϕ(z1)ϕ(z2) + σ2 b (8) where (z1, z2) ∼ N (0, Σ|hl−1,i,hl−1,j ), and (unrolling the restriction notation) Σ|hl−1,i,hl−1,j = (Σ(h l−1,i, h l−1,i) Σ(h l−1,i, h l−1,j) Σ(h l−1,j, h l−1,i) Σ(h l−1,j, h l−1,j) ) . Proof. From Program 6, h li is introduced via LinComb by h li := ˜h li + b l. Thus by the LinComb cases of Eq. (2), we have Σ(h li, h lj) = Σ(˜h li, h lj) + Σ(bl, h lj) = Σ(˜h li, ˜h lj) + Σ(bl, ˜h lj) + Σ(˜h li, b l) + Σ(b l, b l). Now, we cannot pattern match Σ(bl, ˜h lj) with any of the cases of Eq. (2) other than the “otherwise” case, which means Σ(bl, ˜h lj) = 0. Likewise, Σ(˜h li, b l) = 0. Therefore, Σ(h li, h lj) = Σ(˜h li, ˜h lj) + Σ(bl, b l) = Σ(˜h li, ˜h lj) + σ2 b . Now let’s analyze the Σ(˜h li, ˜h lj) term. The G-var ˜h li is introduced via MatMul by ˜h li := W lx l−1,i, where xl−1,i := ϕ(h l−1,i) and likewise for h lj. By the MatMul case of Eq. (2), we have Σ(˜h li, ˜h lj) = σ2 w E Z ϕ(Z h l−1,i )ϕ(Z h l−1,j ) where Z ∼ N (µ, Σ). Since the integrand only depends two components of Z, we can simplify the expression as E Z ϕ(Z h l−1,i)ϕ(Z h l−1,j ) = E z1,z2 ϕ(z1)ϕ(z2), where (z1, z2) ∼ N (0, Σ|hl−1,i,hl−1,j ) . Putting it all together, we recover the expression in the claim, as desired. 23 Computing the GP Kernel K Eq. (8) along with Eq. (6) gives all we need to compute K by recursion. If ϕ has a nice V-transform Vϕ, then we can vectorize this equation and obtain the following algorithm (which, again, is by now well-known [46, 50, 37]) Computing MLP kernel on B inputs Consider an L-layer MLP with nonlinearity ϕ at each layer. Suppose we have B network inputs x1, . . . , xB ∈ Rm, as in Program 6, and we sample the MLP parameters as in Eq. (7). Then the MLP converges in distribution to a GP on those B inputs, with kernel K computed as follows 1. Initialize K ∈ RB×B by Kij ← σ2 wxi⊤x j/m 2. For l = 1, . . . , L − 1, do K ← σ2 wVϕ(K) + σ2 b 3. Return K ← σ2 vVϕ(K) B.2 Simple RNN (Program 2) By Corollary 5.5, we know that the output of Program 2, ( v⊤s 11 √n , . . . , v⊤s t1 √n , v⊤s 12 √n , . . . , v⊤s r2 √n ) is, in the large n limit, distributed as a Gaussian with mean 0 and the covariance K where, for any a, b ∈ {1, 2} (denoting sequence number), Kia,jb def = lim n→∞ Cov ( v⊤s ia √n , v⊤s j b √n ) = σ2 v E Z ϕ(Z h ia )ϕ(Z h jb ) where Z ∼ N (µ, Σ), and Z h ia is the component of Z corresponding to h ia and likewise for Z hjb . Therefore, we need to compute µ and Σ for the G-vars {h 11, . . . , hs1, h 12, . . . , ht2} in Program 2. Setup Suppose the input tokens x ia to the RNN have dimension m. We will obtain the µ and Σ for Program 2 with Uαβ ∼ N (0, σ2 U /m), Wαβ ∼ N (0, σ2 W /n), bα ∼ N (0, σ2 b ), vα ∼ N (0, σ2 v). (9) The randomization of U induces the following covariance structure in the input token embeddings Σin(U x, U y) = σ2 U x⊤y/m (10) for any x, y ∈ {x i1} t i=1 ∪ {xj2}r j=1. For any other pair g, g′ of input G-vars, the sampling implies Σin(g, g′) = 0. Additionally, µ in(g) = 0 for all input G-var g. Computing µ In fact, one can quickly see that µ(g) = 0 for all G-vars g, not just the input G-vars. Computing Σ By Eq. (2), all {h i1, ˜h i1} t i=1 ∪ {h j2, ˜h j2} r j=1 are possibly correlated with each other. They satisfy the following recurrence Lemma B.6. For any a, b ∈ {1, 2}, we have Σ(h ia, h jb) = Σ(˜hia, ˜h jb) + Σin(U xia, U x jb) + σ2 b , ∀i, j ≥ 1 (11) Σ(˜hia, ˜h jb) = σ2 W E ϕ(z1)ϕ(z2), ∀i, j ≥ 2, (12) where (z1, z2) ∼ N (0, Σ|hi−1,a,hj−1,b ), and the base case Σ(˜h ia, ˜h jb) = 0 if i ≤ 1 or j ≤ 1. Here, Σ|set means the submatrix of Σ with rows and columns in set. Note that the notation b appears both as a sequence index and as the bias of the RNN. Since the former will only appear as a superscript and the latter will not, there should be no confusion. 24 Proof. Note that for each i ≥ 2, h ia is introduced via LinComb by h ia := ˜h ia + U x ia + b, where ˜h ia, U x ia, and b are G-vars. Then by the LinComb case of Eq. (2), Σ(h ia, h jb) = Σ(˜h ia, h jb) + Σ(U x ia, h jb) + Σ(b, h jb) = Σ(˜h ia, ˜h jb) + Σ(U x ia, ˜h jb) + Σ(b, ˜h jb) + Σ(˜h ia, U x jb) + Σ(U xia, U x jb) + Σ(b, U x jb) + Σ(˜h ia, b) + Σ(U xia, b) + Σ(b, b). By the “otherwise” case of Eq. (2), Σ(U x ia, ˜h jb) = Σ(b, ˜h jb) = Σ(˜h ia, U x jb) = Σ(˜h ia, b) = 0, and by the “input G-var” case of Eq. (2), Σ(b, U x jb) = Σ(˜h ia, b) = Σ in(b, U x jb) = Σ in(˜h ia, b) = 0. We thus have Σ(h ia, h jb) = Σ(˜h ia, ˜h jb) + Σ(U x ia, U x jb) + Σ(b, b) = Σ(˜h ia, ˜h jb) + Σ(U x ia, U x jb) + σ2 b which is Eq. (11). Next, note that ˜h ia is introduced via MatMul by ˜h ia := W s ia, and s ia is an H-var introduced by s ia := ϕ(h ia). Thus, by the MatMul rule of Eq. (2), Σ(˜h ia, ˜h jb) = σ2 W E Z ϕ(Z h ia )ϕ(Z h jb ) where Z ∼ N (µ, Σ) and Z h ia (resp. Z hjb ) is its component corresponding to h ia (resp. h jb). Since the integrand only depends on the two components Z h ia and Z h jb , we can rewrite Σ(˜h ia, ˜h jb) = σ2 W E z1,z2 ϕ(z1)ϕ(z2) where (z1, z2) ∼ N (0, Σ|hi−1,a,hj−1,b ). This is Eq. (12). Let us formulate the results above in a way more suggestive of the algorithm required to compute the kernel. For any 2 ≤ p ≤ t, 2 ≤ q ≤ r, deﬁne ˜H pq def = {˜h i1} p i=2 ∪ {˜h j2}q j=2 and X pq def = {U x i1}p i=1 ∪ {U xj2} q j=1. Denote by Σ| ˜H pq the restriction of Σ to ˜H pq, and likewise Σin|X pq the restriction of Σin to X pq. We can visualize Σ| ˜H pq = ( App Bpq Bpq ⊤ C qq ) ∈ R(p+q−2)×(p+q−2) Σin|X pq = ( P pp Q pq Q pq ⊤ Rqq ) ∈ R(p+q)×(p+q) (13) where App def = {Σ(˜h i1, ˜h j1)}p,p i,j=2 C qq def = {Σ(˜h i2, ˜h j2)} q,q i,j=2 Bpq def = {Σ(˜h i1, ˜h j2)} p,q i,j=2 P pp def = {Σin(U x i1, U x j1)} p,p i,j=1 Rqq def = {Σin(U x i2, U x j2)} q,q i,j=1 Q pq def = {Σin(U xi1, U x j2)}p,q i,j=1. Let Σ| 0 ˜H pq also denote Σ| ˜H pq padded with an additional column and an additional row of 0s on the left and top of each block App, Bpq, Bpq ⊤, C qq: Σ|0 ˜H pq =    0 0 0 0 0 App 0 Bpq 0 0 0 0 0 Bpq ⊤ 0 C qq  |  ∈ R(p+q)×(p+q). (14) Then Eqs. (11) and (12) can be combined and vectorized as Σ| ˜H p+1,q+1 = Vϕ (Σ| 0 ˜H pq + Σin|X pq + σ2 b ) . (15) Eq. (15) quickly yields to a iterative, vectorized algorithm for computing Σ| ˜H tr (recall t and r are the lengths of the two input sequences), assuming that Vϕ can be efﬁciently computed (such as those under Deﬁnition B.1). Then, with H pq def = {h i1}p i=1 ∪ {h j2} q j=1, we have Σ|H tr = Σ| 0 ˜H tr + Σin|X tr + σ2 b . 25 Computing the GP Kernel Finally, given Σ|H tr , by Corollary 5.5, the covariance of the output of Program 2 in the large n limit is K = σ2 vVϕ (Σ|H tr ) . Computing RNN kernel Consider a simple RNN with nonlinearity ϕ, as in Program 2. Suppose we have 2 input sequences x11, . . . , xt1 and x12, . . . , xr2 ∈ Rm. Assume we sample the RNN parameters as in Eq. (9). Then the outputs of the RNN converge jointly in distribution to a Gaussian with covariance computed as follows. 1. Initialize Σin according to Eq. (10). 2. Starting with p = q = 0, do (a) Σ| ˜H p+1,q+1 ← σ2 wVϕ (Σ| 0 ˜H pq + Σin|X pq + σ2 b ) (see Eqs. (13) and (14) for notations) (b) Set p ← min(p + 1, t), q ← min(q + 1, r). (c) If p and q did not change, break. 3. Compute Σ|H tr ← Σ| 0 ˜H tr + Σin|X tr + σ2 b . 4. The output kernel is given by K ← σ2 vVϕ (Σ|H tr ) . See our repository github.com/thegregyang/GP4A for a reference implementation of this kernel. B.3 Batchnorm (Program 3) As shown in Appendix A, batchnorm (followed by a coordinatewise nonlinearity) can be just thought of a multivariate nonlinearity, and the computation of Σ can largely follow the same pattern as for any other feedforward neural net (see Program 3). However, doing so efﬁciently is not so obvious, especially when the batch size is large. In this section, we describe how to overcome this apparent difﬁculty. B.3.1 Batchnorm with Single Batch Let us compute the GP kernel for Program 3, assuming there is only one batch. Setup The network in Program 3 has parameters W 1 ∈ Rn×m, where m is the input dimension, and W l ∈ Rn×n for 2 ≤ l ≤ L. Since batchnorm is scale-invariant, we will just assume that W 1 αβ ∼ N (0, 1/m), W l αβ ∼ N (0, 1/n), ∀l ≥ 2, and vα ∼ N (0, σ2 v). This means that the initial NETSOR sampling data have values Σin(v, v) = σ2 v Σin(W 1x ia, W 1xi ′a′) = xia⊤xi′a ′/m, and µ in = 0 identically. Computing µ As before, from Eq. (2), it is easy to see that µ in = 0 implies µ = 0. Computing Σ Applying Eq. (2) in the fashion of Lemma B.5, we get Lemma B.7. For any a ∈ [k] and 2 ≤ l ≤ L, let H la denote the set {h lia}i∈[Ba]. Also write H 1a def = {W 1x ia}i∈[Ba]. Recall that Σ|set denotes the square submatrix of Σ with rows and columns in set. Then for any a ∈ [k] and l = 1, . . . , L − 1, Σ|H l+1,a = E ζ∼N (0,Σ| Hla ) ˜ϕ(ζ) ˜ϕ(ζ) ⊤ = V ˜ϕ (Σ|H la ) ∈ RBa×Ba , (16) where ˜ϕ is batchnorm followed by coordinatewise nonlinearity ϕ, as in Appendix A and Program 3. 26 A priori, evaluating this expectation requires a Ba-dimensional Gaussian integral, which becomes intractible when B is large. However, if ϕ = ReLU, then surprisingly one can reduce the B- dimensional integral this Gaussian expectation seems to require to a 1-dimensional integral: By [65] we can express Eq. (16) as Σ|H l+1,a = Ba ∫ ∞ 0 VReLU(Σ G(I + 2sΣG) −1) √ det(I + 2sΣG) ds (17) where VReLU is as in Fact B.2, and ΣG def = GΣ|H laG G def = IB − 1 B 11⊤. B.3.2 Batchnorm with Multiple Batches Now let’s consider the covariance of preactivations between different batches. We maintain the same setup as above, and as usual µ = 0 identically. Computing Σ By another straightforward application of Eq. (2), we get Lemma B.8. With the same notation as in Lemma B.7, for two different batches a ̸= b, Σ(H l+1,a, H l+1,b) = E ˜ϕ(ζ1) ˜ϕ(ζ2)⊤ where the expectation is taken over ( ζ1 ζ2 ) ∼ N (0, (Σ(H la, H la) Σ(H la, H lb) Σ(H lb, H la) Σ(H lb, H lb) )) . Again, this expectaion seems to require an integral in (Ba + Bb) dimensions. However, via some integral tricks, this expectation can be transformed to the following 2D integral: Σ(H l+1,a, H l+1,b) = √BaBbπ−1 ∫ ∞ 0 ds ∫ ∞ 0 dt (st) −1/2 det(IBa+Bb + 2Ω)−1/2VReLU(Π)12 (18) where Ω = D1/2 ( GaΣ(Y, Y )Ga GaΣ(Y, Y ′)Gb GbΣ(Y ′, Y )Ga GbΣ(Y ′, Y ′)Gb ) D1/2 Π = D−1/2Ω(I + 2Ω)−1D−1/2 D = sIBa ⊕ tIBb = (sIBa 0 0 tIBb ) Ga = IBa − B−1 a 11⊤ Gb = IBb − B−1 b 11⊤ and VReLU(Π)12 is the block of VReLU(Π) on the ﬁrst row, second column, of size Ba × Bb. Computing the GP Kernel K The computation is similar to Lemmas B.7 and B.8, so let us directly summarize the entire algorithm below. 27 Computing Batchnorm+ReLU kernel We compute the kernel of a L-layer fully-connected network where each layer has a batchnorm followed by ReLU, as shown in Program 3. Let the input dimension be m and the common width of hidden layers be n. Sample the ﬁrst layer weights W 1 as W 1 αβ ∼ N (0, σ2 W /m) and each higher layer’s weight matrix W l as W l αβ ∼ N (0, σ2 W /n) with σW = 1 (since batchnorm is scale-invariant), and the readout layer as vα ∼ N (0, σ2 v). We omit biases since batchnorm is shift invariant. Suppose we have k batches of inputs {xib : i ∈ [Bb], b ∈ [k]}, with batch b containing Bb inputs. Then the outputs of the network converge jointly in distribution to a Gaussian N (0, K) with K computed as follows. 1. Initialize {K 0 ab ∈ RBa×Bb }k a,b=1 by (K 0 ab)ij ← xia⊤xjb/m. 2. For l = 1, . . . , L, do (a) For a = 1, . . . , k, do i. K l aa ← V ˜ϕ(K l−1 aa ) by evaluating a 1D integral according to Eq. (17). (b) For a, b ∈ [k], a ̸= b, do i. Compute K l ab by using K l−1 ab , K l−1 aa , K l−1 bb and evaluating a 2D integral according to Eq. (18). 3. Return K ← σ2 v    K L 11 · · · K L 1k ... . . . ... K L k1 · · · K L kk.  |  ∈ R∑ a Ba× ∑ a Ba Vectorized Implementation In our repo github.com/thegregyang/GP4A, we show how to implement single- and multi-batch BN using the quadpy [48] package for vectorized quadrature integration, and by using eigendecomposition to simplify the computation of the integrand in the integrals above. B.4 Convolution and Pooling (Program 4) Convolution and pooling, in the context of neural network-Gaussian process correspondence, have already been treated in [43, 18]. In this section we revisit the same derivations from the perspective of tensor programs. B.4.1 CNN with Single Input Let us compute the GP kernel for Program 4, for the following setup: Setup The CNN in Program 4 has parameters {W 1 j }j∈ker1 , {W 2 j }j∈ker2 . It has widths n1 and n2, but for simplicity, let’s assume n1 = n2 = n. Each input image is given as {xi ∈ Rm}i∈pos0 where pos0 denotes “pixel locations” and m denotes number of channels (for example, pos0 = [32] × [32] and m = 3 for the CIFAR10 dataset). Suppose we sample the parameters as (W 1 j )αβ ∼ N (0, σ2 w/m), ∀j ∈ ker1, (W 2 j )αβ ∼ N (0, σ2 w/n1), ∀j ∈ ker2, vα ∼ N (0, σ2 v/n). This induces Σin as follows: Σin(v, v) = σ2 v Σin(W 1 j xi+j, W 1 j′xi′+j′) = σ2 wx⊤ i+jxi′+j′/m for any i, i ′ ∈ pos 1, j, j′ ∈ ker1 such that i + j, i′ + j′ ∈ pos 0; and Σin(g, g′) = 0 for any other pairs of input G-vars. In addition, µ in = 0 identically. Computing µ As before, from Eq. (2), it is easy to see that µ in = 0 implies µ = 0. 28 Computing Σ By straightforward applications of Eq. (2), we obtain the following Lemma B.9. For any i, i ′ ∈ pos1, Σ(h 1 i , h 1 i′) = ∑ j,j′∈ker1 Σin(W 1 j xi+j, W 1 j′xi′+j′). In addition, for any j, j′ ∈ ker2 such that i + j, i′ + j′ ∈ pos 1, Σ(h 2 i;j, h 2 i′;j′) = σ2 w E z1,z2 ϕ(z1)ϕ(z2)I(j = j′), where (z1, z2) ∼ N (0, Σ|h1 i ,h1 i′ ). Finally, for any i, i ′ ∈ pos2, Σ(h 2 i , h 2 i′) = ∑ j,j′∈ker2 Σ(h 2 i;j, h 2 i′;j′) where the sum is over all j, j′ ∈ ker2 such that i + j, i′ + j′ ∈ pos1. Computing the GP kernel K By Corollary 5.5, the output of the CNN converges in distribution to N (0, K) where K is a scalar given by K def = lim n→∞ Var ( v⊤ ¯x 2 n ) = σ2 v E Z ( 1 |pos2| ∑ i ϕ(Z h 2 i ) )2 = σ2 v E i,i′∈pos2 E Z ϕ(Z h 2 i )ϕ(Z h 2 i′ ) where Z ∼ N (µ, Σ) and where in the last expression, i, i ′ are sampled independently and uniformly from pos 2. If we let H 2 def = {h 2 i }i∈pos2 , then K can be computed as K = σ2 v E i,i′∈pos2 Λii′ where Λ def = Vϕ (Σ|H 2 ) . B.4.2 CNN with Multiple Inputs Now consider the general case when we have multiple inputs x 1, . . . , xB and have L layers (but, for simplicity, still no bias), as in Program 7. The derivation is very similar to the single input case, but we will err on the side of completeness. Setup The CNN in Program 7 has parameters {W l}l∈[L],j∈kerl. It has widths n1, . . . , nL, but as before, we shall assume they are all equal to n for simplicity. Each input image xa is given as {xa i ∈ Rm}i∈pos0 where pos 0 denotes “pixel locations” and m denotes number of channels. Suppose we sample the parameters as (W 1 j )αβ ∼ N (0, σ2 w/m), ∀j ∈ ker1, (W l j )αβ ∼ N (0, σ2 w/nl−1), ∀j ∈ kerl, for all l = 2, . . . , L, and vα ∼ N (0, σ2 v/n). This induces Σin as follows: Σin(v, v) = σ2 v Σin(W 1 j xa i+j, W 1 j′xa ′ i′+j′) = σ2 wxa i+j ⊤xa ′ i′+j′/m for any a, a ′ ∈ [B] and any i, i ′ ∈ pos 1, j, j′ ∈ ker1 such that i+j, i′ +j′ ∈ pos0; and Σin(g, g′) = 0 for any other pairs of input G-vars. In addition, µ in = 0 identically. Computing µ As before, from Eq. (2), it is easy to see that µ in = 0 implies µ = 0. Computing Σ By straightforward applications of Eq. (2), we obtain the following Lemma B.10. For any a, a ′ ∈ [B] and any i, i ′ ∈ pos1, Σ(h 1a i , h 1a ′ i′ ) = ∑ j,j′∈ker1 Σin(W 1 j xa i+j, W 1 j′xa ′ i′+j′). (19) In addition, for any 2 ≤ l ≤ L and any j, j′ ∈ ker2 such that i + j, i′ + j′ ∈ pos 1, Σ(h la i;j, h la ′ i′;j′) = σ2 w E z1,z2 ϕ(z1)ϕ(z2)I(j = j′), (20) 29 NETSOR program 7 L-layer Convolutional Network with Global Average Pooling Input: {W 1 j xa i+j : G(n1)} a∈[B], j∈ker1,i∈pos1 s.t. i+j∈pos0 Input: {W l j : A(nl, n l−1)}2≤l≤L,j∈kerl // Readout weights Input: v : G(nL) for a ∈ [B] do // Layer 1 convolution for i ∈ pos1 do // Directly use input embeddings // LinComb // Sum is over all j ∈ ker1 such that // there is i ′ ∈ pos 0 with i ′ = j + i h 1a i := ∑ j W 1 j xa i+j : G(n1) end for // Higher layer convolutions for l = 2, . . . , L do for i ∈ posl−1 do xl−1,a i := ϕ(h l−1,a i ) : H(n l−1) end for for j ∈ kerl, i ∈ pos l s.t. i + j ∈ posl−1 do // MatMul h la i;j := W l j xl−1,a i+j : G(nl) end for for i ∈ posl do // Sum is over all j ∈ kerl such that // there is i ′ ∈ pos l−1 with i ′ = j + i h la i := ∑ j h la i;j : G(nl) end for end for // Nonlinearity & Global Average Pooling ¯xLa := 1 |posL| ∑ i∈kerL ϕ(h La i ) : H(nL) end for Output: v⊤ ¯xL1/ √nL, . . . , v⊤ ¯xLB/ √nL where (z1, z2) ∼ N (0, Σ|h l−1,a i ,hl−1,a′ i′ ). Finally, for any i, i ′ ∈ posl, Σ(h la i , h la ′ i′ ) = ∑ j,j′ Σ(h la i;j, h la ′ i′;j′) (21) where the sum is over all j, j′ ∈ kerl such that i + j, i′ + j′ ∈ posl−1. These equations are all we need to compute the GP kernel K. Computing the GP kernel K By Corollary 5.5, the output of the CNN converges in distribution to N (0, K) where K ∈ RB×B is given by Kaa′ def = lim n→∞ Cov ( v⊤ ¯xLa √n , v⊤ ¯xLa ′ √n ) = σ2 v E Z ( 1 |posL| ∑ i ϕ(Z h La i ) ) ( 1 |posL| ∑ i ϕ(Z h La′ i ) ) = σ2 v E i,i′∈posL E Z ϕ(Z h La i )ϕ(Z h La′ i′ ) (22) where Z ∼ N (µ, Σ) and where in the last expression, i, i ′ are sampled independently and uniformly from posL. Since Σ(h La i , h La ′ i′ ) can be obtained recursively via Lemma B.10, one can compute K easily via recursion. But we can do better by vectorizing the whole computation. 30 Vectorized Implementation Let us deﬁne the 4-tensor Σl = {Σ l aa′ii′ : a, a ′ ∈ [B], i, i′ ∈ pos l} by Σ l aa′ii′ def = Σ(h la i , h la ′ i′ ). Then Eq. (19) corresponds to Σ1 aa′ii′ = σ2 w ∑ j,j′ xa i+j ⊤xa ′ i′+j′/m where the sum is over all j, j′ ∈ ker1 such that i + j, i′ + j′ ∈ pos0. For l = 2, . . . , L − 1, Eqs. (20) and (21) can be vectorized as Σl aa′ = κ l ∗ ˆΣ l aa′, where ˆΣ l = σ2 wVϕ(Σ l−1), treating Σl−1 as a (B · pos l−1) × (B · posl−1) matrix, and κ l∗ is the “convolution” (κ l ∗ ˆΣ l aa′)ii′ = ∑ j,j′∈kerl i+j∈posl−1 i′+j′∈pos l−1 ( ˆΣl aa′)i+j,i′+j′. (23) This κ l convolution can indeed be implemented as a (CUDA) convolutional operation, vectorized over all a, a ′. Finally, to obtain the inﬁnite-width GP kernel K of the CN output, we can vectorize Eq. (22) as Kaa′ = σ2 vE ∗ ˆΣ L, where ˆΣ L = Vϕ(ΣL−1), and E∗ denotes the spacial averaging (E ∗ ˆΣ L)aa′ def = E i,i′∈posL ˆΣL aa′ii′. (24) Again, E∗ can be implemented as a convolution operator vectorized over all a, a ′. In summary, Computing CNN Kernel Suppose we have an L-layer convolutional neural network with coordinatewise nonlinearity ϕ but no bias, as in Program 7, that takes images with m channels and of size pos 0 × pos0. Suppose we have a set of inputs x1, . . . , xB where each input xa is given as xa = {xa i ∈ Rm}i∈pos0. Then the CNN outputs converge in distribution to a Gaussian N (0, K) where K ∈ RB×B can be calculated as follows. 1. Initialize Σ1 ∈ RB×B×posl×posl by Σ1 aa′ii′ = σ2 w ∑ j,j′ xa i+j ⊤xa ′ i′+j′/m where the sum is over all j, j′ ∈ ker1 such that i + j, i′ + j′ ∈ pos 0. 2. For l = 2, . . . , L − 1, do (a) Σl ← σ2 wκ l ∗ Vϕ(Σl−1) (see Eq. (23) for κ l’s deﬁnition) 3. return K ← σ2 vE ∗ Vϕ (Σ L−1) (see Eq. (24) for E’s deﬁnition) B.5 GRU (Program 5) We demonstrate how to compute the GP kernel for GRU as encoded in NETSOR by Program 5. A key distinguishing feature of this conversion is that we will need to compute high dimensional Gaussian expectations, where the dimension is as large as the number of timesteps unrolled, in contrast to the simple RNN case Program 2. These Gaussian expectations correspond to the expected values of multiplication of the gate values across time. We ﬁrst proceed with a single input sequence, as in Program 5. We then comment on the generalization to multiple sequences at the end. 31 Setup We will obtain the µ and Σ for Program 5 with (Uz)αβ, (Ur)αβ, (Uh)αβ ∼ N (0, σ2 U /n), (Wz)αβ, (Wr)αβ, (Wh)αβ ∼ N (0, σ2 W /n), (bz)α, (br)α, (bh)α ∼ N (0, σ2 b ), vα ∼ N (0, σ2 v), and h 0 = 0. (25) Suppose the input tokens xi to the GRU have dimension m. The randomization of U induces the following covariance structure in the input token embeddings Σin(U x, U y) = σ2 U x⊤y/m for any x, y ∈ {xi} T i=1. For any other pair g, g′ of input G-vars, Σin(g, g′) = 0. Additionally, µ in(g) = 0 for all input G-vars g. Computing µ In fact, one can quickly see that µ(g) = 0 for all G-vars g. Computing Σ Applying Eq. (2) to Program 5 and some simpliﬁcation in the manner of Lemma B.6’s proof yields, for any two times t, s, Σ(˜zt, ˜zs) = Σ(h t z, h s z ) + σ2 U xt⊤xs/m + σ2 b (26) Σ(˜rt, ˜rs) = Σ(h t r, h s r ) + σ2 U xt⊤x s/m + σ2 b (27) Σ(˜h t, ˜h s) = Σ(h t h, h s h) + σ2 U xt⊤xs/m + σ2 b (28) Σ(h t z, h s z ) = Σ(h t r, h s r ) = σ2 W t∑ i=1 s∑ j=1 { E ϕ(Z ˜h i)ϕ(Z ˜hj ) × E  σ(Z ˜zi) t∏ p=i+1 (1 − σ(Z ˜zp ))   ×  σ(Z ˜zj ) s∏ q=j+1 (1 − σ(Z ˜zq ))   } (29) Σ(h t h, h s h) = σ2 W Σ(h t z, h s z ) E σ(Z ˜rt)σ(Z ˜rs ) (30) where expectations are taken over Z = {Z g}g is G-var ∼ N (µ, Σ), which has one component for each G-var in the program. Then, applying Corollary 5.5, we see that the output of the GRU (v⊤h 1/ √n, . . . , v⊤hT / √n) converges in distribution to a zero mean Gaussian distribution with covariance matrix K = {Kts} T t,s=1, Kts = σ2 v t∑ i=1 s∑ j=1 { E ϕ(Z ˜h i)ϕ(Z ˜h j ) × E  σ(Z ˜zi) t∏ p=i+1 (1 − σ(Z ˜zp ))   ×  σ(Z ˜zj ) s∏ q=j+1 (1 − σ(Z ˜zq ))   }. (31) Eqs. (27) to (31) yield the complete set of equations to compute the output covariance K, but to do so efﬁciently rests entirely on evaluating the the possibly T -dimensional integral behind E  σ(Z ˜zi) t∏ p=i+1 (1 − σ(Z ˜zp ))   ×  σ(Z ˜zj ) s∏ q=j+1 (1 − σ(Z ˜zq ))   . (32) For general σ and ϕ, this is hopeless. However, when ϕ = erf and σ = (1 + erf)/2 — which approximate ϕ = tanh and σ = sigmoid — Eq. (32) can in fact be evaluated efﬁciently by reducing it to a Gaussian orthant probability, which can be evaluated efﬁciently [19]: 32 NETSOR program 8 GRU, Multiple Input Sequences // Embeddings of B input sequences // with ath sequence having length Ta Input: {Uzx ta : G(n)}a∈[B],t∈[Ta] Input: {Urxta : G(n)}a∈[B],t∈[Ta] Input: {Uhxta : G(n)}a∈[B],t∈[Ta] // Parameters Input: Wz, Wr, Wh : A(n, n) Input: bz, br, bh : G(n) // Initial GRU state Input: h 0 : G(n) // Readout layer Input: v : G(n) for a ∈ [B] do for t ∈ [Ta] do h ta z := Wzht−1,a : G(n) ˜zta := h ta z + Uzx ta + bz : G(n) h ta r := Wrh t−1,a : G(n) ˜rta := h ta r + Urx ta + br : G(n) // Morally, ˆh t−1,a = σ(˜rt−1,a) ⊙ h t−1,a, but we need to unroll h t−1,a to apply Nonlin ˆh t−1,a := σ(˜rt−1,a)⊙(h 0 ⊙ ⊙t−1 i=1(1 − σ(˜zia)) + ∑t−1 j=1 ϕ(˜h ja) ⊙ σ(˜zja) ⊙ ⊙t−1 l=j+1(1 − σ(˜zla)) ) : H(n) h ta h := Whˆh t−1,a : G(n) ˜hta := hta h + Uhxta + bh : G(n) // Unrolling h t to a coordinatewise function of G-vars ht := h 0 ⊙ ⊙t i=1(1 − σ(˜zia)) + ∑t j=1 ϕ(˜h ja) ⊙ σ(˜zja) ⊙ ⊙t l=j+1(1 − σ(˜zla)) : H(n) end for end for Output: {v⊤h ta/ √n}a∈[B],t∈[Ta] Lemma B.11. Let ϕ = erf and σ = (1 + erf)/2. Then for any µ ∈ RT and any PSD Σ ∈ RT ×T , E x∼N (µ,Σ) T∏ i=1 ϕ(xi) = E x∼N (µ,Σ+ 1 2 I) T∏ i=1 sgn(xi) E x∼N (µ,Σ) T∏ i=1 σ(xi) = E x∼N (µ,Σ+ 1 2 I) T∏ i=1 I(xi ≥ 0) = Pr x∼N (µ,Σ+ 1 2 I)[x ≥ 0]. Remark B.12. Observe that if T = 2, then Lemma B.11 recovers the arccosine kernel of erf via Fact B.3: E [erf(x1)erf(x2) : x ∼ N (0, Σ)] = 2 π arcsin Σ12√(Σ11 + 1 2 )(Σ22 + 1 2 ) . 33 To apply Lemma B.11, we can express Eq. (32) as E  σ(Z ˜zi) t∏ p=i+1 σ(−Z ˜zp )   ×  σ(Z ˜zj ) s∏ q=j+1 σ(−Z ˜zq )   = E  σ( ˆY i) s∏ p=i+1 σ( ˆY p)   ×  σ( ˇY j) s∏ q=j+1 σ( ˇY q)   = E   t∏ p=i σ( ˆY p)   ×   s∏ q=j σ( ˇY q)   where ( ˆY i, . . . , ˆY t, ˇY j, . . . , ˇY s) ∼ N (ν, Ω) with b and Ω given as follows ν( ˆY i) = µ(˜zi) ν( ˇY j) = µ(˜zj) ν( ˆY p) = −µ(˜zp), ∀p ≥ i + 1 ν( ˇY q) = −µ(˜zq), ∀q ≥ j + 1 Ω( ˆY p, ˇY q) = {Σ(˜zp, ˜zq) if p ≥ i + 1, q ≥ j + 1, or p = i, q = j −Σ(˜zp, ˜zq) otherwise. Ω( ˆY p, ˆY p′) = {Σ(˜zp, ˜zp′) if p, p ′ ≥ i + 1, or p = p′ = i −Σ(˜zp, ˜zp′) otherwise. Ω( ˇY q, ˇY q′) = {Σ(˜zq, ˜zq′) if q, q′ ≥ j + 1, or q = q′ = j −Σ(˜zq, ˜zq′) otherwise. Using Lemma B.11, one then has E  σ(Z ˜zi ) t∏ p=i+1 σ(−Z ˜zp )   ×  σ(Z ˜zj ) s∏ q=j+1 σ(−Z ˜zq )   = Pr [ X ≥ 0 : X ∼ N (ν, 1 2 I + Ω)] . (33) If we two input sequences, the equations for recursively computing Σ and of K are similar to the above, and we summarize them below 34 Computing the GRU kernel Consider a GRU processing B sequences in the fashion of Program 8, with ϕ = erf and σ = (1 + erf)/2. Sample the GRU’s parameters as in Eq. (25). Then for sequence numbers a, b ∈ [B] and time steps 2 ≤ t ≤ Ta, 2 ≤ s ≤ Tb, we have the following recurrence relations Σ(˜zta, ˜zsb) = Σ(h ta z , h sb z ) + σ2 U xta⊤xsb/m + σ2 b Σ(˜rta, ˜rsb) = Σ(h ta r , h sb r ) + σ2 U xta⊤xsb/m + σ2 b Σ(˜h ta, ˜h sb) = Σ(h ta h , h sb h ) + σ2 U xta⊤xsb/m + σ2 b Σ(h ta z , h sb z ) = Σ(h ta r , h sb r ) = σ2 W t∑ i=1 s∑ j=1 ζ ab i:t,j:s E ϕ(Z ˜hia)ϕ(Z ˜h jb ) Σ(h ta h , h sb h ) = σ2 W Σ(h ta z , h sb z ) E σ(Z ˜rta )σ(Z ˜rsb ) with initial conditions Σ(˜zta, ˜zsb) = Σ(˜rta, ˜rsb) = Σ(˜h ta, ˜h sb) = 0 if t = 1 or s = 1, and the output covariance K of the GRU outputs in the large n limit can be computed as Kta,sb = lim n→∞ Cov ( v⊤xta √n , v⊤xsb √n ) = σ2 v t∑ i=1 s∑ j=1 ζ ab i:t,j:s E ϕ(Z ˜h ia )ϕ(Z ˜h jb ) where Z ∼ N (0, Σ) and ζ ab i:t,j:s def = E Z  σ(Z ˜zia ) t∏ p=i+1(1 − σ(Z ˜zpa ))   ×  σ(Z ˜zjb ) s∏ q=j+1 (1 − σ(Z ˜zqb))   , which can be reduced to a computation of orthant probability in the fashion of Eq. (33). The above equations can be turned into a (relatively) efﬁcient algorithm for computing the GP kernel of a GRU. Our repo github.com/thegregyang/GP4A shows a reference implementation of it (allowing slightly more general initialization hyperparameters). It leverages the R package mvtnorm [19] to evaluate the Gaussian orthant probability involved in Eq. (33). In the rest of the section, we prove Lemma B.11. Review of (Tempered) Distributions Before we begin the proof of Lemma B.11, we brieﬂy recall the notion of a tempered distribution, which is a “pseudo-function” that is formally deﬁned as an element of the dual of Schwartz space (intuitively, the space of functions with rapidly decreasing derivatives of all orders) [52]. Given a Schwartz function f and a tempered distribution τ , the value of τ on f will be denoted here by ⟨τ, f ⟩. For example, if τ is a locally-integrable function, then τ is also a tempered distribution and ⟨τ, f ⟩ can be deﬁned by ⟨τ, f ⟩ = ∫ τ (x)f (x) dx. As all Schwartz functions have Fourier transforms [52], any tempered distribution has Fourier transform deﬁned by ⟨F{τ }, f ⟩ def = ⟨τ, F{f }⟩. 35 In what follows, notationally, Fourier transform will convert functions or distributions in variable t to functions to distributions in variable x, or vice versa. See [52] for more background on distributions. Proof of Lemma B.11. As a tempered distribution, ϕ = erf can be expressed as ϕ(x) = F { −i √2 √π p.v. e −t2/4 t } (x) = 1 √2π p.v. ∫ eixt −i√2 √π e−t 2/4 t dt, where p.v. denotes principal value integration p.v. ∫ eixt −i √2 √π e−t2/4 t dt def = lim ε→0 (∫ −ε −∞ + ∫ ∞ ε ) eixt −i √2 √π e−t2/4 t dt. Over multiple variables, because Fourier transform over RT is equivalent to applying 1D Fourier transform over each coordinate, we have T∏ i=1 ϕ(xi) = F    ( −i √2 √π )T p.v. e− ∑T i=1 t2 i /4 ∏T i=1 ti  ‖  (x) = 1 (2π)T /2 p.v. ∫ eix·t ( −i √2 √π )T e− ∑T i=1 t 2 i /4 ∏T i=1 ti dt. Let γ(x; Σ) def = (det 2πΣ)−T /2e− 1 2 x ⊤Σ −1x be the density of N (0, Σ) for nonsingular Σ. Note that F{γ(x; Σ)}(t) = (2π) −T /2e− 1 2 t⊤Σt. We thus have E [ T∏ i=1 ϕ(xi) : x ∼ N (0, Σ) ] = * F    ( −i √2 √π )T p.v. e− ∑T i=1 t 2 i /4 ∏T i=1 ti  ‖  , γ(x; Σ) + = *( −i √2 √π )T p.v. e − ∑T i=1 t 2 i /4 ∏T i=1 ti , F{γ(x; Σ)} + = *( −i √2 √π )T p.v. e − ∑T i=1 t 2 i /4 ∏T i=1 ti , (2π) −T /2e− 1 2 t⊤Σt+ = ( −i √2 √π )T p.v. ∫ e − ∑T i=1 t 2 i /4 ∏T i=1 ti (2π)−T /2e− 1 2 t ⊤Σt dt = ( −i √2 √π )T p.v. ∫ 1 ∏T i=1 ti (2π)−T /2e− 1 2 t ⊤(Σ+ 1 2 I)t dt = *( −i √2 √π )T p.v. 1 ∏T i=1 ti , (2π) −T /2e− 1 2 t⊤(Σ+ 1 2 I)t+ = *( −i √2 √π )T p.v. 1 ∏T i=1 ti , F{γ(x; Σ + 1 2 I)} + = * F    ( −i √2 √π )T p.v. 1 ∏T i=1 ti  ‖  , γ(x; Σ + 1 2 I) + = *( −i √2 √π )T (i √π/2 )T T∏ i=1 sgn(xi), γ(x; Σ + 1 2 I) + 36 = E [ T∏ i=1 sgn(xi) : x ∼ N (0, Σ + 1 2 I) ] where we used F{p.v.t −1}(x) = i √π/2 sgn(x). Similar reasoning show that this formula also works when the mean is nonzero: E [ T∏ i=1 ϕ(xi) : x ∼ N (µ, Σ) ] = E [ T∏ i=1 sgn(xi) : x ∼ N (µ, Σ + 1 2 I) ] . A standard continuity argument yields the same formula for singular Σ. Some simple arithmetic reduces the σ case to ϕ. 2 −T E [ T∏ i=1 (1 + ϕ(xi)) : x ∼ N (µ, Σ) ] = E [ T∏ i=1 I(xi ≥ 0) : x ∼ N (µ, Σ + 1 2 I) ] . C NETSOR+ Master Theorem In this section, we state the Master Theorem for NETSOR+ . Its proof can be found in Appendix I. We ﬁrst need to extend the notion of controlled functions (Deﬁnition 5.3) to functions with parameters, and additionally require a smoothness assumption. Deﬁnition C.1. We say a parametrized function ϕ(−; −) : Rk × Rl → R is parameter-controlled at ˚Θ ∈ Rl if 1. ϕ(−; ˚Θ) is controlled, and 2. there are some controlled ¯ϕ : Rk → R and some function f : Rl → R≥0 ∪ {∞} that has f (˚Θ) = 0 and that is continuous at ˚Θ, such that, for all x1, . . . , xk ∈ R and Θ ∈ Rl, |ϕ(x1, . . . , xk; Θ) − ϕ(x1, . . . , xk; ˚Θ)| ≤ f (Θ) ¯ϕ(x 1, . . . , xk). Note that f and ¯ϕ here can depend on ˚Θ. Example C.2. Any function that is (pseudo-)Lipschitz 11 in x1, . . . , xk and Θ is parameter-controlled. An example of a discontinuous function that is parameter-controlled is ϕ(x; θ) = step(θx). Then for ˚θ ̸= 0, |ϕ(x; θ) − ϕ(x; ˚θ)| ≤ |˚θ − θ| |˚θ| , so we can set f (θ) = |˚θ−θ| |˚θ| and ¯ϕ = 1 in Deﬁnition C.1. Assumption C.3 (Rank Stability). For any W : A(n, m) and any collection S ⊆ {(h : H(m)) | ∃(g : G(n)), g := W h}, let H ∈ Rm×|S| be the matrix whose columns are h ∈ S. If 1 m H ⊤H ∈ R|S|×|S| converges almost surely to some ˚C as n, m → ∞ with convergent ratio n/m → α, then almost surely rank H = rank ˚C for all large n and m. Note that a common situation where rank stability holds is when all limit ˚C matrices are full rank. By the lower semi-continuity of rank, rank H = rank ˚C must hold asymptotically. 11A pseudo-Lipschitz function ϕ : R r → R is one that satisﬁes |ϕ(x) − ϕ(y)| ≤ C∥x − y∥(∥x∥p + ∥y∥q + 1) for some constants C, p, q ≥ 0. Roughly speaking, pseudo-Lipschitz functions are those that have polynomially bounded weak derivatives. 37 Theorem C.4. Fix any NETSOR+ program satisfying Assumption 5.1 and Assumption C.3. Suppose for each parametrized nonlinearity ϕ(−; Θ) in the program (appearing as part of Nonlin +), the pa- rameters Θ are instantiated with random variables that converge almost surely to some deterministic vector ˚Θ as n → ∞, and assume ϕ is parameter-controlled at ˚Θ. If g1, . . . , gM are all of the G-vars (including all input G-vars), then for any l, for any random vector Θ ∈ Rl that converges almost surely to a deterministic vector ˚Θ, as n → ∞, and for any ψ : RM × Rl → R parameter-controlled at ˚Θ, 1 n n∑ α=1 ψ(g1 α, . . . , gM α ; Θ) a.s. −−→ E Z∼N (µ,Σ) ψ(Z; ˚Θ), (34) where a.s. −−→ means almost sure convergence, Z ∈ RM , and µ ∈ RM and Σ ∈ RM ×M are given in Eq. (2), calculated by replacing each parametrized ϕ(−; Θ) with parameterless nonlinearity ϕ(−; ˚Θ). The proof of this theorem can be found in Appendix I. We will be instantiating the coordinates of Θ typically with “empirical moments” 1 n n∑ α=1 ψ(g1 α, . . . , gM α ) (35) for some controlled ψ, since such “moments” should converge to a deterministic value by Theorem C.4; or even, recursively, 1 n n∑ α=1 ψ(g1 α, . . . , gM α ; Θ ′) (36) for some sequence of random vectors Θ′ that converge a.s. to ˚Θ′ and for ψ parameter-controlled at ˚Θ′. One can keep recursing by replacing Θ′ with further empirical moments. However, there is a slight complication: we are using Theorem C.4 both for the convergence of the parameters in Nonlin + rules in the program, as well as for the convergence Eq. (34), in what seems like could be circular logic. It turns out not hard to straighten out this reasoning, but it requires a bit more notation and setup to state the result. We do this in the next section Appendix C.1, with main theorem Theorem C.11 that will be our primary tool concerning NETSOR+ programs in practice. To ﬁnish up this section, we make several remarks on the assumptions made in Theorem C.4. Remark C.5 (Necessity of parameter-control). Suppose ψ(x; θ) = I(θx ̸= 0). For θ ̸= 0, ψ is 1 everywhere except ψ(0; θ) = 0. For θ = 0, ψ is identically 0. Thus it’s easily seen that ψ is not parameter-controlled at θ = 0. Now, if g : G(n) is sampled like gα ∼ N (0, 1), then 1 n n∑ α=1 ψ(gα; θ) a.s. −−→ 1 if θ = 1/n so that θ → ˚θ = 0, but E Z∼N (µ,Σ) ψ(Z; ˚θ) = E 0 = 0. So our Master Theorem can’t hold in this case. Remark C.6 (Necessity of Rank Stability Assumption Assumption C.3). Suppose we have two input G-vars g1, g2 : G(n) which are sampled independently as g1 α, g2 α ∼ N (0, 1). Let W : A(n, n) be sampled as Wαβ ∼ N (0, 1/n). Then we can deﬁne h 2 := θg2 : H(n) where θ = exp(−n) as a function of n, using Nonlin +, so that h2 α a.s. −−→ 0. Additionally, let ¯g1 := W g1 : G(n) and ¯g2 := W h 2 : G(n). Again, ¯g2 α a.s. −−→ 0 but for any ﬁnite n, ¯g2 is linearly independent from ¯g1. Thus rank stability does not hold here. Now consider the (parameterless) nonlinearity ψ(x, y) that is 1 except on the line y = 0, where it is 0. Then 1 n n∑ α=1 ψ(¯g1 α, ¯g2 α) a.s. −−→ 1 38 but E Z∼N (µ,Σ) ψ(Z ¯g1, Z ¯g2) = E 0 = 0. Remark C.7 (Rank Stability Already Holds for NETSOR Programs). It turns out that, as long as we only have parameterless nonlinearities, we get rank stability Assumption C.3 for free. This is formulated explicitly in Lemma H.4. It is as a result of our proof of Theorem 5.4 that interleaves an inductive proof of this rank stability (more generally, the inductive hypothesis CoreSet) with an inductive proof of the “empirical moment” convergence (the inductive hypothesis Moments). C.1 Self-Parametrized NETSOR+ Programs and Their Master Theorem As stated below Theorem C.4, there could be potentially circular logic when allowing Nonlin + rules to take parameters depending on previously deﬁned variables in the program, such as in the form of Eq. (35). In this section, we untangle this potentially circular logic into a sound reasoning • by introducing a scalar type into NETSOR+ programs to explicitly extract the Nonlin + parameters into their own variables (Deﬁnition C.8). These scalar variables can recursively depend on previously deﬁned scalar variables, making the “recursive parameters” discussed in Eq. (36) much more succinctly and clearly expressed. • and by proving a Master Theorem for such NETSOR+ programs (Theorem C.11). This theorem will be the primary way through which we analyze NETSOR+ programs in practice. Deﬁnition C.8. 12 A self-parametrized NETSOR+ program is a NETSOR program where we have an additional scalar type, called C, which should intuitively be thought of as random variables that tend to a deterministic limit (i.e. a Constant) almost surely. Colloquially, we will call variables of type C “C-vars.” C-vars can be used as parameters of nonlinearities in Nonlin + rules, hence the “self-parametrized” in the name. For completeness, we specify a self-parametrized NETSOR+ program as follows: Input A set of input C-vars, in addition to the G- and A-vars allowed in Deﬁnition 4.1. Body New variables can be introduced and assigned via the following rules MatMul Same as in Deﬁnition 4.1. LinComb Same as in Deﬁnition 4.1. Nonlin + If x 1, . . . , xk : G(n) are G-vars with the same dimension n, θ1, . . . , θl : C are C-vars, and ϕ(−; −) : Rk × Rl → R is a parametrized function, then we may create an H-var ϕ(x1, . . . , xk; θ1, . . . , θl) : H(n) where ϕ(−; θ1, . . . , θl) acts coordinatewise. Moment If x1, . . . , xk : G(n) are G-vars with the same dimension n, θ1, . . . , θl : C are C-vars, and ϕ(−; −) : Rk × Rl → R is a parametrized function, then we may create a C-var 1 n n∑ α=1 ϕ(x1 α, . . . , xk α; θ1, . . . , θl) : C. Output Same as in Deﬁnition 4.1. The self-parametrized NETSOR+ programs we are concerned will have all of its C-vars convergent to a deterministic constant. We thus need this to be true for the input C-vars at the very least. We encapsulate this requirement below. Assumption C.9. Fix a self-parametrized NETSOR+ program satisfying Assumption 5.1. Assume each input C-var θ is sampled in a way such that θ a.s. −−→ ˚θ as n → ∞ for some deterministic scalar ˚θ ∈ R. 12We keep the deﬁnition here informal in terms of programming language convention to be accessible to the general machine learning audience. For those with PL background, see Appendix J. 39 Now, we shall deﬁne µ and Σ for self-parametrized NETSOR+ programs just as in NETSOR+ programs. The only complication here is that we also need to keep track of the limit values of the C-vars in order to do so. See Deﬁnition C.10 below. Deﬁnition C.10. Fix a NETSOR+ program with scalar variables satisfying Assumption C.9. For the purpose of this deﬁnition, write g1, . . . , gM for the entirety of the G-vars in the program, including input G-vars. New Notations For each H-var h introduced by Nonlin +, we introduce the notations ϕ h, Θh, ϑ h i , ℓ h as follows: denote the associated parametrized nonlinearity by ϕ h(−; −) : RM × Rℓ h → R (implicitly padded so that it has as many input slots as G-vars in the program) and its parameters by Θh = (ϑh 1 , . . . , ϑh ℓh ) ∈ Rℓh with length ℓ h. For each G-var gi, we also set ϕgi(x1, . . . , xM ) = xi and Θ gi = () ∈ R0 to be the empty vector (so that ℓ gi = 0). Likewise, for each C-var θ introduced by Moment, we introduce the notations ϕθ, Θθ, ϑ θ i , ℓ θ as follows: denote the associated parametrized nonlinearity by ϕ θ(−; −) : RM × Rℓθ → R (implicitly padded so that it has as many input slots as G-vars in the program) and its parameters by Θθ = (ϑθ 1, . . . , ϑθ ℓθ ) ∈ Rℓ θ with length ℓ θ. Extending the ˚( ) notation from Assumption C.9 and the Recursive Deﬁnition of µ and Σ Given µ in and Σin as in Assumption 5.1, we deﬁne µ and Σ on G-vars, along with “limit scalars” ˚θ for each C-var θ (extending ˚θ given by Assumption C.9 for input θ), as follows: For any pair of G-vars g, g′ (among g1, . . . , gM ), we deﬁne recursively µ(g) def =    µ in(g) if g is input ∑ i aiµ(yi) if g = ∑ i aiyi, (LinComb) 0 otherwise Σ(g, g′) def =  ||||| ||||| Σin(g, g′) if g, g′ are inputs ∑ i aiΣ(yi, g′) if g = ∑ i aiyi, (LinComb) ∑ i aiΣ(g, yi) if g′ = ∑ i aiyi, (LinComb) σ2 W EZ ϕh(Z; ˚Θ h)ϕ h′(Z; ˚Θh ′) if g = W h, g′ = W h ′, (MatMul) 0 otherwise (37) (this is the same as Eq. (2) except the MatMul case) and for each C-var θ introduced by Moment, ˚θ def = E Z ϕ θ(Z; ˚Θθ). (38) In all of the equations above, Z ∼ N (µ, Σ) is a random Gaussian vector with an entry for each G-var in the program, and ˚Θu denotes (˚ϑu 1 , . . . , ˚ϑu ℓu ), for H-var or C-var u. Note that since ϕ h, ϕ h ′, and ϕ θ only depend on entries of Z corresponding to G-vars previous to h, h ′, or θ, the expectations involving Z only depend on entries of µ and Σ already deﬁned, so there is no circular logic in this recursive deﬁnition of µ and Σ. Note that the notation ϕ h will be overloaded in a semantically consistent way in the context of NETSOR◦ programs; see Deﬁnition E.6. We are ﬁnally ready to formulate the Master Theorem for self-parametrized NETSOR+ programs, which basically is just Theorem C.4 but explicitly allowing parameters of the form Eq. (35) (“empirical moments”) in Nonlin +. Theorem C.11 (Self-Parameterized NETSOR+ Master Theorem). Fix any self-parametrized NETSOR+ program satisfying Assumption C.9 and Assumption C.3. For H-var or C-var u, adopt the notation ϕ u, Θu, ℓ u from Deﬁnition C.10 and also let µ, Σ, ˚θ be as computed in Deﬁnition C.10. Let g1, . . . , gM be all of the G-vars in the program (including all input G-vars). Suppose for every H-var or C-var u, ϕ u(−; Θ u) is parameter-controlled at ˚Θu. 40 1. Then for any l, for any random vector Θ ∈ Rl that converges almost surely to a deterministic vector ˚Θ, as n → ∞, and for any ψ(−; −) : RM × Rl → R parameter-controlled at ˚Θ, 1 n n∑ α=1 ψ(g1 α, . . . , gM α ; Θ) a.s. −−→ E Z∼N (µ,Σ) ψ(Z; ˚Θ), where a.s. −−→ means almost sure convergence. 2. In addition, for each C-var θ in the program, θ a.s. −−→ ˚θ. This theorem almost trivially follows from Theorem C.4, since the parameter vectors Θh intuitively should converge to deterministic limits ˚Θh. The only slight complication is that this convergence intuitvely follows from Theorem C.4 itself in what may be a circular logic, so we need to be slightly careful to unwind this logic into a valid inductive argument. We do so below, assuming Theorem C.4 (which is proved in Appendix I). Proof. Notice that the 2nd claim about θ a.s. −−→ ˚θ follows immediately from the 1st claim, so we will prove the 1st claim here. Assume that the G-vars g1, . . . , gM are in order of appearance in the program, and that g1, . . . , gm0 (with m0 ≤ M ) are all of the input G-vars. We perform simultaneous induction on two claims Moments(m) and CVarLimits(m) in m, deﬁned below Moments(m) For any l, for any random vector Θ ∈ Rl that converges almost surely to a determin- istic vector ˚Θ as n → ∞, and for any ψ(−; −) : Rm × Rl → R parameter-controlled at ˚Θ, 1 n n∑ α=1 ψ(g1 α, . . . , gm α ; Θ) a.s. −−→ E Z∼N (µ|m,Σ|m) ψ(Z; ˚Θ) where µ|m and Σ|m are the restriction of µ and Σ to g1, . . . , gm. CVarLimits(m) For each C-var θ introduced before gm, θ a.s. −−→ ˚θ as n → ∞, where ˚θ is as computed in Deﬁnition C.10. When m = M , we would have Theorem C.11 by Moments(M ). Base case: m = m0 (input G-vars only). Moments(m0) trivially follows from Theorem C.4. CVarLimits(m0) follows from Assumption C.9. Now suppose Moments(m) and CVarLimits(m) are true; we aim to show Moments(m + 1) and CVarLimits(m + 1). Inductive case: CVarLimits(m + 1) By CVarLimits(m), it sufﬁces to show θ a.s. −−→ ˚θ for all θ introduced after gm but before gm+1. We do so by another induction (an inner induction) in order of C-var appearance. The inner base case is the ﬁrst C-var θ introduced after gm. Its parameters Θ θ are among those introduced before gm, so by induction hypothesis CVarLimits(m), Θθ a.s. −−→ ˚Θθ. By the assumption of Theorem C.11 that ϕ θ is parameter-controlled at ˚Θθ, we have θ = 1 n n∑ α=1 ϕ θ(g1 α, . . . , gm α ; Θ θ) a.s. −−→ E Z∼N (µ|m,Σ|m) ϕ θ(Z; ˚Θθ) = ˚θ by induction hypothesis Moments(m) (where we have explicitly truncated the input slots of ϕθ to reﬂect its dependence only on g1, . . . , gm). The inner inductive case, for a later θ, follows the same logic, once we assume the inner inductive hypothesis that each θ′ introduced before θ has θ′ a.s. −−→ ˚θ′. 41 Inductive case: Moments(m+1) The claim is trivially true by Moments(m) if gm+1 is introduced via LinComb, so consider the case when gm+1 is introduced via MatMul gm+1 := W h where h : H(n) is an H-var with associated nonlinearity ϕ h and parameters Θh as deﬁned in Deﬁnition C.10. By the claim CVarLimits(m + 1) we proved above, Θh a.s. −−→ ˚Θh. By the assumption of Theorem C.11, ϕ h is parameter-controlled at the parameter limit ˚Θ h. Thus, the subprogram up to and including the introduction of gm+1 satisﬁes the assumptions of Theorem C.4. Consequently, Moments(m + 1) is true by Theorem C.4. This completes the simultaneous induction of Moments and CVarLimits and thus the proof of Theorem C.11. C.2 Gaussian Process Behavior of NETSOR+ Programs We can generalize the Gaussian process behavior (Corollary 5.5) to cases involving Nonlin +: Corollary C.12 (Computing the GP Kernel for NETSOR+ programs). Adopt the same assumptions and notations as in Theorem C.4. Suppose the program outputs (v⊤x1/√n, . . . , v⊤xk/ √n), where • v : G(n), vα ∼ N (0, σ2 v), is an input G-var not used elsewhere in the program and is sampled independently from all other G-vars, and • x i was introduced as xi := ϕi(g1, . . . , gM ; Θ i) for parametrized nonlinearity ϕi and parameter vector Θi that converges a.s. to a deterministic vector ˚Θi as n → ∞. Assume ϕi is parameter-controlled at ˚Θ i. Then the output vector converges in distribution to N (0, K) where Kij = σ2 v E Z∼N (µ,Σ) ϕi(Z; ˚Θi)ϕj(Z; ˚Θ j) (39) with µ, Σ computed by replacing each parametrized ϕ(−; Θ) with the parameterless ϕ(−; ˚Θ) in Eq. (2), as in Theorem C.4. The proof is a straightforward application of Theorem E.8 and Proposition G.4. Likewise, for self-parametrized programs, we have a similar result: Corollary C.13 (Computing the GP Kernel for self-parametrized NETSOR+ programs). Adopt the same assumptions and notations as in Theorem C.11. Suppose the program outputs (v⊤x1/√n, . . . , v⊤xk/ √n), where • v : G(n), vα ∼ N (0, σ2 v), is an input G-var not used elsewhere in the program and is sampled independently from all other G-vars, and • x i was introduced as x i := ϕ xi(g1, . . . , gM ; Θ xi) for self-parametrized nonlinearity ϕ xi and parameter vector Θ xi (composed of C-vars) as deﬁned in Deﬁnition C.10. Let ˚Θxi be the limit parameter as in Deﬁnition C.10. Note that ϕx i is parameter-controlled at ˚Θ xi by assumption of Theorem C.11. Then the output vector converges in distribution to N (0, K) where Kij = σ2 v E Z∼N (µ,Σ) ϕ xi(Z; ˚Θxi)ϕ xj (Z; ˚Θxj ), with µ, Σ deﬁned in Eq. (37). (40) D Example GP Kernel Computation with NETSOR+ D.1 Layernorm: Concrete Example (Program 9) Consider the example layernorm network in Program 9. This is a self-parametrized NETSOR+ program. 42 Self-parametrized NETSOR+ program 9 Layernorm Network Input: W 1x, W 1x′ : G(n) Input: W 2 : G(n) Input: v : G(n) // Mean and variance of W 1x // Moment ν1 := 1 n ∑n α=1(W 1x)α : C var1 := 1 n ∑n α=1(W 1x) 2 α − (ν1)2 : C // Nonlin + x1 := ReLU ( (W 1x)−ν11 √var1 ) : H(n) h 2 := W 2x1 : G(n) // Mean and variance of h 2 // Moment ν2 := 1 n ∑n α=1 h 2 α : C var2 := 1 n ∑n α=1(h 2 α) 2 − (ν2) 2 : C // Nonlin + x2 := ReLU ( h 2−ν21√var2 ) : H(n) // Same thing for x′ // Mean and variance of W 1x′ // Moment ν1′ := 1 n ∑n α=1(W 1x ′)α : C var1′ := 1 n ∑n α=1(W 1x′)2 α − (ν1′) 2 : C // Nonlin + x1′ := ReLU ( (W 1x′)−ν1 ′1 √var1′ ) : H(n) h 2′ := W 2x 1′ : G(n) // Mean and variance of h 2′ // Moment ν2′ := 1 n ∑n α=1 h 2 α′ : C var2′ := 1 n ∑n α=1(h 2 α′) 2 − (ν2′) 2 : C // Nonlin + x2′ := ReLU ( h 2 ′−ν2′1√var2′ ) : H(n) Output: (v⊤x2/√n, v⊤x2′/√n) Setup Suppose the inputs x, x ′ ̸= 0 are in Rm. The network has parameters W 1 ∈ Rn×m, W 2 ∈ Rn×n, and v ∈ Rn. Let us sample them as follows W 1 αβ ∼ N (0, σ2 w/m), W 2 αβ ∼ N (0, σ2 w/n), vα ∼ N (0, σ2 v), for σw, σv > 0. This corresponds to the NETSOR+ sampling data µ in = 0 and Σin given as Σin(W 1x, W 1x) = σ2 w∥x∥ 2/m, Σin(W 1x, W 1x ′) = σ2 wx⊤x′/m, Σin(W 1x′, W 1x′) = σ2 w∥x′∥2/m, Σin(v, v) = σ2 v, and Σin(g, g′) = 0 for any other pairs of input G-vars g, g′. D.1.1 Computing µ, Σ, and Limit Parameters ˚θ Let’s compute the values of µ, Σ, ˚θ in order of the appearance of the variables, according to Deﬁnition C.10. For each C-var or H-var u, we also show that ϕ u is parameter-controlled at ˚ϑu. First, one can quickly notice that µ(g) = 0 for all G-vars g, so we shall focus on computing Σ and ˚θ. C-var ν1 Here we have introduced ν1 via Moment by ν1 := 1 n n∑ α=1 ϕν1((W 1x)α), where ϕ ν1(z) = z, and there are no parameters. The function ϕ ν1 is then obviously controlled and trivially parameter- controlled. Finally, by Eq. (38), we set ˚ν1 def = E z∼N (0,σ2 w∥x∥2/m) z = 0. C-var var1 Note here var1 := 1 n n∑ α=1 ϕ var1((W 1x)α; ν1), where ϕvar1(z; θ) def = z2 − θ2. (Here superscript-2 denotes square, not an index). Since ϕ var1(−; −) is pseudo-Lipschitz in both its inputs and its parameter jointly, it is parameter-controlled at θ = ˚ν1 = 0 by Example C.2. In addition, ˚var1 is computed by Eq. (38) as ˚var1 def = E z∼N (0,σ2 w∥x∥2/m) ϕ var1(z;˚ν1) = E z∼N (0,σ2 w∥x∥2/m) z2 = σ2 w m ∥x∥ 2. 43 H-var x1 The ﬁrst H-var introduced in the program is x1 := ReLU ( (W 1x)−ν11 √ var1 ) . It can be written as a Nonlin + with x1 := ϕ x1(W 1x; ν1, var1) where ϕ x1(−; −) : R × R2 → R, and ϕx 1 (z; θ1, θ2) def = ReLU ( z − θ1√θ2 ) . Since σw > 0 and x ̸= 0, we have ˚var1 ̸= 0, and we claim that ϕ x1 is parameter-controlled at (˚ν1, ˚var1) = (0, σ2 w m ∥x∥ 2). Indeed, ϕ x1(−;˚ν1, ˚var1) is obviously controlled, so that condition 1 of Deﬁnition C.1 is satisﬁed. In addition, for any z ∈ R, \f \f \fϕ x1(z; θ1, θ2) − ϕ x1(z;˚ν1, ˚var1)\f \f \f = \f \f \f \fReLU ( z − θ1√θ2 ) − ReLU ( z − ˚ν1 √ ˚var1 )\f \f \f \f ≤ \f \f \f \f z − θ1√θ2 − z − ˚ν1 √ ˚var1 \f \f \f \f = \f \f \f \fz ( 1 √θ2 − 1 √ ˚var1 ) − ( θ1√θ2 − ˚ν1 √ ˚var1 )\f \f \f \f ≤ \f \f \f \fz ( 1 √θ2 − 1 √ ˚var1 )\f \f \f \f + \f \f \f \f θ1√θ2 − ˚ν1 √ ˚var1 \f \f \f \f ≤ √( 1 √θ2 − 1 √ ˚var1 )2 + ( θ1√θ2 − ˚ν1 √ ˚var1 )2√ z2 + 1 by Cauchy-Schwarz. Note that √( 1√θ2 − 1√ ˚var1 )2 + ( θ1√θ2 − ˚ν1 √ ˚var1 )2 equals 0 and is continuous at (θ1, θ2) = (˚ν1, ˚var1) because ˚var1 ̸= 0. Then since √z2 + 1 is controlled in z, ϕ x1 satisﬁes property 2 of Deﬁnition C.1. Altogther, we have shown that ϕ x1 is indeed parameter-controlled at (˚ν1, ˚var1). G-var h 2 By the MatMul case of Eq. (37), Σ(h 2, h 2) = σ2 w E z ϕ(z;˚ν1, ˚var1) 2, Σ(h 2, W 1x) = Σ(h 2, W 1x′) = 0, where z ∼ N (µ(W 1x), Σ(W 1x, W 1x)) = N (0, σ2 w m ∥x∥ 2), and ϕ(z; θ1, θ2) def = ϕ x1(z; θ1, θ2) = ReLU ( z − θ1√θ2 ) . We can then simplify Σ(h 2, h 2) = σ2 w E z ReLU   z √ σ2 w m ∥x∥2   2 = σ2 w E ζ∼N (0,1) ReLU(ζ)2 = 1 2 σ2 w. C-var ν2 Similar to the case of ν1, we can express ν2 via Moment by ν2 := 1 n n∑ α=1 ϕ ν2(h 2 α), where ϕν2(z) = z, and there are no parameters. The function ϕ ν2 is then obviously controlled and trivially parameter- controlled. Finally, by Eq. (38), we set ˚ν2 def = E z∼N (µ(h2),Σ(h2,h2)) z = E z∼N (0, 1 2 σ2 w) z = 0. 44 C-var var2 Similar to the case of var1, we can express var2 via Moment by var2 := 1 n n∑ α=1 ϕvar2(h 2 α; ν2), where ϕ var2(z; θ) def = z2 − θ2. (Here z2 and θ2 are the squares of z and θ). Since ϕ var2(−; −) is pseudo-Lipschitz in both its inputs and its parameter jointly, it is parameter-controlled at θ = ˚ν2 = 0 by Example C.2. In addition, ˚var2 is computed by Eq. (38) as ˚var2 def = E z∼N (µ(h2),Σ(h2,h2)) ϕ var2 (z;˚ν2) = E z∼N (0, 1 2 σ2 w) z2 = 1 2 σ2 w. H-var x2 Similar to the case of x1, we can express x2 via Nonlin + by x2 := ϕ x2 (h 2; ν2, var2) where ϕx 2 (−; −) : R × R2 → R, and ϕx 2 (z; θ1, θ2) def = ReLU ( z − θ1√θ2 ) . Since σw > 0, we also have ˚var2 > 0. Then by the same reasoning as in the case of x1, ϕ x2 is parameter-controlled at ˚Θ x2 = (˚ν2, ˚var2). C-vars ν1′, var1′ and H-var x1′ These calculations proceed similarly to those for ν1, var1 and x1. We end up with ˚ν1′ = 0, ˚var1′ = σ2 w m ∥x′∥2, and, for each u ∈ {ν1′, var1′, x 1′}, the associated nonlinearity ϕ u is parameter-controlled at limit parameter Θu. G-var h 2′ By the MatMul case of Eq. (37), Σ(h 2, h 2′) = σ2 w E z,z′ ϕ(z;˚ν1, ˚var1)ϕ(z;˚ν1′, ˚var1′), Σ(h 2′, h 2′) = σ2 w E z′ ϕ(z′;˚ν1′, ˚var1′) 2 and Σ(h 2′, g) = 0 for all other G-var g (by the “otherwise” case of Eq. (37)), where (z, z′) ∼ N (µ|W 1x,W 1x′, Σ|W 1x,W 1x′) = N (0, σ2 w m (∥x∥ 2 x⊤x ′ x⊤x ′ ∥x′∥2 )) and ϕ(z; θ1, θ2) def = ϕ x1(z; θ1, θ2) = ϕ x1′(z; θ1, θ2) = ReLU ( z − θ1√θ2 ) . We can simplify Σ|h2,h2′ = σ2 w E ˜z,˜z′ ReLU(˜z)ReLU(˜z′), (˜z, ˜z′) ∼ N ( 0, ( 1 x⊤x ′ ∥x∥∥x′∥ x ⊤x′ ∥x∥∥x′∥ 1 )) = σ2 wVReLU ( 1 x⊤x′ ∥x∥∥x′∥ x⊤x ′ ∥x∥∥x′∥ 1 ) , where VReLU is as given in Fact B.2. In particular, with c def = x⊤x′ ∥x∥∥x′∥ , this yields Σ(h 2′, h 2′) = Σ(h 2, h 2) = 1 2 σ2 w, Σ(h 2, h 2′) = σ2 w 2π (√ 1 − c2 + (π − arccos c)c). (41) 45 C-vars ν2′, var2′ and H-var x2′ These calculations proceed similarly to those for ν2, var2 and x2. We end up with ˚ν2′ = 0, ˚var2′ = 1 2 σ2 w, and, for each u ∈ {ν2′, var2′, x 2′}, the associated nonlinearity ϕ u is parameter-controlled at limit parameter Θu. D.1.2 Computing the GP Kernel It is easy to see that the set of H-vars are all linearly independent almost surely. Therefore we may apply Corollary C.13. By Corollary C.13, (v⊤x2/ √n, v⊤x2′/ √n) converges in distribution to N (0, K) where K = σ2 v E z,z′ ( ϕ(z; ˚Θ x2)2 ϕ(z; ˚Θx2)ϕ(z; ˚Θx2′) ϕ(z; ˚Θx2)ϕ(z; ˚Θx2′) ϕ(z′; ˚Θx 2 ′)2 ) where ϕ(z; θ1, θ2) def = ReLU ( z−θ1√θ2 ) and (z, z′) ∼ N (µ|h2,h2′, Σ|h2,h2′) with µ|h2,h2 ′ = 0 and Σ|h2,h2′ given in Eq. (41). Since ˚ϑx 2 1 = ˚ν2 = ˚ϑx2′ 1 = ˚ν2′ = 0 and ˚ϑx2 2 = ˚var2 = ˚ϑx2′ 2 = ˚var2′ = 1 2 σ2 w, we can simplify K = σ2 vVReLU ((σ2 w/2)−1Σ|h2,h2 ′) = 2σ2 v σ2 w VReLU (Σ|h2,h2′) . D.2 Layernorm: General Case As mentioned in Appendix A, layernorm in general can be implemented with Nonlin +. Suppose y1, . . . , yk : H(n) are H-vars deﬁned by yi := ϕi(g1, . . . , gM ; Θ i) for (possibly self- )parametrized nonlinearities ϕi(−; −) : Rm → R, i ∈ [k] and parameters Θi (possibly dependent on previous G-vars). Suppose that each Θi converges almost surely to a deterministic vector ˚Θi, and suppose each ϕi is parameter-controlled at ˚Θi. Each of yi has mean ν(yi) def = 1 n n∑ α=1 yi α = 1 n n∑ α=1 ϕi(g1 α, . . . , gM α ; Θ i) and variance σ2(yi) def = 1 n n∑ α=1 (yi α)2 − ν(yi) 2 = 1 n n∑ α=1 ϕi(g1 α, . . . , gM α ; Θ i)2 − ν(yi) 2. Under generic conditions (i.e. Assumption C.3 and parameter-control), Theorem C.4 or Theorem C.11 applies, so that ν(yi) a.s. −−→ ˚ν(yi) def = E Z ϕi (Z; ˚Θi) , and σ2(yi) a.s. −−→ ˚σ2(yi) def = E Z ϕi (Z; ˚Θi)2 − [ E Z ϕi (Z; ˚Θi)]2 where Z ∼ N (µ, Σ). Layernorm(yi) can then be expressed via a self-parametrized (Deﬁnition C.8) Nonlin + rule like so Layernorm(yi) = ψ(yi; ν(yi), σ2(yi)), where ψ(z; a, b) def = (z − a)/ √b. It’s easy to check that ψ(z; a, b) is parameter-controlled at ˚a,˚b as long as ˚b ̸= 0. Assuming rank stability (Assumption C.3) is not violated by the new variables, Theorem C.4 holds, so that, intuitively, this application of Nonlin + can be replaced with a straightforward application of Nonlin: “Layernorm(yi) = ψ(yi; ν(yi), σ2(yi))” → “Layernorm(yi) = ψ(yi;˚ν(yi),˚σ2(yi))”. Therefore, if we deﬁne the kernel matrices Ωij = lim n→∞ yi⊤yj/n ¯Ωij = lim n→∞ Layernorm(yi)⊤Layernorm(yj)/n, 46 then ¯Ωij = lim n→∞ 1 n (yi − ν(yi))⊤(yj − ν(yj)) √σ2(yi)σ2(yj) = lim n→∞ y⊤ i yj/n − ν(yi)ν(yj) √σ2(yi)σ2(yj) = lim n→∞ y⊤ i yj/n − ˚ν(yi)˚ν(yj) √ ˚σ2(yi)˚σ2(yj) ¯Ω = D−1/2(Ω − ˚ν˚ν⊤)D−1/2, where ˚ν is the column vector (˚ν(y1), . . . ,˚ν(yk))⊤ and D = Diag(Ω − ˚ν˚ν⊤). In summary, Computing Layernorm Kernel Suppose y1, . . . , yk : H(n) are H-vars deﬁned by yi := ϕi(g1, . . . , gM ; Θ i) for (possibly self-)parametrized nonlinearities ϕi(−; −) : Rm → R, i ∈ [k] and parameters Θi. Assume that each Θi converges almost surely to a deterministic vector ˚Θi, and that each ϕi is parameter-controlled at ˚Θi. If we deﬁne the kernel matrices Ωij = lim n→∞ yi⊤yj/n ¯Ωij = lim n→∞ Layernorm(yi)⊤Layernorm(yj)/n, then, assuming generic conditions (see main text above), ¯Ω = D−1/2(Ω − ˚ν˚ν⊤)D−1/2, where D = Diag(Ω − ˚ν˚ν⊤) and ˚ν is the vector given by ˚νi = E ϕi(Z; ˚Θi), Z ∼ N (µ, Σ). D.3 Transformer (Program 10) The Transformer Variant, in Mathematical Terms We’ll work with the following transformer model. Let x0 1, . . . , x0 t be a sequence of inputs (the superscript will be layer index, and the subscript will be token index). Then each layer l of our transformer works like the following kl i = U lxl−1 i ∈ Rn h l i = Layernorm(kl i + MaskedAttentioni(kl i, {kl j} t j=1, {kl j} t j=1)) xl i = Layernorm(W l2relu(W l1hl i + b l1) + b l2 + W l1h l i) (42) where U l, W l1, W l2 are weights and b l1, b l2 are the biases, and MaskedAttentionj(q, {ki} r i=1, {vi} r i=1) = r∑ i=1 a j i vi, where a j i = SoftMax(q⊤k1/n, . . . , q⊤kj/n, −∞, . . . , −∞)i (43) as described in Appendix A. Note that we make the following simpliﬁcations for ease of presentation, but all of them can be removed at the expense of more complex NETSOR programs. 1. We are forgoing positional embeddings 2. The keys, values, and queries here are the same, compared to the standard version, where they are different linear projections of xl−1 i 3. There is only 1 head, compared to the standard multi-head attention 4. The skip connection has base W l2h l i instead of just h l i 47 Self-parametrized NETSOR+ program 10 Transformer Input: U 1x0 1, . . . , U 1x0 t : G(n) Input: ∀l = 1, . . . , L : W l1, W l2 : A(n, n) Input: ∀l = 2, . . . , L : U l : A(n, n) Input: ∀l = 1, . . . , L : b l1, b l2 : G(n) Input: v : G(n) 1: for l = 1, . . . , L do 2: for i = 1, . . . , t do 3: // if l = 1, apply LinComb 4: // if l ≥ 2, apply MatMul 5: kl i := U lxl−1 i : G(n) 6: end for 7: for i = 1, . . . , t do 8: for j = 1, . . . , t do 9: // Moment 10: cij := kl i⊤kl j/n : C 11: end for 12: // With a i j being shorthand for 13: // SoftMax(ci1, . . . , cii, −∞, . . . , −∞)j 14: // Mean, post attention 15: νi := 1 n ∑n α=1(kl i + ∑t j=1 a i jkl j)α : C 16: // Variance, post attention 17: vari = 1 n ∑n α=1(kl i + ∑t j=1 a i jkl j)2 α − ν2 i : C 18: // applying Nonlin + to express attention+layernorm 19: h l i := (kl i + ∑t j=1 a i jkl j − νi1)/ √vari : H(n) 20: end for 21: for i = 1, . . . , t do 22: yl1 i := W l1h l i : G(n) 23: ˆyl1 i := yl1 i + bl1 : G(n) 24: ˆxl1 i := ReLU(ˆyl1 i ) : H(n) 25: yl2 i := W l2 ˆxl1 i : G(n) 26: ˆyl2 i := yl2 i + bl2 : G(n) 27: // Layernorm mean and variance 28: ν′ i := 1 n ∑n α=1(ˆyl2 i )α + (yl1 i )α : C 29: var′ i := 1 n ∑n α=1((ˆyl2 i )α + (yl1 i )α)2 − (ν′ i)2 : C 30: // Layernorm 31: xl i := (ˆyl2 i + yl1 i − ν′ i1)/ √var′ i : H(n) 32: end for 33: end for Output: (v⊤xL 1 / √n, . . . , v⊤xL t /√n) Setup assume for all α, β ∈ [n], • W l1 αβ, W l2 αβ ∼ N (0, σ2 w/n) for all l ≥ 1 • U l αβ ∼ N (0, σ2 u/n) for all l ≥ 2 and U 1 αβ ∼ N (0, σ2 u/m) • b l1 α , b l2 α ∼ N (0, σ2 b ) for all l. • vα ∼ N (0, σ2 v) D.3.1 Expressing the Composition of Attention, Skip Connection, and Layernorm via Nonlin + and Moment Program 10 captures the computation of this transformer on an input sequence. Let us explain how Eq. (42) is expressed in Program 10. Throughout the below, we will use the easy observation that µ(g) = 0 for all G-vars g. For any layer l, we proceed as follows. 48 Attention Weights First, cij in Line 10 represents a pre-SoftMax logit for the attention weights. They are introduced via Moment by cij := 1 n n∑ α=1 ϕ cij ((kl i)α, (kl j)α), where ϕcij (z1, z2) = z1z2. This implies ˚cij = E Z∼N (µ,Σ) Z kl iZ kl j = Σ(kl i, kl j), (44) where we used µ(g) = 0 for all G-vars g. Layernorm Mean and Variance Next, νi in Line 15 and vari in Line 17 represent the mean and variance of the post-attention embedding of the ith token. They are introduced via Moment by νi := 1 n n∑ α=1 ϕ νi((kl 1)α, . . . , (kl t)α; ci1, . . . , cii) vari := 1 n n∑ α=1 ϕ vari((kl 1)α, . . . , (kl t)α; ci1, . . . , cii, νi) where ϕ νi(z1, . . . , zt; θ1, . . . , θi) def = zi + t∑ j=1 ajzj, where (a1, . . . , at) = SoftMax(θ1, . . . , θi, −∞, . . . , −∞), (45) and similarly, ϕ vari(z1, . . . , zt; θ1, . . . , θi, ν) def = (zi + t∑ j=1 ajzj) 2 − ν2, where (a1, . . . , at) are as in Eq. (45). Note that both ϕ νi and ϕ vari are pseudo-Lipschitz in both their inputs and parameters jointly, so that they are parameter-controlled by Example C.2. Their limit parameters can be computed as ˚νi = µ(kl i) + t∑ j=1˚ajµ(kl j) = 0 where (˚a1, . . . ,˚at) = SoftMax(˚θ1, . . . , ˚θi, −∞, . . . , −∞), (46) since µ = 0 identically, and ˚vari = Σ(kl i, kl i) + 2 ∑ j ˚ajΣ(kl i, kl j) + ∑ j,j′ ˚aj˚aj′Σ(kl j, kl j′) (47) with ˚aj same as in Eq. (46). Putting Them All Together Finally, h l i in Line 19 represents the post-layernorm activations and is introduced via Nonlin + by h l i := ϕh l i(kl 1, . . . , kl t; ci1, . . . , cii, νi, vari) where ϕ h l i (z1, . . . , zt; θ1, . . . , θi, ν, var) := (zi + t∑ j=1 ajzj − ν)/√var (48) where (a1, . . . , at) are as in Eq. (45). If ˚vari > 0, then one can show that ϕ h l i is parameter-controlled at (˚ci1, . . . ,˚cii,˚νi, ˚vari) via the same reasoning as in Appendix D. When is ˚vari > 0? From Eq. (47), because the ai are all nonnegative, ˚vari = 0 implies that Σ(kl i, kl i) = 0. This is impossible if all of the input tokens xi are nonzero and the weight variances satisfy σw, σu > 0, as one can easily see. 49 D.3.2 Computing the GP Kernel By Corollary C.13, the output vector converges in distribution to N (0, K), where K ∈ Rt×t, and Kij = σ2 v E Z∼N (µ,Σ) ϕ xL i (Z; ˚ΘxL i )ϕ xL j (Z; ˚ΘxL j ). Here Θx L i = {ν′ i, var′ i} as given in Lines 28 and 29, and ϕ xL i (Z; ν, var) = (Z ˆyL2 i + Z yl1 i − ν)/√var. Simultaneously, ˚ν′ i = E Z Z ˆyL2 i + Z yL1 i , ˚var′ i = E Z(Z ˆyL2 i + Z yL1 i ) 2 − (˚ν′ i)2. Thus, to compute K, it sufﬁces to compute the restriction Σ|yL1 1 ,...,yL1 t ,ˆyL2 1 ,...,ˆyL2 t , from which K can be computed by the equations above. However, notice that by the “otherwise” case Eq. (37), Σ(h L2 i , h L1 i ) = 0 because ˆh L2 i and h L1 i are introduced by MatMul with different A-vars, and consequently Σ(ˆh L2 i , h L1 i ) = Σ(h L2 i , h L1 i ) + Σ(bl2, h L1 i ) = 0. Therefore, we only need to compute Σ|yL1 1 ,...,yL1 t and Σ|ˆyL2 1 ,...,ˆyL2 t separately. Then K is given by K = σ2 vD−1/2(Σ|yL1 1 ,...,yL1 t + Σ|ˆyL2 1 ,...,ˆyL2 t )D−1/2, (49) where D is the diagonal matrix with diagonal equal to the diagonal of Σ|yL1 1 ,...,yL1 t + Σ|ˆyL2 1 ,...,ˆyL2 t . D.3.3 Computing Σ Let Σˆyl2 def = Σ|ˆyl2 1 ,...,ˆyl2 t , Σyl1 def = Σ|yl1 1 ,...,yl1 t , Σkl def = Σ|kl 1,...,kl t resp. be the restriction of Σ to {ˆyl2 i }i, {yl1 i }i, and {ˆkl i}i. As explained above, the kernel of the Gaussian process underlying the output vector (v⊤xL 1 / √n, . . . , v⊤x L t /√n) can be computed from ΣˆyL2 . In this section, we shall describe equations tying together Σˆyl2, Σyl1 , Σkl that will allow us to compute ΣˆyL2 recursively. Computing Σyl1 from Σkl . The G-var yl1 i is introduced as yl1 i := W l1h l i. Then given Eq. (48), we have, for any i, i ′ ∈ [t], Σ(yl1 i , yl1 i′ ) = σ2 w√ ˚vari ˚vari′  Σ(kl i, kl i′) + ∑ j ˚a i jΣ(kl j, kl i′) + ∑ j′ ˚a i′ j′Σ(kl i, kl j′) + ∑ j,j′ ˚a i j˚a i′ j′Σ(kl j, kl j′)   , (50) where (˚a i 1, . . . ,˚a i t) = SoftMax(˚ci1, . . . ,˚cii, −∞, . . . , −∞) = SoftMax(Σ(kl i, kl 1), . . . , Σ(kl i, kl i), −∞, . . . , −∞) by Eq. (44), and likewise for i ′. This reduces computing Σyl1 to computing Σkl . Computing Σˆyl2 from Σyl1 . By some simple calculations in the vein of Appendix B.1.2, we can also see Σˆyl2 = σ2 wVReLU (Σyl1 + σ2 b ) + σ2 b . (51) 50 Computing Σkl+1 from Σˆyl2. Finally, following the same reasoning as in Appendix D.1, we get ˚ν′ i = 0, ˚var′ i = Σ(ˆyl2 i , ˆyl2 i ) + Σ(yl1 i , yl1 i ), ϕ xl i is parameter-controlled at ˚Θxl i as long as ˚var′ i > 0, and Σkl+1 = σ2 uD−1/2(Σˆyl2 + Σyl1)D−1/2 (52) where D = Diag(Σˆyl2 + Σyl1 ). Putting them all together, Eqs. (50) to (52) along with Eq. (49) yield the complete set of equations to compute the GP kernel of a transformer. D.3.4 Vectorized Implementation: Single Sequence Eqs. (49), (51) and (52) are already in vectorized forms. The following equation expresses Eq. (50) in a vectorized form as well: Σyl = σ2 wD−1/2(I + ∆)Σkl (I + ∆)⊤D−1/2 where • ∆ = SoftMax(Mask(Σkl )), with SoftMax applied to each row, and Mask(Σkl ) is the same as Σkl , except that its upper triangular portion (above the diagonal) is all set to −∞, and • D is the diagonal matrix with diagonal equal to the diagonal of (I + ∆)Σkl (I + ∆)⊤. Here, ∆ is the attention weights, masked so that a token’s embedding cannot depend on those of future tokens. The identity matrix I appears due to the skip connection. And the multiplication by D−1/2 is as result of layernorm. D.3.5 Vectorized Implementation: Double Sequence Program 10 only expresses the computation of a transformer on a single sequence. In general, the GP kernel will also have covariances between the embeddings of tokens of one sequence and those of tokens of another sequence. One can derive the computation of these covariances just as we did above for a single sequence. Below, we will just summarize the vectorized implementation for computing the joint kernel over multiple input sequences. One should think of Σ l below as the tensor of Σkl over every pair of sequences, and one should think of ˆΣl as the same for Σyl . 51 Computing Transformer Kernel Suppose we have p input sequences {(x1a, . . . , xta)} p a=1, each with t tokens. Suppose each sequence is processed by a transformer as in Program 10, and the transformer’s parameters are sampled with nonzero variances as follows. • W l1 αβ, W l2 αβ ∼ N (0, σ2 w/n) for all l ≥ 1 • U l αβ ∼ N (0, σ2 u/n) for all l ≥ 2 and U 1 αβ ∼ N (0, σ2 u/m) • bl1 α , b l2 α ∼ N (0, σ2 b ) for all l. • vα ∼ N (0, σ2 v) Then the transformer’s outputs, one scalar for each input token, converge in distribution to a Gaussian N (0, K) where K ∈ Rpt×pt can be computed as follows: 1. Initialize Σ0 ∈ Rt×p×t×p by Σ 0 iajb ← σ2 ux⊤ iaxjb/m for all a, b ∈ [p] and i, j ∈ [t]. 2. For l = 1, . . . , L, do (a) For a = 1, . . . , p, do i. Σl−1,a ← Σl−1 •a•a be the ath “diagonal block” ii. ∆la ← SoftMax(Mask(Σ l−1,a)), where Mask replaces the upper trian- gular portion (above the diagonal) with −∞, and SoftMax is applied row-wise. (b) ∆l ← block diagonal matrix with ∆l1, . . . , ∆lp as blocks. (c) // below, we treat each tensor as a (pt × pt) matrix. (d) ˆΣl ← (I + ∆l)Σl−1(I + ∆l) ⊤ (e) ˆΣl ← σ2 wD−1/2 ˆΣlD−1/2, where D = Diag(Σl) (f) Σl ← σ2 wVReLU( ˆΣ l + σ2 b ) + σ2 b (g) Σl ← σ2 uD−1/2(Σ l + ˆΣ l)D−1/2, where D = Diag(Σ l + ˆΣ l) 3. Return σ2 v σ2 u ΣL See our repo github.com/thegregyang/GP4A for an implementation of this algorithm. E Different Versions of Tensor Programs Deﬁnition E.1. A NETSOR− program is a NETSOR program without the LinComb rule. Remark E.2. Any NETSOR program is semantically identical to a NETSOR− program, by absorbing any usage of LinComb into a downstream nonlinearity (e.g., if g := g1 + g2, and h := ϕ(g), write h := ϕ(g1 + g2) directly as an application of Nonlin), or if there is no downstream nonlinearity, treat it as an application of Nonlin. Because LinComb allows one to express certain gadgets such as skip connection and convolutions more easily, we chose to present NETSOR as the canonical version of Tensor Program here. See Appendix J for a formal speciﬁcation of NETSOR− . By the remark above, the following NETSOR− Master Theorem is equivalent to Theorem 5.4. Theorem E.3 (NETSOR− Master Theorem). Fix any NETSOR− program satisfying Assumption 5.1 and with all nonlinearities controlled. If g1, . . . , gM are all of the G-vars (including all input G-vars), then for any controlled ψ : RM → R, as n → ∞, 1 n n∑ α=1 ψ(g1 α, . . . , gM α ) a.s. −−→ E Z∼N (µ,Σ) ψ(Z) = E Z∼N (µ,Σ) ψ(Z g1, . . . , Z gM ), where a.s. −−→ means almost sure convergence, Z = (Z g1, . . . , Z gM ) ∈ RM , and µ = {µ(gi)} M i=1 ∈ RM and Σ = {Σ(gi, gj)} M i,j=1 ∈ RM ×M are given in Eq. (2) (note that the cases involving LinComb in Eq. (2) are now vacuous in this setting with NETSOR− program). See Fig. 1 for an illustration. To prove Theorem 5.4, we will in fact prove Theorem E.3; see Appendix H. 52 Deﬁnition E.4. A NETSOR◦ program (pronounced “Net-Sor-O”) is a NETSOR program but where Nonlin rules allow nonlinearities ϕ to take H-vars. NETSOR◦ programs are thus a superset of NETSOR programs. Similarly, a NETSOR◦ + (pronounced “Net-Sor-O-Plus”) program is a NETSOR+ program but where Nonlin + rules allow nonlinearities ϕ to take H-vars. Remark E.5. Any NETSOR◦ program is semantically identical to a NETSOR program: If g := W h is any application of MatMul, we can rewrite h as a function of G-vars only by unwinding its deﬁnition recursively (e.g., if h := ϕ(h 1, g) and h 1 := ψ(g1, g2), then we can write directly h := ϕ(ψ(g1, g2), g) using a single application of Nonlin in G-vars). Likewise, any NETSOR◦+ program can be rewritten as a NETSOR+ program without losing any information. NETSOR◦ programs can be more concise than NETSOR programs by reusing H-vars more efﬁciently; see Program 11 for GRU expressed in NETSOR◦ , and compare to Program 5. However, the Master Theorem is more complicated to state, and the task of unwinding the nonlinearity just shifts from the program to the scaling limit computation stage; see Eq. (53) below. This is why we did not present NETSOR◦ as the canonical version of Tensor Programs. Deﬁnition E.6. Fix a NETSOR◦ program. For any H-var h, let ϕ h be the unwinded nonlinearity ex- pressing h as a function of only G-vars, as described in Remark E.5, i.e. h = ϕh(g1, . . . , gM ). For ex- ample, if h := ϕ(h 1, g3) and h 1 := ψ(g1, g2), then h = ϕ(ψ(g1, g2), g3) and ϕ h = ϕ(ψ(−, −), −). Similarly, in a NETSOR◦ + program, if h is an H-var, let ϕh be the unwinded nonlinearity (possibly with parameters) expressing h as a function only G-vars, h = ϕ h(g1, . . . , gM ; Θ). For example, if h := ϕ(h 1, g3; θ2) and h 1 := ψ(g1, g2; θ1), then h = ϕ(ψ(g1, g2; θ1), g3; θ2) and ϕ h(−, −, −; θ1, θ2) = ϕ(ψ(−, −; θ1), −; θ2). Note that this ϕ h notation is consistent with the semantics of the same notation deﬁned in Deﬁnition C.10, where there is nothing to unwind. The extended mean and covariance µ and Σ can still be computed as before in a NETSOR◦ program. The only difference is that we are using the unwinded nonlinearities ϕ h instead. µ(g) =    µ in(g) if g is input ∑ i aiµ(yi) if g = ∑ i aiyi, introduced by LinComb 0 otherwise , Σ(g, g′) =  ||||| ||||| Σin(g, g′) if g, g′ are inputs ∑ i aiΣ(yi, g′) if g = ∑ i aiyi, introduced by LinComb ∑ i aiΣ(g, yi) if g′ = ∑ i aiyi, introduced by LinComb σ2 W EZ ϕ h(Z)ϕ h′(Z) if g = W h, g′ = W h ′, introduced by MatMul w/ same A-var W 0 otherwise (53) where ϕ h and ϕ h ′ is as deﬁned in Deﬁnition E.6 and Z ∼ N (µ, Σ). Theorem E.7 (NETSOR◦ Master Theorem). Fix any NETSOR◦ program satisfying Assumption 5.1 and with all unwinded nonlinearities ϕh controlled, for all H-vars h. If g1, . . . , gM are all of the G-vars (including all input G-vars), then for any controlled ψ : RM → R, as n → ∞, 1 n n∑ α=1 ψ(g1 α, . . . , gM α ) a.s. −−→ E Z∼N (µ,Σ) ψ(Z) = E Z∼N (µ,Σ) ψ(Z g1, . . . , Z gM ), where a.s. −−→ means almost sure convergence, Z = (Z g1, . . . , Z gM ) ∈ RM , and µ = {µ(gi)} M i=1 ∈ RM and Σ = {Σ(gi, gj)} M i,j=1 ∈ RM ×M are given in Eq. (53). See Fig. 1 for an illustration. Theorem E.8. Fix any NETSOR◦+ program satisfying Assumption 5.1 and Assumption C.3. Suppose for each parametrized unwinded nonlinearity ϕ h(−; Θ), the parameters Θ are instantiated with random variables that converge almost surely to some deterministic vector ˚Θ as n → ∞, and assume ϕ h is parameter-controlled at ˚Θ. If g1, . . . , gM are all of the G-vars (including all input G-vars), then for any l, for any random vector Θ ∈ Rl that converges almost surely to a deterministic vector 53 NETSOR◦ program 11 GRU, with Gating Function σ and Activation Function ϕ // Embeddings of input sequence Input: Uzx1, . . . , Uzxt : G(n) Input: Urx1, . . . , Urxt : G(n) Input: Uhx 1, . . . , Uhxt : G(n) // Parameters Input: Wz, Wr, Wh : A(n, n) Input: bz, br, bh : G(n) // Initial GRU state Input: h 0 : G(n) // Readout layer Input: v : G(n) // Time step 1 h 1 z := Wzh 0 : G(n) ˜z1 := h 1 z + Uzx1 + bz : G(n) h 1 r := Wrh0 : G(n) ˜r1 := h 1 r + Urx1 + br : G(n) // σ is gating function, typically sigmoid; applying Nonlin ˆh 0 := h 0 ⊙ σ(˜r1) : H(n) h 1 h := Whˆh 0 : G(n) ˜h 1 := h 1 h + Uhx1 + bh : G(n) // Apply Nonlin // ϕ is activation function, typically tanh h 1 := (1 − σ(˜z1)) ⊙ h 0 + σ(˜z1) ⊙ ϕ(˜h 1) : H(n) // Time step 2 h 2 z := Wzh 1 : G(n) ˜z2 := h 2 z + Uzx2 + bz : G(n) h 2 r := Wrh 1 : G(n) ˜r2 := h 2 r + Urx2 + br : G(n) // No longer need to unwind h1 as in Program 5 ˆh 1 = σ(˜r1) ⊙ h 1 : H(n) h 2 h := Whˆh 1 : G(n) ˜h 2 := h 2 h + Uhx2 + bh : G(n) // No longer need to unwind h 1 as in Program 5 h 2 := (1 − σ(˜z2)) ⊙ h 1 + σ(˜z2) ⊙ ϕ(˜h 2) : H(n) // Time step 3 ... // Time step t // Deﬁne ˜zt, ˜rt, ˜h t just like above ... // No longer need to unwind ht − 1 as in Program 5 h t := (1 − σ(˜zt)) ⊙ ht−1 + σ(˜zt) ⊙ ϕ(˜h t) : H(n) Output: (v⊤h 1/ √n, . . . , v⊤h t/ √n) ˚Θ, as n → ∞, and for any ψ : RM × Rl → R parameter-controlled at ˚Θ, 1 n n∑ α=1 ψ(g1 α, . . . , gM α ; Θ) a.s. −−→ E Z∼N (µ,Σ) ψ(Z; ˚Θ), where a.s. −−→ means almost sure convergence, Z ∈ RM , and µ ∈ RM and Σ ∈ RM ×M are given in Eq. (53), calculated by replacing each parametrized unwinded nonlinearity ϕ(−; Θ) with parameter- less nonlinearity ϕ(−; ˚Θ). 54 F Programs with Variable Dimensions Notation In this section, we let dim(x) denote the dimension of an H-var x. Before this section, we have mostly assumed that all dimensions in a NETSOR (or NETSOR+ ) program are equal. This is not necessary, and was done only to more quickly present the main ideas of this work. In general, we can allow the H-vars in a program to vary in dimension, subject to the obvious dimensionality constraints imposed by the different rules: {If y := ∑k i=1 aixi or y := ϕ(x1, . . . , xk), then the dim(y) = dim(xi) for each i. If y := W x and y′ := W x ′, then dim(x) = dim(x′) and dim(y) = dim(y′). (54) Deﬁnition F.1. Given an equivalence relation ≃ on the input G-vars of a program, we extend this to an equivalence relation on all H-vars of the program by h ≡ h ′ ⇐⇒ h ≃ h ′ OR h and h ′ are constrained to have the same dimension by (54). (55) We call any such equivalence class a Common Dimension Class, or CDC. Intuitively, the dimensions of H-vars in each CDC are all the same, and this common dimension is allowed to vary between CDCs. Example F.2. In Program 1, the CDCs are {W 1x, b 1, h 1, x 1} and {b2, v, ˜h 2, h 2, x 2}. In Program 2, all G-vars are in the same CDC, and given the body of the program, this is the only way to partition the H-vars into CDCs, because the reuse of W across time step ties all H-var dimensions to be equal. Assumption F.3. Fix a NETSOR program with some equivalence relation on the input G-vars, and thus with induced CDCs over its H-vars. Assume the dimensions in each CDC are the same, but the dimensions of different CDCs can vary. Suppose for each A-var W : A(m′, m), we sample Wαβ ∼ N (σ2 W /m) for some σ2 W > 0. Suppose further for each CDC C with dimension n, for each α ∈ [n], we sample, i.i.d., {xα : x ∈ C and x is input G-var} ∼ N (µ C, ΣC) for some mean µ C and covariance ΣC over input G-vars in C. Then the following result is an easy extension of Theorem 5.4. Theorem F.4 (NETSOR Master Theorem; Variable Dimensions). Fix any NETSOR program satisfying Assumption F.3 and with all nonlinearities controlled. For any CDC C, if g1, . . . , gM are all of the G-vars (including all input G-vars) in C, then for any controlled ψ : RM → R, as all dimensions in the program tend to inﬁnity (not just the dimension of C) 13, 1 n n∑ α=1 ψ(g1 α, . . . , gM α ) a.s. −−→ E Z∼N (µC,ΣC) ψ(Z) = E Z∼N (µC,ΣC) ψ(Z g1, . . . , Z gM ), where a.s. −−→ means almost sure convergence, Z = (Z g1, . . . , Z gM ) ∈ RM , and µ C = {µ C(gi)}M i=1 ∈ RM and ΣC = {ΣC(gi, gj)} M i,j=1 ∈ RM ×M are given in Eq. (56). See Fig. 1 for an illustration. Deﬁnition F.5. For any CDC C and G-vars g, g′ in C, deﬁne recursively µ C(g) =    µ C(g) if g is input ∑ i aiµ C(yi) if g = ∑ i aiyi, introduced by LinComb 0 otherwise , ΣC(g, g′) =  ||||| ||||| ΣC(g, g′) if g, g′ are inputs ∑ i aiΣC(yi, g′) if g = ∑ i aiyi, introduced by LinComb ∑ i aiΣC(g, yi) if g′ = ∑ i aiyi, introduced by LinComb σ2 W EZ ϕ h(Z)ϕ h ′(Z) if g = W h, g′ = W h ′, introduced by MatMul w/ same A-var W 0 otherwise (56) where Z ∼ N (µ C′, ΣC′) with C′ denoting the CDC of h and h ′. Essentially the same proof of Theorem 5.4 goes through for Theorem F.4, by noting that this proof only requires the minimum of all dimensions to go to inﬁnity. 13Note that we do not require the dimensions of different CDCs to have a convergent, ﬁnite but nonzero, ratio 55 G Theoretical Tools In this section, we list a series of theoretical tools needed to prove the Master Theorems. G.1 Probability Facts Notations Given two random variables X, Y , and a σ-algebra A, the notation X d =A Y means that for any integrable function ϕ and for any random varible Z measurable on A, E ϕ(X)Z = E ϕ(Y )Z. We say that X is distributed as (or is equal in distribution to) Y conditional on A. In case A is the trivial σ-algebra, we just write X d = Y . The expression X d −→ Y (resp. X a.s. −−→ Y ) means X converges to Y in distribution (resp. almost surely). Lemma G.1. Let {Xn}n≥1 be a sequence of random variables with zero mean. If for some p ∈ N and for all n, E X 2p n ≤ cn −1−ρ, for some ρ > 0, then Xn → 0 almost surely. Proof. By Markov’s inequality, for any ϵ > 0, Pr(|Xn| > ϵ) = Pr(X 2p n > ϵ 2p) ≤ E X 2p n /ϵ 2p ≤ cn −1−ρ/ϵ2p ∑ n Pr(|Xn| > ϵ) ≤ ∑ n cn −1−ρ/ϵ 2p < ∞. By Borel-Cantelli Lemma, almost surely, |Xn| ≤ ϵ for all large n. Then, if we pick a sequence {ϵk > 0}k converging to 0, we have that, almost surely, for each k, |Xn| ≤ ϵk for large enough n — i.e. almost surely, Xn → 0. The following is a standard fact about multivariate Gaussian conditioning Proposition G.2. Suppose Rn1+n2 ∋ x ∼ N (µ, K), where we partition x = (x1, x2) ∈ Rn1 × Rn2 , µ = (µ1, µ2) ∈ Rn1 × Rn2 , and K = (K11 K12 K21 K22 ). Then x1 d =x2 N (µ|x2, K|x2) where µ|x2 = µ1 − K12K + 22(x2 − µ2) K|x2 = K11 − K12K + 22K21. Lemma G.3 (Stein’s lemma). For jointly Gaussian random variables Z1, Z2 with zero mean, and any function ϕ : R → R where E ϕ′(Z1) and E Z1ϕ(Z2) exists, we have E Z1ϕ(Z2) = Cov(Z1, Z2) E ϕ′(Z2). Proposition G.4 (Convergence of output vector to Gaussian given convergent 2nd moments). Con- sider a sequence (in t ∈ N) of collections of random vectors {xab ∈ Rna}ra b=1, a = 1, . . . , m, where na and xab can depend on t but m and ra are ﬁxed. Suppose as t → ∞, 1 na xab⊤xab ′ d −→ Σ∞ ab,ab′ for some deterministic PSD matrix Σ∞ = {Σ∞ ab,a′b′}a,b,a′,b′. If va ∼ N (0, σ2 aI) is sampled indepen- dently for each a, and independently from {xab}a,b, then {va⊤xab/ √na}a,b d −→ N (0, Σ) where the covariance Σ = {Σab,a′b′}a,b,a′,b′ has Σab,a′b′ = {σ2 aΣ∞ ab,ab′ if a = a ′ 0 else. Proof. WLOG, we assume σa = 1 for all a = 1, . . . , m. Let f : R∑ a ra → R be a bounded continuous function. We need to show that E f ({va⊤xab}a,b) → E Z∼N (0,Σ) f (Z) Note that if we deﬁne the PSD matrix ˆΣ by ˆΣab,ab′ = 1 na xab⊤xab ′ and ˆΣab,a′b′ = 0 if a ̸= a ′, then E f ({va⊤xab}a,b) = E ˆΣ E Z∼N (0, ˆΣ) f (Z) 56 where the distribution over ˆΣ is induced by the distribution over {xab}. The function ˜f ( ˆΣ) := EZ∼N (0, ˆΣ) f (Z) is bounded because f is bounded. Thus, as ˆΣ d −→ Σ∞ by assumption, we have E f ({va⊤xab}a,b) = E ˆΣ ˜f ( ˆΣ) → ˜f (Σ ∞) = E Z∼N (0,Σ∞) f (Z) as t → ∞. G.2 Review of Moore-Penrose Pseudoinverse We recall Moore-Penrose pseudoinverse and some properties of it. Deﬁnition G.5. For A ∈ Rn×m, a pseudoinverse of A is deﬁned as a matrix A+ ∈ Rm×n that satisﬁes all of the following criteria • AA+A = A • A+AA+ = A+ • (AA+) ⊤ = AA+ • (A+A) ⊤ = A+A The following facts are standard • if A has real entries, then so does A+. • The pseudoinverse always exists and is unique. • When A is invertible, A+ = A−1. • (A⊤) + = (A+)⊤, which we denote as A+⊤. • A+ = (A⊤A)+A⊤ = A⊤(AA⊤) +. • AA+ is the orthogonal projector to the column space of A; I − A+A is the orthogonal project to the null space of A. • if A has singular value decomposition A = U ΛV where U and V are orthogonal and Λ has the singular values on its diagonal, then A+ = V ⊤Λ +U ⊤ where Λ+ inverts all nonzero entries of Λ. • For any collection of vectors {vi} n i=1 in a Hilbert space, w 7→ ∑n i,j=1 vi(Σ+)ij⟨vj, w⟩, where Σij = ⟨vi, vj⟩, is the projection operator to the linear span of {vi}n i=1. G.3 Gaussian Conditioning Trick The Gaussian conditioning trick was introduced by Bolthausen [5] for solving the TAP equation in statistical physics. Later, this idea was used in Bayati and Montanari [3] to study the Approximate Message Passing algorithm in compressed sensing. We present a slightly more general versions of lemmas from Bayati and Montanari [3] that deal with singular matrices. Lemma G.6. Let z ∈ Rn be a random vector with i.i.d. N (0, σ2) entries and let D ∈ Rm×n be a linear operator. Then for any constant vector b ∈ Rn the distribution of z conditioned on Dz = b satisﬁes: z d =Dz=b D+b + Π˜z where D+ is the (Moore-Penrose) pseudoinverse, Π is the orthogonal projection onto subspace {z : Dz = 0}, and ˜z is a random vector of i.i.d. N (0, σ2). Proof. When D = [Im×m|0m×n−m], this claim is immediate. By rotational symmetry, this shows that, for any vector space V and vector v orthogonal to it, conditioning z on V + v yields a Gaussian centered on v with covariance determined by ΠV z. Then the lemma in the general case is implied by noting that {z : Dz = b} can be decomposed as {z : Dz = 0} + D+b. 57 Lemma G.7. Let A ∈ Rn×m be a matrix with random Gaussian entries, Aij ∼ N (0, σ2). Consider ﬁxed matrices Q ∈ Rm×q, Y ∈ Rn×q, P ∈ Rn×p, X ∈ Rm×p. Suppose there exists a solution in A to the equations Y = AQ and X = A⊤P . Then the distribution of A conditioned on Y = AQ and X = A⊤P is A d =Y =AQ,X=A⊤P E + Π⊥ P ˜AΠ⊥ Q where E = Y Q+ + P +⊤X ⊤ − P +⊤P ⊤Y Q+, ˜A is an iid copy of A, and Π⊥ P = I − ΠP and Π⊥ Q = I − ΠQ in which ΠP = P P + and ΠQ = QQ + are the orthogonal projection to the space spanned by the column spaces of P and Q respectively. Proof. We apply Lemma G.6 to D : A 7→ (AQ, P ⊤A). The pseudoinverse of D applied to (Y, X ⊤) can be formulated as the unique solution of argmin A { ∥A∥ 2 F : AQ = Y, P ⊤A = X ⊤} where ∥ − ∥F denotes Frobenius norm. We check that E is a 1) a solution to AQ = Y, P ⊤A = X ⊤ and 2) the minimal norm solution. We have EQ = Y Q+Q + P +⊤X ⊤Q − P +⊤P ⊤Y Q+Q. Note that Y Q+Q = Y because Y = AQ =⇒ Y Q+Q = AQQ +Q = AQ = Y . So EQ = Y + P +T (X ⊤Q − P ⊤Y ). But X ⊤Q = P ⊤AQ = P ⊤Y , so EQ = Y as desired. A similar, but easier reasoning, gives P ⊤E = X ⊤. This veriﬁes that E is a solution. To check that E is minimal norm, we show that it satisﬁes the stationarity of the Lagrangian L(A, Θ, Γ) = ∥A∥ 2 F + ⟨Θ, Y − AQ⟩ + ⟨Γ, X − A⊤P ⟩. So ∂L ∂A = 0 =⇒ 2A = ΘQ ⊤ + P Γ⊤ for some choices of Θ ∈ Rn×q and Γ ∈ Rm×p. For Θ = 2Y (Q ⊤Q) + and Γ ⊤ = 2(P ⊤P ) +[X ⊤ − P ⊤Y Q⊤], we can check that ΘQ ⊤ + P Γ⊤ = 2Y (Q ⊤Q)+Q ⊤ + 2P (P ⊤P ) +[X ⊤ − P ⊤Y Q+] = 2Y Q+ + 2P +⊤X ⊤ − 2P +⊤P ⊤Y Q+ = 2E as desired. G.4 α-Controlled Functions We generalize Deﬁnition 5.3 slightly as follows. Deﬁnition G.8 (α-controlled). For α > 0, a function ϕ : Rk → R is said to be α-controlled if for some C, c > 0, |ϕ(x)| ≤ eC ∑k i=1 |xi|α+c for all x ∈ Rk. We present a few helper lemmas to facilitate our reasoning with α-controlled functions. The next lemma is easy to show using the equivalence of norms in ﬁnite dimensional Euclidean space. Lemma G.9. Let ϕ : Rk → R. The following are equivalent 1. ϕ is α-controlled 2. For some p ≥ 1 and some g(x) = o∥x∥p→∞(∥x∥α p ), C, c > 0, |ϕ(x)| ≤ eC∥x∥α p +g(x) 3. For all p ≥ 1, there is some C, c > 0, |ϕ(x)| ≤ e C∥x∥α p +c Lemma G.10. Let C k α : R≥0 → R, c 7→ Ez∼N (0,Ik) ec∥z∥α 2 . Then 1. C k α < ∞ iff α < 2 2. for α ≥ 1, E z∼N (µ,Σ) eC∥z∥α 2 ≤ eC∥µ∥α 2 C k α(Cα∥Σ∥α/2 2 ) where ∥Σ∥2 denotes the spectral norm of Σ. 58 3. for any α-controlled ϕ : Rk → R with α ≥ 1, there is C > 0 such that for all µ ∈ Rk and k × k PSD matrix Σ, E z∼N (µ,Σ) |ϕ(z)| ≤ Ce C∥µ∥α 2 C k α(Cα∥Σ∥α/2 2 ) where ∥Σ∥2 denotes the spectral norm of Σ. Note that the RHS is a montonic function in ∥µ∥2 and ∥Σ∥2, in the sense that if ∥µ∥2 and ∥Σ∥2 don’t decrease, then the RHS will not decrease either. Proof. The ﬁrst claim is obvious and the third follows from the second easily. For the second, E z∼N (µ,Σ) eC∥z∥α 2 ≤ E z∼N (0,I) eC∥√Σz+µ∥α 2 ≤ E z∼N (0,I) eCα(∥√Σz∥α 2 +∥µ∥α 2 ) ≤ eC∥µ∥ α 2 E z∼N (0,I) eCα∥Σ∥α/2 2 ∥z∥α 2 = eC∥µ∥α 2 C k α(Cα∥Σ∥ α/2 2 ). H Proof of NETSOR Master Theorem In this section, we prove Theorem E.3, i.e. the Master Theorem for programs without LinComb. By Remark E.2, this would also show Theorem 5.4. A Bit of Notation and Terminology Note that, for each n, the randomness of our program speciﬁed by Theorem 5.4 comes from the sampling of the input variables. Let U be the product space obtained from multiplying together the corresponding probability space for each n. Each sample from this product probability space thus correspond to a sequence {S(n)}n of instantiatiations of input variables. Below, when we say “almost surely” (often abbreviated “a.s.”), we mean “almost surely over the probability of U.” We will also often make statements of the form almost surely (or, a.s.), for all large n, A(n) is true where A(n) is a claim parametrized by n. This means that for all but a U-probability-zero set of sequences {S(n)}n of input variable instantiations, A(n) is true for large enough n. Note that the order of the qualiﬁers is very important here. We induct, but on what? A natural way of going about proving Theorem 5.4 is by inducting on the number of variables in a program. It turns out this is not enough to prove our claim in its full generality (see below), and it would be more fruitful to perform a simultaneous induction on our claim (Moments) along with another statement, parametrized by m, Moments(m) For any controlled ψ : Rm → R, as n → ∞, 1 n n∑ α=1 ψ(g1 α, . . . , gm α ) a.s. −−→ E Z∼N (µ,Σ) ψ(Z). CoreSet(m) There exists a “core set” M ⊆ [m] such that, Basis(m) almost surely, for large enough n, for every i ∈ [m], there exist unique constants (not depending on n) {aj}j∈M such that gi = ∑ j∈M ajgj. Note the uniqueness implies that {gi}i∈M is linearly independent. NullAvoid(m) for every triangular array of Lesbegue measure zero sets {Anα ∈ RM}n∈N,α∈[n], almost surely for all large enough n, for all α ∈ [n], we have {gi α}i∈M ̸∈ Anα. In other words, the values {gi α}α∈M of the core set “avoid” Lebesgue measure zero sets asymptotically. Intuitively, this says that the distribution of these values are not singular. (Note the LHS depends on n although we are suppressing it notationally) 59 Let us explain in brief why we need to consider CoreSet satisfying Basis and NullAvoid. • Basis reduces the consideration of Moments to only the core set G-vars, since every other G-var is asymptotically a linear combination of them. • When we apply the Gaussian conditioning technique Proposition G.2, we need to reason about the pseudo-inverse Λ + of some submatrix Λ of a covariance matrix. Each entry of Λ is of the form 1 n ∑n α=1 ϕi(g1 α, . . . , gm−1 α )ϕj(g1 α, . . . , gm−1 α ) for a collection of controlled scalar functions {ϕi}i. This Λ will be a random variable which converges a.s. to a deter- minstic limit ˚Λ as n → ∞. It should be generically true that Λ+ a.s. −−→ ˚Λ + as well, which is essential to make the Gaussian conditioning argument go through. But in general, this is guaranteed only if Λ’s rank doesn’t drop suddenly in the n → ∞ limit. We thus need to guard against the possibility that g1, . . . , gm, in the limit, suddenly concentrate on a small set on which {ϕi(g1, . . . , gm)}i are linearly dependent. This is where NullAvoid comes in. It tells us that g1, . . . , gm will avoid any such small set asymptotically, so that indeed the rank of Λ will not drop in the limit. Proof organization We will show that Moments and CoreSet are true for input variables, as the base case, and Moments(m − 1) and CoreSet(m − 1) =⇒ Moments(m) and CoreSet(m) as the inductive step. By induction, we obtain Moments(M ), which is Theorem 5.4. The base cases are easy and we will dispatch with them immediately after this in Appendix H.1, but the inductive step is much more complicated, and we will need to set up notation in Appendix H.2. During this setup, we prove some basic limit theorems using the induction hypothesis. However, the full generality of these claims requires some consequences of CoreSet, which we call “rank stability” and “zero stability” (related to Assumption C.3). These notions are introduced and proved in Appendix H.3. We would then ﬁnally be able to handle the inductive steps at this point. We ﬁrst prove Moments(m − 1) and CoreSet(m − 1) =⇒ CoreSet(m) in Appendix H.4 because it is easier. Then we prove Moments(m − 1) and CoreSet(m − 1) =⇒ Moments(m) in Appendix H.5. H.1 Base Cases: Moments and CoreSet for Input Variables Base case: Moments(input vars) Suppose the input variables are x1, . . . , xk : G(n) (so that µ in ∈ Rk, Σin ∈ Rk×k). We need to show that for any controlled function ψ : Rk → R, 1 n n∑ α=1 ψ(x1 α, . . . , xk α) a.s. −−→ E Z∼N (µ,Σ) ψ(Z), where ψ on the RHS ignores all coordinates corresponding to non-input G-vars. Since µ and Σ restricted to input variables are just µ in and Σin (see Eq. (2)), the RHS expectation is just E Z∼N (µ,Σ) ψ(Z) = E Zin∼N (µin,Σin) ψ(Z in) and the almost sure convergence we desire is just a result of the law of large numbers. Base Case: CoreSet(input vars) Let x1, . . . , xk be the input G-vars as above. Pick the core set M to be any subset of [k] such that rank Σ in|M = rank Σ in. Then it’s straightforward to verify Basis and NullAvoid. 60 H.2 Inductive Case: Setup We now assume Moments(m − 1) and CoreSet(m − 1) and want to reason about gm to show Moments(m) and CoreSet(m). Suppose gm := Ah where A : A(n, n) and h : H(n) was introduced by h := ϕ(g1, . . . , gm−1) (WLOG padding coordinates if necessary; if h = gi is a G-var, then pretend ϕ just projects to the ith coordinate). For brevity, we will just write g = gm. Consider all previous instances where A is used: ˆgi := Aˆh i, i = 1, . . . , r. Deﬁne ˆG def = [ˆg1| . . . |ˆgr] ∈ Rn×r, ˆH def = [ˆh 1| . . . |ˆh r]. (57) We will also use ˆG to denote the set of G-vars {ˆg1, . . . , ˆgr} when we later write expressions like Σ( ˆG, ˆG). Let B be the σ-algebra spanned by all previous G-vars g1, . . . , gm−1 (and hence also all previous H-vars). Conditioning on B, A is constrained by ˆG = A ˆH, and we have by Lemma G.7, g d =B ( ˆG ˆH + + ˜AΠ⊥ ˆH )h where ˜A is an independent copy of A and Π ˆH = ˆH ˆH + = ˆH( ˆH ⊤ ˆH) + ˆH ⊤ is the projection to the column space of ˆH. If we deﬁne ω def = ˆG ˆH +h, σ def = σA√ ∥Π⊥ ˆH h∥2/n (58) then g d =B ω + σy, with y ∼ N (0, In) (59) For brevity, we will deﬁne the following matrices and vectors of ﬁxed dimension ˆΛ def = ˆH ⊤ ˆH/n ∈ Rr×r, ˆη def = ˆH ⊤h/n ∈ Rr. (60) Suppose ˆh i was introduced by ˆh i := ˆϕi(g1, . . . , gM ), where ˆϕi depends at most on g1, . . . , gm−1. By induction hypothesis Moments(m − 1), ˆΛ and ˆη all converge a.s. to corresponding limit values ˚ˆΛ and ˚ˆη, since their entries are moments of Z 1, . . . , Z m−1: ˆΛij a.s. −−→ ˚ˆΛij def = E ˆϕi(Z) ˆϕj(Z) = (σA) −2Σ(ˆgi, ˆgj) ˆηi a.s. −−→ ˚ˆηi def = E ˆϕi(Z)ϕ(Z) = (σA)−2Σ(ˆgi, g). It turns out that, as a consequence of Lemma H.4 below, a.s. for all large enough n, rank ˆΛ = rank ˚ˆΛ. Therefore, as pseudoinverse is continuous on matrices of ﬁxed rank, we get the following proposition Proposition H.1. ˆΛ + a.s. −−→ ˚ˆΛ +. Using this proposition, we compute the limits of the conditional mean ω and variance σ2. Lemma H.2. σ2 a.s. −−→ ˚σ2 def = Σ(g, g) − Σ(g, ˆG)Σ( ˆG, ˆG) +Σ( ˆG, g) Proof. Note that σ2 = σ2 A n (h ⊤h − h ⊤Π ˆH h) = σ2 A n (h ⊤h − h⊤ ˆH( ˆH ⊤ ˆH) + ˆH ⊤h) = σ2 A n (h ⊤h − ˆη⊤ ˆΛ+ ˆη). Because ϕ is polynomially-bounded, so is ϕ(z) 2 as well. By induction hypothesis, 1 n h ⊤h = 1 n n∑ α=1 ϕ(g1 α, . . . , gm−1 α ) 2 a.s. −−→ E Z∼N (µ,Σ) ϕ(Z)2 = σ−2 A Σ(g, g). Likewise, ˆη a.s. −−→ ˚ˆη and ˆΛ a.s. −−→ ˚ˆΛ. By Proposition H.1, ˆΛ+ a.s. −−→ ˚ˆΛ +. Combining all of these limits together yields the desired claim. 61 Lemma H.3. Let v def = ˆΛ+ ˆη, so that v a.s. −−→ ˚v def = ˚ˆΛ +˚ˆη. Then for some vector ˆε ∈ Rr that go to 0 a.s. with n, ω = Eh = ˆG(˚v + ˆε) Proof. Using Eq. (60), we can re-express ω as ω = ˆGˆΛ+ ˆη. By Proposition H.1, ˆΛ+ a.s. −−→ ˚ˆΛ+, so that setting ˆε def = v − ˚v, we get ˆε a.s. −−→ 0. Thus, ω = ˆG(˚v + ˆε) as desired. H.3 Rank Stability and Zero Stability In this section, we prove the following consequence of CoreSet(m − 1) and Moments(m − 1). Lemma H.4 (Rank Stability). For any collection of controlled functions {ψj : Rm−1 → R} l j=1, let K ∈ Rl×l be the random matrix (depending on n) deﬁned by Kij = 1 n n∑ α=1 ψi(g1 α, . . . , gm−1 α )ψj(g1 α, . . . , gm−1 α ). By Moments(m − 1), K a.s. −−→ ˚K for some matrix ˚K ∈ Rl×l. 1. Then, almost surely, for large enough n, ker K = ker ˚K, im K = im ˚K, and rank K = rank ˚K. Here ker denotes null space and im denotes image space. 2. Suppose I ⊆ [l] is any subset such that ˚K|I , the restriction of ˚K to rows and columns corresponding to I, satisﬁes |I| = rank ˚K|I = rank ˚K. There are unique coefﬁcients {Fij}i∈[l],j∈I that expresses each row of ˚K as linear combina- tions of rows corresponding to I: ∀i ∈ [l], ˚Ki = ∑ j∈I Fij ˚Kj. Then, a.s. for all large n, for all α ∈ [n], ψi(g1 α, . . . , gm−1 α ) = ∑ j∈I Fijψj(g1 α, . . . , gm−1 α ). This will be primarily a corollary of the following Lemma H.5. Lemma H.5 (Zero Stability). If ψ : Rm−1 → R≥0 is a nonnegative function such that 1 n n∑ α=1 ψ(g1 α, . . . , gm−1 α ) a.s. −−→ 0 then, almost surely, for large enough n, ψ(g1 α, . . . , gm−1 α ) = 0 for all α ∈ [n]. We give the proof of Lemma H.4 now, assuming Lemma H.5. Proof. Let v ∈ Rl be in the null space of ˚K, i.e. v⊤ ˚Kv = 0. Then we also have v⊤Kv a.s. −−→ v⊤ ˚Kv = 0. But v⊤Kv = 1 n n∑ α=1 Ψ(g1 α, . . . , gm−1 α ), where Ψ(g1 α, . . . , gm−1 α ) def = (∑ i=1 viψi(g1 α, . . . , gm−1 α ) )2 62 and Ψ is a nonnegative function. By Lemma H.5, we have that: almost surely, for large enough n, Ψ(g1 α, . . . , gm−1 α ) = 0 for all α ∈ [n] =⇒ v⊤Kv = 0 Claim 1. If we apply this argument to a basis {v1, . . . , vt} of ker ˚K, then we get, a.s. for all large n, ker ˚K ⊆ ker K, so that a.s. for all large n, rank ˚K ≥ rank K. Because the rank function is lower semicontinuous (i.e. the rank can drop suddenly, but cannot increase suddenly), and K a.s. −−→ ˚K, we also have a.s. for all large n, rank ˚K ≤ rank K. Combined with the above, this gives the desired result on rank. The equality of null space then follows from the equality of rank, and the equality of image space follows immediately, as the image space is the orthogonal complement of the null space. Claim 2. If we apply the above argument to each vi deﬁned by inner product as ∀x ∈ Rl, x ⊤vi def = xi − ∑ j∈I Fijxj, (note that only for i ̸∈ I is vi nonzero), then we have, a.s. for large n, vi⊤Kvi = 0, or ψi(g1 α, . . . , gm−1 α ) = ∑ j∈I Fijψj(g1 α, . . . , gm−1 α ). In the rest of this section, we prove Lemma H.5. It helps to ﬁrst show that the linear relations given in Basis carries over to the n → ∞ limit. Proposition H.6. Let Σ|M be the submatrix of Σ with rows and columns corresponding to {gi : i ∈ M}. Then rank Σ = rank Σ|M = |M|. Furthermore, if Z = (Z 1, . . . , Z m−1) ∼ N (µ|m−1, Σ|m−1), where µ|m−1, Σ|m−1 are the restrictions of µ, Σ to g1, . . . , gm−1, then Z i d = ∑ j∈M ajZ j where {aj}j∈M are the coefﬁcients corresponding to gi given in Basis. Proof. By Basis property, each gi, i ∈ M, has a set of unique constants {aj}j∈M (independent of n) such that, almost surely, for large enough n, gi = ∑ j∈M ajgj. Let ψ(x1, . . . , xm−1) def = (xi − ∑ j∈M ajxj) 2. Then by Basis(m − 1) and Moments(m − 1), 1 n n∑ α=1 ψ(g1 α, . . . , gm−1 α ) a.s. −−→ E Z∼N (µ|m−1,Σ|m−1) ψ(Z) = 0. where µ|m−1, Σ|m−1 are the restrictions of µ, Σ to g1, . . . , gm−1. This implies that for Z = (Z 1, . . . , Z m−1) ∼ N (µ|m−1, Σ|m−1), Z i d = ∑ j∈M ajZ j. Repeating this argument for all i ∈ [m − 1] implies that {Z j}j∈M is a “spanning set” of Z 1, . . . , Z m−1. Furthermore, by the uniqueness of the coefﬁcients, we also have that {Z j}j∈M is linearly independent as well. This then implies the rank consequence we want. 63 Now we show Lemma H.5. Proof of Lemma H.5. By Moments(m − 1), 1 n n∑ α=1 ψ(g1 α, . . . , gm−1 α ) → E Z∼N (µ|m−1,Σ|m−1) ψ(Z). By Proposition H.6, if Z ∼ N (µ|m−1, Σ|m−1) and Z|M is the part of Z corresponding to M, then Z|M has density. The law of Z|M (namely N (µ|M, Σ|M), where µ|M, Σ|M are the restriction of µ and Σ to M) is absolutely continuous against the Lebesgue measure of RM and vice versa, so that a set of Lebesgue measure zero is measure zero under N (µ|M, Σ|M), and vice versa; and Z|M is basis of Z. Basis yields a linear function λ such that λ({gj α}j∈M) = {gi α} m−1 i=1 for all α, almost surely asymptotically, and λ(Z|M) d = Z, so that E Z∼N (µ|m−1,Σ|m−1) ψ(Z) = E Z′∼N (µ|M,Σ|M) ψ ◦ λ(Z ′). This expectation is 0 by our premise. Because ψ, and thus ψ ◦ λ, is a nonnegative function, the nullity of the expectation implies that, other than a set U of N (µ|M, Σ|M)-measure zero, ψ ◦ λ is 0. This set U also has Lebesgue measure zero as Z|M has density, by our reasoning above. If in NullAvoid, we set Anα = U for all n and all α ∈ [n], then we get that: almost surely, for all large enough n, for all α ∈ [n], {gi α}i∈M ̸∈ U ⇐⇒ ψ ◦ λ({gi α}i∈M) = 0 ⇐⇒ ψ(g1 α, . . . , gm−1 α ) = 0, as desired. H.4 Inductive Step: CoreSet(m) In this section, we show Moments(m − 1) and CoreSet(m − 1) =⇒ CoreSet(m). More explicitly, we need to think about whether to add m to the core set M of [m − 1] in order to maintain the Basis and NullAvoid properties. We proceed by casework on whether ˚σ = 0. H.4.1 If ˚σ = 0 We will show that the core set properties are maintained if we don’t add m to the core set. Consider the space L def = L 2(N (µ|M, Σ|M)) of square-integrable real functions against the measure N (µ|M, Σ|M) deﬁned on RM. Let ⟨ϕ, ψ⟩ = EY ∼N (µ|M,Σ|M) ϕ(Y )ψ(Y ) be the inner product of this space. Just like in a ﬁnite-dimensional inner product space, given a ﬁnite collection of functions S = {ψi} k i=1, the orthogonal projection operator ΠS to the span of S (inside L) is given by ΠSϕ = k∑ i=1 aiψi, for any ϕ ∈ L, where a = Λ +b ∈ Rk, bj = ⟨ψj, ϕ⟩, b ∈ Rk, Λij = ⟨ψi, ψj⟩, Λ ∈ Rk×k. 64 Recall that g = Ah where h was introduced by h := ϕ(g1, . . . , gm−1), for some controlled ϕ, and likewise ˆgi = Aˆh i where ˆh i = ˆϕi(g1, . . . , gm−1), for each i ∈ [r]. By Basis, we know that, a.s. for large enough n, each of g1, . . . , gm−1 is a (unique, constant-in-n) linear combination of {gj}j∈M. Therefore, we can express h = ϕ({gj}j∈M), and ∀i ∈ [r], ˆh i = ˆϕi({gj}j∈M) for some functions ϕ, ˆϕi ∈ L. For convenience, set S def = { ˆϕi}i. One can see then, as in the proof of Lemma H.2, ˚σ2 = σ2 A(E ϕ(Z) 2 − ˚ˆη⊤˚ˆΛ+˚ˆη) = σ2 A(⟨ϕ, ϕ⟩ − ⟨ϕ, ΠSϕ⟩) by expanding the deﬁnition of ˚ˆη and ˚ˆΛ. Therefore, ˚σ = 0 implies that ⟨ϕ, ϕ⟩ = ⟨ϕ, ΠSϕ⟩ so that: after changing its values on a set U of measure zero under N (µ|M, Σ|M) (and thus also under Lebesgue measure by Lemma H.4), ϕ is a linear combination of { ˆϕi}r i=1, i.e. ∀x ̸∈ U, ϕ(x) = ∑ i∈[r] ci ˆϕi(x) for some coefﬁcients {ci}i∈[r]. By NullAvoid applied to Anα = U for all n and α ∈ [n], we also have that: a.s. for large enough n, ϕ(g1, . . . , gα) = ϕ({gj}j∈M) = ∑ i∈[r] ci ˆϕi({gj}j∈M) = ∑ i∈[r] ci ˆϕi(g1, . . . , gα), and therefore, under the same condition, (recall A is the matrix giving rise to g in g := Ah) g = Aϕ(g1, . . . , gα) = ∑ i∈[r] ciA ˆϕi(g1, . . . , gα) = ∑ i∈[r] ciˆgi. This shows that, if we keep the core set as M, then Basis is still satisﬁed. Since the core set is not changing, NullAvoid just follows from the induction hypothesis. For usage later in the proof of Moments(m), we record our observation here as follows Lemma H.7. If ˚σ = 0, then there are coefﬁcients {ci} r i=1 such that a.s. for large enough n, g = ∑ i∈[r] ciˆgi. H.4.2 If ˚σ > 0 It’s clear that g cannot be in the linear span of {ˆgi}i∈[r] asymptotically, so we will add g to the core set, and the Basis property follows immediately. In the below, we shall write M for the old core set, and M ′ def = M ∪ {g} for the new one. It remains to show NullAvoid for M′. Because the conditional variance of gm α given g1, . . . , gm−1 is σ2, and because ˚σ > 0, this assumption implies that, a.s. for all large enough n, gm α |g1, . . . , gm−1 has density for all α ∈ [n]. (61) By “has density” here, we in particular mean that any Lesbegue measure zero set in R has zero probability under the conditional distribution of gm α given g1, . . . , gm−1. Now, to prove NullAvoid holds for M ′: Let {Anα ⊆ RM′}n∈N,α∈[n] be a triangular array of Lesbegue measure zero sets. For each Anα, deﬁne Bnα def = {x ∈ RM : λ(Anα|x) ̸= 0}, where Anα|x = {y ∈ R : (x, y) ∈ Anα ⊆ RM × R} is the “slice” of Anα at x, and λ is the 1-dimensional Lebesgue measure. Because each Anα has measure zero in RM ′, necessarily each Bnα also has measure zero in RM. Applying NullAvoid to the triangular array {Bnα ⊆ RM}n∈N,α∈[n], we get that: a.s. for large enough n, ∀α ∈ [n], {gi α}i∈M ̸∈ Bnα. Therefore, by Eq. (61), a.s. for large enough n, ∀α ∈ [n], {gi α}i∈M′ ̸∈ Anα. This ﬁnishes the proof of NullAvoid for M ′, and also CoreSet(m). 65 Lemma H.8. Assume Moments(m − 1). Suppose ψ : Rm−1 → R is controlled. Then as n → ∞, 1 np max α∈[n] |ψ(g1 α, . . . , gm−1 α )| a.s. −−→ 0 for any p > 0. Proof. For any q > 0, we have the elementary bound max α∈[n] |ψ(g1 α, . . . , gm−1 α )| ≤ q √ ∑ α∈[n] |ψ(g1 α, . . . , gm−1 α )|q. Thus, for any q > 0, 1 np max α∈[n] |ψ(g1 α, . . . , gm−1 α )| ≤ 1 np−1/q q v u u √ 1 n ∑ α∈[n] |ψ(g1 α, . . . , gm−1 α )|q. Because, by Moments(m − 1), 1 n ∑ α∈[n] |ψ(g1 α, . . . , gm−1 α )|q a.s. −−→ C for some constant C as n → ∞, the RHS above converges a.s. to 0 as soon as we take q > 1/p, and therefore so does the LHS. H.5 Inductive Step: Moments(m) In this section, we show Moments(m − 1) and CoreSet(m − 1) =⇒ Moments(m). More speciﬁcally, we will show that for any controlled ψ : Rm → R, 1 n n∑ α=1 ψ(g1 α, . . . , gm α ) a.s. −−→ E Z∼N (µ,Σ) ψ(Z) where again on the RHS ψ ignores all coordinates Z m+1, . . . , Z M (corresponding to gm+1, . . . , gM ). By Lemma H.7, if ˚σ = 0, then almost surely, for large enough n, g = gm is just a (ﬁxed) linear combination of g1, . . . , gm−1, so Moments is trivially true. Therefore, in the below, we assume ˚σ > 0. (⋆) This assumption will be crucial for our arguments involving smoothness induced by Gaussian averaging. To clarify notation in the following, we will write EX [expression] to denote the expectation over only the randomization in X, and E [ expression| B] to denote the expectation taken over all randomness except those in B. Proof Plan Note that\f \f \f \f \f 1 n n∑ α=1 ψ(g1 α, . . . , gm α ) − E Z∼N (µ,Σ) ψ(Z) \f \f \f \f \f ≤ A + B + C (62) where A def = \f \f \f \f \f 1 n n∑ α=1 ψ(g1 α, . . . , gm α ) − E z ψ (g1 α, . . . , gm−1 α , ωα + σz)\f \f \f \f \f B def = \f \f \f \f \f 1 n n∑ α=1 E z ψ (g1 α, . . . , gm−1 α , ωα + σz) − E z ψ ( g1 α, . . . , gm−1 α , r∑ i=1 ˚viˆgi α + ˚σz )\f \f \f \f \f C def = \f \f \f \f \f 1 n n∑ α=1 E z ψ ( g1 α, . . . , gm−1 α , r∑ i=1 ˚viˆgi α + ˚σz ) − E Z∼N (µ,Σ) ψ(Z) \f \f \f \f \f 66 with z ∼ N (0, 1). Note that B and C are random variables in B. We will show that each of A, B, C goes to 0 almost surely, which would ﬁnish the proof of Theorem 5.4. Roughly speaking, A a.s. −−→ 0 because of a law of large number, B a.s. −−→ 0 because of the smoothness in Ez ψ induced by Gaussian averaging, and C a.s. −−→ 0 by induction hypothesis. We start with the last item, since it’s the easiest. H.5.1 C Converges Almost Surely to 0 In this section we show that C a.s. −−→ 0 by a straightforward reduction to the inductive hypothesis. Let ˆZ 1, . . . , ˆZ r be the components of Z ∼ N (µ, Σ) corresponding to ˆg1, . . . , ˆgr, and let ˆZ be the col- umn vector with these entries. Note that, by Proposition G.2, Z m (corresponding to gm), conditioned on Z 1, . . . , Z m−1, is distributed as a Gaussian with mean Σ(g, ˆG)Σ( ˆG, ˆG) + ˆZ = ˚ˆη⊤˚ˆΛ + ˆZ = ˚v⊤ ˆZ and variance Σ(g, g) − Σ(g, ˆG)Σ( ˆG, ˆG) +Σ( ˆG, g) = ˚σ. Thus E Z ψ(Z) = E Z1,...,Zm−1 E[ψ(Z)|Z 1, . . . , Z m−1] = E Z1,...,Zm−1 E z∼N (0,1) ψ(Z 1, . . . , Z m−1,˚v⊤ ˆZ + ˚σz) = E Z1,...,Zm−1 Ψ(Z 1, . . . , Z m−1) where we have set Ψ(Z 1, . . . , Z m−1) def = Ez∼N (0,1) ψ(Z 1, . . . , Z m−1,˚v⊤ ˆZ +˚σz). Ψ is a controlled function since ψ is. Applying the induction hypothesis to Ψ, we obtain 1 n n∑ α=1 E z ψ ( g1 α, . . . , gm−1 α , r∑ i=1 ˚viˆgi α + ˚σz ) = 1 n n∑ α=1 Ψ (g1 α, . . . , gm−1 α ) a.s. −−→ E Z1,...,Zm−1 Ψ(Z 1, . . . , Z m−1) by induction hypothesis = E Z1,...,Zm−1 E z∼N (0,1) ψ(Z 1, . . . , Z m−1,˚v⊤ ˆZ + ˚σz) = E Z ψ(Z) as desired. H.5.2 A Converges Almost Surely to 0 In this section we show A a.s. −−→ 0 by a bounding moments of A and then ﬁnishing with Lemma G.1. For each α ∈ [n], let ψα(x) def = ψ(g1 α, . . . , gm−1 α , ωα + σx), with ω and σ deﬁned in Eq. (58). This is a random function depending on the randomness of g1 α, . . . , gm−1 α , and it changes with n as well. Note by Eq. (59), A d =B 1 n n∑ α=1 ψα(ξα) − E ξ′ ψα(ξ′ α) where ξ, ξ′ ∼ N (0, I). Now the 2k-moment of A for any integer k ≥ 1 satisﬁes E[A 2k | B] = 1 n2k E [ n∑ α=1 ( ψα(ξα) − E ξ′ ψα(ξ′ α) )2k + · · · \f \f \f \f B ] where the · · · include only terms that involve only powers of ψα(ξα) − Eξ′ ψα(ξ′ α) greater than 1 for each α. Indeed, other terms are killed by the conditional mean, since each ψα(ξα) − Eξ′ ψα(ξ′ α) has 67 zero (conditional) mean and is independent from others when conditioned on B. We can push the conditional mean operator inside each product by conditional independence. Then, applying power mean inequality and AM-GM to bound each mixed moment with linear combinations of the 2kth powers, we get E[A 2k | B] ≤ Dn 2k−1 n2k · 1 n n∑ α=1 E [(ψα(ξα) − E ξ′ ψα(ξ′ α) )2k \f \f \f \f B ] def = D n U (63) where D is some absolute constant. Thus, to show A a.s. −−→ 0, it sufﬁces to bound U and then apply Lemma G.1. This is equivalent to bounding the uncentered moments Ez∼N (0,1) |ψα(x)|q for q = 2k. Suppose ψ is λ-controlled and satisﬁes |ψ(x)| ≤ eC ∑ i |xi| λ+c for some C, c > 0 and λ < 2. (64) We have E z∼N (0,1) |ψα(z)| q ≤ E z [ eCq(|ωα+σz| λ+ ∑m−1 i=1 |gi α| λ)+cq] ≤ E z [ eCq2 λ(|ωα| λ+|σz| λ+ ∑m−1 i=1 |gi α|λ)+cq] = eCq2 λ(|ωα| λ+ ∑m−1 i=1 |gi α|λ)+cq E z [ eCq2λσλ|z| λ] = eCq2 λ(|ωα| λ+ ∑m−1 i=1 |gi α|λ)+cqR where R = C 1 λ(Cq2λσλ) > 0 is deterministic and C k λ is as deﬁned in Lemma G.10. Now, |ωα| λ = \f \f \f \f \f r∑ i=1 viˆgi α \f \f \f \f \f λ ≤ rλ r∑ i=1 |vi| λ|ˆgi α|λ. Additionally, almost surely, |vi| < |˚vi| + 1, for all i ∈ [r] simultaneously, for large enough n because vi a.s. −−→ ˚vi. Let L = Cq2λrλ max r i=1(|˚vi| + 1) and L ′ = cq, where C, c are as in Eq. (64). Then, almost surely, for large enough n, for all z1, . . . , zm−1 ∈ R, e Cq2 λ(| ∑r i=1 vizi|λ+ ∑m−1 i=1 |zi| λ)+cq ≤ eL′+L ∑m−1 i=1 |zi| λ def = ˆψ(z1, . . . , zm−1). Obviously ˆψ is λ-controlled. Then, again a.s. for large enough n, simultaneously for all α, E z∼N (0,1) |ψα(z)| q ≤ R ˆψ(g1 α, . . . , gm−1 α ), so that 1 n n∑ α=1 E z∼N (0,1) |ψα(z)| q ≤ R 1 n n∑ α=1 ˆψ(g1 α, . . . , gm−1 α ) a.s. −−→ R E Z ˆψ(Z) as n → ∞, by induction hypothesis, where Z ∼ N (µ, Σ). Consequently, almost surely, the U in Eq. (63) (as a function of g1, . . . , gm−1) is uniformly bounded in n. Applying Lemma G.1 for large enough q yields the result. H.5.3 B Converges Almost Surely to 0 In this section we show B a.s. −−→ 0. The main insight here is integrating a function against Gaussian induces smoothness in the function. We will assume that ˚σ > 0, so that σ > 0 almost surely for large enough n. This is because ˚σ = 0 implies that gm is in the linear span of {g1, . . . , gm−1} almost surely by Lemma H.4, and Moments(m) then holds trivially. For each α ∈ [n], w ∈ R, τ ≥ 0, let Ψα(w; τ 2) def = E z∼N (0,1) ψ (g1 α, . . . , gm−1 α , w + τ z) . (Here and in all that follows, τ 2 is the square of τ , and the 2 is not an index). This is a random function, with randomness induced by g1, . . . , gm−1. By Lemma G.3, Ψα is differentiable in w, and ∂wΨα(w; τ 2) = τ −1 E z∼N (0,1) zψ(g1 α, . . . , gm−1 α , w + τ z). We can obtain the following smoothness condition on Ψα. 68 Lemma H.9. For any w, τ, ϵ ∈ R with ϵ, τ > 0, |Ψα(w; τ 2) − Ψα(w + ϵ; τ 2)| ≤ |ϵ|τ −1R(τ ) ˆΨ(g1 α, . . . , gm−1 α )eC4 λ(|w| λ+|ϵ| λ), where ˆΨ(g1 α, . . . , gm−1 α ) def = eC2 λ ∑m−1 i=1 |gi α| λ+c and R(τ ) def = Ez |z|eC2 λτ λ|z| λ. Proof. Clearly, with z ∼ N (0, 1), |∂wΨα(w; τ 2)| ≤ τ −1 E z |zψ(g1 α, . . . , gm−1 α , w + τ z)| ≤ τ −1 E z |z|eC(|w+τ z| λ+ ∑m−1 i=1 |gi α| λ)+c ≤ τ −1 E z |z|eC2λ(|w| λ+τ λ|z|λ+ ∑m−1 i=1 |gi α| λ)+c = τ −1 ˆΨ(g1 α, . . . , gm−1 α )R(τ )eC2 λ|w| λ. Then |Ψα(w; τ 2) − Ψα(w + ϵ; τ 2)| ≤ \f \f \f \f ∫ w+ϵ w dξ ∂ξΨα(ξ; τ 2) \f \f \f \f ≤ τ −1R(τ ) ˆΨ(g1 α, . . . , gm−1 α ) ∫ w+ϵ w dξ e C2 λ|ξ| λ = τ −1R(τ ) ˆΨ(g1 α, . . . , gm−1 α ) ∫ ϵ 0 dξ e C2 λ|w+ξ| λ ≤ τ −1R(τ ) ˆΨ(g1 α, . . . , gm−1 α ) ∫ ϵ 0 dξ e C4 λ|w| λeC4 λ|ξ| λ = τ −1R(τ ) ˆΨ(g1 α, . . . , gm−1 α )e C4 λ|w|λ|ϵ|eC4 λ|ϵ| λ. Therefore, with z ∼ N (0, 1), \f \f \f \f \fE z [ ψ(g1 α, . . . , gm−1 α , r∑ i=1 viˆgi α + σz) ] − E z [ ψ(g1 α, . . . , gm−1 α , r∑ i=1 ˚viˆgi α + σz) ]\f \f \f \f \f = \f \f \f \f \fΨα ( r∑ i=1 viˆgi α; σ2) − Ψα ( r∑ i=1 ˚viˆgi α; σ2)\f \f \f \f \f ≤ σ−1R(σ) ˆΨ(g1 α, . . . , gm−1 α )e C4 λ(| ∑r i=1 ˚vi ˆgi α| λ+|ϵα| λ)|ϵα|, where ϵα def = ∑r i=1(vi − ˚vi)ˆgi α. Because ˆΨ is λ-controlled, 1 n n∑ α=1 ˆΨ(g1 α, . . . , gm−1 α ) converges almost surely to a deterministic limit. At the same time, since vi a.s. −−→ ˚vi, we also have ϵα a.s. −−→ 0 as n → ∞, so that 1 n n∑ α=1 \f \f \f \f \fE z [ ψ(g1 α, . . . , gm−1 α , r∑ i=1 viˆgi α + σz) ] − E z [ ψ(g1 α, . . . , gm−1 α , r∑ i=1 ˚viˆgi α + σz) ]\f \f \f \f \f a.s. −−→ 0. A similar argument shows that we can replace σ with ˚σ: 1 n n∑ α=1 \f \f \f \f \fE z [ ψ(g1 α, . . . , gm−1 α , r∑ i=1 ˚viˆgi α + σz) ] − E z [ ψ(g1 α, . . . , gm−1 α , r∑ i=1 ˚viˆgi α + ˚σz) ]\f \f \f \f \f a.s. −−→ 0. By triangular inequality, these limits show that B a.s. −−→ 0 as desired. 69 I Proof of NETSOR+ Master Theorem In this section we describe how to augment the proof of Theorem E.3 given in Appendix H to yield the proof of Theorem C.4. The key points to note here are 1) the presence of LinComb rules in NETSOR+ but not in NETSOR− , 2) the rank stability assumption Assumption C.3 used in Theorem C.4, and 3) an additional term in Eq. (62) due to ﬂuctuations in the parameter Θ. I.1 LinComb As remarked in Remark E.2, any usage of LinComb in a NETSOR+ program can be absorbed into downstream nonlinearities or be expressed as Nonlin + rule. So WLOG, we can assume that the NETSOR+ program has no applications of LinComb. I.2 Rank Stability By Remark C.6, we see that rank stability assumption is necessary for the NETSOR+ Master Theorem. Whereas in Appendix H, we had to intricately weave together an induction on rank stability (more generally, CoreSet) and an induction on moment convergence (Moments), here to show Theorem C.4, we just need 1) to induct on Moments and 2) to invoke Assumption C.3 whenever we need to use Lemma H.4, which is when we need to show that pseudo-inverse commutes with almost surely limit, such as in Proposition H.1, and when we need to ensure either σ is almost surely 0 or is almost surely positive, as in Appendix H.5.3. I.3 Fluctuation of the Parameters When we have parameters in nonlinearities, Eq. (62) needs to be modiﬁed to contain an additional term D: \f \f \f \f \f 1 n n∑ α=1 ψ(g1 α, . . . , gm α ; Θ) − E Z∼N (µ,Σ) ψ(Z; ˚Θ) \f \f \f \f \f ≤ D + A + B + C where D def = \f \f \f \f \f 1 n n∑ α=1 ψ(g1 α, . . . , gm α ; Θ) − ψ(g1 α, . . . , gm α ; ˚Θ) \f \f \f \f \f and A, B, C are as in Eq. (62) but replacing ψ(−) there with ψ(−; ˚Θ). Because ψ(−; −) is parameter- controlled at ˚Θ by assumption, ψ(−; ˚Θ) is controlled, and A, B, C a.s. −−→ 0 with the same arguments as before (except using rank stability assumption Assumption C.3 where appropriate, instead of CoreSet). Now, by the other property of parameter-control, we have D ≤ 1 n n∑ α=1 \f \f \fψ(g1 α, . . . , gm α ; Θ) − ψ(g1 α, . . . , gm α ; ˚Θ) \f \f \f ≤ 1 n n∑ α=1 f (Θ) ¯ψ(g1 α, . . . , gm α ) = f (Θ) 1 n n∑ α=1 ¯ψ(g1 α, . . . , gm α ) for some controlled ¯ψ : Rm → R and some f : Rl → R≥0 ∪ {∞} that is continuous at ˚Θ and has f (˚Θ) = 0 (where ¯ψ and f can both depend on ˚Θ). Since Θ a.s. −−→ ˚Θ, we have f (Θ) a.s. −−→ 0. In addition, by Moments, 1 n ∑n α=1 ¯ψ(g1 α, . . . , gm α ) converges a.s. as well to a ﬁnite constant. Therefore, D a.s. −−→ 0 as desired. 70 program ::= stmt* stmt ::= Input var :: type | var := expr :: type expr ::= MatMul (var, var ) | fun( var* ) var ::= ⟨ id ⟩ fun ::= ⟨ function Rk → R for some k ≥ 0 ⟩ type ::= G(nat) | H(nat) | A(nat, nat) nat ::= ⟨ any integer ≥ 1 ⟩ Figure 4: NETSOR− Grammar; see Deﬁnition E.1. expr : type var := expr :: type var : type a : A(n1, n2) h : H(n2) MatMul(a, h) : G(n1) g1, . . . , gk : G(n) f : Rk → R f(g1, . . . , gk) : H(n) Figure 5: NETSOR− Inference Rules I.4 Summary The proof of Theorem C.4, WLOG for programs without LinComb, would proceed as follows: We induct on Moments with the same setup as Appendix H.2, except using Assumption C.3 for Proposition H.1. Then we prove the inductive step for Moments as in Appendix H.5. We modify Eq. (62) to add a term D as in Appendix I.3, which goes to 0 a.s. as argued there. The same arguments for A, B, C a.s. −−→ 0, exhibited in Appendix H.5 still hold, except that in the proof of B a.s. −−→ 0, we apply Assumption C.3 (instead of Lemma H.4) to allow us to assume ˚σ > 0 and σ > 0 almost surely. J Formal Speciﬁcation of Tensor Programs In the main text, we have adopted an informal approach to specifying the NETSOR language and its siblings, in order to make the material accessible to a wide audience. Here we give the formal speciﬁcations for NETSOR− (Figs. 4 to 6), NETSOR (Figs. 7 to 9), and self-parametrized NETSOR+ (Figs. 10 to 12). For ease of presentation, we have represented matrix multiplication explicitly via an operation MatMul (likewise for Moment in self-parametrized NETSOR+ ), and have we used double colon :: instead of single colon : for type annotation. JaK = W ∈ Rn1×n2 JhK = v ∈ Rn2 JMatMul(a, h)K = W v ∀i ∈ [k], JgiK = vi ∈ Rn JfK = f : Rk → R Jf(g1, . . . , gk)K = u ∈ Rn with uα = f (v1α, . . . , vkα) Figure 6: NETSOR− Semantics 71 program ::= stmt* stmt ::= Input var :: type | var := expr :: type expr ::= MatMul (var, var ) | fun( var* ) | var (+ var) + var ::= ⟨ id ⟩ fun ::= ⟨ function Rk → R for some k ≥ 0 ⟩ type ::= G(nat) | H(nat) | A(nat, nat) nat ::= ⟨ any integer ≥ 1 ⟩ Figure 7: NETSOR Grammar; see Deﬁnition 4.1. Compared to NETSOR− grammar, the only new item is LinComb in expr. expr : type var := expr :: type var : type a : A(n1, n2) h : H(n2) MatMul(a, h) : G(n1) g1, . . . , gk : G(n) f : Rk → R f(g1, . . . , gk) : H(n) g1, . . . , gk : G(n) g1 + · · · + gk : G(n) Figure 8: NETSOR Inference Rules JaK = W ∈ Rn1×n2 JhK = v ∈ Rn2 JMatMul(a, h)K = W v ∀i ∈ [k], JgiK = vi ∈ Rn JfK = f : Rk → R Jf(g1, . . . , gk)K = u ∈ Rn with uα = f (v1α, . . . , vkα) ∀i ∈ [k], JgiK = vi ∈ Rn Jg1 + · · · + gkK = v1 + · · · + vk ∈ Rn Figure 9: NETSOR Semantics 72 program ::= stmt* stmt ::= Input var :: type | var := expr :: type expr ::= MatMul (var, var ) | fun( var* ; var*) | var (+ var) + | Moment(fun; var*; var*) var ::= ⟨ id ⟩ fun ::= ⟨ parametrized function Rk × Rl → R for some k, l ≥ 0 ⟩ type ::= C | G(nat) | H(nat) | A(nat, nat) nat ::= ⟨ any integer ≥ 1 ⟩ Figure 10: Self-Parametrized NETSOR+ Grammar; see Deﬁnition C.8. Compared to NETSOR grammar, we have added a new type C and a new expression Moment. expr : type var := expr :: type var : type a : A(n1, n2) h : H(n2) MatMul(a, h) : G(n1) g1, . . . , gk : G(n) g1 + · · · + gk : G(n) g1, . . . , gk : G(n) c1, . . . , cl : C f : Rk × Rl → R f(g1, . . . , gk; c1, . . . , cl) : H(n) g1, . . . , gk : G(n) c1, . . . , cl : C f : Rk × Rl → R Moment(f; g1, . . . , gk; c1, . . . , cl) : C Figure 11: Self-Parametrized NETSOR+ Inference Rules JaK = W ∈ Rn1×n2 JhK = v ∈ Rn2 JMatMul(a, h)K = W v ∀i ∈ [k], JgiK = vi ∈ Rn Jg1 + · · · + gkK = v1 + · · · + vk ∈ Rn ∀i ∈ [k], JgiK = vi ∈ Rn ∀j ∈ [l], JcjK = cj ∈ R JfK = f : Rk × Rl → R Jf(g1, . . . , gk; c1, . . . , cl)K = u ∈ Rn with uα = f (v1α, . . . , vkα; c1, . . . , cl) ∀i ∈ [k], JgiK = vi ∈ Rn ∀j ∈ [l], JcjK = cj ∈ R JfK = f : Rk × Rl → R JMoment(f; g1, . . . , gk; c1, . . . , cl)K = 1 n n∑ α=1 f (v1α, . . . , vkα; c1, . . . , cl) Figure 12: Self-Parametrized NETSOR+ Semantics 73","libVersion":"0.2.3","langs":""}