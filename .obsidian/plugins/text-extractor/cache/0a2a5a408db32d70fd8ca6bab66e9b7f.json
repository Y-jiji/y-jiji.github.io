{"path":"_assets/tensor-program.pdf","text":"Tensor Programs I: Wide Feedforward or Recurrent Neural Networks of Any Architecture are Gaussian Processes Greg Yang ‚àó Microsoft Research AI gregyang@microsoft.com Abstract Wide neural networks with random weights and biases are Gaussian processes, as originally observed by Neal (1995) and more recently by Lee et al. (2018) and Matthews et al. (2018) for deep fully-connected networks, as well as by Novak et al. (2019) and Garriga-Alonso et al. (2019) for deep convolutional networks. We show that this Neural Network-Gaussian Process correspondence surprisingly extends to all modern feedforward or recurrent neural networks composed of multi- layer perceptron, RNNs (e.g. LSTMs, GRUs), (nD or graph) convolution, pooling, skip connection, attention, batch normalization, and/or layer normalization. More generally, we introduce a language for expressing neural network computations, and our result encompasses all such expressible neural networks. This work serves as a tutorial on the tensor programs technique formulated in Yang (2019) and elucidates the Gaussian Process results obtained there. We provide open-source im- plementations of the Gaussian Process kernels of simple RNN, GRU, transformer, and batchnorm+ReLU network at github.com/thegregyang/GP4A. 1 Introduction Motivated to understand the Bayesian prior in neural networks (NNs), Neal [41] theoretically showed that inÔ¨Ånitely wide, shallow neural networks with random weights and biases are Gaussian processes (GPs). He empirically explored this phenomenon over deep networks as well, but this was not proven rigorously until recently [37, 40, 43, 18], with concrete progress made over the intervening years [56, 34, 22, 13]. This neural network-Gaussian process correspondence (NN-GP correspondence) has not only allowed one to transform the implicit prior of NNs into explicit priors that can be understood analytically [46, 49, 63, 59, 65], but has also created new state-of-the-art kernels by converting from deep neural networks [37, 43]. Yet, so far the focus has dwelled entirely on multilayer perceptrons (MLPs) or simple convolutional neural networks (CNNs). As new architectures are created with blistering speed, a question starts to emerge and reverberate: Do all inÔ¨Ånitely wide, randomly initialized neural networks correspond to Gaussian processes? Even if the answer is yes, at the current rate where each new architecture warrants its own NN-GP correspondence paper, theory will never catch up to practice. On a more basic level, what does this question even mean for recurrent neural networks? Our Contributions In this paper, we formulate the notion of a Gaussian process with variable- dimensional output (see DeÔ¨Ånition 2.1), and show that feedforward and recurrent neural networks of standard architectures converge to Gaussian processes in this sense as their widths or number ‚àóPlease see https://arxiv.org/abs/1910.12478 for the full version of this paper. 33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.arXiv:1910.12478v3 [cs.NE] 8 May 2021 of channels go to inÔ¨Ånity, when their weights and biases are randomized. By standard architecture we mean any architecture that is some composition of multilayer perceptrons (MLPs), recurrent neural networks (RNNs) (e.g., Long-Short Term Memory (LSTM) [26] or Gated Recurrent Unit (GRU) [10]), skip connections [24, 27], convolutions [16, 17, 47, 35, 36] or graph convolutions [8, 25, 15, 38, 14, 31], pooling [35, 36], batch normalization (batchnorm) [28], layer normalization [1] and/or attention [2, 55]. Even more broadly, we design a new language, NETSOR , for expressing neural network computations, and show the GP convergence for all such expressible networks. By demonstrating that NETSOR can implement any network of standard architectures, we obtain the aforementioned results as a corollary. The results for RNNs, batchnorm, layernorm, attention, and their combination with other layers are new. We open-source reference implementations 2 for the GP kernels of simple RNN, GRU, transformer, and feedforward batchnorm network; see Fig. 3 for an illustration. Relation of This Paper with [60] This paper serves several purposes. 1) Introduce the reader to the tensor programs technique formulated in [60], using the Neural Network-Gaussian Process Correspondence as motivation. 2) Promote a redesigned set of notations for tensor programs that hopefully makes the understanding and the application of this technique easier. 3) Prove a more general version of the Gaussian Process results Ô¨Årst presented in [60]. 4) Provide example calculations and reference implementations2 of the GP kernels for several architectures like the vanilla RNN, GRU, batchnorm network, and transformers. We assume the reader has not read [60] and seek to explain all results in elementary terms. However, we will provide commentary in footnotes throughout the paper on differences from [60]. Regarding 1), this paper will be the Ô¨Årst in a series to explain the tensor programs technique, each covering a more powerful type of tensor programs, and each motivated by speciÔ¨Åc theorems that can be proved or calculations made possible by these new tensor programs. In particular, here we will only talk about tensor programs without matrix transposes. Regarding 3), the results presented here will supersede all results in [60] concerning Gaussian Processes, with one caveat that here we will not cover architectures using both a weight W and its transpose W ‚ä§ in its forward pass (but this result will come for free in a later paper in this series). 2 Gaussian Process with Variable-Dimensional Output We Ô¨Årst clarify the notion of a Gaussian process with variable dimension output. DeÔ¨Ånition 2.1 (Gaussian Process). We say a random function f : X ‚Üí Rm (with Ô¨Åxed dimen- sional output) is a Gaussian process if for any Ô¨Ånite subset {x1, . . . , xk} ‚äÜ X, the random vector (f (x1), . . . , f (xk)) ‚àà Rm√ók is distributed as a km-dimensional Gaussian. If f has variable dimen- sional output (e.g. f is an RNN), such as when f (x) ‚àà Rl(x) for some length function l : X ‚Üí N 3, then we say f is a Gaussian process if for any Ô¨Ånite subset {x1, . . . , xk} ‚äÜ X, the random vector (f (x1), . . . , f (xk)) is distributed as a ( ‚àëi l(xi))-dimensional Gaussian. To illustrate a GP with variable-dimensional output, consider a simple RNN that runs on two input sequences given by the GloVe embeddings [44] 4 of the words of the two sentences sentence 1 (7 words): ‚ÄúThe brown fox jumps over the dog.‚Äù sentence 2 (9 words): ‚ÄúThe quick brown fox jumps over the lazy dog.‚Äù (‚ãÜ) A pseudocode is given in Program 2 in Section 4 (ignore the type annotations like G(n), H(n), A(n) for now). The RNN emits a single scalar after reading each token (in Program 2, this is v‚ä§s ia/‚àön, where s ia is the RNN state after reading the ith token of the ath sentence, and v is the readout layer); this number takes into account all of the word embeddings read so far. Thus, it will output a total of 7 scalars after reading sentence 1, and a total of 9 scalars after reading sentence 2. To say that this RNN is a GP would imply that all 7 + 9 = 16 scalars are jointly Gaussian-distributed (corresponding to a 16 √ó 16 kernel), over the randomness of the weights and biases imbued during initialization. This 2github.com/thegregyang/GP4A 3i.e. f : ‚àè x‚ààX Rl(x) is a dependent function 4The embedding associates each word to a real vector of 100 dimensions such that semantically similar words are mapped to closer vectors 2 is indeed the empirical phenomenon with a width-1000 RNN, and Fig. 2(E) visualizes the the joint distribution of the last scalars output by the RNN at the end of each sentence. It clearly exhibits a Gaussian nature, and perfectly Ô¨Åts the theoretically predicted Gaussian distribution (dashed ovals), which we shall describe in Corollary 5.5. 3 Recap: GP Behavior of a Multilayer Perceptron (MLP) Before explaining our main results, we Ô¨Årst review the argument from prior works [37, 40, 43] for the GP convergence of a wide MLP with randomly initialized weights and biases, and we also demonstrate why such an argument is inadequate for RNNs. Consider an MLP with widths {nl}l, weight matrices {W l ‚àà Rnl√ónl‚àí1}l, and biases {bl ‚àà Rnl }l, where l ranges among the layer numbers of the MLP. Its computation is given recursively as h 1(x) = W 1x + b 1 and h l(x) = W lœï(h l‚àí1(x)) + bl for l ‚â• 2. (1) At initialization time, suppose W l Œ±Œ≤ ‚àº N (0, œÉ2 w/nl‚àí1) for each Œ± ‚àà [nl], Œ≤ ‚àà [nl‚àí1], and bl Œ± ‚àº N (0, œÉ2 b ). Consider two inputs x, x ‚Ä≤. Conditioned on h l‚àí1(x) and h l‚àí1(x‚Ä≤), iid for each Œ±, (h l(x)Œ±, h l(x‚Ä≤)Œ±) is distributed as N ( 0, œÉ2 w nl‚àí1 ( ‚à•œï(h l‚àí1(x))‚à• 2 œï(h l‚àí1(x)) ¬∑ œï(h l‚àí1(x‚Ä≤)) œï(h l‚àí1(x)) ¬∑ œï(h l‚àí1(x‚Ä≤)) ‚à•œï(h l‚àí1(x‚Ä≤))‚à• 2 ) + œÉ2 b . ) If (h l‚àí1(x)Œ±, h l‚àí1(x ‚Ä≤)Œ±) is distributed as N (0, Œ£l‚àí1), iid for each Œ±, then by a law of large number argument, the covariance matrix above converges to a deterministic limit Œ£l def = œÉ2 w E (z,z‚Ä≤)‚àºN (0,Œ£l‚àí1) ( œï(z) 2 œï(z)œï(z‚Ä≤) œï(z)œï(z‚Ä≤) œï(z‚Ä≤) 2 ) + œÉ2 b as the width nl‚àí1 ‚Üí ‚àû, making (h l(x)Œ±, h l(x‚Ä≤)Œ±) Gaussian distributed as N (0, Œ£l). Iteratively applying this argument for each l yields the result for a deep MLP. A similar logic works for feedforward CNNs. Unfortunately, this argument breaks down if the weights {W l}l are tied, i.e. all W l are equal to a common matrix W , as in the case of an RNN. In this case, when we condition on the preactivations h l‚àí1(x), h l‚àí1(x‚Ä≤) of the previous layer, W is no longer conditionally an iid random Gaussian matrix, and all subsequent reasoning breaks down. We can repair this situation for RNNs in an ad hoc way via the Gaussian conditioning technique (Lemma G.7), but we prefer to set our sights wider, and deal with all standard architectures, and more, in one fell swoop. To this end, we develop a framework based on our new NETSOR language. 4 NETSOR : Language for Expressing Neural Network Computation To show that networks of all standard architectures converge to GPs, we Ô¨Årst show that they can be expressed by the following very general NETSOR language (see Programs 1 and 2 for examples)5, and then show that any computation expressed this way exhibits GP behavior when its dimensions are large. DeÔ¨Ånition 4.1. 6 NETSOR programs are straight-line programs, where each variable follows one of three types, G, H, or A (such variables are called G-vars, H-vars, and A-vars), and after input variables, new variables can be introduced by one of the rules MatMul, LinComb, Nonlin to be discussed shortly. G and H are vector types and A is a matrix type; intuitively, G-vars should be thought of as vectors that are asymptotically Gaussian, H-vars are images of G-vars by coordinatewise nonlinearities, and A-vars are random matrices with iid Gaussian entries. Each type is annotated by dimensionality information: ‚Ä¢ If x is a (vector) variable of type G (or H) and has dimension n, we write x : G(n) (or x : H(n)). 5NETSOR is a speciÔ¨Åc kind of tensor program; for other variants, see Appendix E. 6We keep the deÔ¨Ånition here informal in terms of programming language convention to be accessible to the general machine learning audience. For those with PL background, see Appendix J. 3 NETSOR program 1 MLP Computation on Network Input x Input: W 1x : G(n 1) ‚ñ∑ layer 1 embedding of input Input: b1 : G(n1) ‚ñ∑ layer 1 bias Input: W 2 : A(n2, n 1) ‚ñ∑ layer 2 weights Input: b2 : G(n2) ‚ñ∑ layer 2 bias Input: v : G(n2) ‚ñ∑ readout layer weights 1: h1 := W 1x + b1 : G(n1) ‚ñ∑ layer 1 preactivation; LinComb 2: x1 := œï(h 1) : H(n1) ‚ñ∑ layer 1 activation; Nonlin 3: Àúh 2 := W 2x1 : G(n2) ‚ñ∑ MatMul 4: h 2 := Àúh 2 + b 2 : G(n2) ‚ñ∑ layer 2 preactivation; LinComb 5: x2 := œï(h 2) : H(n2) ‚ñ∑ layer 2 activation; Nonlin Output: v‚ä§x2/ ‚àön2 ‚Ä¢ If A is a (matrix) variable of type A and has size n1 √ó n2, we write A : A(n1, n2). G is a subtype of H, so that x : G(n) implies x : H(n). A NETSOR program consists of the following three parts. Input A set of input G- or A-vars. Body New variables can be introduced and assigned via the following rules (with intuition in italics) MatMul if A : A(n1, n2) and x : H(n2), we can form a G-var via matrix-vector product: Ax : G(n1), ‚Äúrandom iid matrix times a vector is roughly a Gaussian vector.‚Äù 7 LinComb Suppose x1, . . . , xk : G(n) are G-vars with the same dimension and a1, . . . ak ‚àà R are constants. Then we can form their linear combination as a G-var: n‚àë i=1 aixi : G(n), ‚Äúlinear combination of Gaussian vectors is Gaussian.‚Äù Nonlin If x1, . . . , xk : G(n) are G-vars with the same dimension n and œï : Rk ‚Üí R, then œï(x1, . . . , xk) : H(n), ‚Äúimage of Gaussian vector is not always Gaussian‚Äù where œï acts coordinatewise. Output For the purpose of this paper 8, the output of a NETSOR program can be any tuple of scalars, (v1‚ä§y1/‚àön1, . . . , vk‚ä§yk/ ‚àönk), where v1 : G(n1); . . . ; vk : G(nk) are some input G-vars not used elsewhere (and possibly with duplicates vi = vj), and y1 : H(n1); . . . ; yk : H(nk) are some H-vars (possibly with duplicates yi = yj). Examples Program 1 gives an example of a NETSOR program representing an MLP computation. Note that we account for the input x through its embedding W 1x, not x itself. This is because 1) our theorems concern the case where all input G-vars are random; in the context of expressing neural network computation, x is a deterministic input, while W 1x is a Gaussian vector when W 1 has iid Gaussian entries; 2) x has a Ô¨Åxed dimension, while we intend all dimensions (like n1, n 2) in the NETSOR program to tend to inÔ¨Ånity, as we‚Äôll describe shortly. Similarly, Program 2 expresses in NETSOR the computation of a simple RNN on two separate input sequences; computation on more input sequences follows the same pattern. Note how weight-sharing is easily expressed in NETSOR because we can re-use A-vars arbitrarily. Appendix A shows more examples of standard architectures in NETSOR and NETSOR+ . More generally, we can allow the nonlinearities in Nonlin to depend on parameters; this will be necessary to express layernorm and attention (see Appendix A). We capture this idea in a new rule: 7Beware: in a later paper (and in [60], tensor program general case), we will introduce matrix transpose as a valid operation, and in that case, Ax can be very far from a Gaussian, and this intuition is no longer correct. Thus, this intuition is more subtle than it might seem at face value. 8In general, the output of a tensor program need not be deÔ¨Åned, as most of the time we are concerned with how the H-vars produced over the course of the program interact with each other. 4 NETSOR program 2 Simple RNN Computation on Two Input Sequences // Embeddings of two inputs sequences Input: U x11, . . . , U xt1 : G(n) Input: U x12, . . . , U xr2 : G(n) // Weight and bias Input: W : A(n, n) Input: b : G(n) // Readout weights Input: v : G(n) // Computation on sequence 1 h 11 := U x11 + b : G(n) s 11 := œï(h 11) : H(n) Àúh 21 := W s 11 : G(n) h 21 := Àúh 21 + U x21 + b : G(n) s 21 := œï(h 21) : H(n) ... Àúh t1 := W s t‚àí1,1 : G(n) h t1 := Àúh t1 + U xt1 + b : G(n) s t1 := œï(h t1) : H(n) // Computation on sequence 2 h 12 := U x 12 + b : G(n) s 12 := œï(h 12) : H(n) Àúh 22 := W s 12 : G(n) h 22 := Àúh 22 + U x 22 + b : G(n) s 22 := œï(h 22) : H(n) ... Àúh r2 := W s r‚àí1,2 : G(n) h r2 := Àúh r2 + U xr2 + b : G(n) s r2 := œï(h r2) : H(n) Output: (v‚ä§s 11/ ‚àön, . . . , v‚ä§s t1/ ‚àön, v‚ä§s 12/‚àön, . . . , v‚ä§s r2/ ‚àön) Nonlin + Suppose x1, . . . , xk : G(n) are G-vars with the same dimension n and Œ∏1, . . . , Œ∏t ‚àà R possibly depend on G-vars already deÔ¨Åned. If œï(‚àí; ‚àí) : Rk √ó Rt ‚Üí R, then œï(x1, . . . , xk; Œ∏1, . . . , Œ∏t) : H(n), where œï acts coordinatewise. DeÔ¨Ånition 4.2. NETSOR+ programs are NETSOR programs allowing Nonlin + rules. NETSOR and NETSOR+ specify different kinds of tensor programs; in Appendix E we discuss other kinds that are semantically equivalent. In a future paper, we shall study the effect of allowing matrix transposes as an operation on A-vars. Remark 4.3. In this paper, in Nonlin +, we will only instantiate Œ∏j with continuous functions of ‚Äúempirical moments‚Äù of the form n‚àí1 ‚àën i=1 œà(y1, . . . , yr) for some set of G-vars {yi}i. A key consequence of our scaling limit result is that these ‚Äúempirical moments‚Äù converge almost surely to a deterministic limit under very general conditions (Theorems 5.4 and C.4), so that œï(‚àí; Œò) is, under suitable smoothness conditions (DeÔ¨Ånition C.1), approximately a Ô¨Åxed nonlinearity when n is large. Thus, we should intuitively treat Nonlin + as Nonlin but with the nonlinearity determined automatically by the NETSOR program itself. Nonlin + expands the expressible computation quite broadly, but to keep the main text lean and focused on the key ideas behind tensor programs, we relegate a more thorough discussion of Nonlin + in the appendix (see Appendices C, D and I). 5 Computing the GP Kernel from a NETSOR Encoding of a Neural Network For readers who wish to be convinced that NETSOR (or NETSOR+ ) can express standard architectures, see Appendix A. In this section, we show that any architecture expressible in NETSOR and satisÔ¨Åes some mild conditions will exhibit Gaussian Process behavior in the large width limit. In this section, we make the following simplifying assumption on the dimensions of the program and the randomness of the variables. Assumption 5.1. Fix a NETSOR program. For simplicity, assume all dimensions in the program are equal to n. Suppose for each A-var W : A(n, n), we sample WŒ±Œ≤ ‚àº N (0, œÉ2 W /n) for some œÉ2 W > 0, and for each Œ± ‚àà [n], we sample, i.i.d., {xŒ± : x is input G-var} ‚àº N (¬µ in, Œ£in) for some mean ¬µ in and (possibly singular) covariance Œ£in over input G-vars. 5 The constraint on the dimensions can be removed easily; see Appendix F. This sampling induces randomness in all variables created in the program, and we shall characterize this randomness shortly. We Ô¨Årst review some notation that will be used immediately. Notation In this paper, a kernel Œ£ on a set X is a symmetric function Œ£ : X √ó X ‚Üí R such that m‚àë i=1 m‚àë j=1 cicjŒ£(xi, xj) ‚â• 0 holds for any m ‚àà N, x1, . . . , xm ‚àà X, and c1, . . . , cm ‚àà R. Given a kernel Œ£ on a set of G-vars, we will both treat it as matrix and as a function, depending on the context. Function Notation As a function, Œ£(g, g‚Ä≤) is the value of Œ£ on the pair of G-vars (g, g‚Ä≤). If G = {g1, . . . , gk} is a set of G-vars, then we also denote by Œ£(g, G) the row vector {Œ£(g, g1), . . . , Œ£(g, gk)}. Likewise Œ£(G, g) is the column vector with the same values. If G ‚Ä≤ = {g1‚Ä≤, . . . , gl‚Ä≤} is another set of G-vars (possible with overlap with G), then Œ£(G, G ‚Ä≤) is the matrix {Œ£(gi, gj ‚Ä≤) : i ‚àà [k], j ‚àà [l]}. Restriction Notation We also use the ‚Äúrestriction‚Äù notation Œ£|G to denote the square matrix Œ£(G, G) in a more concise way. Matrix Notation When an association of indices to G-vars is clear from context, we will also write Œ£ij for the corresponding value of Œ£ on the pair of ith and jth G-vars. Juxtaposition implies matrix multiplication, e.g. Œ£‚Ñ¶ means matrix product if ‚Ñ¶ is a matrix of appropriate size. Indices Notation We will both use superscripts and subscripts for indices. We will never multiply in subscript or superscript, so juxtaposition of indices like W ib Œ±Œ≤ is the same as W i,b Œ±,Œ≤. H-vars as Both Symbols and Vectors An H-var will be considered both as a symbol (like in Œ£(g, g‚Ä≤) above) as well as the corresponding length n vector (like in Theorem 5.4 below), depending on the context. DeÔ¨Ånition 5.2. In the setting of Assumption 5.1, we extend ¬µ in and Œ£in to ¬µ and Œ£ that resp. take a single and a pair of G-vars and both output to R. Intuitively, ¬µ speciÔ¨Åes the mean coordinate of a G-var, and Œ£ speciÔ¨Åes the coordinatewise covariance of a pair of G-vars; this is formalized in Theorem 5.4 below. Index all the G-vars in the program as g1, . . . , gM (including input G-vars), in the order of appearance in the program. For any pair of G-vars g, g‚Ä≤ (among g1, . . . , gM ), we deÔ¨Åne recursively ¬µ(g) = Ô£± Ô£≤ Ô£≥ ¬µ in(g) if g is input ‚àë i ai¬µ(yi) if g = ‚àë i aiyi, introduced by LinComb 0 otherwise , Œ£(g, g‚Ä≤) = Ô£± |||||Ô£≤ |||||Ô£≥ Œ£in(g, g‚Ä≤) if g, g‚Ä≤ are inputs ‚àë i aiŒ£(yi, g‚Ä≤) if g = ‚àë i aiyi, introduced by LinComb ‚àë i aiŒ£(g, yi) if g‚Ä≤ = ‚àë i aiyi, introduced by LinComb œÉ2 W EZ œï(Z) ¬Øœï(Z) if g = W h, g‚Ä≤ = W h ‚Ä≤, introduced by MatMul w/ same A-var W 0 otherwise (2) where ‚Ä¢ yi are G-vars for all i ‚Ä¢ (h : H(n)) was introduced by the Nonlin with h := œï(g1, . . . , gM ), h ‚Ä≤ was introduced by Nonlin with h ‚Ä≤ := ¬Øœï(g1, . . . , gM ) (where WLOG we have padded the input slots of œï and ¬Øœï to account for all G-vars) ‚Ä¢ Z ‚àº N (¬µ, Œ£) is a random Gaussian vector with 1 entry for each G-var in the program. Note that since œï and ¬Øœï only depends on entries of Z corresponding to previous G-vars, the expectation EZ œï(Z) ¬Øœï(Z) only depends on entries of ¬µ and Œ£ already deÔ¨Åned, so there is no circular logic in this recursive deÔ¨Ånition of ¬µ and Œ£. See Appendix B.1.1 for a simple, worked-out example of how to recursively compute ¬µ and Œ£ for Program 1. For our main theorems, we isolate a very general class of nonlinearities that we are concerned with. DeÔ¨Ånition 5.3. We say a function œï : Rk ‚Üí R is controlled if |œï(x)| is bounded by a function of the form eC‚à•x‚à•2‚àíœµ+c with C, c, œµ > 0 6 ùëî1 ùëî2 ùëî3 ùëîùëÄùëõ‚Üí‚àû ùúì ùúì ùúì ùúì ùúì ùúì ( Average 1 ùëõ ‡∑ç 1 ùëõ ( ( ( ( ( ) ) ) ) ) ) ùîº ùëç ~ ùëÅ ùúá,Œ£ ùúì ( )ùëçùëî1 ùëçùëî2 ùëçùëî3 ùëçùëîùëÄùëé. ùë†. Figure 1: An illustration of the NETSOR Master Theorem Theorem 5.4. Controlled functions can explode faster than exponential but are still L 1 and L 2-integrable against Gaussian measures. Additionally, there is no constraint on the smoothness of œï here. Thus this deÔ¨Ånition captures almost all functions we would care about in practice. The metric structure of the Ô¨Ånal layer representations of inputs under a deep neural network often reveals semantical information about the inputs. This structure is reÔ¨Çected in the inner products between pairs of such representations, e.g. s t1‚ä§s r2/n for s t1 and s r2 in Program 2. The following Master Theorem allows one to compute such inner products, and much more, for a wide network at initialization time (take œà below to be œà(z1, . . . , zM ) def = zM ‚àí1zM ). Theorem 5.4 (NETSOR Master Theorem). 9 Fix any NETSOR program satisfying Assumption 5.1 and with all nonlinearities controlled. If g1, . . . , gM are all of the G-vars in the entire program, including all input G-vars, then for any controlled œà : RM ‚Üí R, as n ‚Üí ‚àû, 1 n n‚àë Œ±=1 œà(g1 Œ±, . . . , gM Œ± ) a.s. ‚àí‚àí‚Üí E Z‚àºN (¬µ,Œ£) œà(Z) = E Z‚àºN (¬µ,Œ£) œà(Z g1, . . . , Z gM ), where a.s. ‚àí‚àí‚Üí means almost sure convergence, Z = (Z g1, . . . , Z gM ) ‚àà RM , and ¬µ = {¬µ(gi)} M i=1 ‚àà RM and Œ£ = {Œ£(gi, gj)} M i,j=1 ‚àà RM √óM are given in Eq. (2). See Fig. 1 for an illustration. Intuitively, Theorem 5.4 says, for each Œ±, (g1 Œ±, . . . , gM Œ± ) ‚âà N (¬µ, Œ£) in the large n limit, and each Œ±-slice appears to be ‚Äúiid‚Äù from the point of view of the empirical average by any controlled function œà. The proof of Theorem 5.4 is given in Appendix H. Combining Theorem 5.4 with Proposition G.4, we can straightforwardly calculate the output distri- bution of a NETSOR program. Corollary 5.5 (Computing the GP Kernel). Adopt the same assumptions and notations as in Theorem 5.4. If the program outputs (v‚ä§x1/ ‚àön, . . . , v‚ä§xk/‚àön), where ‚Ä¢ v : G(n), vŒ± ‚àº N (0, œÉ2 v), is an input G-var not used elsewhere in the program and is sampled independently from all other G-vars, and ‚Ä¢ x i was introduced as xi := œïi(g1, . . . , gM ) then the output vector converges in distribution to N (0, K) where Kij = œÉ2 v E Z‚àºN (¬µ,Œ£) œïi(Z)œïj(Z), with ¬µ, Œ£ deÔ¨Åned in Eq. (2). (3) Intuitively, this corollary follows from the fact that, for any Ô¨Ånite n, the output vector is some Gaussian N (0, ÀúK) conditioned on x1, . . . , xk, and the covariance ÀúK converges to a deterministic covariance K, causing the output vector to converge in distribution to N (0, K) as well. The case 9Difference with [60, Thm 4.3]: We have gotten rid of the ‚Äúrank convergence‚Äù assumption by showing that it comes for free. See CoreSet and Lemma H.4 in Appendix H. 7ThebrownfoxjumpsoverthedogThequickbrownfoxjumpsoverthelazydog sent1 sent2 The brown fox jumps over the dog The quick brown fox jumps over the lazy dogsent2 sent1 (A) GloVe correlations 0.0 0.2 0.4 0.6 0.8 1.0ThebrownfoxjumpsoverthedogThequickbrownfoxjumpsoverthelazydog sent1 sent2(B) randRNN correlations (theory) 0.0 0.2 0.4 0.6 0.8 1.0ThebrownfoxjumpsoverthedogThequickbrownfoxjumpsoverthelazydog sent1 sent2(C) randRNN covariances (theory) 0.00 0.05 0.10 0.15 0.20 0.25 0.30ThebrownfoxjumpsoverthedogThequickbrownfoxjumpsoverthelazydog sent1 sent2(D) randRNN covariance std (empirical) 0.006 0.008 0.010 0.012 0.014 0.016 0.018 0.020 2 1 0 1 2 randRNN(\"...lazy dog\") 2.0 1.5 1.0 0.5 0.0 0.5 1.0 1.5 2.0randRNN(\"...the dog\") (E) randRNN: Theory vs Empirics theory Figure 2: InÔ¨Ånite-width theory is highly predictive for simple RNN (Program 2) with 1000 neurons and erf activation. We pass two sentences (‚ÄúThe brown fox jumps over the dog‚Äù and ‚ÄúThe quick brown fox jumps over the lazy dog‚Äù) by their word GloVe embeddings into randomly initialized simple RNNs. (A) Cosine distances between each pair of word GloVe embeddings. (B) Correlation matrix of the limiting Gaussian that Program 2 output vector converges to. Each row/column corresponds to an embedding of of the sentence up to that word. (C) Covariance matrix of the same. See Appendix B.2 for algorithm to compute this covariance. (D) Entrywise standard deviation of empirical covariance around (C), as measured from 100 random simple RNNs. Note the deviations are at least an order of magnitude smaller than the limiting values (C), for 1000 neurons. (E) Visualizing the joint distribution of the Ô¨Ånal outputs of the RNN at the end of each sentence, i.e. (v‚ä§s t1/ ‚àön, v‚ä§s r2/ ‚àön) in Program 2. We sampled 100,000 simple RNNs and plotted the 2d histogram as a heatmap. Simultaneously, we plot the limiting Gaussian level curves predicted by our theory, which Ô¨Åt the simulations perfectly. when we have multiple distinct vi (allowed by DeÔ¨Ånition 4.1) can be obtained easily as well (see Proposition G.4). Following Corollary 5.5 and its extensions below, the convergence of standard architectures to Gaussian Processes becomes obvious: Express the marginal of the distribution on every Ô¨Ånite set of inputs as a NETSOR (or NETSOR+ ) program, and then apply Corollary 5.5. We summarize the result below. Corollary 5.6. If its nonlinearities are controlled (DeÔ¨Ånition 5.3), then a (possibly recurrent) neural network of standard architecture converges to a Gaussian process in Ô¨Ånite-dimensional distribu- tion 10 in the sense of DeÔ¨Ånition 2.1 as its widths go to inÔ¨Ånity and each of its weights W and biases b are randomized as WŒ±Œ≤ ‚àº N (0, œÉ2 W /n), bŒ± ‚àº N (¬µb, œÉ2 b ) for a collection of sampling hyperparameters {œÉW }W , {¬µb, œÉb}b. If its nonlinearities are more generally parametrized and are parameter-controlled (DeÔ¨Ånition C.1), such as in the case of attention models or where layernorm is involved, then the same result holds as long as Assumption C.3 also holds. An Empirical Demonstration Despite being about inÔ¨Ånite width, our theory is highly predictive for Ô¨Ånite-width networks, as shown in Fig. 2. As in Section 2, we randomly initialize a simple RNN (Program 2) with 1000 neurons and erf activation (we choose erf instead of tanh because it simpliÔ¨Åes kernel calculations; see Appendix B.2 for the derivation of the algorithm to compute the kernel). We pass the two sentences in (‚ãÜ) to the random RNN by their GloVe embeddings. After processing each token, the RNN outputs a scalar, as before, and over the two input sequences, the RNN outputs 7 + 9 = 16 scalars in total. Our result Corollary 5.5 implies that, as the width of the RNN grows to inÔ¨Ånity, these 16 scalars are distributed jointly as a Gaussian. Fig. 2(E) illustrates this is indeed the case for the marginal on 2 scalars, as discussed in Section 2. We also compare our theoretically derived, inÔ¨Ånite-width covariance of the 16 scalars (Fig. 2(C)) with the empirical Ô¨Ånite-width covariance obtained from multiple random initializations. We Ô¨Ånd that the empirical covariance, as predicted, concentrates around the theoretical, and the entrywise standard deviation is typically at least an order of magnitude lower than the values themselves (Fig. 2(D)) (with width 1000 RNNs). The random RNN is clearly doing nontrivial context embedding, as seen by comparing the correlation matrix of the 16 scalars Fig. 2(B) (context-sensitive) with the matrix of cosine distances (i.e. correlations) between the GloVe embeddings Fig. 2(A) (context-insensitive). A tell-tale sign is the entry corresponding to (‚Äúlazy‚Äù, ‚Äúdog‚Äù): even though as words, they are not semantically similar 10Stronger convergence results, such as convergence in distribution with respect to some topology on functions on Rd, would be available if one can show additionally the tightness of the random neural networks under this topology. However, here we are content with convergence of Ô¨Ånite-dimensional marginals of the stochastic processes. 8ThebrownfoxjumpsoverthedogThequickbrownfoxjumpsoverthelazydog sent1 sent2 The brown fox jumps over the dog The quick brown fox jumps over the lazy dogsent2 sent1 (A) RNN correlations (theory) 0.0 0.2 0.4 0.6 0.8 1.0ThebrownfoxjumpsoverthedogThequickbrownfoxjumpsoverthelazydog sent1 sent2(B) GRU correlations (theory) 0.80 0.85 0.90 0.95 1.00ThebrownfoxjumpsoverthedogThequickbrownfoxjumpsoverthelazydog sent1 sent2(C) transformer correlation (theory) 0.5 0.6 0.7 0.8 0.9 1.0 0 20 40 60 CIFAR10 sample id (D) BN+relu fc net correlations (theory) 0.2 0.4 0.6 0.8 1.0 4 6 8 10 12 log2width 12 10 8 6 4 22log2(|KsimKth|F/|Kth|F) (E) relative deviation from theory model BN GRU transformer simpleRNN log2width Figure 3: InÔ¨Ånite-width GP kernels (more precisely, their correlation matrices) for which we provide reference implementations, and the deviation of Ô¨Ånite-width simulations from the corresponding inÔ¨Ånite-width limits. (A) ‚Äì (C) The correlation matrices of the GP kernels for the simple RNN (same as in Fig. 2; see Program 2 for the architecture and Appendix B.2 for derivation), GRU (Program 5; Appendix B.5), and transformer (Program 10; Appendix D.3), with input sequences given by the GloVe embeddings of (‚ãÜ). (D) The correlation matrix of the GP kernel for a feedforward, fully-connected network with batchnorm+ReLU (batchnorm followed by ReLU) as nonlinearity (see Appendix B.3 for derivation). The inputs are the Ô¨Årst 64 CIFAR10 images, split into two batches of 32 each. The red lines indicate the batch split. (E) For each architecture above, we independently randomly initialize 100 networks for each width among [25, 26, . . . , 2 13]. We calculate the empirical kernel of the network outputs and plot its (relative) Frobenius distance to the inÔ¨Ånite-width kernel. This Frobenius distance drops like 1/‚àöwidth as one would expect from a central limit intuition. See our code 2 for Python implementations of these kernels and the code for generating this Ô¨Ågure. (so that the entry in Fig. 2(A) is small), the random RNN understands that the two sentences resp. up to ‚Äúlazy‚Äù and ‚Äúdog‚Äù have been very similar (so that the entry in Fig. 2(B) is large). Given the precision of our theoretical predictions, we expect analyses of the equations derived here will lead to many nontrivial insights about recurrent (and other) neural network behavior in practice, which we leave for future work. Examples and Extensions: A Brief Guide to the Appendix Appendix B contains a plethora of worked-out examples of the kernel computation for different architectures, starting from the known case of MLP to the new results of RNN (as shown in Fig. 2), GRU, batchnorm, and others. At this point, we recommend the reader to follow along some of those examples to solidify the understanding of Theorem 5.4. A Master Theorem for NETSOR+ can be similarly proved. This is stated in Appendix C and can be proved easily given the proof of Theorem 5.4; see Appendix I. Appendix D works out examples of kernel computations for layernorm and transformer, which can only be expressed through NETSOR+ . Fig. 3 illustrates the kernels of simple RNN, GRU, transformer, and a batchnorm+ReLU network, and conÔ¨Årms that the Ô¨Ånite width simulations tend to the inÔ¨Ånite-width, theoretical kernels. We also discuss different variants of NETSOR and NETSOR+ in Appendix E which trade off syntacti- cal simplicity with ease of use, but are semantically equivalent to NETSOR or NETSOR+ . Appendix F discusses the case when the dimensions of a program need not be equal. With the appropriate setup, a Master Theorem in this case can be proved similarly (Theorem F.4). 6 Related Works NN-GP Correspondence Many works have observed the neural network-Gaussian process corre- spondence (NN-GP correspondence) for subsets of standard architectures [56, 34, 22, 13, 37, 40, 43]. Others have exploited this NN-GP correspondence implicitly or explicitly to build new models [11, 33, 12, 57, 58, 7, 54, 32, 4, 6, 18, 43]. In particular, by directly converting NN to GP using this correspondence, Lee et al. [37] constructed the state-of-the-art (SOTA) permutation-invariant GP on MNIST, and Novak et al. [43] was until recently SOTA on CIFAR10 for any GP with untrainable kernel. Additionally, the NN-GP correspondence has led to new understanding of neural network training and generalization [42, 53, 61]. In this paper, we generalized the NN-GP correspondence to standard architectures and very general nonlinearities (controlled functions; see DeÔ¨Ånition 5.3). In contrast, Matthews et al. [40] requires œï to be linearly bounded in norm; Daniely et al. [13] requires œï be twice-differentiable with |œï|, |œï‚Ä≤|, |œï‚Ä≤‚Ä≤| 9 all bounded, or that œï = ReLU; and a sufÔ¨Åcient condition given in Novak et al. [43] is that œï‚Ä≤ exists and is bounded by exp(O(x2‚àíœµ)), though it is unclear how the more general set of 3 conditions given there (in their section E.4) compares with ours. Signal Propagation in Neural Networks A long line of work starting with Glorot and Bengio [20] and He et al. [23] studies the effect of initialization in deep neural networks [46, 50, 63, 62, 21, 9, 64, 45], for example, what is the best initialization scheme to avoid gradient vanishing? These works apply the same calculations of covariances as we do for calculating Œ£ here, though in a much more restricted set of architectures, and they are typically more concerned with the dynamics of such covariances with depth. Reservoir Computing In reservoir computing [30, 39, 51], sequence processing is typically done by a randomly initialized recurrent neural network. A sequence of inputs is fed step by step into the network, and a Ô¨Ånal readout layer transforms the random RNN‚Äôs state into an output. The only trainable parameters are the readout layer, but not the random RNN itself. Thus, in the inÔ¨Ånite-width limit, reservoir computing corresponds exactly to GP inference with the RNN kernel computed in Appendix B.2. 7 Conclusion We formulated the notion of Gaussian process with variable-dimensional outputs and showed that randomly initialized, wide feedforward and recurrent neural networks of standard architectures converge in distribution to Gaussian processes in such a sense. This signiÔ¨Åcantly generalizes prior work on the NN-GP correspondence. We did so by introducing NETSOR , a language for expressing computation common in deep learning, including neural networks of standard architecture, along with a theorem (Theorem 5.4) characterizing the behavior of a NETSOR program as its tensors are randomized and their dimensions tend to inÔ¨Ånity; many examples and extensions are exhibited in the appendix. Finally, we empirically veriÔ¨Åed our theory for simple RNN, GRU, transformer, and batchnorm (Fig. 3) and open-sourced implementations of the corresponding inÔ¨Ånite-width limit kernels at github.com/thegregyang/GP4A. In the next paper in this series, we will introduce a more powerful version of tensor program that allows matrix transposes, and use this tool to compute Neural Tangent Kernel [29] for any architecture. 10 Acknowlegements I‚Äôm very thankful for my buddy Hadi Salman who is always ready to help and who donated a lot of time helping me write the detailed examples for MLP and RNN kernels. I‚Äôd also like to thank Mimee Xu who read the Ô¨Årst versions of this paper and provided valuable feedback. Finally, allow me to express my appreciation for myriads of friends and collaborators who have helped me improve this paper in one way or another: Sam Schoenholz, Yihe Dong, Judy Shen, Alessandro Sordoni, Huishuai Zhang, George Phillip, Vinay Rao, Sebastien Bubeck, Zeyuan Allen-Zhu, Kyle Aitkens, Chunyuan Li, Alex Polozov, Ilya Razenshteyn, Jianfeng Gao, Pengchuan Zhang, Jascha Sohl-Dickstein, Jeffrey Pennington, and others. References [1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer Normalization. arXiv:1607.06450 [cs, stat], July 2016. URL http://arxiv.org/abs/1607.06450. 00329 arXiv: 1607.06450. [2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural Machine Translation by Jointly Learning to Align and Translate. arXiv:1409.0473 [cs, stat], September 2014. URL http://arxiv.org/abs/1409.0473. arXiv: 1409.0473. [3] Mohsen Bayati and Andrea Montanari. The dynamics of message passing on dense graphs, with applications to compressed sensing. IEEE Transactions on Information Theory, 57(2): 764‚Äì785, February 2011. ISSN 0018-9448, 1557-9654. doi: 10.1109/TIT.2010.2094817. URL http://arxiv.org/abs/1001.3448. arXiv: 1001.3448. [4] Kenneth Blomqvist, Samuel Kaski, and Markus Heinonen. Deep convolutional Gaussian processes. arXiv preprint arXiv:1810.03052, 2018. [5] Erwin Bolthausen. An iterative construction of solutions of the TAP equations for the Sherrington-Kirkpatrick model. arXiv:1201.2891 [cond-mat, physics:math-ph], January 2012. URL http://arxiv.org/abs/1201.2891. arXiv: 1201.2891. [6] Anastasia Borovykh. A gaussian process perspective on convolutional neural networks. arXiv preprint arXiv:1810.10798, 2018. [7] John Bradshaw, Alexander G de G Matthews, and Zoubin Ghahramani. Adversarial examples, uncertainty, and transfer testing robustness in gaussian process hybrid deep networks. arXiv preprint arXiv:1707.02476, 2017. [8] Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral Networks and Locally Connected Networks on Graphs. arXiv:1312.6203 [cs], December 2013. [9] Minmin Chen, Jeffrey Pennington, and Samuel Schoenholz. Dynamical Isometry and a Mean Field Theory of RNNs: Gating Enables Signal Propagation in Recurrent Neural Networks. In Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 873‚Äì882, Stockholmsm√§ssan, Stockholm Sweden, July 2018. PMLR. URL http://proceedings.mlr.press/v80/chen18i.html. [10] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning Phrase Representations using RNN Encoder- Decoder for Statistical Machine Translation. arXiv:1406.1078 [cs, stat], June 2014. URL http://arxiv.org/abs/1406.1078. arXiv: 1406.1078. [11] Youngmin Cho and Lawrence K. Saul. Kernel methods for deep learning. In Advances in neural information processing systems, pages 342‚Äì350, 2009. URL http://papers.nips. cc/paper/3628-kernel-methods-for-deep-learning. [12] Andreas Damianou and Neil Lawrence. Deep gaussian processes. In ArtiÔ¨Åcial Intelligence and Statistics, pages 207‚Äì215, 2013. 11 [13] Amit Daniely, Roy Frostig, and Yoram Singer. Toward Deeper Understanding of Neural Networks: The Power of Initialization and a Dual View on Expressivity. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems 29, pages 2253‚Äì2261. Curran Associates, Inc., 2016. [14] Micha√´l Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering. arXiv:1606.09375 [cs, stat], June 2016. [15] David K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel, Alan Aspuru-Guzik, and Ryan P Adams. Convolutional Networks on Graphs for Learning Molecular Fingerprints. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 2224‚Äì2232. Curran Associates, Inc., 2015. [16] Kunihiko Fukushima. Cognitron: A self-organizing multilayered neural network. Biological cybernetics, 20(3-4):121‚Äì136, 1975. [17] Kunihiko Fukushima and Sei Miyake. Neocognitron: A self-organizing neural network model for a mechanism of visual pattern recognition. In Competition and cooperation in neural nets, pages 267‚Äì285. Springer, 1982. [18] Adri√† Garriga-Alonso, Laurence Aitchison, and Carl Edward Rasmussen. Deep Convolutional Networks as shallow Gaussian Processes. arXiv:1808.05587 [cs, stat], August 2018. URL http://arxiv.org/abs/1808.05587. arXiv: 1808.05587. [19] Alan Genz, Frank Bretz, Tetsuhisa Miwa, Xuefei Mi, Friedrich Leisch, Fabian Scheipl, and Torsten Hothorn. mvtnorm: Multivariate Normal and t Distributions, 2019. URL https: //CRAN.R-project.org/package=mvtnorm. R package version 1.0-11. [20] Xavier Glorot and Yoshua Bengio. Understanding the difÔ¨Åculty of training deep feedforward neural networks. In Yee Whye Teh and Mike Titterington, editors, Proceedings of the Thirteenth International Conference on ArtiÔ¨Åcial Intelligence and Statistics, volume 9 of Proceedings of Machine Learning Research, pages 249‚Äì256, Chia Laguna Resort, Sardinia, Italy, May 2010. PMLR. URL http://proceedings.mlr.press/v9/glorot10a.html. 02641. [21] Boris Hanin and David Rolnick. How to Start Training: The Effect of Initialization and Architecture. arXiv:1803.01719 [cs, stat], March 2018. URL http://arxiv.org/abs/1803. 01719. arXiv: 1803.01719. [22] Tamir Hazan and Tommi Jaakkola. Steps Toward Deep Kernel Methods from InÔ¨Ånite Neural Net- works. arXiv:1508.05133 [cs], August 2015. URL http://arxiv.org/abs/1508.05133. arXiv: 1508.05133. [23] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectiÔ¨Åers: Surpassing human-level performance on imagenet classiÔ¨Åcation. In Pro- ceedings of the IEEE international conference on computer vision, pages 1026‚Äì 1034, 2015. URL http://www.cv-foundation.org/openaccess/content_iccv_2015/ html/He_Delving_Deep_into_ICCV_2015_paper.html. [24] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image Recognition. pages 770‚Äì778, 2016. URL https://www.cv-foundation.org/ openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_ paper.html. [25] Mikael Henaff, Joan Bruna, and Yann LeCun. Deep Convolutional Networks on Graph- Structured Data. arXiv:1506.05163 [cs], June 2015. [26] Sepp Hochreiter and J√ºrgen Schmidhuber. Long Short-Term Memory. Neural Comput., 9 (8):1735‚Äì1780, November 1997. ISSN 0899-7667. doi: 10.1162/neco.1997.9.8.1735. URL http://dx.doi.org/10.1162/neco.1997.9.8.1735. [27] Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Weinberger. Densely Connected Convolutional Networks. arXiv:1608.06993 [cs], August 2016. URL http://arxiv.org/ abs/1608.06993. arXiv: 1608.06993. 12 [28] Sergey Ioffe and Christian Szegedy. Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. In PMLR, pages 448‚Äì456, June 2015. URL http: //proceedings.mlr.press/v37/ioffe15.html. [29] Arthur Jacot, Franck Gabriel, and Cl√©ment Hongler. Neural Tangent Kernel: Convergence and Generalization in Neural Networks. arXiv:1806.07572 [cs, math, stat], June 2018. URL http://arxiv.org/abs/1806.07572. 00000 arXiv: 1806.07572. [30] Herbert Jaeger. Echo state network. Scholarpedia, 2(9):2330, September 2007. ISSN 1941-6016. doi: 10.4249/scholarpedia.2330. [31] Thomas N. Kipf and Max Welling. Semi-Supervised ClassiÔ¨Åcation with Graph Convolutional Networks. arXiv:1609.02907 [cs, stat], September 2016. [32] Vinayak Kumar, Vaibhav Singh, PK Srijith, and Andreas Damianou. Deep Gaussian Processes with Convolutional Kernels. arXiv preprint arXiv:1806.01655, 2018. [33] Neil D Lawrence and Andrew J Moore. Hierarchical Gaussian process latent variable models. In Proceedings of the 24th international conference on Machine learning, pages 481‚Äì488. ACM, 2007. [34] Nicolas Le Roux and Yoshua Bengio. Continuous neural networks. In ArtiÔ¨Åcial Intelligence and Statistics, pages 404‚Äì411, 2007. [35] Yann LeCun, L√©on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278‚Äì2324, 1998. [36] Yann LeCun, Patrick Haffner, L√©on Bottou, and Yoshua Bengio. Object recognition with gradient-based learning. In Shape, contour and grouping in computer vision, pages 319‚Äì345. Springer, 1999. [37] Jaehoon Lee, Yasaman Bahri, Roman Novak, Sam Schoenholz, Jeffrey Pennington, and Jascha Sohl-dickstein. Deep Neural Networks as Gaussian Processes. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=B1EA-M-0Z. [38] Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated Graph Sequence Neural Networks. arXiv:1511.05493 [cs, stat], November 2015. [39] Wolfgang Maass, Thomas Natschl√§ger, and Henry Markram. Real-Time Computing Without Stable States: A New Framework for Neural Computation Based on Perturbations. Neural Computation, 14(11):2531‚Äì2560, November 2002. ISSN 0899-7667, 1530-888X. doi: 10.1162/ 089976602760407955. [40] Alexander G. de G. Matthews, Jiri Hron, Mark Rowland, Richard E. Turner, and Zoubin Ghahramani. Gaussian Process Behaviour in Wide Deep Neural Networks. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum? id=H1-nGgWC-. [41] Radford M Neal. BAYESIAN LEARNING FOR NEURAL NETWORKS. PhD Thesis, University of Toronto, 1995. [42] Roman Novak, Yasaman Bahri, Daniel A. AbolaÔ¨Åa, Jeffrey Pennington, and Jascha Sohl- Dickstein. Sensitivity and Generalization in Neural Networks: an Empirical Study. In Inter- national Conference on Learning Representations, 2018. URL https://openreview.net/ forum?id=HJC2SzZCW. [43] Roman Novak, Lechao Xiao, Jaehoon Lee, Yasaman Bahri, Daniel A AbolaÔ¨Åa, Jeffrey Penning- ton, and Jascha Sohl-Dickstein. Bayesian Deep Convolutional Networks with Many Channels are Gaussian Processes. arXiv preprint arXiv:1810.05148, 2018. [44] Jeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global Vectors for Word Representation. pages 1532‚Äì1543. Association for Computational Linguistics, 2014. doi: 10.3115/v1/D14-1162. URL http://aclweb.org/anthology/D14-1162. 04219. 13 [45] Jeffrey Pennington, Samuel Schoenholz, and Surya Ganguli. Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30, pages 4788‚Äì4798. Curran Associates, Inc., 2017. 00004. [46] Ben Poole, Subhaneil Lahiri, Maithreyi Raghu, Jascha Sohl-Dickstein, and Surya Ganguli. Exponential expressivity in deep neural networks through transient chaos. In Advances In Neural Information Processing Systems, pages 3360‚Äì3368, 2016. 00047. [47] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning internal represen- tations by error propagation. Technical report, California Univ San Diego La Jolla Inst for Cognitive Science, 1985. [48] Nico Schl√∂mer. QuadPy: Numerical integration (quadrature, cubature) in Python, 2016‚Äì. URL https://github.com/nschloe/quadpy. [Online; accessed <today>]. [49] Samuel S. Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. Deep Infor- mation Propagation. arXiv:1611.01232 [cs, stat], November 2016. URL http://arxiv.org/ abs/1611.01232. arXiv: 1611.01232. [50] Samuel S. Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. Deep Infor- mation Propagation. 2017. URL https://openreview.net/pdf?id=H1W1UN9gg. [51] Benjamin Schrauwen. An overview of reservoir computing: Theory, applications and imple- mentations. page 12, 2007. [52] Elias M. Stein and Rami Shakarchi. Functional analysis: introduction to further topics in analysis. Princeton University Press, 2011. [53] Guillermo Valle-P√©rez, Chico Q. Camargo, and Ard A. Louis. Deep learning generalizes because the parameter-function map is biased towards simple functions. arXiv:1805.08522 [cs, stat], May 2018. [54] Mark van der Wilk, Carl Edward Rasmussen, and James Hensman. Convolutional Gaussian Processes. In Advances in Neural Information Processing Systems 30, pages 2849‚Äì2858, 2017. [55] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \\Lukasz Kaiser, and Illia Polosukhin. Attention is All You Need. In Advances in Neural Information Processing Systems, pages 5998‚Äì6008, 2017. [56] Christopher K I Williams. Computing with InÔ¨Ånite Networks. In Advances in neural information processing systems, page 7, 1997. [57] Andrew G Wilson, Zhiting Hu, Ruslan R Salakhutdinov, and Eric P Xing. Stochastic Variational Deep Kernel Learning. In Advances in Neural Information Processing Systems, pages 2586‚Äì 2594, 2016. [58] Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, and Eric P Xing. Deep kernel learning. In ArtiÔ¨Åcial Intelligence and Statistics, pages 370‚Äì378, 2016. [59] Lechao Xiao, Yasaman Bahri, Jascha Sohl-Dickstein, Samuel Schoenholz, and Jeffrey Pen- nington. Dynamical Isometry and a Mean Field Theory of CNNs: How to Train 10,000- Layer Vanilla Convolutional Neural Networks. In Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Re- search, pages 5393‚Äì5402, Stockholmsm√§ssan, Stockholm Sweden, July 2018. PMLR. URL http://proceedings.mlr.press/v80/xiao18a.html. [60] Greg Yang. Scaling Limits of Wide Neural Networks with Weight Sharing: Gaussian Process Behavior, Gradient Independence, and Neural Tangent Kernel Derivation. arXiv:1902.04760 [cond-mat, physics:math-ph, stat], February 2019. [61] Greg Yang and Hadi Salman. A Fine-Grained Spectral Perspective on Neural Networks. July 2019. 14 [62] Greg Yang and Sam S. Schoenholz. Deep Mean Field Theory: Layerwise Variance and Width Variation as Methods to Control Gradient Explosion. February 2018. URL https: //openreview.net/forum?id=rJGY8GbR-. [63] Greg Yang and Samuel S. Schoenholz. Mean Field Residual Network: On the Edge of Chaos. In Advances in neural information processing systems, 2017. [64] Greg Yang, Jeffrey Pennington, Vinay Rao, Jascha Sohl-Dickstein, and Samuel S. Schoenholz. A Mean Field Theory of Batch Normalization. September 2018. URL https://openreview. net/forum?id=SyMDXnCcF7. [65] Greg Yang, Jeffrey Pennington, Vinay Rao, Jascha Sohl-Dickstein, and Samuel S. Schoenholz. A Mean Field Theory of Batch Normalization. arXiv:1902.08129 [cond-mat], February 2019. URL http://arxiv.org/abs/1902.08129. arXiv: 1902.08129. 15 NETSOR program 3 Batchnorm (with Fully Connected Layers) over Multiple Batches // For œï : R ‚Üí R, h ‚àà RB, // let Àúœï(h) = œï((h ‚àí ŒΩ(h))/œÉ(h)), // where ŒΩ(h) = 1 B ‚àëB i=1 hi, // œÉ(h) = ‚àö 1 B ‚àëB i=1(hi ‚àí ŒΩ(h))2. // Let Àúœïi, i ‚àà [B], be the ith coordinate of Àúœï. // Embeddings of k batches of inputs // with ath batch having size Ba Input: {W 1xia : G(n)}a‚àà[k],i‚àà[Ba] Input: W 2, . . . , W L : A(n, n) // Readout layer Input: v : G(n) for a ‚àà [k] and i ‚àà [Ba] do x1ia := Àúœïi(W 1x1a, . . . , W 1xBaa) : H(n) end for for l = 2, . . . , L do for a ‚àà [k] and i ‚àà [Ba] do h lia := W lxl‚àí1,ia : G(n) end for for a ‚àà [k] and i ‚àà [Ba] do xlia := Àúœïi(h l1a, . . . , hlBaa) : H(n) end for end for Output: {v‚ä§xLia/ ‚àön}a‚àà[k],i‚àà[Ba] A Writing Standard Architectures in NETSOR In this section, we showcase example programs for batchnorm, skip connection, convolution, pooling, GRU/LSTM, and (scaled) attention. In most cases, we demonstrate the computation on a single input batch, image, or sequence. Generalization to multiple inputs is obvious and follows the pattern of Program 2. It should also be apparent that any composition of these gadgets can be expressed in NETSOR . Additionally, observe that all nonlinearities used in these programs are controlled (DeÔ¨Ånition 5.3) or parameter-controlled (DeÔ¨Ånition C.1), so that Theorem 5.4 or Theorem C.4 applies. Also we remark that the GP convergence results for batchnorm, GRU/LSTM (and RNNs in general), and attention are new. Batchnorm, Fully-Connected Let Àúœï : RB ‚Üí RB, Àúœï(h) def = œï ( h ‚àí ŒΩ(h) œÉ(h) ) , where ŒΩ(h) def = 1 B B‚àë i=1 hi, œÉ(h) def = v u u ‚àö 1 B B‚àë i=1(hi ‚àí ŒΩ(h))2, (4) be batchnorm followed by coordinatewise nonlinearity œï, where h ‚àà RB should be interpreted as a single neuron across a batch, and ŒΩ and œÉ are the batch mean and standard deviations. Here, B should be thought of as Ô¨Åxed while n ‚Üí ‚àû. Then, given a batch of G-vars y1, . . . , yB : G(n) (for example, they could be the preactivations after applying a linear layer), we can express the application of batchnorm via Nonlin as x1 := Àúœï1(y1, . . . , yB), . . . , xB := ÀúœïB(y1, . . . , yB), (5) producing B H-vars x1, . . . , xB : H(n). Program 3 expresses more generally the computation of a batchnorm network on multiple batches and over multiple layers. Here, each ‚Äúfor-loop‚Äù is just shorthand for the corresponding unrolled program (since DeÔ¨Ånition 4.1 doesn‚Äôt allow for-loops). Skip Connection If x : H(n) is previous layer activation, and we have weights W : A(n, n) and bias b : G(n), then we can express skip connection as Àúh := W x MatMul h := Àúh + b LinComb ¬Øx := x + œï(h) Nonlin (Graph) Convolution Consider a convolution network with L layers, with layer l having nl channels, with kernel positions kerl (for example, for a 3 √ó 5 kernel, kerl = [3] √ó [5]), and with feature map pixel positions pos l (for example, for a 128 √ó 256 feature map, pos l = [128] √ó [256]). 16 NETSOR program 4 2-Layer Convolutional Network with Global Average Pooling Input: {W 1 j xi+j : G(n1)}j‚ààker1,i‚ààpos1 s.t. i+j‚ààpos0 Input: {W 2 j : A(n2, n 1)}j‚ààker2 // Readout weights Input: v : G(n2) // Layer 1 Convolution for i ‚àà pos1 do // Directly use input embeddings // LinComb // Sum is over all j ‚àà ker1 such that // there is i ‚Ä≤ ‚àà pos 0 with i ‚Ä≤ = j + i h 1 i := ‚àë j W 1 j xi+j : G(n1) x1 i := œï(h 1 i ) : H(n1) end for // Layer 2 Convolution for j ‚àà ker2, i ‚àà pos 1 s.t. i + j ‚àà pos2 do // Convolution Weights Multiplication // MatMul h 2 i;j := W 1 j x1 i+j : G(n2) end for for i ‚àà pos 2 do // Sum is over all j ‚àà ker2 such that // there is i ‚Ä≤ ‚àà pos 1 with i‚Ä≤ = j + i h 2 i := ‚àë j h 2 i;j : G(n2) end for // Nonlinearity & Global Average Pooling ¬Øx2 := 1 |pos2| ‚àë i‚ààpos2 œï(h 2 i ) : H(n2) Output: v‚ä§ ¬Øx2/‚àön2 Then we can describe the weights of a stride-1 convolution as {W l iŒ±Œ≤}l‚àà[L],i‚ààkerl,Œ±‚àà[nl],Œ≤‚àà[nl‚àí1], so that for each i ‚àà kerl, W l i ‚àà Rnl√ónl‚àí1 is a dense matrix. Further, suppose x is an image input to the network with pixel coordinates pos 0 and n0 channels, {xiŒ±}i‚ààpos0,Œ±‚àà[n0], so that xi is a vector of dimension n0. Then the application of the convolution W 1 on x is given by h 1 i := (W 1 ‚àó x)i = ‚àë j W 1 j xi+j ‚àà Rn 1 , h 1 iŒ± = (W 1 ‚àó x)iŒ± = ‚àë jŒ≤ W 1 jŒ±Œ≤xi+j,Œ≤ ‚àà R, where the sums are over j ‚àà ker1, i + j ‚àà pos 0, and Œ≤ ‚àà [n0]. For higher layers we have similar formulas, with the only difference that we treat W 1 j xi+j as input G-vars but treat W l+1 j x l i+j, l ‚â• 2, as a MatMul operation. Thus, our framework can express CNN computations by expressing the individual matrix multiplications Wjxi+j and reusing the matrices Wj. Program 4 shows a simple example program, and Program 7 shows a full-Ô¨Çedged example over multiple inputs. Again, each ‚Äúfor-loop‚Äù is just shorthand for the corresponding unrolled program. Higher stride and general graph convolution can be expressed likewise. Pooling We continue the notational scheme of the exposition on convolutions above. Given the feature maps of layer l, {xiŒ±}i‚ààposl,Œ±‚àà[nl], global average pooling produces a single vector ¬Øx = {¬ØxŒ±}Œ±‚àà[nl] given by ¬Øx := 1 |posl| ‚àë i‚ààposl xi, using LinComb. See Program 4 for an example in combination with convolutional layers. We can similarly express max pooling. Suppose pos l = [2k] √ó [2k]. Then max pooling with, for example, 2 √ó 2 kernel and stride of 2 would produce {ÀÜxjŒ±}j‚àà[k]√ó[k],Œ±‚àà[nl], with ÀÜxjŒ± := max({x2j+i,Œ±}i‚àà{0,1}√ó{0,1},2j+i‚ààposl ), using Nonlin. GRU and LSTM We present a program expressing GRU computation in Program 5; the program for LSTM is similar and is omitted. The overall pattern is similar to the program for simple RNN (Program 2), but with a crucial subtlety regarding the gating mechanism. In Program 5, Àúzs, Àúrs, Àúh s are G-vars, but the gates œÉ(Àúzs), œÉ(Àúrs) (where œÉ is sigmoid) and the candidate update œï(Àúh s) are not G-vars. As we can only apply Nonlin to G-vars, this requires us to unroll the deÔ¨Ånition of h s to be a function of only G-vars. However, Appendix E presents expanded, but semantically equivalent versions of NETSOR which allow a more succinct representation of the same computation; see Program 11. Finally, Program 8 presents a full-Ô¨Çedged program with multiple input sequences. 17 NETSOR program 5 GRU, with Gating Function œÉ and Activation Function œï // Embeddings of input sequence Input: Uzx1, . . . , UzxT : G(n) Input: Urx1, . . . , UrxT : G(n) Input: Uhx 1, . . . , UhxT : G(n) // Parameters Input: Wz, Wr, Wh : A(n, n) Input: bz, br, bh : G(n) // Initial GRU state Input: h 0 : G(n) // Readout layer Input: v : G(n) // Time step 1 h 1 z := Wzh 0 : G(n) Àúz1 := h 1 z + Uzx1 + bz : G(n) h 1 r := Wrh0 : G(n) Àúr1 := h 1 r + Urx1 + br : G(n) // œÉ is gating function, typically sigmoid; applying Nonlin ÀÜh 0 := h 0 ‚äô œÉ(Àúr1) : H(n) h 1 h := WhÀÜh 0 : G(n) Àúh 1 := h 1 h + Uhx1 + bh : G(n) // Apply Nonlin // œï is activation function, typically tanh h 1 := (1 ‚àí œÉ(Àúz1)) ‚äô h 0 + œÉ(Àúz1) ‚äô œï(Àúh 1) : H(n) // Time step 2 h 2 z := Wzh 1 : G(n) Àúz2 := h 2 z + Uzx2 + bz : G(n) h 2 r := Wrh 1 : G(n) Àúr2 := h 2 r + Urx2 + br : G(n) // Morally, ÀÜh 1 = œÉ(Àúr1) ‚äô h 1, but we need to unroll h 1 to apply Nonlin // This can be expressed with more brevity using NETSOR‚ó¶ ; see Remark E.5 ÀÜh 1 := œÉ(Àúr1) ‚äô ((1 ‚àí œÉ(Àúz1)) ‚äô h 0 + œÉ(Àúz1) ‚äô œï(Àúh 1)) : H(n) h 2 h := WhÀÜh 1 : G(n) Àúh 2 := h 2 h + Uhx2 + bh : G(n) // Unrolling h 2 to a coordinatewise function of G-vars h 2 := (1 ‚àí œÉ(Àúz2)) ‚äô (1 ‚àí œÉ(Àúz1)) ‚äô h 0 + (1 ‚àí œÉ(Àúz2)) ‚äô œÉ(Àúz1) ‚äô œï(Àúh 1) + œÉ(Àúz2) ‚äô œï(Àúh 2) : H(n) // Time step 3 ... // Time step T // DeÔ¨Åne ÀúzT , ÀúrT , Àúh T just like above ... // Unrolling h T to a coordinatewise function of G-vars h T := h 0 ‚äô ‚äôT i=1(1 ‚àí œÉ(Àúzi)) + ‚àëT j=1 œï(Àúhj) ‚äô œÉ(Àúzj) ‚äô ‚äôT l=j+1(1 ‚àí œÉ(Àúzl)) : H(n) Output: (v‚ä§h 1/‚àön, . . . , v‚ä§h T /‚àön) Layernorm Layernorm requires the extended rule Nonlin + to express. Recall for x ‚àà Rn (thought of as the vector of activations with one entry per neuron in a layer; contrast with batchnorm), Layernorm(x) = x‚àíŒΩ(x) œÉ(x) where ŒΩ(x) = 1 n ‚àë Œ± xŒ± and œÉ(x) = ‚àö 1 n ‚àën Œ±=1(xŒ± ‚àí ŒΩ(x))2. As we will see, ŒΩ(x) and œÉ(x) will both converge a.s. to a deterministic limit. Thus Layernorm(x) is just a linear combination of x with the constant-1s vector (considered as a input G-var), with (roughly 18 deterministic) coefÔ¨Åcients ¬µ(x) and œÉ(x). This is expressible using the Nonlin + rule: Layernorm(x) := œà(x; ŒΩ(x), œÉ(x)), where œà(x; Œ∏1, Œ∏2) def = x ‚àí Œ∏1 Œ∏2 . Similarly, if layernorm is followed up by a nonlinearity œï, then we can express œï(Layernorm(x)) := œà(x; ŒΩ(x), œÉ(x)), where œà(x; Œ∏1, Œ∏2) def = œï ( x ‚àí Œ∏1 Œ∏2 ) . If layernorm is preceded by a nonlinearity, then likewise we can write Layernorm(œï(x)) := œà(x; ŒΩ(œï(x)), œÉ(œï(x))), where œà(x; Œ∏1, Œ∏2) def = œï(x) ‚àí Œ∏1 Œ∏2 . Scaled Attention Scaled attention requires the extended rule Nonlin + to express. Consider the following version of scaled attention: Given a query vector q ‚àà Rn, keys ki ‚àà Rn for each i ‚àà [r], and corresponding values vi ‚àà Rm, i ‚àà [r], we can deÔ¨Åne the following scaled attention Attention(q, {ki}i, {vi}i) def = r‚àë i=1 aivi, ai def = SoftMax(q‚ä§k1/n, . . . , q‚ä§kr/n)i. If q, ki are given as H-vars in a NETSOR program, then Theorem 5.4 will show that q‚ä§ki/n converges almost surely to a deterministic limit, so that each ai converges likewise. If each vi = œà(gi) for some G-var gi and Ô¨Åxed nonlinearity œà, then attention can be expressed as follows using Nonlin +: Attention(q, {ki}i, {vi}i) = œï(g1, . . . , gr; a1, . . . , ar) where œï(x1, . . . , xr; Œ∏1, . . . , Œ∏r) def = Œ∏1œà(x1) + ¬∑ ¬∑ ¬∑ + Œ∏rœà(xr). More complicated variants, such as allowing œà to take multiple G-vars as inputs, is likewise express- ible. When used as part of a decoder, a mask needs to be placed on the pre-softmax values so that no attention is paid to tokens in the future. This is aptly called masked attention and is given by the following formula: for j ‚àà [r], MaskedAttentionj(q, {ki} r i=1, {vi} r i=1) = r‚àë i=1 a j i vi, where a j i = SoftMax(q‚ä§k1/n, . . . , q‚ä§kj/n, ‚àí‚àû, . . . , ‚àí‚àû)i. It can obviously be expressed in NETSOR in the same fashion as (non-masked) attention. Note that Vaswani et al. [55] scales the pre-softmax values by 1/ ‚àön instead of 1/n: ai = SoftMax(q‚ä§k1/‚àön, . . . , q‚ä§kr/‚àön)i. This is useful if qŒ±ki Œ± has mean zero (averaged over Œ±) for each i, so that q‚ä§ki/‚àön becomes roughly Gaussian, whereas q‚ä§ki/n converges to 0. However, when the zero-mean condition doesn‚Äôt hold, q‚ä§ki/ ‚àön would only blow up to inÔ¨Ånity. B Example GP Kernel Computation with NETSOR In this section, we show how to compute the GP kernel of different architecture, following the recursive construction of Theorem 5.4. First, we review the V-transform of a nonlinearity. DeÔ¨Ånition B.1. Given a multivariate nonlinearity Œ¶ : RB ‚Üí RB, its V-transform VŒ¶ is a function taking B √ó B positive semideÔ¨Ånite matrices to B √ó B positive semideÔ¨Ånite matrices, and is given by the following formula VŒ¶(K) def = E z‚àºN (0,K) Œ¶(z)Œ¶(z) ‚ä§. When œï : R ‚Üí R, we take Vœï to be V-transform of the RB ‚Üí RB function that applies œï to each coordinate. 19 We collect below some of the common V-transforms. Here we describe the V-transforms using the function notation of kernels, but we shall freely switch between the function notation and the matrix notation in what follows. Fact B.2 ([11]). For any kernel K, VReLU(K)(x, x ‚Ä≤) = 1 2œÄ ( ‚àö1 ‚àí c2 + (œÄ ‚àí arccos c)c) ‚àöK(x, x)K(x‚Ä≤, x‚Ä≤) VReLU‚Ä≤(K)(x, x ‚Ä≤) = 1 2œÄ (œÄ ‚àí arccos c) where c = K(x, x ‚Ä≤)/ ‚àöK(x, x)K(x‚Ä≤, x‚Ä≤). Fact B.3 ([41]). For any kernel K, Verf (K)(x, x ‚Ä≤) = 2 œÄ arcsin K(x, x ‚Ä≤) ‚àö(K(x, x) + 0.5)(K(x‚Ä≤, x‚Ä≤) + 0.5) Verf ‚Ä≤(K)(x, x ‚Ä≤) = 4 œÄ‚àö(1 + 2K(x, x))(1 + 2K(x‚Ä≤, x‚Ä≤)) ‚àí 4K(x, x‚Ä≤)2 . Fact B.4. Let œï(x) = exp(x/œÉ) for some œÉ > 0. For any kernel K, Vœï(K)(x, x ‚Ä≤) = exp ( K(x, x) + 2K(x, x ‚Ä≤) + K(x‚Ä≤, x ‚Ä≤) 2œÉ2 ) . In Appendix B.3, we will also discuss the V-transform of batchnorm (which has been derived in [65]). B.1 MLP The kernel computation of a multilayer perceptron is by now well-known [46, 50, 37]. In this section, we work out an example of how to recover the usual kernel computation via tensor programs. B.1.1 MLP with Single Input (Program 1) The aim here is to illustrate step-by-step applications of Eq. (2) for Program 1, but note that in practice, it is often more convenient to Ô¨Ånd some bulk recursive or compositional structure of Œ£, and to leverage that structure for computing Œ£ (see Appendix B.2 for an example). For simplicity, assume the nonlinearities œï are ReLUs and the widths n1 = n2 = n to satisfy Assumption 5.1. By Corollary 5.5, we know that the output of the MLP, vT x2/ ‚àön, is distributed as a Gaussian with mean 0 and variance of œÉ2 Ez œï2(z), where z ‚àº N (¬µ(h 2), Œ£(h 2, h 2)), and h 2 is the layer 2 preactivation (also a G-var) in Program 1. Therefore, all we need is to compute ¬µ(h 2) and Œ£(h 2, h 2) using (2), which requires the calculation of ¬µ and Œ£ for possibly all the other G-vars in Program 1 due to the recursive nature of (2). In this example, we shall compute all entries of ¬µ and Œ£ explicitly as a demonstration. We do so for each G-var in the order of appearance in the NETSOR program. Explicitly, the ordering is G = (W 1x, b 1, b 2, v, h1, Àúh 2, h 2). The input G-vars are W 1x, b 1, b 2, and v, and the sole input A-var is W 2. Setup In the fashion of typical Glorot initializations, we shall sample the parameters W 1, W 2, b 1, b 2 of the network as W 1 Œ±Œ≤ ‚àº N (0, 1/ dim x), W 2 Œ±Œ≤ ‚àº N (0, 1/n), vŒ± ‚àº N (0, 1), b 1 Œ± ‚àº N (0, 1), b 2 Œ± ‚àº N (0, 1). This corresponds, in the context of Assumption 5.1, to setting œÉW 2 = 1, ¬µ in = 0 (in particular, ¬µ(W 1x) = ¬µ in(W 1x) = 0 due to the sampling of W 1), and Œ£in as follows: 20 Œ£in(W 1x, W 1x) = 1 Œ£in(b1, b 1) = 1 Œ£in(b2, b 2) = 1 Œ£in(b2, b 1) = 0 Œ£in(v, v) = 1 Œ£in(bi, W 1x) = 0 for i ‚àà {1, 2} Œ£in(bi, v) = 0 for i ‚àà {1, 2} Œ£in(v, W 1x) = 0 Calculating ¬µ and Œ£ Now we will show a detailed calculation of ¬µ and Œ£ for all of the G-vars G appearing here. By the input G-var case of Eq. (2), Œ£(W 1x, W 1x) = Œ£ in(W 1x, W 1x) = 1 Œ£(b 1, b 1) = Œ£in(b 1, b 1) = 1 Œ£(b 2, b 2) = Œ£in(b 2, b 2) = 1 Œ£(b 1, b 2) = Œ£in(b 2, b 1) = 0 Œ£(v, v) = Œ£in(v, v) = 1 Œ£(W 1x, b i) = Œ£(b i, W 1x) = Œ£ in(bi, W 1x) = 0 for i ‚àà {1, 2} Œ£(v, b i) = Œ£(bi, v) = Œ£ in(bi, v) = 0 for i ‚àà {1, 2} Œ£(W 1x, v) = Œ£(v, W 1x) = Œ£ in(v, W 1x) = 0 Next, we extend ¬µ and Œ£ to h 1, introduced via LinComb by h 1 := W 1x + b1. Note that h 1 is a G-var of the form g = ‚àë i aiyi where a1 = a2 = 1, y1 = W 1x, and y2 = b1. Therefore, by the LinComb case of Eq. (2), ¬µ(h 1) = ¬µ(W 1x) + ¬µ(b1) = 0 Œ£(h 1, W 1x) = Œ£(W 1x, W 1x) + Œ£(b1, W 1x) = 1 + 0 = 1 Œ£(h 1, b 1) = Œ£(W 1x, b 1) + Œ£(b 1, b 1) = 0 + 1 = 1 Œ£(h 1, b 2) = Œ£(W 1x, b 2) + Œ£(b 1, b 2) = 0 + 0 = 0 Œ£(h 1, v) = Œ£(W 1x, v) + Œ£(b1, v) = 0 + 0 = 0 Œ£(h 1, h 1) = Œ£(h 1, W 1x) + Œ£(h 1, b 1) = 1 + 1 = 2. So h 1 is correlated with W 1x and b1 in obvious ways, and is independent from b2 and v. Next, we extend ¬µ and Œ£ to, introduced via MatMul by Àúh 2 := W 2x1. Note that Àúh 2 is a G-var of the form g = W h where W = W 2 is an A-var, and h = x1 is an H-var introduced by x1 := ReLU(h 1). Therefore, by the ‚Äúotherwise‚Äù case of Eq. (2), ¬µ(Àúh 2) = 0 Œ£(Àúh 2, W 1x) = 0 Œ£(Àúh 2, b 1) = 0 Œ£(Àúh 2, b 2) = 0 Œ£(Àúh 2, v) = 0 Œ£(Àúh 2, h 1) = 0 and by the MatMul case of Eq. (2) (setting œï and ¬Øœï there to both be ReLU), Œ£(Àúh 2, Àúh 2) = œÉ2 W 2 E z œï(z) ¬Øœï(z) = E z ReLU(z) 2 = 1 where z ‚àº N (¬µ(h 1), Œ£(h 1, h 1)) = N (0, 2). 21 NETSOR program 6 MLP Computation on a Set of Inputs // Embeddings of inputs Input: W 1x1, . . . , W 1xB : G(n) // Biases across L layers Input: b1, . . . , bL : G(n) // Weights from layer 2 on Input: W 2, . . . , W L : A(n, n) // Readout weights Input: v : G(n) for i = 1, . . . , B do h 1i := W 1xi + b1 : G(n) x1i := œï(h 1i) : H(n) for l = 2, . . . , L do Àúh li := W lxl‚àí1,i : G(n) h li := Àúh li + b l : G(n) xli := œï(h li) : H(n) end for end for Output: (v‚ä§xL1/ ‚àön, . . . , v‚ä§xLB/ ‚àön) Thus, Àúh 2 can be thought of as ‚Äúindependent‚Äù from all other G-vars. Finally, we extend ¬µ and Œ£ to h 2, introduced via LinComb by h 2 := Àúh2 + b2. Note that h 2 is a G-var of the form g = ‚àë i aiyi where a1 = a2 = 1, y1 = Àúh 2, and y2 = b2. Then by the LinComb case of Eq. (2), ¬µ(h 2) = ¬µ(Àúh 2) + ¬µ(b2) = 0 Œ£(h 2, W 1x) = Œ£(Àúh 2, W 1x) + Œ£(b 2, W 1x) = 0 + 0 = 0 Œ£(h 2, b 1) = Œ£(Àúh 2, b 1) + Œ£(b2, b 1) = 0 + 0 = 0 Œ£(h 2, b 2) = Œ£(Àúh 2, b 2) + Œ£(b2, b 2) = 0 + 1 = 1 Œ£(h 2, v) = Œ£(Àúh 2, v) + Œ£(b 2, v) = 0 + 0 = 0 Œ£(h 2, h 1) = Œ£(Àúh 2, h 1) + Œ£(b2, h 1) = 0 Œ£(h 2, Àúh 2) = Œ£(Àúh 2, Àúh 2) + Œ£(b2, Àúh 2) = 1 + 0 = 1 Œ£(h 2, h 2) = Œ£(Àúh 2, h 2) + Œ£(b2, h 2) = Œ£(h 2, Àúh 2) + Œ£(h 2, b 2) = 1 + 1 = 2. Note that h 2 turns out to be ‚Äúindependent‚Äù from h 1, i.e. Œ£(h 2, h 1) = 0, just as one might expect from the mean Ô¨Åeld or the NNGP literature. Distribution of the Program Output We are now done with calculating ¬µ(g) and Œ£(g, g‚Ä≤) for all g, g‚Ä≤ ‚àà G. Recall the output of the program is v‚ä§x2/ ‚àön, where x2 was introduced via Nonlin by x2 := ReLU(h 2). According to Corollary 5.5, the output variance is then given by œÉ2 v E z œï2(z) = E z ReLU(z) 2 = 1, where z ‚àº N (¬µ(h 2), Œ£(h 2, h 2)) = N (0, 2). B.1.2 MLP with Multiple Inputs (Program 6) Now suppose we have an L-layer MLP with B inputs x1, . . . , xB. Program 6 expresses its computa- tion (again, the ‚Äúfor‚Äù loops are shorthands for the unrolled series of assignments). Here we will avoid computing out all values of Œ£ but only those that affect the inÔ¨Ånite-width GP. By Corollary 5.5, the output of Program 6 is distributed as Kij def = Cov ( v‚ä§xLi ‚àön , v‚ä§xLj ‚àön ) = œÉ2 v E Z œï(Z h Li)œï(Z hLj ) (6) where œÉ2 v is the coordinatewise variance of v, Z ‚àº N (¬µ, Œ£), and Z h Li is the component of Z corresponding to h Li (likewise for Z h Lj ). Therefore, we need to compute ¬µ and Œ£ for the G-vars {h L1, . . . , hLB}. 22 Setup Suppose the inputs xi have dimension m. If we sample the neural network parameters in the usual Glorot fashion, W 1 Œ±Œ≤ ‚àº N (0, œÉ2 w/m), W l Œ±Œ≤ ‚àº N (0, œÉ2 w/n), ‚àÄ2 ‚â§ l ‚â§ L, vŒ± ‚àº N (0, œÉ2 v), b l Œ± ‚àº N (0, œÉ2 b ), ‚àÄl ‚àà [L], (7) then we have Œ£in deÔ¨Åned by Œ£in(W 1x i, W 1xj) = œÉ2 wxi‚ä§xj/m, Œ£in(b l, b l) = œÉ2 b , Œ£in(v, v) = œÉ2 v for all i, j ‚àà [B] and l ‚àà [L], and Œ£in(g, g‚Ä≤) = 0 for any other pairs of input G-vars g, g‚Ä≤. On the other hand, ¬µ in(g) = 0 for all input G-vars g. Computing ¬µ From Eq. (2), it is clear that ¬µ in = 0 implies ¬µ = 0. Computing Œ£ Again, our goal is to compute Œ£ restricted to the G-vars {h L1, . . . , hLB}. Lemma B.5. For any l = 2, . . . , L and any i, j ‚àà [B], Œ£(h li, h lj) = œÉ2 w E z1,z2 œï(z1)œï(z2) + œÉ2 b (8) where (z1, z2) ‚àº N (0, Œ£|hl‚àí1,i,hl‚àí1,j ), and (unrolling the restriction notation) Œ£|hl‚àí1,i,hl‚àí1,j = (Œ£(h l‚àí1,i, h l‚àí1,i) Œ£(h l‚àí1,i, h l‚àí1,j) Œ£(h l‚àí1,j, h l‚àí1,i) Œ£(h l‚àí1,j, h l‚àí1,j) ) . Proof. From Program 6, h li is introduced via LinComb by h li := Àúh li + b l. Thus by the LinComb cases of Eq. (2), we have Œ£(h li, h lj) = Œ£(Àúh li, h lj) + Œ£(bl, h lj) = Œ£(Àúh li, Àúh lj) + Œ£(bl, Àúh lj) + Œ£(Àúh li, b l) + Œ£(b l, b l). Now, we cannot pattern match Œ£(bl, Àúh lj) with any of the cases of Eq. (2) other than the ‚Äúotherwise‚Äù case, which means Œ£(bl, Àúh lj) = 0. Likewise, Œ£(Àúh li, b l) = 0. Therefore, Œ£(h li, h lj) = Œ£(Àúh li, Àúh lj) + Œ£(bl, b l) = Œ£(Àúh li, Àúh lj) + œÉ2 b . Now let‚Äôs analyze the Œ£(Àúh li, Àúh lj) term. The G-var Àúh li is introduced via MatMul by Àúh li := W lx l‚àí1,i, where xl‚àí1,i := œï(h l‚àí1,i) and likewise for h lj. By the MatMul case of Eq. (2), we have Œ£(Àúh li, Àúh lj) = œÉ2 w E Z œï(Z h l‚àí1,i )œï(Z h l‚àí1,j ) where Z ‚àº N (¬µ, Œ£). Since the integrand only depends two components of Z, we can simplify the expression as E Z œï(Z h l‚àí1,i)œï(Z h l‚àí1,j ) = E z1,z2 œï(z1)œï(z2), where (z1, z2) ‚àº N (0, Œ£|hl‚àí1,i,hl‚àí1,j ) . Putting it all together, we recover the expression in the claim, as desired. 23 Computing the GP Kernel K Eq. (8) along with Eq. (6) gives all we need to compute K by recursion. If œï has a nice V-transform Vœï, then we can vectorize this equation and obtain the following algorithm (which, again, is by now well-known [46, 50, 37]) Computing MLP kernel on B inputs Consider an L-layer MLP with nonlinearity œï at each layer. Suppose we have B network inputs x1, . . . , xB ‚àà Rm, as in Program 6, and we sample the MLP parameters as in Eq. (7). Then the MLP converges in distribution to a GP on those B inputs, with kernel K computed as follows 1. Initialize K ‚àà RB√óB by Kij ‚Üê œÉ2 wxi‚ä§x j/m 2. For l = 1, . . . , L ‚àí 1, do K ‚Üê œÉ2 wVœï(K) + œÉ2 b 3. Return K ‚Üê œÉ2 vVœï(K) B.2 Simple RNN (Program 2) By Corollary 5.5, we know that the output of Program 2, ( v‚ä§s 11 ‚àön , . . . , v‚ä§s t1 ‚àön , v‚ä§s 12 ‚àön , . . . , v‚ä§s r2 ‚àön ) is, in the large n limit, distributed as a Gaussian with mean 0 and the covariance K where, for any a, b ‚àà {1, 2} (denoting sequence number), Kia,jb def = lim n‚Üí‚àû Cov ( v‚ä§s ia ‚àön , v‚ä§s j b ‚àön ) = œÉ2 v E Z œï(Z h ia )œï(Z h jb ) where Z ‚àº N (¬µ, Œ£), and Z h ia is the component of Z corresponding to h ia and likewise for Z hjb . Therefore, we need to compute ¬µ and Œ£ for the G-vars {h 11, . . . , hs1, h 12, . . . , ht2} in Program 2. Setup Suppose the input tokens x ia to the RNN have dimension m. We will obtain the ¬µ and Œ£ for Program 2 with UŒ±Œ≤ ‚àº N (0, œÉ2 U /m), WŒ±Œ≤ ‚àº N (0, œÉ2 W /n), bŒ± ‚àº N (0, œÉ2 b ), vŒ± ‚àº N (0, œÉ2 v). (9) The randomization of U induces the following covariance structure in the input token embeddings Œ£in(U x, U y) = œÉ2 U x‚ä§y/m (10) for any x, y ‚àà {x i1} t i=1 ‚à™ {xj2}r j=1. For any other pair g, g‚Ä≤ of input G-vars, the sampling implies Œ£in(g, g‚Ä≤) = 0. Additionally, ¬µ in(g) = 0 for all input G-var g. Computing ¬µ In fact, one can quickly see that ¬µ(g) = 0 for all G-vars g, not just the input G-vars. Computing Œ£ By Eq. (2), all {h i1, Àúh i1} t i=1 ‚à™ {h j2, Àúh j2} r j=1 are possibly correlated with each other. They satisfy the following recurrence Lemma B.6. For any a, b ‚àà {1, 2}, we have Œ£(h ia, h jb) = Œ£(Àúhia, Àúh jb) + Œ£in(U xia, U x jb) + œÉ2 b , ‚àÄi, j ‚â• 1 (11) Œ£(Àúhia, Àúh jb) = œÉ2 W E œï(z1)œï(z2), ‚àÄi, j ‚â• 2, (12) where (z1, z2) ‚àº N (0, Œ£|hi‚àí1,a,hj‚àí1,b ), and the base case Œ£(Àúh ia, Àúh jb) = 0 if i ‚â§ 1 or j ‚â§ 1. Here, Œ£|set means the submatrix of Œ£ with rows and columns in set. Note that the notation b appears both as a sequence index and as the bias of the RNN. Since the former will only appear as a superscript and the latter will not, there should be no confusion. 24 Proof. Note that for each i ‚â• 2, h ia is introduced via LinComb by h ia := Àúh ia + U x ia + b, where Àúh ia, U x ia, and b are G-vars. Then by the LinComb case of Eq. (2), Œ£(h ia, h jb) = Œ£(Àúh ia, h jb) + Œ£(U x ia, h jb) + Œ£(b, h jb) = Œ£(Àúh ia, Àúh jb) + Œ£(U x ia, Àúh jb) + Œ£(b, Àúh jb) + Œ£(Àúh ia, U x jb) + Œ£(U xia, U x jb) + Œ£(b, U x jb) + Œ£(Àúh ia, b) + Œ£(U xia, b) + Œ£(b, b). By the ‚Äúotherwise‚Äù case of Eq. (2), Œ£(U x ia, Àúh jb) = Œ£(b, Àúh jb) = Œ£(Àúh ia, U x jb) = Œ£(Àúh ia, b) = 0, and by the ‚Äúinput G-var‚Äù case of Eq. (2), Œ£(b, U x jb) = Œ£(Àúh ia, b) = Œ£ in(b, U x jb) = Œ£ in(Àúh ia, b) = 0. We thus have Œ£(h ia, h jb) = Œ£(Àúh ia, Àúh jb) + Œ£(U x ia, U x jb) + Œ£(b, b) = Œ£(Àúh ia, Àúh jb) + Œ£(U x ia, U x jb) + œÉ2 b which is Eq. (11). Next, note that Àúh ia is introduced via MatMul by Àúh ia := W s ia, and s ia is an H-var introduced by s ia := œï(h ia). Thus, by the MatMul rule of Eq. (2), Œ£(Àúh ia, Àúh jb) = œÉ2 W E Z œï(Z h ia )œï(Z h jb ) where Z ‚àº N (¬µ, Œ£) and Z h ia (resp. Z hjb ) is its component corresponding to h ia (resp. h jb). Since the integrand only depends on the two components Z h ia and Z h jb , we can rewrite Œ£(Àúh ia, Àúh jb) = œÉ2 W E z1,z2 œï(z1)œï(z2) where (z1, z2) ‚àº N (0, Œ£|hi‚àí1,a,hj‚àí1,b ). This is Eq. (12). Let us formulate the results above in a way more suggestive of the algorithm required to compute the kernel. For any 2 ‚â§ p ‚â§ t, 2 ‚â§ q ‚â§ r, deÔ¨Åne ÀúH pq def = {Àúh i1} p i=2 ‚à™ {Àúh j2}q j=2 and X pq def = {U x i1}p i=1 ‚à™ {U xj2} q j=1. Denote by Œ£| ÀúH pq the restriction of Œ£ to ÀúH pq, and likewise Œ£in|X pq the restriction of Œ£in to X pq. We can visualize Œ£| ÀúH pq = ( App Bpq Bpq ‚ä§ C qq ) ‚àà R(p+q‚àí2)√ó(p+q‚àí2) Œ£in|X pq = ( P pp Q pq Q pq ‚ä§ Rqq ) ‚àà R(p+q)√ó(p+q) (13) where App def = {Œ£(Àúh i1, Àúh j1)}p,p i,j=2 C qq def = {Œ£(Àúh i2, Àúh j2)} q,q i,j=2 Bpq def = {Œ£(Àúh i1, Àúh j2)} p,q i,j=2 P pp def = {Œ£in(U x i1, U x j1)} p,p i,j=1 Rqq def = {Œ£in(U x i2, U x j2)} q,q i,j=1 Q pq def = {Œ£in(U xi1, U x j2)}p,q i,j=1. Let Œ£| 0 ÀúH pq also denote Œ£| ÀúH pq padded with an additional column and an additional row of 0s on the left and top of each block App, Bpq, Bpq ‚ä§, C qq: Œ£|0 ÀúH pq = Ô£´ Ô£¨ Ô£≠ 0 0 0 0 0 App 0 Bpq 0 0 0 0 0 Bpq ‚ä§ 0 C qq Ô£∂ | Ô£∏ ‚àà R(p+q)√ó(p+q). (14) Then Eqs. (11) and (12) can be combined and vectorized as Œ£| ÀúH p+1,q+1 = Vœï (Œ£| 0 ÀúH pq + Œ£in|X pq + œÉ2 b ) . (15) Eq. (15) quickly yields to a iterative, vectorized algorithm for computing Œ£| ÀúH tr (recall t and r are the lengths of the two input sequences), assuming that Vœï can be efÔ¨Åciently computed (such as those under DeÔ¨Ånition B.1). Then, with H pq def = {h i1}p i=1 ‚à™ {h j2} q j=1, we have Œ£|H tr = Œ£| 0 ÀúH tr + Œ£in|X tr + œÉ2 b . 25 Computing the GP Kernel Finally, given Œ£|H tr , by Corollary 5.5, the covariance of the output of Program 2 in the large n limit is K = œÉ2 vVœï (Œ£|H tr ) . Computing RNN kernel Consider a simple RNN with nonlinearity œï, as in Program 2. Suppose we have 2 input sequences x11, . . . , xt1 and x12, . . . , xr2 ‚àà Rm. Assume we sample the RNN parameters as in Eq. (9). Then the outputs of the RNN converge jointly in distribution to a Gaussian with covariance computed as follows. 1. Initialize Œ£in according to Eq. (10). 2. Starting with p = q = 0, do (a) Œ£| ÀúH p+1,q+1 ‚Üê œÉ2 wVœï (Œ£| 0 ÀúH pq + Œ£in|X pq + œÉ2 b ) (see Eqs. (13) and (14) for notations) (b) Set p ‚Üê min(p + 1, t), q ‚Üê min(q + 1, r). (c) If p and q did not change, break. 3. Compute Œ£|H tr ‚Üê Œ£| 0 ÀúH tr + Œ£in|X tr + œÉ2 b . 4. The output kernel is given by K ‚Üê œÉ2 vVœï (Œ£|H tr ) . See our repository github.com/thegregyang/GP4A for a reference implementation of this kernel. B.3 Batchnorm (Program 3) As shown in Appendix A, batchnorm (followed by a coordinatewise nonlinearity) can be just thought of a multivariate nonlinearity, and the computation of Œ£ can largely follow the same pattern as for any other feedforward neural net (see Program 3). However, doing so efÔ¨Åciently is not so obvious, especially when the batch size is large. In this section, we describe how to overcome this apparent difÔ¨Åculty. B.3.1 Batchnorm with Single Batch Let us compute the GP kernel for Program 3, assuming there is only one batch. Setup The network in Program 3 has parameters W 1 ‚àà Rn√óm, where m is the input dimension, and W l ‚àà Rn√ón for 2 ‚â§ l ‚â§ L. Since batchnorm is scale-invariant, we will just assume that W 1 Œ±Œ≤ ‚àº N (0, 1/m), W l Œ±Œ≤ ‚àº N (0, 1/n), ‚àÄl ‚â• 2, and vŒ± ‚àº N (0, œÉ2 v). This means that the initial NETSOR sampling data have values Œ£in(v, v) = œÉ2 v Œ£in(W 1x ia, W 1xi ‚Ä≤a‚Ä≤) = xia‚ä§xi‚Ä≤a ‚Ä≤/m, and ¬µ in = 0 identically. Computing ¬µ As before, from Eq. (2), it is easy to see that ¬µ in = 0 implies ¬µ = 0. Computing Œ£ Applying Eq. (2) in the fashion of Lemma B.5, we get Lemma B.7. For any a ‚àà [k] and 2 ‚â§ l ‚â§ L, let H la denote the set {h lia}i‚àà[Ba]. Also write H 1a def = {W 1x ia}i‚àà[Ba]. Recall that Œ£|set denotes the square submatrix of Œ£ with rows and columns in set. Then for any a ‚àà [k] and l = 1, . . . , L ‚àí 1, Œ£|H l+1,a = E Œ∂‚àºN (0,Œ£| Hla ) Àúœï(Œ∂) Àúœï(Œ∂) ‚ä§ = V Àúœï (Œ£|H la ) ‚àà RBa√óBa , (16) where Àúœï is batchnorm followed by coordinatewise nonlinearity œï, as in Appendix A and Program 3. 26 A priori, evaluating this expectation requires a Ba-dimensional Gaussian integral, which becomes intractible when B is large. However, if œï = ReLU, then surprisingly one can reduce the B- dimensional integral this Gaussian expectation seems to require to a 1-dimensional integral: By [65] we can express Eq. (16) as Œ£|H l+1,a = Ba ‚à´ ‚àû 0 VReLU(Œ£ G(I + 2sŒ£G) ‚àí1) ‚àö det(I + 2sŒ£G) ds (17) where VReLU is as in Fact B.2, and Œ£G def = GŒ£|H laG G def = IB ‚àí 1 B 11‚ä§. B.3.2 Batchnorm with Multiple Batches Now let‚Äôs consider the covariance of preactivations between different batches. We maintain the same setup as above, and as usual ¬µ = 0 identically. Computing Œ£ By another straightforward application of Eq. (2), we get Lemma B.8. With the same notation as in Lemma B.7, for two different batches a Ã∏= b, Œ£(H l+1,a, H l+1,b) = E Àúœï(Œ∂1) Àúœï(Œ∂2)‚ä§ where the expectation is taken over ( Œ∂1 Œ∂2 ) ‚àº N (0, (Œ£(H la, H la) Œ£(H la, H lb) Œ£(H lb, H la) Œ£(H lb, H lb) )) . Again, this expectaion seems to require an integral in (Ba + Bb) dimensions. However, via some integral tricks, this expectation can be transformed to the following 2D integral: Œ£(H l+1,a, H l+1,b) = ‚àöBaBbœÄ‚àí1 ‚à´ ‚àû 0 ds ‚à´ ‚àû 0 dt (st) ‚àí1/2 det(IBa+Bb + 2‚Ñ¶)‚àí1/2VReLU(Œ†)12 (18) where ‚Ñ¶ = D1/2 ( GaŒ£(Y, Y )Ga GaŒ£(Y, Y ‚Ä≤)Gb GbŒ£(Y ‚Ä≤, Y )Ga GbŒ£(Y ‚Ä≤, Y ‚Ä≤)Gb ) D1/2 Œ† = D‚àí1/2‚Ñ¶(I + 2‚Ñ¶)‚àí1D‚àí1/2 D = sIBa ‚äï tIBb = (sIBa 0 0 tIBb ) Ga = IBa ‚àí B‚àí1 a 11‚ä§ Gb = IBb ‚àí B‚àí1 b 11‚ä§ and VReLU(Œ†)12 is the block of VReLU(Œ†) on the Ô¨Årst row, second column, of size Ba √ó Bb. Computing the GP Kernel K The computation is similar to Lemmas B.7 and B.8, so let us directly summarize the entire algorithm below. 27 Computing Batchnorm+ReLU kernel We compute the kernel of a L-layer fully-connected network where each layer has a batchnorm followed by ReLU, as shown in Program 3. Let the input dimension be m and the common width of hidden layers be n. Sample the Ô¨Årst layer weights W 1 as W 1 Œ±Œ≤ ‚àº N (0, œÉ2 W /m) and each higher layer‚Äôs weight matrix W l as W l Œ±Œ≤ ‚àº N (0, œÉ2 W /n) with œÉW = 1 (since batchnorm is scale-invariant), and the readout layer as vŒ± ‚àº N (0, œÉ2 v). We omit biases since batchnorm is shift invariant. Suppose we have k batches of inputs {xib : i ‚àà [Bb], b ‚àà [k]}, with batch b containing Bb inputs. Then the outputs of the network converge jointly in distribution to a Gaussian N (0, K) with K computed as follows. 1. Initialize {K 0 ab ‚àà RBa√óBb }k a,b=1 by (K 0 ab)ij ‚Üê xia‚ä§xjb/m. 2. For l = 1, . . . , L, do (a) For a = 1, . . . , k, do i. K l aa ‚Üê V Àúœï(K l‚àí1 aa ) by evaluating a 1D integral according to Eq. (17). (b) For a, b ‚àà [k], a Ã∏= b, do i. Compute K l ab by using K l‚àí1 ab , K l‚àí1 aa , K l‚àí1 bb and evaluating a 2D integral according to Eq. (18). 3. Return K ‚Üê œÉ2 v Ô£´ Ô£¨ Ô£≠ K L 11 ¬∑ ¬∑ ¬∑ K L 1k ... . . . ... K L k1 ¬∑ ¬∑ ¬∑ K L kk. Ô£∂ | Ô£∏ ‚àà R‚àë a Ba√ó ‚àë a Ba Vectorized Implementation In our repo github.com/thegregyang/GP4A, we show how to implement single- and multi-batch BN using the quadpy [48] package for vectorized quadrature integration, and by using eigendecomposition to simplify the computation of the integrand in the integrals above. B.4 Convolution and Pooling (Program 4) Convolution and pooling, in the context of neural network-Gaussian process correspondence, have already been treated in [43, 18]. In this section we revisit the same derivations from the perspective of tensor programs. B.4.1 CNN with Single Input Let us compute the GP kernel for Program 4, for the following setup: Setup The CNN in Program 4 has parameters {W 1 j }j‚ààker1 , {W 2 j }j‚ààker2 . It has widths n1 and n2, but for simplicity, let‚Äôs assume n1 = n2 = n. Each input image is given as {xi ‚àà Rm}i‚ààpos0 where pos0 denotes ‚Äúpixel locations‚Äù and m denotes number of channels (for example, pos0 = [32] √ó [32] and m = 3 for the CIFAR10 dataset). Suppose we sample the parameters as (W 1 j )Œ±Œ≤ ‚àº N (0, œÉ2 w/m), ‚àÄj ‚àà ker1, (W 2 j )Œ±Œ≤ ‚àº N (0, œÉ2 w/n1), ‚àÄj ‚àà ker2, vŒ± ‚àº N (0, œÉ2 v/n). This induces Œ£in as follows: Œ£in(v, v) = œÉ2 v Œ£in(W 1 j xi+j, W 1 j‚Ä≤xi‚Ä≤+j‚Ä≤) = œÉ2 wx‚ä§ i+jxi‚Ä≤+j‚Ä≤/m for any i, i ‚Ä≤ ‚àà pos 1, j, j‚Ä≤ ‚àà ker1 such that i + j, i‚Ä≤ + j‚Ä≤ ‚àà pos 0; and Œ£in(g, g‚Ä≤) = 0 for any other pairs of input G-vars. In addition, ¬µ in = 0 identically. Computing ¬µ As before, from Eq. (2), it is easy to see that ¬µ in = 0 implies ¬µ = 0. 28 Computing Œ£ By straightforward applications of Eq. (2), we obtain the following Lemma B.9. For any i, i ‚Ä≤ ‚àà pos1, Œ£(h 1 i , h 1 i‚Ä≤) = ‚àë j,j‚Ä≤‚ààker1 Œ£in(W 1 j xi+j, W 1 j‚Ä≤xi‚Ä≤+j‚Ä≤). In addition, for any j, j‚Ä≤ ‚àà ker2 such that i + j, i‚Ä≤ + j‚Ä≤ ‚àà pos 1, Œ£(h 2 i;j, h 2 i‚Ä≤;j‚Ä≤) = œÉ2 w E z1,z2 œï(z1)œï(z2)I(j = j‚Ä≤), where (z1, z2) ‚àº N (0, Œ£|h1 i ,h1 i‚Ä≤ ). Finally, for any i, i ‚Ä≤ ‚àà pos2, Œ£(h 2 i , h 2 i‚Ä≤) = ‚àë j,j‚Ä≤‚ààker2 Œ£(h 2 i;j, h 2 i‚Ä≤;j‚Ä≤) where the sum is over all j, j‚Ä≤ ‚àà ker2 such that i + j, i‚Ä≤ + j‚Ä≤ ‚àà pos1. Computing the GP kernel K By Corollary 5.5, the output of the CNN converges in distribution to N (0, K) where K is a scalar given by K def = lim n‚Üí‚àû Var ( v‚ä§ ¬Øx 2 n ) = œÉ2 v E Z ( 1 |pos2| ‚àë i œï(Z h 2 i ) )2 = œÉ2 v E i,i‚Ä≤‚ààpos2 E Z œï(Z h 2 i )œï(Z h 2 i‚Ä≤ ) where Z ‚àº N (¬µ, Œ£) and where in the last expression, i, i ‚Ä≤ are sampled independently and uniformly from pos 2. If we let H 2 def = {h 2 i }i‚ààpos2 , then K can be computed as K = œÉ2 v E i,i‚Ä≤‚ààpos2 Œõii‚Ä≤ where Œõ def = Vœï (Œ£|H 2 ) . B.4.2 CNN with Multiple Inputs Now consider the general case when we have multiple inputs x 1, . . . , xB and have L layers (but, for simplicity, still no bias), as in Program 7. The derivation is very similar to the single input case, but we will err on the side of completeness. Setup The CNN in Program 7 has parameters {W l}l‚àà[L],j‚ààkerl. It has widths n1, . . . , nL, but as before, we shall assume they are all equal to n for simplicity. Each input image xa is given as {xa i ‚àà Rm}i‚ààpos0 where pos 0 denotes ‚Äúpixel locations‚Äù and m denotes number of channels. Suppose we sample the parameters as (W 1 j )Œ±Œ≤ ‚àº N (0, œÉ2 w/m), ‚àÄj ‚àà ker1, (W l j )Œ±Œ≤ ‚àº N (0, œÉ2 w/nl‚àí1), ‚àÄj ‚àà kerl, for all l = 2, . . . , L, and vŒ± ‚àº N (0, œÉ2 v/n). This induces Œ£in as follows: Œ£in(v, v) = œÉ2 v Œ£in(W 1 j xa i+j, W 1 j‚Ä≤xa ‚Ä≤ i‚Ä≤+j‚Ä≤) = œÉ2 wxa i+j ‚ä§xa ‚Ä≤ i‚Ä≤+j‚Ä≤/m for any a, a ‚Ä≤ ‚àà [B] and any i, i ‚Ä≤ ‚àà pos 1, j, j‚Ä≤ ‚àà ker1 such that i+j, i‚Ä≤ +j‚Ä≤ ‚àà pos0; and Œ£in(g, g‚Ä≤) = 0 for any other pairs of input G-vars. In addition, ¬µ in = 0 identically. Computing ¬µ As before, from Eq. (2), it is easy to see that ¬µ in = 0 implies ¬µ = 0. Computing Œ£ By straightforward applications of Eq. (2), we obtain the following Lemma B.10. For any a, a ‚Ä≤ ‚àà [B] and any i, i ‚Ä≤ ‚àà pos1, Œ£(h 1a i , h 1a ‚Ä≤ i‚Ä≤ ) = ‚àë j,j‚Ä≤‚ààker1 Œ£in(W 1 j xa i+j, W 1 j‚Ä≤xa ‚Ä≤ i‚Ä≤+j‚Ä≤). (19) In addition, for any 2 ‚â§ l ‚â§ L and any j, j‚Ä≤ ‚àà ker2 such that i + j, i‚Ä≤ + j‚Ä≤ ‚àà pos 1, Œ£(h la i;j, h la ‚Ä≤ i‚Ä≤;j‚Ä≤) = œÉ2 w E z1,z2 œï(z1)œï(z2)I(j = j‚Ä≤), (20) 29 NETSOR program 7 L-layer Convolutional Network with Global Average Pooling Input: {W 1 j xa i+j : G(n1)} a‚àà[B], j‚ààker1,i‚ààpos1 s.t. i+j‚ààpos0 Input: {W l j : A(nl, n l‚àí1)}2‚â§l‚â§L,j‚ààkerl // Readout weights Input: v : G(nL) for a ‚àà [B] do // Layer 1 convolution for i ‚àà pos1 do // Directly use input embeddings // LinComb // Sum is over all j ‚àà ker1 such that // there is i ‚Ä≤ ‚àà pos 0 with i ‚Ä≤ = j + i h 1a i := ‚àë j W 1 j xa i+j : G(n1) end for // Higher layer convolutions for l = 2, . . . , L do for i ‚àà posl‚àí1 do xl‚àí1,a i := œï(h l‚àí1,a i ) : H(n l‚àí1) end for for j ‚àà kerl, i ‚àà pos l s.t. i + j ‚àà posl‚àí1 do // MatMul h la i;j := W l j xl‚àí1,a i+j : G(nl) end for for i ‚àà posl do // Sum is over all j ‚àà kerl such that // there is i ‚Ä≤ ‚àà pos l‚àí1 with i ‚Ä≤ = j + i h la i := ‚àë j h la i;j : G(nl) end for end for // Nonlinearity & Global Average Pooling ¬ØxLa := 1 |posL| ‚àë i‚ààkerL œï(h La i ) : H(nL) end for Output: v‚ä§ ¬ØxL1/ ‚àönL, . . . , v‚ä§ ¬ØxLB/ ‚àönL where (z1, z2) ‚àº N (0, Œ£|h l‚àí1,a i ,hl‚àí1,a‚Ä≤ i‚Ä≤ ). Finally, for any i, i ‚Ä≤ ‚àà posl, Œ£(h la i , h la ‚Ä≤ i‚Ä≤ ) = ‚àë j,j‚Ä≤ Œ£(h la i;j, h la ‚Ä≤ i‚Ä≤;j‚Ä≤) (21) where the sum is over all j, j‚Ä≤ ‚àà kerl such that i + j, i‚Ä≤ + j‚Ä≤ ‚àà posl‚àí1. These equations are all we need to compute the GP kernel K. Computing the GP kernel K By Corollary 5.5, the output of the CNN converges in distribution to N (0, K) where K ‚àà RB√óB is given by Kaa‚Ä≤ def = lim n‚Üí‚àû Cov ( v‚ä§ ¬ØxLa ‚àön , v‚ä§ ¬ØxLa ‚Ä≤ ‚àön ) = œÉ2 v E Z ( 1 |posL| ‚àë i œï(Z h La i ) ) ( 1 |posL| ‚àë i œï(Z h La‚Ä≤ i ) ) = œÉ2 v E i,i‚Ä≤‚ààposL E Z œï(Z h La i )œï(Z h La‚Ä≤ i‚Ä≤ ) (22) where Z ‚àº N (¬µ, Œ£) and where in the last expression, i, i ‚Ä≤ are sampled independently and uniformly from posL. Since Œ£(h La i , h La ‚Ä≤ i‚Ä≤ ) can be obtained recursively via Lemma B.10, one can compute K easily via recursion. But we can do better by vectorizing the whole computation. 30 Vectorized Implementation Let us deÔ¨Åne the 4-tensor Œ£l = {Œ£ l aa‚Ä≤ii‚Ä≤ : a, a ‚Ä≤ ‚àà [B], i, i‚Ä≤ ‚àà pos l} by Œ£ l aa‚Ä≤ii‚Ä≤ def = Œ£(h la i , h la ‚Ä≤ i‚Ä≤ ). Then Eq. (19) corresponds to Œ£1 aa‚Ä≤ii‚Ä≤ = œÉ2 w ‚àë j,j‚Ä≤ xa i+j ‚ä§xa ‚Ä≤ i‚Ä≤+j‚Ä≤/m where the sum is over all j, j‚Ä≤ ‚àà ker1 such that i + j, i‚Ä≤ + j‚Ä≤ ‚àà pos0. For l = 2, . . . , L ‚àí 1, Eqs. (20) and (21) can be vectorized as Œ£l aa‚Ä≤ = Œ∫ l ‚àó ÀÜŒ£ l aa‚Ä≤, where ÀÜŒ£ l = œÉ2 wVœï(Œ£ l‚àí1), treating Œ£l‚àí1 as a (B ¬∑ pos l‚àí1) √ó (B ¬∑ posl‚àí1) matrix, and Œ∫ l‚àó is the ‚Äúconvolution‚Äù (Œ∫ l ‚àó ÀÜŒ£ l aa‚Ä≤)ii‚Ä≤ = ‚àë j,j‚Ä≤‚ààkerl i+j‚ààposl‚àí1 i‚Ä≤+j‚Ä≤‚ààpos l‚àí1 ( ÀÜŒ£l aa‚Ä≤)i+j,i‚Ä≤+j‚Ä≤. (23) This Œ∫ l convolution can indeed be implemented as a (CUDA) convolutional operation, vectorized over all a, a ‚Ä≤. Finally, to obtain the inÔ¨Ånite-width GP kernel K of the CN output, we can vectorize Eq. (22) as Kaa‚Ä≤ = œÉ2 vE ‚àó ÀÜŒ£ L, where ÀÜŒ£ L = Vœï(Œ£L‚àí1), and E‚àó denotes the spacial averaging (E ‚àó ÀÜŒ£ L)aa‚Ä≤ def = E i,i‚Ä≤‚ààposL ÀÜŒ£L aa‚Ä≤ii‚Ä≤. (24) Again, E‚àó can be implemented as a convolution operator vectorized over all a, a ‚Ä≤. In summary, Computing CNN Kernel Suppose we have an L-layer convolutional neural network with coordinatewise nonlinearity œï but no bias, as in Program 7, that takes images with m channels and of size pos 0 √ó pos0. Suppose we have a set of inputs x1, . . . , xB where each input xa is given as xa = {xa i ‚àà Rm}i‚ààpos0. Then the CNN outputs converge in distribution to a Gaussian N (0, K) where K ‚àà RB√óB can be calculated as follows. 1. Initialize Œ£1 ‚àà RB√óB√óposl√óposl by Œ£1 aa‚Ä≤ii‚Ä≤ = œÉ2 w ‚àë j,j‚Ä≤ xa i+j ‚ä§xa ‚Ä≤ i‚Ä≤+j‚Ä≤/m where the sum is over all j, j‚Ä≤ ‚àà ker1 such that i + j, i‚Ä≤ + j‚Ä≤ ‚àà pos 0. 2. For l = 2, . . . , L ‚àí 1, do (a) Œ£l ‚Üê œÉ2 wŒ∫ l ‚àó Vœï(Œ£l‚àí1) (see Eq. (23) for Œ∫ l‚Äôs deÔ¨Ånition) 3. return K ‚Üê œÉ2 vE ‚àó Vœï (Œ£ L‚àí1) (see Eq. (24) for E‚Äôs deÔ¨Ånition) B.5 GRU (Program 5) We demonstrate how to compute the GP kernel for GRU as encoded in NETSOR by Program 5. A key distinguishing feature of this conversion is that we will need to compute high dimensional Gaussian expectations, where the dimension is as large as the number of timesteps unrolled, in contrast to the simple RNN case Program 2. These Gaussian expectations correspond to the expected values of multiplication of the gate values across time. We Ô¨Årst proceed with a single input sequence, as in Program 5. We then comment on the generalization to multiple sequences at the end. 31 Setup We will obtain the ¬µ and Œ£ for Program 5 with (Uz)Œ±Œ≤, (Ur)Œ±Œ≤, (Uh)Œ±Œ≤ ‚àº N (0, œÉ2 U /n), (Wz)Œ±Œ≤, (Wr)Œ±Œ≤, (Wh)Œ±Œ≤ ‚àº N (0, œÉ2 W /n), (bz)Œ±, (br)Œ±, (bh)Œ± ‚àº N (0, œÉ2 b ), vŒ± ‚àº N (0, œÉ2 v), and h 0 = 0. (25) Suppose the input tokens xi to the GRU have dimension m. The randomization of U induces the following covariance structure in the input token embeddings Œ£in(U x, U y) = œÉ2 U x‚ä§y/m for any x, y ‚àà {xi} T i=1. For any other pair g, g‚Ä≤ of input G-vars, Œ£in(g, g‚Ä≤) = 0. Additionally, ¬µ in(g) = 0 for all input G-vars g. Computing ¬µ In fact, one can quickly see that ¬µ(g) = 0 for all G-vars g. Computing Œ£ Applying Eq. (2) to Program 5 and some simpliÔ¨Åcation in the manner of Lemma B.6‚Äôs proof yields, for any two times t, s, Œ£(Àúzt, Àúzs) = Œ£(h t z, h s z ) + œÉ2 U xt‚ä§xs/m + œÉ2 b (26) Œ£(Àúrt, Àúrs) = Œ£(h t r, h s r ) + œÉ2 U xt‚ä§x s/m + œÉ2 b (27) Œ£(Àúh t, Àúh s) = Œ£(h t h, h s h) + œÉ2 U xt‚ä§xs/m + œÉ2 b (28) Œ£(h t z, h s z ) = Œ£(h t r, h s r ) = œÉ2 W t‚àë i=1 s‚àë j=1 { E œï(Z Àúh i)œï(Z Àúhj ) √ó E Ô£Æ Ô£∞œÉ(Z Àúzi) t‚àè p=i+1 (1 ‚àí œÉ(Z Àúzp )) Ô£π Ô£ª √ó Ô£Æ Ô£∞œÉ(Z Àúzj ) s‚àè q=j+1 (1 ‚àí œÉ(Z Àúzq )) Ô£π Ô£ª } (29) Œ£(h t h, h s h) = œÉ2 W Œ£(h t z, h s z ) E œÉ(Z Àúrt)œÉ(Z Àúrs ) (30) where expectations are taken over Z = {Z g}g is G-var ‚àº N (¬µ, Œ£), which has one component for each G-var in the program. Then, applying Corollary 5.5, we see that the output of the GRU (v‚ä§h 1/ ‚àön, . . . , v‚ä§hT / ‚àön) converges in distribution to a zero mean Gaussian distribution with covariance matrix K = {Kts} T t,s=1, Kts = œÉ2 v t‚àë i=1 s‚àë j=1 { E œï(Z Àúh i)œï(Z Àúh j ) √ó E Ô£Æ Ô£∞œÉ(Z Àúzi) t‚àè p=i+1 (1 ‚àí œÉ(Z Àúzp )) Ô£π Ô£ª √ó Ô£Æ Ô£∞œÉ(Z Àúzj ) s‚àè q=j+1 (1 ‚àí œÉ(Z Àúzq )) Ô£π Ô£ª }. (31) Eqs. (27) to (31) yield the complete set of equations to compute the output covariance K, but to do so efÔ¨Åciently rests entirely on evaluating the the possibly T -dimensional integral behind E Ô£Æ Ô£∞œÉ(Z Àúzi) t‚àè p=i+1 (1 ‚àí œÉ(Z Àúzp )) Ô£π Ô£ª √ó Ô£Æ Ô£∞œÉ(Z Àúzj ) s‚àè q=j+1 (1 ‚àí œÉ(Z Àúzq )) Ô£π Ô£ª . (32) For general œÉ and œï, this is hopeless. However, when œï = erf and œÉ = (1 + erf)/2 ‚Äî which approximate œï = tanh and œÉ = sigmoid ‚Äî Eq. (32) can in fact be evaluated efÔ¨Åciently by reducing it to a Gaussian orthant probability, which can be evaluated efÔ¨Åciently [19]: 32 NETSOR program 8 GRU, Multiple Input Sequences // Embeddings of B input sequences // with ath sequence having length Ta Input: {Uzx ta : G(n)}a‚àà[B],t‚àà[Ta] Input: {Urxta : G(n)}a‚àà[B],t‚àà[Ta] Input: {Uhxta : G(n)}a‚àà[B],t‚àà[Ta] // Parameters Input: Wz, Wr, Wh : A(n, n) Input: bz, br, bh : G(n) // Initial GRU state Input: h 0 : G(n) // Readout layer Input: v : G(n) for a ‚àà [B] do for t ‚àà [Ta] do h ta z := Wzht‚àí1,a : G(n) Àúzta := h ta z + Uzx ta + bz : G(n) h ta r := Wrh t‚àí1,a : G(n) Àúrta := h ta r + Urx ta + br : G(n) // Morally, ÀÜh t‚àí1,a = œÉ(Àúrt‚àí1,a) ‚äô h t‚àí1,a, but we need to unroll h t‚àí1,a to apply Nonlin ÀÜh t‚àí1,a := œÉ(Àúrt‚àí1,a)‚äô(h 0 ‚äô ‚äôt‚àí1 i=1(1 ‚àí œÉ(Àúzia)) + ‚àët‚àí1 j=1 œï(Àúh ja) ‚äô œÉ(Àúzja) ‚äô ‚äôt‚àí1 l=j+1(1 ‚àí œÉ(Àúzla)) ) : H(n) h ta h := WhÀÜh t‚àí1,a : G(n) Àúhta := hta h + Uhxta + bh : G(n) // Unrolling h t to a coordinatewise function of G-vars ht := h 0 ‚äô ‚äôt i=1(1 ‚àí œÉ(Àúzia)) + ‚àët j=1 œï(Àúh ja) ‚äô œÉ(Àúzja) ‚äô ‚äôt l=j+1(1 ‚àí œÉ(Àúzla)) : H(n) end for end for Output: {v‚ä§h ta/ ‚àön}a‚àà[B],t‚àà[Ta] Lemma B.11. Let œï = erf and œÉ = (1 + erf)/2. Then for any ¬µ ‚àà RT and any PSD Œ£ ‚àà RT √óT , E x‚àºN (¬µ,Œ£) T‚àè i=1 œï(xi) = E x‚àºN (¬µ,Œ£+ 1 2 I) T‚àè i=1 sgn(xi) E x‚àºN (¬µ,Œ£) T‚àè i=1 œÉ(xi) = E x‚àºN (¬µ,Œ£+ 1 2 I) T‚àè i=1 I(xi ‚â• 0) = Pr x‚àºN (¬µ,Œ£+ 1 2 I)[x ‚â• 0]. Remark B.12. Observe that if T = 2, then Lemma B.11 recovers the arccosine kernel of erf via Fact B.3: E [erf(x1)erf(x2) : x ‚àº N (0, Œ£)] = 2 œÄ arcsin Œ£12‚àö(Œ£11 + 1 2 )(Œ£22 + 1 2 ) . 33 To apply Lemma B.11, we can express Eq. (32) as E Ô£Æ Ô£∞œÉ(Z Àúzi) t‚àè p=i+1 œÉ(‚àíZ Àúzp ) Ô£π Ô£ª √ó Ô£Æ Ô£∞œÉ(Z Àúzj ) s‚àè q=j+1 œÉ(‚àíZ Àúzq ) Ô£π Ô£ª = E Ô£Æ Ô£∞œÉ( ÀÜY i) s‚àè p=i+1 œÉ( ÀÜY p) Ô£π Ô£ª √ó Ô£Æ Ô£∞œÉ( ÀáY j) s‚àè q=j+1 œÉ( ÀáY q) Ô£π Ô£ª = E Ô£Æ Ô£∞ t‚àè p=i œÉ( ÀÜY p) Ô£π Ô£ª √ó Ô£Æ Ô£∞ s‚àè q=j œÉ( ÀáY q) Ô£π Ô£ª where ( ÀÜY i, . . . , ÀÜY t, ÀáY j, . . . , ÀáY s) ‚àº N (ŒΩ, ‚Ñ¶) with b and ‚Ñ¶ given as follows ŒΩ( ÀÜY i) = ¬µ(Àúzi) ŒΩ( ÀáY j) = ¬µ(Àúzj) ŒΩ( ÀÜY p) = ‚àí¬µ(Àúzp), ‚àÄp ‚â• i + 1 ŒΩ( ÀáY q) = ‚àí¬µ(Àúzq), ‚àÄq ‚â• j + 1 ‚Ñ¶( ÀÜY p, ÀáY q) = {Œ£(Àúzp, Àúzq) if p ‚â• i + 1, q ‚â• j + 1, or p = i, q = j ‚àíŒ£(Àúzp, Àúzq) otherwise. ‚Ñ¶( ÀÜY p, ÀÜY p‚Ä≤) = {Œ£(Àúzp, Àúzp‚Ä≤) if p, p ‚Ä≤ ‚â• i + 1, or p = p‚Ä≤ = i ‚àíŒ£(Àúzp, Àúzp‚Ä≤) otherwise. ‚Ñ¶( ÀáY q, ÀáY q‚Ä≤) = {Œ£(Àúzq, Àúzq‚Ä≤) if q, q‚Ä≤ ‚â• j + 1, or q = q‚Ä≤ = j ‚àíŒ£(Àúzq, Àúzq‚Ä≤) otherwise. Using Lemma B.11, one then has E Ô£Æ Ô£∞œÉ(Z Àúzi ) t‚àè p=i+1 œÉ(‚àíZ Àúzp ) Ô£π Ô£ª √ó Ô£Æ Ô£∞œÉ(Z Àúzj ) s‚àè q=j+1 œÉ(‚àíZ Àúzq ) Ô£π Ô£ª = Pr [ X ‚â• 0 : X ‚àº N (ŒΩ, 1 2 I + ‚Ñ¶)] . (33) If we two input sequences, the equations for recursively computing Œ£ and of K are similar to the above, and we summarize them below 34 Computing the GRU kernel Consider a GRU processing B sequences in the fashion of Program 8, with œï = erf and œÉ = (1 + erf)/2. Sample the GRU‚Äôs parameters as in Eq. (25). Then for sequence numbers a, b ‚àà [B] and time steps 2 ‚â§ t ‚â§ Ta, 2 ‚â§ s ‚â§ Tb, we have the following recurrence relations Œ£(Àúzta, Àúzsb) = Œ£(h ta z , h sb z ) + œÉ2 U xta‚ä§xsb/m + œÉ2 b Œ£(Àúrta, Àúrsb) = Œ£(h ta r , h sb r ) + œÉ2 U xta‚ä§xsb/m + œÉ2 b Œ£(Àúh ta, Àúh sb) = Œ£(h ta h , h sb h ) + œÉ2 U xta‚ä§xsb/m + œÉ2 b Œ£(h ta z , h sb z ) = Œ£(h ta r , h sb r ) = œÉ2 W t‚àë i=1 s‚àë j=1 Œ∂ ab i:t,j:s E œï(Z Àúhia)œï(Z Àúh jb ) Œ£(h ta h , h sb h ) = œÉ2 W Œ£(h ta z , h sb z ) E œÉ(Z Àúrta )œÉ(Z Àúrsb ) with initial conditions Œ£(Àúzta, Àúzsb) = Œ£(Àúrta, Àúrsb) = Œ£(Àúh ta, Àúh sb) = 0 if t = 1 or s = 1, and the output covariance K of the GRU outputs in the large n limit can be computed as Kta,sb = lim n‚Üí‚àû Cov ( v‚ä§xta ‚àön , v‚ä§xsb ‚àön ) = œÉ2 v t‚àë i=1 s‚àë j=1 Œ∂ ab i:t,j:s E œï(Z Àúh ia )œï(Z Àúh jb ) where Z ‚àº N (0, Œ£) and Œ∂ ab i:t,j:s def = E Z Ô£Æ Ô£∞œÉ(Z Àúzia ) t‚àè p=i+1(1 ‚àí œÉ(Z Àúzpa )) Ô£π Ô£ª √ó Ô£Æ Ô£∞œÉ(Z Àúzjb ) s‚àè q=j+1 (1 ‚àí œÉ(Z Àúzqb)) Ô£π Ô£ª , which can be reduced to a computation of orthant probability in the fashion of Eq. (33). The above equations can be turned into a (relatively) efÔ¨Åcient algorithm for computing the GP kernel of a GRU. Our repo github.com/thegregyang/GP4A shows a reference implementation of it (allowing slightly more general initialization hyperparameters). It leverages the R package mvtnorm [19] to evaluate the Gaussian orthant probability involved in Eq. (33). In the rest of the section, we prove Lemma B.11. Review of (Tempered) Distributions Before we begin the proof of Lemma B.11, we brieÔ¨Çy recall the notion of a tempered distribution, which is a ‚Äúpseudo-function‚Äù that is formally deÔ¨Åned as an element of the dual of Schwartz space (intuitively, the space of functions with rapidly decreasing derivatives of all orders) [52]. Given a Schwartz function f and a tempered distribution œÑ , the value of œÑ on f will be denoted here by ‚ü®œÑ, f ‚ü©. For example, if œÑ is a locally-integrable function, then œÑ is also a tempered distribution and ‚ü®œÑ, f ‚ü© can be deÔ¨Åned by ‚ü®œÑ, f ‚ü© = ‚à´ œÑ (x)f (x) dx. As all Schwartz functions have Fourier transforms [52], any tempered distribution has Fourier transform deÔ¨Åned by ‚ü®F{œÑ }, f ‚ü© def = ‚ü®œÑ, F{f }‚ü©. 35 In what follows, notationally, Fourier transform will convert functions or distributions in variable t to functions to distributions in variable x, or vice versa. See [52] for more background on distributions. Proof of Lemma B.11. As a tempered distribution, œï = erf can be expressed as œï(x) = F { ‚àíi ‚àö2 ‚àöœÄ p.v. e ‚àít2/4 t } (x) = 1 ‚àö2œÄ p.v. ‚à´ eixt ‚àíi‚àö2 ‚àöœÄ e‚àít 2/4 t dt, where p.v. denotes principal value integration p.v. ‚à´ eixt ‚àíi ‚àö2 ‚àöœÄ e‚àít2/4 t dt def = lim Œµ‚Üí0 (‚à´ ‚àíŒµ ‚àí‚àû + ‚à´ ‚àû Œµ ) eixt ‚àíi ‚àö2 ‚àöœÄ e‚àít2/4 t dt. Over multiple variables, because Fourier transform over RT is equivalent to applying 1D Fourier transform over each coordinate, we have T‚àè i=1 œï(xi) = F Ô£± Ô£≤ Ô£≥ ( ‚àíi ‚àö2 ‚àöœÄ )T p.v. e‚àí ‚àëT i=1 t2 i /4 ‚àèT i=1 ti Ô£º ‚Äñ Ô£æ (x) = 1 (2œÄ)T /2 p.v. ‚à´ eix¬∑t ( ‚àíi ‚àö2 ‚àöœÄ )T e‚àí ‚àëT i=1 t 2 i /4 ‚àèT i=1 ti dt. Let Œ≥(x; Œ£) def = (det 2œÄŒ£)‚àíT /2e‚àí 1 2 x ‚ä§Œ£ ‚àí1x be the density of N (0, Œ£) for nonsingular Œ£. Note that F{Œ≥(x; Œ£)}(t) = (2œÄ) ‚àíT /2e‚àí 1 2 t‚ä§Œ£t. We thus have E [ T‚àè i=1 œï(xi) : x ‚àº N (0, Œ£) ] = * F Ô£± Ô£≤ Ô£≥ ( ‚àíi ‚àö2 ‚àöœÄ )T p.v. e‚àí ‚àëT i=1 t 2 i /4 ‚àèT i=1 ti Ô£º ‚Äñ Ô£æ , Œ≥(x; Œ£) + = *( ‚àíi ‚àö2 ‚àöœÄ )T p.v. e ‚àí ‚àëT i=1 t 2 i /4 ‚àèT i=1 ti , F{Œ≥(x; Œ£)} + = *( ‚àíi ‚àö2 ‚àöœÄ )T p.v. e ‚àí ‚àëT i=1 t 2 i /4 ‚àèT i=1 ti , (2œÄ) ‚àíT /2e‚àí 1 2 t‚ä§Œ£t+ = ( ‚àíi ‚àö2 ‚àöœÄ )T p.v. ‚à´ e ‚àí ‚àëT i=1 t 2 i /4 ‚àèT i=1 ti (2œÄ)‚àíT /2e‚àí 1 2 t ‚ä§Œ£t dt = ( ‚àíi ‚àö2 ‚àöœÄ )T p.v. ‚à´ 1 ‚àèT i=1 ti (2œÄ)‚àíT /2e‚àí 1 2 t ‚ä§(Œ£+ 1 2 I)t dt = *( ‚àíi ‚àö2 ‚àöœÄ )T p.v. 1 ‚àèT i=1 ti , (2œÄ) ‚àíT /2e‚àí 1 2 t‚ä§(Œ£+ 1 2 I)t+ = *( ‚àíi ‚àö2 ‚àöœÄ )T p.v. 1 ‚àèT i=1 ti , F{Œ≥(x; Œ£ + 1 2 I)} + = * F Ô£± Ô£≤ Ô£≥ ( ‚àíi ‚àö2 ‚àöœÄ )T p.v. 1 ‚àèT i=1 ti Ô£º ‚Äñ Ô£æ , Œ≥(x; Œ£ + 1 2 I) + = *( ‚àíi ‚àö2 ‚àöœÄ )T (i ‚àöœÄ/2 )T T‚àè i=1 sgn(xi), Œ≥(x; Œ£ + 1 2 I) + 36 = E [ T‚àè i=1 sgn(xi) : x ‚àº N (0, Œ£ + 1 2 I) ] where we used F{p.v.t ‚àí1}(x) = i ‚àöœÄ/2 sgn(x). Similar reasoning show that this formula also works when the mean is nonzero: E [ T‚àè i=1 œï(xi) : x ‚àº N (¬µ, Œ£) ] = E [ T‚àè i=1 sgn(xi) : x ‚àº N (¬µ, Œ£ + 1 2 I) ] . A standard continuity argument yields the same formula for singular Œ£. Some simple arithmetic reduces the œÉ case to œï. 2 ‚àíT E [ T‚àè i=1 (1 + œï(xi)) : x ‚àº N (¬µ, Œ£) ] = E [ T‚àè i=1 I(xi ‚â• 0) : x ‚àº N (¬µ, Œ£ + 1 2 I) ] . C NETSOR+ Master Theorem In this section, we state the Master Theorem for NETSOR+ . Its proof can be found in Appendix I. We Ô¨Årst need to extend the notion of controlled functions (DeÔ¨Ånition 5.3) to functions with parameters, and additionally require a smoothness assumption. DeÔ¨Ånition C.1. We say a parametrized function œï(‚àí; ‚àí) : Rk √ó Rl ‚Üí R is parameter-controlled at ÀöŒò ‚àà Rl if 1. œï(‚àí; ÀöŒò) is controlled, and 2. there are some controlled ¬Øœï : Rk ‚Üí R and some function f : Rl ‚Üí R‚â•0 ‚à™ {‚àû} that has f (ÀöŒò) = 0 and that is continuous at ÀöŒò, such that, for all x1, . . . , xk ‚àà R and Œò ‚àà Rl, |œï(x1, . . . , xk; Œò) ‚àí œï(x1, . . . , xk; ÀöŒò)| ‚â§ f (Œò) ¬Øœï(x 1, . . . , xk). Note that f and ¬Øœï here can depend on ÀöŒò. Example C.2. Any function that is (pseudo-)Lipschitz 11 in x1, . . . , xk and Œò is parameter-controlled. An example of a discontinuous function that is parameter-controlled is œï(x; Œ∏) = step(Œ∏x). Then for ÀöŒ∏ Ã∏= 0, |œï(x; Œ∏) ‚àí œï(x; ÀöŒ∏)| ‚â§ |ÀöŒ∏ ‚àí Œ∏| |ÀöŒ∏| , so we can set f (Œ∏) = |ÀöŒ∏‚àíŒ∏| |ÀöŒ∏| and ¬Øœï = 1 in DeÔ¨Ånition C.1. Assumption C.3 (Rank Stability). For any W : A(n, m) and any collection S ‚äÜ {(h : H(m)) | ‚àÉ(g : G(n)), g := W h}, let H ‚àà Rm√ó|S| be the matrix whose columns are h ‚àà S. If 1 m H ‚ä§H ‚àà R|S|√ó|S| converges almost surely to some ÀöC as n, m ‚Üí ‚àû with convergent ratio n/m ‚Üí Œ±, then almost surely rank H = rank ÀöC for all large n and m. Note that a common situation where rank stability holds is when all limit ÀöC matrices are full rank. By the lower semi-continuity of rank, rank H = rank ÀöC must hold asymptotically. 11A pseudo-Lipschitz function œï : R r ‚Üí R is one that satisÔ¨Åes |œï(x) ‚àí œï(y)| ‚â§ C‚à•x ‚àí y‚à•(‚à•x‚à•p + ‚à•y‚à•q + 1) for some constants C, p, q ‚â• 0. Roughly speaking, pseudo-Lipschitz functions are those that have polynomially bounded weak derivatives. 37 Theorem C.4. Fix any NETSOR+ program satisfying Assumption 5.1 and Assumption C.3. Suppose for each parametrized nonlinearity œï(‚àí; Œò) in the program (appearing as part of Nonlin +), the pa- rameters Œò are instantiated with random variables that converge almost surely to some deterministic vector ÀöŒò as n ‚Üí ‚àû, and assume œï is parameter-controlled at ÀöŒò. If g1, . . . , gM are all of the G-vars (including all input G-vars), then for any l, for any random vector Œò ‚àà Rl that converges almost surely to a deterministic vector ÀöŒò, as n ‚Üí ‚àû, and for any œà : RM √ó Rl ‚Üí R parameter-controlled at ÀöŒò, 1 n n‚àë Œ±=1 œà(g1 Œ±, . . . , gM Œ± ; Œò) a.s. ‚àí‚àí‚Üí E Z‚àºN (¬µ,Œ£) œà(Z; ÀöŒò), (34) where a.s. ‚àí‚àí‚Üí means almost sure convergence, Z ‚àà RM , and ¬µ ‚àà RM and Œ£ ‚àà RM √óM are given in Eq. (2), calculated by replacing each parametrized œï(‚àí; Œò) with parameterless nonlinearity œï(‚àí; ÀöŒò). The proof of this theorem can be found in Appendix I. We will be instantiating the coordinates of Œò typically with ‚Äúempirical moments‚Äù 1 n n‚àë Œ±=1 œà(g1 Œ±, . . . , gM Œ± ) (35) for some controlled œà, since such ‚Äúmoments‚Äù should converge to a deterministic value by Theorem C.4; or even, recursively, 1 n n‚àë Œ±=1 œà(g1 Œ±, . . . , gM Œ± ; Œò ‚Ä≤) (36) for some sequence of random vectors Œò‚Ä≤ that converge a.s. to ÀöŒò‚Ä≤ and for œà parameter-controlled at ÀöŒò‚Ä≤. One can keep recursing by replacing Œò‚Ä≤ with further empirical moments. However, there is a slight complication: we are using Theorem C.4 both for the convergence of the parameters in Nonlin + rules in the program, as well as for the convergence Eq. (34), in what seems like could be circular logic. It turns out not hard to straighten out this reasoning, but it requires a bit more notation and setup to state the result. We do this in the next section Appendix C.1, with main theorem Theorem C.11 that will be our primary tool concerning NETSOR+ programs in practice. To Ô¨Ånish up this section, we make several remarks on the assumptions made in Theorem C.4. Remark C.5 (Necessity of parameter-control). Suppose œà(x; Œ∏) = I(Œ∏x Ã∏= 0). For Œ∏ Ã∏= 0, œà is 1 everywhere except œà(0; Œ∏) = 0. For Œ∏ = 0, œà is identically 0. Thus it‚Äôs easily seen that œà is not parameter-controlled at Œ∏ = 0. Now, if g : G(n) is sampled like gŒ± ‚àº N (0, 1), then 1 n n‚àë Œ±=1 œà(gŒ±; Œ∏) a.s. ‚àí‚àí‚Üí 1 if Œ∏ = 1/n so that Œ∏ ‚Üí ÀöŒ∏ = 0, but E Z‚àºN (¬µ,Œ£) œà(Z; ÀöŒ∏) = E 0 = 0. So our Master Theorem can‚Äôt hold in this case. Remark C.6 (Necessity of Rank Stability Assumption Assumption C.3). Suppose we have two input G-vars g1, g2 : G(n) which are sampled independently as g1 Œ±, g2 Œ± ‚àº N (0, 1). Let W : A(n, n) be sampled as WŒ±Œ≤ ‚àº N (0, 1/n). Then we can deÔ¨Åne h 2 := Œ∏g2 : H(n) where Œ∏ = exp(‚àín) as a function of n, using Nonlin +, so that h2 Œ± a.s. ‚àí‚àí‚Üí 0. Additionally, let ¬Øg1 := W g1 : G(n) and ¬Øg2 := W h 2 : G(n). Again, ¬Øg2 Œ± a.s. ‚àí‚àí‚Üí 0 but for any Ô¨Ånite n, ¬Øg2 is linearly independent from ¬Øg1. Thus rank stability does not hold here. Now consider the (parameterless) nonlinearity œà(x, y) that is 1 except on the line y = 0, where it is 0. Then 1 n n‚àë Œ±=1 œà(¬Øg1 Œ±, ¬Øg2 Œ±) a.s. ‚àí‚àí‚Üí 1 38 but E Z‚àºN (¬µ,Œ£) œà(Z ¬Øg1, Z ¬Øg2) = E 0 = 0. Remark C.7 (Rank Stability Already Holds for NETSOR Programs). It turns out that, as long as we only have parameterless nonlinearities, we get rank stability Assumption C.3 for free. This is formulated explicitly in Lemma H.4. It is as a result of our proof of Theorem 5.4 that interleaves an inductive proof of this rank stability (more generally, the inductive hypothesis CoreSet) with an inductive proof of the ‚Äúempirical moment‚Äù convergence (the inductive hypothesis Moments). C.1 Self-Parametrized NETSOR+ Programs and Their Master Theorem As stated below Theorem C.4, there could be potentially circular logic when allowing Nonlin + rules to take parameters depending on previously deÔ¨Åned variables in the program, such as in the form of Eq. (35). In this section, we untangle this potentially circular logic into a sound reasoning ‚Ä¢ by introducing a scalar type into NETSOR+ programs to explicitly extract the Nonlin + parameters into their own variables (DeÔ¨Ånition C.8). These scalar variables can recursively depend on previously deÔ¨Åned scalar variables, making the ‚Äúrecursive parameters‚Äù discussed in Eq. (36) much more succinctly and clearly expressed. ‚Ä¢ and by proving a Master Theorem for such NETSOR+ programs (Theorem C.11). This theorem will be the primary way through which we analyze NETSOR+ programs in practice. DeÔ¨Ånition C.8. 12 A self-parametrized NETSOR+ program is a NETSOR program where we have an additional scalar type, called C, which should intuitively be thought of as random variables that tend to a deterministic limit (i.e. a Constant) almost surely. Colloquially, we will call variables of type C ‚ÄúC-vars.‚Äù C-vars can be used as parameters of nonlinearities in Nonlin + rules, hence the ‚Äúself-parametrized‚Äù in the name. For completeness, we specify a self-parametrized NETSOR+ program as follows: Input A set of input C-vars, in addition to the G- and A-vars allowed in DeÔ¨Ånition 4.1. Body New variables can be introduced and assigned via the following rules MatMul Same as in DeÔ¨Ånition 4.1. LinComb Same as in DeÔ¨Ånition 4.1. Nonlin + If x 1, . . . , xk : G(n) are G-vars with the same dimension n, Œ∏1, . . . , Œ∏l : C are C-vars, and œï(‚àí; ‚àí) : Rk √ó Rl ‚Üí R is a parametrized function, then we may create an H-var œï(x1, . . . , xk; Œ∏1, . . . , Œ∏l) : H(n) where œï(‚àí; Œ∏1, . . . , Œ∏l) acts coordinatewise. Moment If x1, . . . , xk : G(n) are G-vars with the same dimension n, Œ∏1, . . . , Œ∏l : C are C-vars, and œï(‚àí; ‚àí) : Rk √ó Rl ‚Üí R is a parametrized function, then we may create a C-var 1 n n‚àë Œ±=1 œï(x1 Œ±, . . . , xk Œ±; Œ∏1, . . . , Œ∏l) : C. Output Same as in DeÔ¨Ånition 4.1. The self-parametrized NETSOR+ programs we are concerned will have all of its C-vars convergent to a deterministic constant. We thus need this to be true for the input C-vars at the very least. We encapsulate this requirement below. Assumption C.9. Fix a self-parametrized NETSOR+ program satisfying Assumption 5.1. Assume each input C-var Œ∏ is sampled in a way such that Œ∏ a.s. ‚àí‚àí‚Üí ÀöŒ∏ as n ‚Üí ‚àû for some deterministic scalar ÀöŒ∏ ‚àà R. 12We keep the deÔ¨Ånition here informal in terms of programming language convention to be accessible to the general machine learning audience. For those with PL background, see Appendix J. 39 Now, we shall deÔ¨Åne ¬µ and Œ£ for self-parametrized NETSOR+ programs just as in NETSOR+ programs. The only complication here is that we also need to keep track of the limit values of the C-vars in order to do so. See DeÔ¨Ånition C.10 below. DeÔ¨Ånition C.10. Fix a NETSOR+ program with scalar variables satisfying Assumption C.9. For the purpose of this deÔ¨Ånition, write g1, . . . , gM for the entirety of the G-vars in the program, including input G-vars. New Notations For each H-var h introduced by Nonlin +, we introduce the notations œï h, Œòh, œë h i , ‚Ñì h as follows: denote the associated parametrized nonlinearity by œï h(‚àí; ‚àí) : RM √ó R‚Ñì h ‚Üí R (implicitly padded so that it has as many input slots as G-vars in the program) and its parameters by Œòh = (œëh 1 , . . . , œëh ‚Ñìh ) ‚àà R‚Ñìh with length ‚Ñì h. For each G-var gi, we also set œïgi(x1, . . . , xM ) = xi and Œò gi = () ‚àà R0 to be the empty vector (so that ‚Ñì gi = 0). Likewise, for each C-var Œ∏ introduced by Moment, we introduce the notations œïŒ∏, ŒòŒ∏, œë Œ∏ i , ‚Ñì Œ∏ as follows: denote the associated parametrized nonlinearity by œï Œ∏(‚àí; ‚àí) : RM √ó R‚ÑìŒ∏ ‚Üí R (implicitly padded so that it has as many input slots as G-vars in the program) and its parameters by ŒòŒ∏ = (œëŒ∏ 1, . . . , œëŒ∏ ‚ÑìŒ∏ ) ‚àà R‚Ñì Œ∏ with length ‚Ñì Œ∏. Extending the Àö( ) notation from Assumption C.9 and the Recursive DeÔ¨Ånition of ¬µ and Œ£ Given ¬µ in and Œ£in as in Assumption 5.1, we deÔ¨Åne ¬µ and Œ£ on G-vars, along with ‚Äúlimit scalars‚Äù ÀöŒ∏ for each C-var Œ∏ (extending ÀöŒ∏ given by Assumption C.9 for input Œ∏), as follows: For any pair of G-vars g, g‚Ä≤ (among g1, . . . , gM ), we deÔ¨Åne recursively ¬µ(g) def = Ô£± Ô£≤ Ô£≥ ¬µ in(g) if g is input ‚àë i ai¬µ(yi) if g = ‚àë i aiyi, (LinComb) 0 otherwise Œ£(g, g‚Ä≤) def = Ô£± |||||Ô£≤ |||||Ô£≥ Œ£in(g, g‚Ä≤) if g, g‚Ä≤ are inputs ‚àë i aiŒ£(yi, g‚Ä≤) if g = ‚àë i aiyi, (LinComb) ‚àë i aiŒ£(g, yi) if g‚Ä≤ = ‚àë i aiyi, (LinComb) œÉ2 W EZ œïh(Z; ÀöŒò h)œï h‚Ä≤(Z; ÀöŒòh ‚Ä≤) if g = W h, g‚Ä≤ = W h ‚Ä≤, (MatMul) 0 otherwise (37) (this is the same as Eq. (2) except the MatMul case) and for each C-var Œ∏ introduced by Moment, ÀöŒ∏ def = E Z œï Œ∏(Z; ÀöŒòŒ∏). (38) In all of the equations above, Z ‚àº N (¬µ, Œ£) is a random Gaussian vector with an entry for each G-var in the program, and ÀöŒòu denotes (Àöœëu 1 , . . . , Àöœëu ‚Ñìu ), for H-var or C-var u. Note that since œï h, œï h ‚Ä≤, and œï Œ∏ only depend on entries of Z corresponding to G-vars previous to h, h ‚Ä≤, or Œ∏, the expectations involving Z only depend on entries of ¬µ and Œ£ already deÔ¨Åned, so there is no circular logic in this recursive deÔ¨Ånition of ¬µ and Œ£. Note that the notation œï h will be overloaded in a semantically consistent way in the context of NETSOR‚ó¶ programs; see DeÔ¨Ånition E.6. We are Ô¨Ånally ready to formulate the Master Theorem for self-parametrized NETSOR+ programs, which basically is just Theorem C.4 but explicitly allowing parameters of the form Eq. (35) (‚Äúempirical moments‚Äù) in Nonlin +. Theorem C.11 (Self-Parameterized NETSOR+ Master Theorem). Fix any self-parametrized NETSOR+ program satisfying Assumption C.9 and Assumption C.3. For H-var or C-var u, adopt the notation œï u, Œòu, ‚Ñì u from DeÔ¨Ånition C.10 and also let ¬µ, Œ£, ÀöŒ∏ be as computed in DeÔ¨Ånition C.10. Let g1, . . . , gM be all of the G-vars in the program (including all input G-vars). Suppose for every H-var or C-var u, œï u(‚àí; Œò u) is parameter-controlled at ÀöŒòu. 40 1. Then for any l, for any random vector Œò ‚àà Rl that converges almost surely to a deterministic vector ÀöŒò, as n ‚Üí ‚àû, and for any œà(‚àí; ‚àí) : RM √ó Rl ‚Üí R parameter-controlled at ÀöŒò, 1 n n‚àë Œ±=1 œà(g1 Œ±, . . . , gM Œ± ; Œò) a.s. ‚àí‚àí‚Üí E Z‚àºN (¬µ,Œ£) œà(Z; ÀöŒò), where a.s. ‚àí‚àí‚Üí means almost sure convergence. 2. In addition, for each C-var Œ∏ in the program, Œ∏ a.s. ‚àí‚àí‚Üí ÀöŒ∏. This theorem almost trivially follows from Theorem C.4, since the parameter vectors Œòh intuitively should converge to deterministic limits ÀöŒòh. The only slight complication is that this convergence intuitvely follows from Theorem C.4 itself in what may be a circular logic, so we need to be slightly careful to unwind this logic into a valid inductive argument. We do so below, assuming Theorem C.4 (which is proved in Appendix I). Proof. Notice that the 2nd claim about Œ∏ a.s. ‚àí‚àí‚Üí ÀöŒ∏ follows immediately from the 1st claim, so we will prove the 1st claim here. Assume that the G-vars g1, . . . , gM are in order of appearance in the program, and that g1, . . . , gm0 (with m0 ‚â§ M ) are all of the input G-vars. We perform simultaneous induction on two claims Moments(m) and CVarLimits(m) in m, deÔ¨Åned below Moments(m) For any l, for any random vector Œò ‚àà Rl that converges almost surely to a determin- istic vector ÀöŒò as n ‚Üí ‚àû, and for any œà(‚àí; ‚àí) : Rm √ó Rl ‚Üí R parameter-controlled at ÀöŒò, 1 n n‚àë Œ±=1 œà(g1 Œ±, . . . , gm Œ± ; Œò) a.s. ‚àí‚àí‚Üí E Z‚àºN (¬µ|m,Œ£|m) œà(Z; ÀöŒò) where ¬µ|m and Œ£|m are the restriction of ¬µ and Œ£ to g1, . . . , gm. CVarLimits(m) For each C-var Œ∏ introduced before gm, Œ∏ a.s. ‚àí‚àí‚Üí ÀöŒ∏ as n ‚Üí ‚àû, where ÀöŒ∏ is as computed in DeÔ¨Ånition C.10. When m = M , we would have Theorem C.11 by Moments(M ). Base case: m = m0 (input G-vars only). Moments(m0) trivially follows from Theorem C.4. CVarLimits(m0) follows from Assumption C.9. Now suppose Moments(m) and CVarLimits(m) are true; we aim to show Moments(m + 1) and CVarLimits(m + 1). Inductive case: CVarLimits(m + 1) By CVarLimits(m), it sufÔ¨Åces to show Œ∏ a.s. ‚àí‚àí‚Üí ÀöŒ∏ for all Œ∏ introduced after gm but before gm+1. We do so by another induction (an inner induction) in order of C-var appearance. The inner base case is the Ô¨Årst C-var Œ∏ introduced after gm. Its parameters Œò Œ∏ are among those introduced before gm, so by induction hypothesis CVarLimits(m), ŒòŒ∏ a.s. ‚àí‚àí‚Üí ÀöŒòŒ∏. By the assumption of Theorem C.11 that œï Œ∏ is parameter-controlled at ÀöŒòŒ∏, we have Œ∏ = 1 n n‚àë Œ±=1 œï Œ∏(g1 Œ±, . . . , gm Œ± ; Œò Œ∏) a.s. ‚àí‚àí‚Üí E Z‚àºN (¬µ|m,Œ£|m) œï Œ∏(Z; ÀöŒòŒ∏) = ÀöŒ∏ by induction hypothesis Moments(m) (where we have explicitly truncated the input slots of œïŒ∏ to reÔ¨Çect its dependence only on g1, . . . , gm). The inner inductive case, for a later Œ∏, follows the same logic, once we assume the inner inductive hypothesis that each Œ∏‚Ä≤ introduced before Œ∏ has Œ∏‚Ä≤ a.s. ‚àí‚àí‚Üí ÀöŒ∏‚Ä≤. 41 Inductive case: Moments(m+1) The claim is trivially true by Moments(m) if gm+1 is introduced via LinComb, so consider the case when gm+1 is introduced via MatMul gm+1 := W h where h : H(n) is an H-var with associated nonlinearity œï h and parameters Œòh as deÔ¨Åned in DeÔ¨Ånition C.10. By the claim CVarLimits(m + 1) we proved above, Œòh a.s. ‚àí‚àí‚Üí ÀöŒòh. By the assumption of Theorem C.11, œï h is parameter-controlled at the parameter limit ÀöŒò h. Thus, the subprogram up to and including the introduction of gm+1 satisÔ¨Åes the assumptions of Theorem C.4. Consequently, Moments(m + 1) is true by Theorem C.4. This completes the simultaneous induction of Moments and CVarLimits and thus the proof of Theorem C.11. C.2 Gaussian Process Behavior of NETSOR+ Programs We can generalize the Gaussian process behavior (Corollary 5.5) to cases involving Nonlin +: Corollary C.12 (Computing the GP Kernel for NETSOR+ programs). Adopt the same assumptions and notations as in Theorem C.4. Suppose the program outputs (v‚ä§x1/‚àön, . . . , v‚ä§xk/ ‚àön), where ‚Ä¢ v : G(n), vŒ± ‚àº N (0, œÉ2 v), is an input G-var not used elsewhere in the program and is sampled independently from all other G-vars, and ‚Ä¢ x i was introduced as xi := œïi(g1, . . . , gM ; Œò i) for parametrized nonlinearity œïi and parameter vector Œòi that converges a.s. to a deterministic vector ÀöŒòi as n ‚Üí ‚àû. Assume œïi is parameter-controlled at ÀöŒò i. Then the output vector converges in distribution to N (0, K) where Kij = œÉ2 v E Z‚àºN (¬µ,Œ£) œïi(Z; ÀöŒòi)œïj(Z; ÀöŒò j) (39) with ¬µ, Œ£ computed by replacing each parametrized œï(‚àí; Œò) with the parameterless œï(‚àí; ÀöŒò) in Eq. (2), as in Theorem C.4. The proof is a straightforward application of Theorem E.8 and Proposition G.4. Likewise, for self-parametrized programs, we have a similar result: Corollary C.13 (Computing the GP Kernel for self-parametrized NETSOR+ programs). Adopt the same assumptions and notations as in Theorem C.11. Suppose the program outputs (v‚ä§x1/‚àön, . . . , v‚ä§xk/ ‚àön), where ‚Ä¢ v : G(n), vŒ± ‚àº N (0, œÉ2 v), is an input G-var not used elsewhere in the program and is sampled independently from all other G-vars, and ‚Ä¢ x i was introduced as x i := œï xi(g1, . . . , gM ; Œò xi) for self-parametrized nonlinearity œï xi and parameter vector Œò xi (composed of C-vars) as deÔ¨Åned in DeÔ¨Ånition C.10. Let ÀöŒòxi be the limit parameter as in DeÔ¨Ånition C.10. Note that œïx i is parameter-controlled at ÀöŒò xi by assumption of Theorem C.11. Then the output vector converges in distribution to N (0, K) where Kij = œÉ2 v E Z‚àºN (¬µ,Œ£) œï xi(Z; ÀöŒòxi)œï xj (Z; ÀöŒòxj ), with ¬µ, Œ£ deÔ¨Åned in Eq. (37). (40) D Example GP Kernel Computation with NETSOR+ D.1 Layernorm: Concrete Example (Program 9) Consider the example layernorm network in Program 9. This is a self-parametrized NETSOR+ program. 42 Self-parametrized NETSOR+ program 9 Layernorm Network Input: W 1x, W 1x‚Ä≤ : G(n) Input: W 2 : G(n) Input: v : G(n) // Mean and variance of W 1x // Moment ŒΩ1 := 1 n ‚àën Œ±=1(W 1x)Œ± : C var1 := 1 n ‚àën Œ±=1(W 1x) 2 Œ± ‚àí (ŒΩ1)2 : C // Nonlin + x1 := ReLU ( (W 1x)‚àíŒΩ11 ‚àövar1 ) : H(n) h 2 := W 2x1 : G(n) // Mean and variance of h 2 // Moment ŒΩ2 := 1 n ‚àën Œ±=1 h 2 Œ± : C var2 := 1 n ‚àën Œ±=1(h 2 Œ±) 2 ‚àí (ŒΩ2) 2 : C // Nonlin + x2 := ReLU ( h 2‚àíŒΩ21‚àövar2 ) : H(n) // Same thing for x‚Ä≤ // Mean and variance of W 1x‚Ä≤ // Moment ŒΩ1‚Ä≤ := 1 n ‚àën Œ±=1(W 1x ‚Ä≤)Œ± : C var1‚Ä≤ := 1 n ‚àën Œ±=1(W 1x‚Ä≤)2 Œ± ‚àí (ŒΩ1‚Ä≤) 2 : C // Nonlin + x1‚Ä≤ := ReLU ( (W 1x‚Ä≤)‚àíŒΩ1 ‚Ä≤1 ‚àövar1‚Ä≤ ) : H(n) h 2‚Ä≤ := W 2x 1‚Ä≤ : G(n) // Mean and variance of h 2‚Ä≤ // Moment ŒΩ2‚Ä≤ := 1 n ‚àën Œ±=1 h 2 Œ±‚Ä≤ : C var2‚Ä≤ := 1 n ‚àën Œ±=1(h 2 Œ±‚Ä≤) 2 ‚àí (ŒΩ2‚Ä≤) 2 : C // Nonlin + x2‚Ä≤ := ReLU ( h 2 ‚Ä≤‚àíŒΩ2‚Ä≤1‚àövar2‚Ä≤ ) : H(n) Output: (v‚ä§x2/‚àön, v‚ä§x2‚Ä≤/‚àön) Setup Suppose the inputs x, x ‚Ä≤ Ã∏= 0 are in Rm. The network has parameters W 1 ‚àà Rn√óm, W 2 ‚àà Rn√ón, and v ‚àà Rn. Let us sample them as follows W 1 Œ±Œ≤ ‚àº N (0, œÉ2 w/m), W 2 Œ±Œ≤ ‚àº N (0, œÉ2 w/n), vŒ± ‚àº N (0, œÉ2 v), for œÉw, œÉv > 0. This corresponds to the NETSOR+ sampling data ¬µ in = 0 and Œ£in given as Œ£in(W 1x, W 1x) = œÉ2 w‚à•x‚à• 2/m, Œ£in(W 1x, W 1x ‚Ä≤) = œÉ2 wx‚ä§x‚Ä≤/m, Œ£in(W 1x‚Ä≤, W 1x‚Ä≤) = œÉ2 w‚à•x‚Ä≤‚à•2/m, Œ£in(v, v) = œÉ2 v, and Œ£in(g, g‚Ä≤) = 0 for any other pairs of input G-vars g, g‚Ä≤. D.1.1 Computing ¬µ, Œ£, and Limit Parameters ÀöŒ∏ Let‚Äôs compute the values of ¬µ, Œ£, ÀöŒ∏ in order of the appearance of the variables, according to DeÔ¨Ånition C.10. For each C-var or H-var u, we also show that œï u is parameter-controlled at Àöœëu. First, one can quickly notice that ¬µ(g) = 0 for all G-vars g, so we shall focus on computing Œ£ and ÀöŒ∏. C-var ŒΩ1 Here we have introduced ŒΩ1 via Moment by ŒΩ1 := 1 n n‚àë Œ±=1 œïŒΩ1((W 1x)Œ±), where œï ŒΩ1(z) = z, and there are no parameters. The function œï ŒΩ1 is then obviously controlled and trivially parameter- controlled. Finally, by Eq. (38), we set ÀöŒΩ1 def = E z‚àºN (0,œÉ2 w‚à•x‚à•2/m) z = 0. C-var var1 Note here var1 := 1 n n‚àë Œ±=1 œï var1((W 1x)Œ±; ŒΩ1), where œïvar1(z; Œ∏) def = z2 ‚àí Œ∏2. (Here superscript-2 denotes square, not an index). Since œï var1(‚àí; ‚àí) is pseudo-Lipschitz in both its inputs and its parameter jointly, it is parameter-controlled at Œ∏ = ÀöŒΩ1 = 0 by Example C.2. In addition, Àövar1 is computed by Eq. (38) as Àövar1 def = E z‚àºN (0,œÉ2 w‚à•x‚à•2/m) œï var1(z;ÀöŒΩ1) = E z‚àºN (0,œÉ2 w‚à•x‚à•2/m) z2 = œÉ2 w m ‚à•x‚à• 2. 43 H-var x1 The Ô¨Årst H-var introduced in the program is x1 := ReLU ( (W 1x)‚àíŒΩ11 ‚àö var1 ) . It can be written as a Nonlin + with x1 := œï x1(W 1x; ŒΩ1, var1) where œï x1(‚àí; ‚àí) : R √ó R2 ‚Üí R, and œïx 1 (z; Œ∏1, Œ∏2) def = ReLU ( z ‚àí Œ∏1‚àöŒ∏2 ) . Since œÉw > 0 and x Ã∏= 0, we have Àövar1 Ã∏= 0, and we claim that œï x1 is parameter-controlled at (ÀöŒΩ1, Àövar1) = (0, œÉ2 w m ‚à•x‚à• 2). Indeed, œï x1(‚àí;ÀöŒΩ1, Àövar1) is obviously controlled, so that condition 1 of DeÔ¨Ånition C.1 is satisÔ¨Åed. In addition, for any z ‚àà R, \f \f \fœï x1(z; Œ∏1, Œ∏2) ‚àí œï x1(z;ÀöŒΩ1, Àövar1)\f \f \f = \f \f \f \fReLU ( z ‚àí Œ∏1‚àöŒ∏2 ) ‚àí ReLU ( z ‚àí ÀöŒΩ1 ‚àö Àövar1 )\f \f \f \f ‚â§ \f \f \f \f z ‚àí Œ∏1‚àöŒ∏2 ‚àí z ‚àí ÀöŒΩ1 ‚àö Àövar1 \f \f \f \f = \f \f \f \fz ( 1 ‚àöŒ∏2 ‚àí 1 ‚àö Àövar1 ) ‚àí ( Œ∏1‚àöŒ∏2 ‚àí ÀöŒΩ1 ‚àö Àövar1 )\f \f \f \f ‚â§ \f \f \f \fz ( 1 ‚àöŒ∏2 ‚àí 1 ‚àö Àövar1 )\f \f \f \f + \f \f \f \f Œ∏1‚àöŒ∏2 ‚àí ÀöŒΩ1 ‚àö Àövar1 \f \f \f \f ‚â§ ‚àö( 1 ‚àöŒ∏2 ‚àí 1 ‚àö Àövar1 )2 + ( Œ∏1‚àöŒ∏2 ‚àí ÀöŒΩ1 ‚àö Àövar1 )2‚àö z2 + 1 by Cauchy-Schwarz. Note that ‚àö( 1‚àöŒ∏2 ‚àí 1‚àö Àövar1 )2 + ( Œ∏1‚àöŒ∏2 ‚àí ÀöŒΩ1 ‚àö Àövar1 )2 equals 0 and is continuous at (Œ∏1, Œ∏2) = (ÀöŒΩ1, Àövar1) because Àövar1 Ã∏= 0. Then since ‚àöz2 + 1 is controlled in z, œï x1 satisÔ¨Åes property 2 of DeÔ¨Ånition C.1. Altogther, we have shown that œï x1 is indeed parameter-controlled at (ÀöŒΩ1, Àövar1). G-var h 2 By the MatMul case of Eq. (37), Œ£(h 2, h 2) = œÉ2 w E z œï(z;ÀöŒΩ1, Àövar1) 2, Œ£(h 2, W 1x) = Œ£(h 2, W 1x‚Ä≤) = 0, where z ‚àº N (¬µ(W 1x), Œ£(W 1x, W 1x)) = N (0, œÉ2 w m ‚à•x‚à• 2), and œï(z; Œ∏1, Œ∏2) def = œï x1(z; Œ∏1, Œ∏2) = ReLU ( z ‚àí Œ∏1‚àöŒ∏2 ) . We can then simplify Œ£(h 2, h 2) = œÉ2 w E z ReLU Ô£´ Ô£≠ z ‚àö œÉ2 w m ‚à•x‚à•2 Ô£∂ Ô£∏ 2 = œÉ2 w E Œ∂‚àºN (0,1) ReLU(Œ∂)2 = 1 2 œÉ2 w. C-var ŒΩ2 Similar to the case of ŒΩ1, we can express ŒΩ2 via Moment by ŒΩ2 := 1 n n‚àë Œ±=1 œï ŒΩ2(h 2 Œ±), where œïŒΩ2(z) = z, and there are no parameters. The function œï ŒΩ2 is then obviously controlled and trivially parameter- controlled. Finally, by Eq. (38), we set ÀöŒΩ2 def = E z‚àºN (¬µ(h2),Œ£(h2,h2)) z = E z‚àºN (0, 1 2 œÉ2 w) z = 0. 44 C-var var2 Similar to the case of var1, we can express var2 via Moment by var2 := 1 n n‚àë Œ±=1 œïvar2(h 2 Œ±; ŒΩ2), where œï var2(z; Œ∏) def = z2 ‚àí Œ∏2. (Here z2 and Œ∏2 are the squares of z and Œ∏). Since œï var2(‚àí; ‚àí) is pseudo-Lipschitz in both its inputs and its parameter jointly, it is parameter-controlled at Œ∏ = ÀöŒΩ2 = 0 by Example C.2. In addition, Àövar2 is computed by Eq. (38) as Àövar2 def = E z‚àºN (¬µ(h2),Œ£(h2,h2)) œï var2 (z;ÀöŒΩ2) = E z‚àºN (0, 1 2 œÉ2 w) z2 = 1 2 œÉ2 w. H-var x2 Similar to the case of x1, we can express x2 via Nonlin + by x2 := œï x2 (h 2; ŒΩ2, var2) where œïx 2 (‚àí; ‚àí) : R √ó R2 ‚Üí R, and œïx 2 (z; Œ∏1, Œ∏2) def = ReLU ( z ‚àí Œ∏1‚àöŒ∏2 ) . Since œÉw > 0, we also have Àövar2 > 0. Then by the same reasoning as in the case of x1, œï x2 is parameter-controlled at ÀöŒò x2 = (ÀöŒΩ2, Àövar2). C-vars ŒΩ1‚Ä≤, var1‚Ä≤ and H-var x1‚Ä≤ These calculations proceed similarly to those for ŒΩ1, var1 and x1. We end up with ÀöŒΩ1‚Ä≤ = 0, Àövar1‚Ä≤ = œÉ2 w m ‚à•x‚Ä≤‚à•2, and, for each u ‚àà {ŒΩ1‚Ä≤, var1‚Ä≤, x 1‚Ä≤}, the associated nonlinearity œï u is parameter-controlled at limit parameter Œòu. G-var h 2‚Ä≤ By the MatMul case of Eq. (37), Œ£(h 2, h 2‚Ä≤) = œÉ2 w E z,z‚Ä≤ œï(z;ÀöŒΩ1, Àövar1)œï(z;ÀöŒΩ1‚Ä≤, Àövar1‚Ä≤), Œ£(h 2‚Ä≤, h 2‚Ä≤) = œÉ2 w E z‚Ä≤ œï(z‚Ä≤;ÀöŒΩ1‚Ä≤, Àövar1‚Ä≤) 2 and Œ£(h 2‚Ä≤, g) = 0 for all other G-var g (by the ‚Äúotherwise‚Äù case of Eq. (37)), where (z, z‚Ä≤) ‚àº N (¬µ|W 1x,W 1x‚Ä≤, Œ£|W 1x,W 1x‚Ä≤) = N (0, œÉ2 w m (‚à•x‚à• 2 x‚ä§x ‚Ä≤ x‚ä§x ‚Ä≤ ‚à•x‚Ä≤‚à•2 )) and œï(z; Œ∏1, Œ∏2) def = œï x1(z; Œ∏1, Œ∏2) = œï x1‚Ä≤(z; Œ∏1, Œ∏2) = ReLU ( z ‚àí Œ∏1‚àöŒ∏2 ) . We can simplify Œ£|h2,h2‚Ä≤ = œÉ2 w E Àúz,Àúz‚Ä≤ ReLU(Àúz)ReLU(Àúz‚Ä≤), (Àúz, Àúz‚Ä≤) ‚àº N ( 0, ( 1 x‚ä§x ‚Ä≤ ‚à•x‚à•‚à•x‚Ä≤‚à• x ‚ä§x‚Ä≤ ‚à•x‚à•‚à•x‚Ä≤‚à• 1 )) = œÉ2 wVReLU ( 1 x‚ä§x‚Ä≤ ‚à•x‚à•‚à•x‚Ä≤‚à• x‚ä§x ‚Ä≤ ‚à•x‚à•‚à•x‚Ä≤‚à• 1 ) , where VReLU is as given in Fact B.2. In particular, with c def = x‚ä§x‚Ä≤ ‚à•x‚à•‚à•x‚Ä≤‚à• , this yields Œ£(h 2‚Ä≤, h 2‚Ä≤) = Œ£(h 2, h 2) = 1 2 œÉ2 w, Œ£(h 2, h 2‚Ä≤) = œÉ2 w 2œÄ (‚àö 1 ‚àí c2 + (œÄ ‚àí arccos c)c). (41) 45 C-vars ŒΩ2‚Ä≤, var2‚Ä≤ and H-var x2‚Ä≤ These calculations proceed similarly to those for ŒΩ2, var2 and x2. We end up with ÀöŒΩ2‚Ä≤ = 0, Àövar2‚Ä≤ = 1 2 œÉ2 w, and, for each u ‚àà {ŒΩ2‚Ä≤, var2‚Ä≤, x 2‚Ä≤}, the associated nonlinearity œï u is parameter-controlled at limit parameter Œòu. D.1.2 Computing the GP Kernel It is easy to see that the set of H-vars are all linearly independent almost surely. Therefore we may apply Corollary C.13. By Corollary C.13, (v‚ä§x2/ ‚àön, v‚ä§x2‚Ä≤/ ‚àön) converges in distribution to N (0, K) where K = œÉ2 v E z,z‚Ä≤ ( œï(z; ÀöŒò x2)2 œï(z; ÀöŒòx2)œï(z; ÀöŒòx2‚Ä≤) œï(z; ÀöŒòx2)œï(z; ÀöŒòx2‚Ä≤) œï(z‚Ä≤; ÀöŒòx 2 ‚Ä≤)2 ) where œï(z; Œ∏1, Œ∏2) def = ReLU ( z‚àíŒ∏1‚àöŒ∏2 ) and (z, z‚Ä≤) ‚àº N (¬µ|h2,h2‚Ä≤, Œ£|h2,h2‚Ä≤) with ¬µ|h2,h2 ‚Ä≤ = 0 and Œ£|h2,h2‚Ä≤ given in Eq. (41). Since Àöœëx 2 1 = ÀöŒΩ2 = Àöœëx2‚Ä≤ 1 = ÀöŒΩ2‚Ä≤ = 0 and Àöœëx2 2 = Àövar2 = Àöœëx2‚Ä≤ 2 = Àövar2‚Ä≤ = 1 2 œÉ2 w, we can simplify K = œÉ2 vVReLU ((œÉ2 w/2)‚àí1Œ£|h2,h2 ‚Ä≤) = 2œÉ2 v œÉ2 w VReLU (Œ£|h2,h2‚Ä≤) . D.2 Layernorm: General Case As mentioned in Appendix A, layernorm in general can be implemented with Nonlin +. Suppose y1, . . . , yk : H(n) are H-vars deÔ¨Åned by yi := œïi(g1, . . . , gM ; Œò i) for (possibly self- )parametrized nonlinearities œïi(‚àí; ‚àí) : Rm ‚Üí R, i ‚àà [k] and parameters Œòi (possibly dependent on previous G-vars). Suppose that each Œòi converges almost surely to a deterministic vector ÀöŒòi, and suppose each œïi is parameter-controlled at ÀöŒòi. Each of yi has mean ŒΩ(yi) def = 1 n n‚àë Œ±=1 yi Œ± = 1 n n‚àë Œ±=1 œïi(g1 Œ±, . . . , gM Œ± ; Œò i) and variance œÉ2(yi) def = 1 n n‚àë Œ±=1 (yi Œ±)2 ‚àí ŒΩ(yi) 2 = 1 n n‚àë Œ±=1 œïi(g1 Œ±, . . . , gM Œ± ; Œò i)2 ‚àí ŒΩ(yi) 2. Under generic conditions (i.e. Assumption C.3 and parameter-control), Theorem C.4 or Theorem C.11 applies, so that ŒΩ(yi) a.s. ‚àí‚àí‚Üí ÀöŒΩ(yi) def = E Z œïi (Z; ÀöŒòi) , and œÉ2(yi) a.s. ‚àí‚àí‚Üí ÀöœÉ2(yi) def = E Z œïi (Z; ÀöŒòi)2 ‚àí [ E Z œïi (Z; ÀöŒòi)]2 where Z ‚àº N (¬µ, Œ£). Layernorm(yi) can then be expressed via a self-parametrized (DeÔ¨Ånition C.8) Nonlin + rule like so Layernorm(yi) = œà(yi; ŒΩ(yi), œÉ2(yi)), where œà(z; a, b) def = (z ‚àí a)/ ‚àöb. It‚Äôs easy to check that œà(z; a, b) is parameter-controlled at Àöa,Àöb as long as Àöb Ã∏= 0. Assuming rank stability (Assumption C.3) is not violated by the new variables, Theorem C.4 holds, so that, intuitively, this application of Nonlin + can be replaced with a straightforward application of Nonlin: ‚ÄúLayernorm(yi) = œà(yi; ŒΩ(yi), œÉ2(yi))‚Äù ‚Üí ‚ÄúLayernorm(yi) = œà(yi;ÀöŒΩ(yi),ÀöœÉ2(yi))‚Äù. Therefore, if we deÔ¨Åne the kernel matrices ‚Ñ¶ij = lim n‚Üí‚àû yi‚ä§yj/n ¬Ø‚Ñ¶ij = lim n‚Üí‚àû Layernorm(yi)‚ä§Layernorm(yj)/n, 46 then ¬Ø‚Ñ¶ij = lim n‚Üí‚àû 1 n (yi ‚àí ŒΩ(yi))‚ä§(yj ‚àí ŒΩ(yj)) ‚àöœÉ2(yi)œÉ2(yj) = lim n‚Üí‚àû y‚ä§ i yj/n ‚àí ŒΩ(yi)ŒΩ(yj) ‚àöœÉ2(yi)œÉ2(yj) = lim n‚Üí‚àû y‚ä§ i yj/n ‚àí ÀöŒΩ(yi)ÀöŒΩ(yj) ‚àö ÀöœÉ2(yi)ÀöœÉ2(yj) ¬Ø‚Ñ¶ = D‚àí1/2(‚Ñ¶ ‚àí ÀöŒΩÀöŒΩ‚ä§)D‚àí1/2, where ÀöŒΩ is the column vector (ÀöŒΩ(y1), . . . ,ÀöŒΩ(yk))‚ä§ and D = Diag(‚Ñ¶ ‚àí ÀöŒΩÀöŒΩ‚ä§). In summary, Computing Layernorm Kernel Suppose y1, . . . , yk : H(n) are H-vars deÔ¨Åned by yi := œïi(g1, . . . , gM ; Œò i) for (possibly self-)parametrized nonlinearities œïi(‚àí; ‚àí) : Rm ‚Üí R, i ‚àà [k] and parameters Œòi. Assume that each Œòi converges almost surely to a deterministic vector ÀöŒòi, and that each œïi is parameter-controlled at ÀöŒòi. If we deÔ¨Åne the kernel matrices ‚Ñ¶ij = lim n‚Üí‚àû yi‚ä§yj/n ¬Ø‚Ñ¶ij = lim n‚Üí‚àû Layernorm(yi)‚ä§Layernorm(yj)/n, then, assuming generic conditions (see main text above), ¬Ø‚Ñ¶ = D‚àí1/2(‚Ñ¶ ‚àí ÀöŒΩÀöŒΩ‚ä§)D‚àí1/2, where D = Diag(‚Ñ¶ ‚àí ÀöŒΩÀöŒΩ‚ä§) and ÀöŒΩ is the vector given by ÀöŒΩi = E œïi(Z; ÀöŒòi), Z ‚àº N (¬µ, Œ£). D.3 Transformer (Program 10) The Transformer Variant, in Mathematical Terms We‚Äôll work with the following transformer model. Let x0 1, . . . , x0 t be a sequence of inputs (the superscript will be layer index, and the subscript will be token index). Then each layer l of our transformer works like the following kl i = U lxl‚àí1 i ‚àà Rn h l i = Layernorm(kl i + MaskedAttentioni(kl i, {kl j} t j=1, {kl j} t j=1)) xl i = Layernorm(W l2relu(W l1hl i + b l1) + b l2 + W l1h l i) (42) where U l, W l1, W l2 are weights and b l1, b l2 are the biases, and MaskedAttentionj(q, {ki} r i=1, {vi} r i=1) = r‚àë i=1 a j i vi, where a j i = SoftMax(q‚ä§k1/n, . . . , q‚ä§kj/n, ‚àí‚àû, . . . , ‚àí‚àû)i (43) as described in Appendix A. Note that we make the following simpliÔ¨Åcations for ease of presentation, but all of them can be removed at the expense of more complex NETSOR programs. 1. We are forgoing positional embeddings 2. The keys, values, and queries here are the same, compared to the standard version, where they are different linear projections of xl‚àí1 i 3. There is only 1 head, compared to the standard multi-head attention 4. The skip connection has base W l2h l i instead of just h l i 47 Self-parametrized NETSOR+ program 10 Transformer Input: U 1x0 1, . . . , U 1x0 t : G(n) Input: ‚àÄl = 1, . . . , L : W l1, W l2 : A(n, n) Input: ‚àÄl = 2, . . . , L : U l : A(n, n) Input: ‚àÄl = 1, . . . , L : b l1, b l2 : G(n) Input: v : G(n) 1: for l = 1, . . . , L do 2: for i = 1, . . . , t do 3: // if l = 1, apply LinComb 4: // if l ‚â• 2, apply MatMul 5: kl i := U lxl‚àí1 i : G(n) 6: end for 7: for i = 1, . . . , t do 8: for j = 1, . . . , t do 9: // Moment 10: cij := kl i‚ä§kl j/n : C 11: end for 12: // With a i j being shorthand for 13: // SoftMax(ci1, . . . , cii, ‚àí‚àû, . . . , ‚àí‚àû)j 14: // Mean, post attention 15: ŒΩi := 1 n ‚àën Œ±=1(kl i + ‚àët j=1 a i jkl j)Œ± : C 16: // Variance, post attention 17: vari = 1 n ‚àën Œ±=1(kl i + ‚àët j=1 a i jkl j)2 Œ± ‚àí ŒΩ2 i : C 18: // applying Nonlin + to express attention+layernorm 19: h l i := (kl i + ‚àët j=1 a i jkl j ‚àí ŒΩi1)/ ‚àövari : H(n) 20: end for 21: for i = 1, . . . , t do 22: yl1 i := W l1h l i : G(n) 23: ÀÜyl1 i := yl1 i + bl1 : G(n) 24: ÀÜxl1 i := ReLU(ÀÜyl1 i ) : H(n) 25: yl2 i := W l2 ÀÜxl1 i : G(n) 26: ÀÜyl2 i := yl2 i + bl2 : G(n) 27: // Layernorm mean and variance 28: ŒΩ‚Ä≤ i := 1 n ‚àën Œ±=1(ÀÜyl2 i )Œ± + (yl1 i )Œ± : C 29: var‚Ä≤ i := 1 n ‚àën Œ±=1((ÀÜyl2 i )Œ± + (yl1 i )Œ±)2 ‚àí (ŒΩ‚Ä≤ i)2 : C 30: // Layernorm 31: xl i := (ÀÜyl2 i + yl1 i ‚àí ŒΩ‚Ä≤ i1)/ ‚àövar‚Ä≤ i : H(n) 32: end for 33: end for Output: (v‚ä§xL 1 / ‚àön, . . . , v‚ä§xL t /‚àön) Setup assume for all Œ±, Œ≤ ‚àà [n], ‚Ä¢ W l1 Œ±Œ≤, W l2 Œ±Œ≤ ‚àº N (0, œÉ2 w/n) for all l ‚â• 1 ‚Ä¢ U l Œ±Œ≤ ‚àº N (0, œÉ2 u/n) for all l ‚â• 2 and U 1 Œ±Œ≤ ‚àº N (0, œÉ2 u/m) ‚Ä¢ b l1 Œ± , b l2 Œ± ‚àº N (0, œÉ2 b ) for all l. ‚Ä¢ vŒ± ‚àº N (0, œÉ2 v) D.3.1 Expressing the Composition of Attention, Skip Connection, and Layernorm via Nonlin + and Moment Program 10 captures the computation of this transformer on an input sequence. Let us explain how Eq. (42) is expressed in Program 10. Throughout the below, we will use the easy observation that ¬µ(g) = 0 for all G-vars g. For any layer l, we proceed as follows. 48 Attention Weights First, cij in Line 10 represents a pre-SoftMax logit for the attention weights. They are introduced via Moment by cij := 1 n n‚àë Œ±=1 œï cij ((kl i)Œ±, (kl j)Œ±), where œïcij (z1, z2) = z1z2. This implies Àöcij = E Z‚àºN (¬µ,Œ£) Z kl iZ kl j = Œ£(kl i, kl j), (44) where we used ¬µ(g) = 0 for all G-vars g. Layernorm Mean and Variance Next, ŒΩi in Line 15 and vari in Line 17 represent the mean and variance of the post-attention embedding of the ith token. They are introduced via Moment by ŒΩi := 1 n n‚àë Œ±=1 œï ŒΩi((kl 1)Œ±, . . . , (kl t)Œ±; ci1, . . . , cii) vari := 1 n n‚àë Œ±=1 œï vari((kl 1)Œ±, . . . , (kl t)Œ±; ci1, . . . , cii, ŒΩi) where œï ŒΩi(z1, . . . , zt; Œ∏1, . . . , Œ∏i) def = zi + t‚àë j=1 ajzj, where (a1, . . . , at) = SoftMax(Œ∏1, . . . , Œ∏i, ‚àí‚àû, . . . , ‚àí‚àû), (45) and similarly, œï vari(z1, . . . , zt; Œ∏1, . . . , Œ∏i, ŒΩ) def = (zi + t‚àë j=1 ajzj) 2 ‚àí ŒΩ2, where (a1, . . . , at) are as in Eq. (45). Note that both œï ŒΩi and œï vari are pseudo-Lipschitz in both their inputs and parameters jointly, so that they are parameter-controlled by Example C.2. Their limit parameters can be computed as ÀöŒΩi = ¬µ(kl i) + t‚àë j=1Àöaj¬µ(kl j) = 0 where (Àöa1, . . . ,Àöat) = SoftMax(ÀöŒ∏1, . . . , ÀöŒ∏i, ‚àí‚àû, . . . , ‚àí‚àû), (46) since ¬µ = 0 identically, and Àövari = Œ£(kl i, kl i) + 2 ‚àë j ÀöajŒ£(kl i, kl j) + ‚àë j,j‚Ä≤ ÀöajÀöaj‚Ä≤Œ£(kl j, kl j‚Ä≤) (47) with Àöaj same as in Eq. (46). Putting Them All Together Finally, h l i in Line 19 represents the post-layernorm activations and is introduced via Nonlin + by h l i := œïh l i(kl 1, . . . , kl t; ci1, . . . , cii, ŒΩi, vari) where œï h l i (z1, . . . , zt; Œ∏1, . . . , Œ∏i, ŒΩ, var) := (zi + t‚àë j=1 ajzj ‚àí ŒΩ)/‚àövar (48) where (a1, . . . , at) are as in Eq. (45). If Àövari > 0, then one can show that œï h l i is parameter-controlled at (Àöci1, . . . ,Àöcii,ÀöŒΩi, Àövari) via the same reasoning as in Appendix D. When is Àövari > 0? From Eq. (47), because the ai are all nonnegative, Àövari = 0 implies that Œ£(kl i, kl i) = 0. This is impossible if all of the input tokens xi are nonzero and the weight variances satisfy œÉw, œÉu > 0, as one can easily see. 49 D.3.2 Computing the GP Kernel By Corollary C.13, the output vector converges in distribution to N (0, K), where K ‚àà Rt√ót, and Kij = œÉ2 v E Z‚àºN (¬µ,Œ£) œï xL i (Z; ÀöŒòxL i )œï xL j (Z; ÀöŒòxL j ). Here Œòx L i = {ŒΩ‚Ä≤ i, var‚Ä≤ i} as given in Lines 28 and 29, and œï xL i (Z; ŒΩ, var) = (Z ÀÜyL2 i + Z yl1 i ‚àí ŒΩ)/‚àövar. Simultaneously, ÀöŒΩ‚Ä≤ i = E Z Z ÀÜyL2 i + Z yL1 i , Àövar‚Ä≤ i = E Z(Z ÀÜyL2 i + Z yL1 i ) 2 ‚àí (ÀöŒΩ‚Ä≤ i)2. Thus, to compute K, it sufÔ¨Åces to compute the restriction Œ£|yL1 1 ,...,yL1 t ,ÀÜyL2 1 ,...,ÀÜyL2 t , from which K can be computed by the equations above. However, notice that by the ‚Äúotherwise‚Äù case Eq. (37), Œ£(h L2 i , h L1 i ) = 0 because ÀÜh L2 i and h L1 i are introduced by MatMul with different A-vars, and consequently Œ£(ÀÜh L2 i , h L1 i ) = Œ£(h L2 i , h L1 i ) + Œ£(bl2, h L1 i ) = 0. Therefore, we only need to compute Œ£|yL1 1 ,...,yL1 t and Œ£|ÀÜyL2 1 ,...,ÀÜyL2 t separately. Then K is given by K = œÉ2 vD‚àí1/2(Œ£|yL1 1 ,...,yL1 t + Œ£|ÀÜyL2 1 ,...,ÀÜyL2 t )D‚àí1/2, (49) where D is the diagonal matrix with diagonal equal to the diagonal of Œ£|yL1 1 ,...,yL1 t + Œ£|ÀÜyL2 1 ,...,ÀÜyL2 t . D.3.3 Computing Œ£ Let Œ£ÀÜyl2 def = Œ£|ÀÜyl2 1 ,...,ÀÜyl2 t , Œ£yl1 def = Œ£|yl1 1 ,...,yl1 t , Œ£kl def = Œ£|kl 1,...,kl t resp. be the restriction of Œ£ to {ÀÜyl2 i }i, {yl1 i }i, and {ÀÜkl i}i. As explained above, the kernel of the Gaussian process underlying the output vector (v‚ä§xL 1 / ‚àön, . . . , v‚ä§x L t /‚àön) can be computed from Œ£ÀÜyL2 . In this section, we shall describe equations tying together Œ£ÀÜyl2, Œ£yl1 , Œ£kl that will allow us to compute Œ£ÀÜyL2 recursively. Computing Œ£yl1 from Œ£kl . The G-var yl1 i is introduced as yl1 i := W l1h l i. Then given Eq. (48), we have, for any i, i ‚Ä≤ ‚àà [t], Œ£(yl1 i , yl1 i‚Ä≤ ) = œÉ2 w‚àö Àövari Àövari‚Ä≤ Ô£´ Ô£≠Œ£(kl i, kl i‚Ä≤) + ‚àë j Àöa i jŒ£(kl j, kl i‚Ä≤) + ‚àë j‚Ä≤ Àöa i‚Ä≤ j‚Ä≤Œ£(kl i, kl j‚Ä≤) + ‚àë j,j‚Ä≤ Àöa i jÀöa i‚Ä≤ j‚Ä≤Œ£(kl j, kl j‚Ä≤) Ô£∂ Ô£∏ , (50) where (Àöa i 1, . . . ,Àöa i t) = SoftMax(Àöci1, . . . ,Àöcii, ‚àí‚àû, . . . , ‚àí‚àû) = SoftMax(Œ£(kl i, kl 1), . . . , Œ£(kl i, kl i), ‚àí‚àû, . . . , ‚àí‚àû) by Eq. (44), and likewise for i ‚Ä≤. This reduces computing Œ£yl1 to computing Œ£kl . Computing Œ£ÀÜyl2 from Œ£yl1 . By some simple calculations in the vein of Appendix B.1.2, we can also see Œ£ÀÜyl2 = œÉ2 wVReLU (Œ£yl1 + œÉ2 b ) + œÉ2 b . (51) 50 Computing Œ£kl+1 from Œ£ÀÜyl2. Finally, following the same reasoning as in Appendix D.1, we get ÀöŒΩ‚Ä≤ i = 0, Àövar‚Ä≤ i = Œ£(ÀÜyl2 i , ÀÜyl2 i ) + Œ£(yl1 i , yl1 i ), œï xl i is parameter-controlled at ÀöŒòxl i as long as Àövar‚Ä≤ i > 0, and Œ£kl+1 = œÉ2 uD‚àí1/2(Œ£ÀÜyl2 + Œ£yl1)D‚àí1/2 (52) where D = Diag(Œ£ÀÜyl2 + Œ£yl1 ). Putting them all together, Eqs. (50) to (52) along with Eq. (49) yield the complete set of equations to compute the GP kernel of a transformer. D.3.4 Vectorized Implementation: Single Sequence Eqs. (49), (51) and (52) are already in vectorized forms. The following equation expresses Eq. (50) in a vectorized form as well: Œ£yl = œÉ2 wD‚àí1/2(I + ‚àÜ)Œ£kl (I + ‚àÜ)‚ä§D‚àí1/2 where ‚Ä¢ ‚àÜ = SoftMax(Mask(Œ£kl )), with SoftMax applied to each row, and Mask(Œ£kl ) is the same as Œ£kl , except that its upper triangular portion (above the diagonal) is all set to ‚àí‚àû, and ‚Ä¢ D is the diagonal matrix with diagonal equal to the diagonal of (I + ‚àÜ)Œ£kl (I + ‚àÜ)‚ä§. Here, ‚àÜ is the attention weights, masked so that a token‚Äôs embedding cannot depend on those of future tokens. The identity matrix I appears due to the skip connection. And the multiplication by D‚àí1/2 is as result of layernorm. D.3.5 Vectorized Implementation: Double Sequence Program 10 only expresses the computation of a transformer on a single sequence. In general, the GP kernel will also have covariances between the embeddings of tokens of one sequence and those of tokens of another sequence. One can derive the computation of these covariances just as we did above for a single sequence. Below, we will just summarize the vectorized implementation for computing the joint kernel over multiple input sequences. One should think of Œ£ l below as the tensor of Œ£kl over every pair of sequences, and one should think of ÀÜŒ£l as the same for Œ£yl . 51 Computing Transformer Kernel Suppose we have p input sequences {(x1a, . . . , xta)} p a=1, each with t tokens. Suppose each sequence is processed by a transformer as in Program 10, and the transformer‚Äôs parameters are sampled with nonzero variances as follows. ‚Ä¢ W l1 Œ±Œ≤, W l2 Œ±Œ≤ ‚àº N (0, œÉ2 w/n) for all l ‚â• 1 ‚Ä¢ U l Œ±Œ≤ ‚àº N (0, œÉ2 u/n) for all l ‚â• 2 and U 1 Œ±Œ≤ ‚àº N (0, œÉ2 u/m) ‚Ä¢ bl1 Œ± , b l2 Œ± ‚àº N (0, œÉ2 b ) for all l. ‚Ä¢ vŒ± ‚àº N (0, œÉ2 v) Then the transformer‚Äôs outputs, one scalar for each input token, converge in distribution to a Gaussian N (0, K) where K ‚àà Rpt√ópt can be computed as follows: 1. Initialize Œ£0 ‚àà Rt√óp√ót√óp by Œ£ 0 iajb ‚Üê œÉ2 ux‚ä§ iaxjb/m for all a, b ‚àà [p] and i, j ‚àà [t]. 2. For l = 1, . . . , L, do (a) For a = 1, . . . , p, do i. Œ£l‚àí1,a ‚Üê Œ£l‚àí1 ‚Ä¢a‚Ä¢a be the ath ‚Äúdiagonal block‚Äù ii. ‚àÜla ‚Üê SoftMax(Mask(Œ£ l‚àí1,a)), where Mask replaces the upper trian- gular portion (above the diagonal) with ‚àí‚àû, and SoftMax is applied row-wise. (b) ‚àÜl ‚Üê block diagonal matrix with ‚àÜl1, . . . , ‚àÜlp as blocks. (c) // below, we treat each tensor as a (pt √ó pt) matrix. (d) ÀÜŒ£l ‚Üê (I + ‚àÜl)Œ£l‚àí1(I + ‚àÜl) ‚ä§ (e) ÀÜŒ£l ‚Üê œÉ2 wD‚àí1/2 ÀÜŒ£lD‚àí1/2, where D = Diag(Œ£l) (f) Œ£l ‚Üê œÉ2 wVReLU( ÀÜŒ£ l + œÉ2 b ) + œÉ2 b (g) Œ£l ‚Üê œÉ2 uD‚àí1/2(Œ£ l + ÀÜŒ£ l)D‚àí1/2, where D = Diag(Œ£ l + ÀÜŒ£ l) 3. Return œÉ2 v œÉ2 u Œ£L See our repo github.com/thegregyang/GP4A for an implementation of this algorithm. E Different Versions of Tensor Programs DeÔ¨Ånition E.1. A NETSOR‚àí program is a NETSOR program without the LinComb rule. Remark E.2. Any NETSOR program is semantically identical to a NETSOR‚àí program, by absorbing any usage of LinComb into a downstream nonlinearity (e.g., if g := g1 + g2, and h := œï(g), write h := œï(g1 + g2) directly as an application of Nonlin), or if there is no downstream nonlinearity, treat it as an application of Nonlin. Because LinComb allows one to express certain gadgets such as skip connection and convolutions more easily, we chose to present NETSOR as the canonical version of Tensor Program here. See Appendix J for a formal speciÔ¨Åcation of NETSOR‚àí . By the remark above, the following NETSOR‚àí Master Theorem is equivalent to Theorem 5.4. Theorem E.3 (NETSOR‚àí Master Theorem). Fix any NETSOR‚àí program satisfying Assumption 5.1 and with all nonlinearities controlled. If g1, . . . , gM are all of the G-vars (including all input G-vars), then for any controlled œà : RM ‚Üí R, as n ‚Üí ‚àû, 1 n n‚àë Œ±=1 œà(g1 Œ±, . . . , gM Œ± ) a.s. ‚àí‚àí‚Üí E Z‚àºN (¬µ,Œ£) œà(Z) = E Z‚àºN (¬µ,Œ£) œà(Z g1, . . . , Z gM ), where a.s. ‚àí‚àí‚Üí means almost sure convergence, Z = (Z g1, . . . , Z gM ) ‚àà RM , and ¬µ = {¬µ(gi)} M i=1 ‚àà RM and Œ£ = {Œ£(gi, gj)} M i,j=1 ‚àà RM √óM are given in Eq. (2) (note that the cases involving LinComb in Eq. (2) are now vacuous in this setting with NETSOR‚àí program). See Fig. 1 for an illustration. To prove Theorem 5.4, we will in fact prove Theorem E.3; see Appendix H. 52 DeÔ¨Ånition E.4. A NETSOR‚ó¶ program (pronounced ‚ÄúNet-Sor-O‚Äù) is a NETSOR program but where Nonlin rules allow nonlinearities œï to take H-vars. NETSOR‚ó¶ programs are thus a superset of NETSOR programs. Similarly, a NETSOR‚ó¶ + (pronounced ‚ÄúNet-Sor-O-Plus‚Äù) program is a NETSOR+ program but where Nonlin + rules allow nonlinearities œï to take H-vars. Remark E.5. Any NETSOR‚ó¶ program is semantically identical to a NETSOR program: If g := W h is any application of MatMul, we can rewrite h as a function of G-vars only by unwinding its deÔ¨Ånition recursively (e.g., if h := œï(h 1, g) and h 1 := œà(g1, g2), then we can write directly h := œï(œà(g1, g2), g) using a single application of Nonlin in G-vars). Likewise, any NETSOR‚ó¶+ program can be rewritten as a NETSOR+ program without losing any information. NETSOR‚ó¶ programs can be more concise than NETSOR programs by reusing H-vars more efÔ¨Åciently; see Program 11 for GRU expressed in NETSOR‚ó¶ , and compare to Program 5. However, the Master Theorem is more complicated to state, and the task of unwinding the nonlinearity just shifts from the program to the scaling limit computation stage; see Eq. (53) below. This is why we did not present NETSOR‚ó¶ as the canonical version of Tensor Programs. DeÔ¨Ånition E.6. Fix a NETSOR‚ó¶ program. For any H-var h, let œï h be the unwinded nonlinearity ex- pressing h as a function of only G-vars, as described in Remark E.5, i.e. h = œïh(g1, . . . , gM ). For ex- ample, if h := œï(h 1, g3) and h 1 := œà(g1, g2), then h = œï(œà(g1, g2), g3) and œï h = œï(œà(‚àí, ‚àí), ‚àí). Similarly, in a NETSOR‚ó¶ + program, if h is an H-var, let œïh be the unwinded nonlinearity (possibly with parameters) expressing h as a function only G-vars, h = œï h(g1, . . . , gM ; Œò). For example, if h := œï(h 1, g3; Œ∏2) and h 1 := œà(g1, g2; Œ∏1), then h = œï(œà(g1, g2; Œ∏1), g3; Œ∏2) and œï h(‚àí, ‚àí, ‚àí; Œ∏1, Œ∏2) = œï(œà(‚àí, ‚àí; Œ∏1), ‚àí; Œ∏2). Note that this œï h notation is consistent with the semantics of the same notation deÔ¨Åned in DeÔ¨Ånition C.10, where there is nothing to unwind. The extended mean and covariance ¬µ and Œ£ can still be computed as before in a NETSOR‚ó¶ program. The only difference is that we are using the unwinded nonlinearities œï h instead. ¬µ(g) = Ô£± Ô£≤ Ô£≥ ¬µ in(g) if g is input ‚àë i ai¬µ(yi) if g = ‚àë i aiyi, introduced by LinComb 0 otherwise , Œ£(g, g‚Ä≤) = Ô£± |||||Ô£≤ |||||Ô£≥ Œ£in(g, g‚Ä≤) if g, g‚Ä≤ are inputs ‚àë i aiŒ£(yi, g‚Ä≤) if g = ‚àë i aiyi, introduced by LinComb ‚àë i aiŒ£(g, yi) if g‚Ä≤ = ‚àë i aiyi, introduced by LinComb œÉ2 W EZ œï h(Z)œï h‚Ä≤(Z) if g = W h, g‚Ä≤ = W h ‚Ä≤, introduced by MatMul w/ same A-var W 0 otherwise (53) where œï h and œï h ‚Ä≤ is as deÔ¨Åned in DeÔ¨Ånition E.6 and Z ‚àº N (¬µ, Œ£). Theorem E.7 (NETSOR‚ó¶ Master Theorem). Fix any NETSOR‚ó¶ program satisfying Assumption 5.1 and with all unwinded nonlinearities œïh controlled, for all H-vars h. If g1, . . . , gM are all of the G-vars (including all input G-vars), then for any controlled œà : RM ‚Üí R, as n ‚Üí ‚àû, 1 n n‚àë Œ±=1 œà(g1 Œ±, . . . , gM Œ± ) a.s. ‚àí‚àí‚Üí E Z‚àºN (¬µ,Œ£) œà(Z) = E Z‚àºN (¬µ,Œ£) œà(Z g1, . . . , Z gM ), where a.s. ‚àí‚àí‚Üí means almost sure convergence, Z = (Z g1, . . . , Z gM ) ‚àà RM , and ¬µ = {¬µ(gi)} M i=1 ‚àà RM and Œ£ = {Œ£(gi, gj)} M i,j=1 ‚àà RM √óM are given in Eq. (53). See Fig. 1 for an illustration. Theorem E.8. Fix any NETSOR‚ó¶+ program satisfying Assumption 5.1 and Assumption C.3. Suppose for each parametrized unwinded nonlinearity œï h(‚àí; Œò), the parameters Œò are instantiated with random variables that converge almost surely to some deterministic vector ÀöŒò as n ‚Üí ‚àû, and assume œï h is parameter-controlled at ÀöŒò. If g1, . . . , gM are all of the G-vars (including all input G-vars), then for any l, for any random vector Œò ‚àà Rl that converges almost surely to a deterministic vector 53 NETSOR‚ó¶ program 11 GRU, with Gating Function œÉ and Activation Function œï // Embeddings of input sequence Input: Uzx1, . . . , Uzxt : G(n) Input: Urx1, . . . , Urxt : G(n) Input: Uhx 1, . . . , Uhxt : G(n) // Parameters Input: Wz, Wr, Wh : A(n, n) Input: bz, br, bh : G(n) // Initial GRU state Input: h 0 : G(n) // Readout layer Input: v : G(n) // Time step 1 h 1 z := Wzh 0 : G(n) Àúz1 := h 1 z + Uzx1 + bz : G(n) h 1 r := Wrh0 : G(n) Àúr1 := h 1 r + Urx1 + br : G(n) // œÉ is gating function, typically sigmoid; applying Nonlin ÀÜh 0 := h 0 ‚äô œÉ(Àúr1) : H(n) h 1 h := WhÀÜh 0 : G(n) Àúh 1 := h 1 h + Uhx1 + bh : G(n) // Apply Nonlin // œï is activation function, typically tanh h 1 := (1 ‚àí œÉ(Àúz1)) ‚äô h 0 + œÉ(Àúz1) ‚äô œï(Àúh 1) : H(n) // Time step 2 h 2 z := Wzh 1 : G(n) Àúz2 := h 2 z + Uzx2 + bz : G(n) h 2 r := Wrh 1 : G(n) Àúr2 := h 2 r + Urx2 + br : G(n) // No longer need to unwind h1 as in Program 5 ÀÜh 1 = œÉ(Àúr1) ‚äô h 1 : H(n) h 2 h := WhÀÜh 1 : G(n) Àúh 2 := h 2 h + Uhx2 + bh : G(n) // No longer need to unwind h 1 as in Program 5 h 2 := (1 ‚àí œÉ(Àúz2)) ‚äô h 1 + œÉ(Àúz2) ‚äô œï(Àúh 2) : H(n) // Time step 3 ... // Time step t // DeÔ¨Åne Àúzt, Àúrt, Àúh t just like above ... // No longer need to unwind ht ‚àí 1 as in Program 5 h t := (1 ‚àí œÉ(Àúzt)) ‚äô ht‚àí1 + œÉ(Àúzt) ‚äô œï(Àúh t) : H(n) Output: (v‚ä§h 1/ ‚àön, . . . , v‚ä§h t/ ‚àön) ÀöŒò, as n ‚Üí ‚àû, and for any œà : RM √ó Rl ‚Üí R parameter-controlled at ÀöŒò, 1 n n‚àë Œ±=1 œà(g1 Œ±, . . . , gM Œ± ; Œò) a.s. ‚àí‚àí‚Üí E Z‚àºN (¬µ,Œ£) œà(Z; ÀöŒò), where a.s. ‚àí‚àí‚Üí means almost sure convergence, Z ‚àà RM , and ¬µ ‚àà RM and Œ£ ‚àà RM √óM are given in Eq. (53), calculated by replacing each parametrized unwinded nonlinearity œï(‚àí; Œò) with parameter- less nonlinearity œï(‚àí; ÀöŒò). 54 F Programs with Variable Dimensions Notation In this section, we let dim(x) denote the dimension of an H-var x. Before this section, we have mostly assumed that all dimensions in a NETSOR (or NETSOR+ ) program are equal. This is not necessary, and was done only to more quickly present the main ideas of this work. In general, we can allow the H-vars in a program to vary in dimension, subject to the obvious dimensionality constraints imposed by the different rules: {If y := ‚àëk i=1 aixi or y := œï(x1, . . . , xk), then the dim(y) = dim(xi) for each i. If y := W x and y‚Ä≤ := W x ‚Ä≤, then dim(x) = dim(x‚Ä≤) and dim(y) = dim(y‚Ä≤). (54) DeÔ¨Ånition F.1. Given an equivalence relation ‚âÉ on the input G-vars of a program, we extend this to an equivalence relation on all H-vars of the program by h ‚â° h ‚Ä≤ ‚áê‚áí h ‚âÉ h ‚Ä≤ OR h and h ‚Ä≤ are constrained to have the same dimension by (54). (55) We call any such equivalence class a Common Dimension Class, or CDC. Intuitively, the dimensions of H-vars in each CDC are all the same, and this common dimension is allowed to vary between CDCs. Example F.2. In Program 1, the CDCs are {W 1x, b 1, h 1, x 1} and {b2, v, Àúh 2, h 2, x 2}. In Program 2, all G-vars are in the same CDC, and given the body of the program, this is the only way to partition the H-vars into CDCs, because the reuse of W across time step ties all H-var dimensions to be equal. Assumption F.3. Fix a NETSOR program with some equivalence relation on the input G-vars, and thus with induced CDCs over its H-vars. Assume the dimensions in each CDC are the same, but the dimensions of different CDCs can vary. Suppose for each A-var W : A(m‚Ä≤, m), we sample WŒ±Œ≤ ‚àº N (œÉ2 W /m) for some œÉ2 W > 0. Suppose further for each CDC C with dimension n, for each Œ± ‚àà [n], we sample, i.i.d., {xŒ± : x ‚àà C and x is input G-var} ‚àº N (¬µ C, Œ£C) for some mean ¬µ C and covariance Œ£C over input G-vars in C. Then the following result is an easy extension of Theorem 5.4. Theorem F.4 (NETSOR Master Theorem; Variable Dimensions). Fix any NETSOR program satisfying Assumption F.3 and with all nonlinearities controlled. For any CDC C, if g1, . . . , gM are all of the G-vars (including all input G-vars) in C, then for any controlled œà : RM ‚Üí R, as all dimensions in the program tend to inÔ¨Ånity (not just the dimension of C) 13, 1 n n‚àë Œ±=1 œà(g1 Œ±, . . . , gM Œ± ) a.s. ‚àí‚àí‚Üí E Z‚àºN (¬µC,Œ£C) œà(Z) = E Z‚àºN (¬µC,Œ£C) œà(Z g1, . . . , Z gM ), where a.s. ‚àí‚àí‚Üí means almost sure convergence, Z = (Z g1, . . . , Z gM ) ‚àà RM , and ¬µ C = {¬µ C(gi)}M i=1 ‚àà RM and Œ£C = {Œ£C(gi, gj)} M i,j=1 ‚àà RM √óM are given in Eq. (56). See Fig. 1 for an illustration. DeÔ¨Ånition F.5. For any CDC C and G-vars g, g‚Ä≤ in C, deÔ¨Åne recursively ¬µ C(g) = Ô£± Ô£≤ Ô£≥ ¬µ C(g) if g is input ‚àë i ai¬µ C(yi) if g = ‚àë i aiyi, introduced by LinComb 0 otherwise , Œ£C(g, g‚Ä≤) = Ô£± |||||Ô£≤ |||||Ô£≥ Œ£C(g, g‚Ä≤) if g, g‚Ä≤ are inputs ‚àë i aiŒ£C(yi, g‚Ä≤) if g = ‚àë i aiyi, introduced by LinComb ‚àë i aiŒ£C(g, yi) if g‚Ä≤ = ‚àë i aiyi, introduced by LinComb œÉ2 W EZ œï h(Z)œï h ‚Ä≤(Z) if g = W h, g‚Ä≤ = W h ‚Ä≤, introduced by MatMul w/ same A-var W 0 otherwise (56) where Z ‚àº N (¬µ C‚Ä≤, Œ£C‚Ä≤) with C‚Ä≤ denoting the CDC of h and h ‚Ä≤. Essentially the same proof of Theorem 5.4 goes through for Theorem F.4, by noting that this proof only requires the minimum of all dimensions to go to inÔ¨Ånity. 13Note that we do not require the dimensions of different CDCs to have a convergent, Ô¨Ånite but nonzero, ratio 55 G Theoretical Tools In this section, we list a series of theoretical tools needed to prove the Master Theorems. G.1 Probability Facts Notations Given two random variables X, Y , and a œÉ-algebra A, the notation X d =A Y means that for any integrable function œï and for any random varible Z measurable on A, E œï(X)Z = E œï(Y )Z. We say that X is distributed as (or is equal in distribution to) Y conditional on A. In case A is the trivial œÉ-algebra, we just write X d = Y . The expression X d ‚àí‚Üí Y (resp. X a.s. ‚àí‚àí‚Üí Y ) means X converges to Y in distribution (resp. almost surely). Lemma G.1. Let {Xn}n‚â•1 be a sequence of random variables with zero mean. If for some p ‚àà N and for all n, E X 2p n ‚â§ cn ‚àí1‚àíœÅ, for some œÅ > 0, then Xn ‚Üí 0 almost surely. Proof. By Markov‚Äôs inequality, for any œµ > 0, Pr(|Xn| > œµ) = Pr(X 2p n > œµ 2p) ‚â§ E X 2p n /œµ 2p ‚â§ cn ‚àí1‚àíœÅ/œµ2p ‚àë n Pr(|Xn| > œµ) ‚â§ ‚àë n cn ‚àí1‚àíœÅ/œµ 2p < ‚àû. By Borel-Cantelli Lemma, almost surely, |Xn| ‚â§ œµ for all large n. Then, if we pick a sequence {œµk > 0}k converging to 0, we have that, almost surely, for each k, |Xn| ‚â§ œµk for large enough n ‚Äî i.e. almost surely, Xn ‚Üí 0. The following is a standard fact about multivariate Gaussian conditioning Proposition G.2. Suppose Rn1+n2 ‚àã x ‚àº N (¬µ, K), where we partition x = (x1, x2) ‚àà Rn1 √ó Rn2 , ¬µ = (¬µ1, ¬µ2) ‚àà Rn1 √ó Rn2 , and K = (K11 K12 K21 K22 ). Then x1 d =x2 N (¬µ|x2, K|x2) where ¬µ|x2 = ¬µ1 ‚àí K12K + 22(x2 ‚àí ¬µ2) K|x2 = K11 ‚àí K12K + 22K21. Lemma G.3 (Stein‚Äôs lemma). For jointly Gaussian random variables Z1, Z2 with zero mean, and any function œï : R ‚Üí R where E œï‚Ä≤(Z1) and E Z1œï(Z2) exists, we have E Z1œï(Z2) = Cov(Z1, Z2) E œï‚Ä≤(Z2). Proposition G.4 (Convergence of output vector to Gaussian given convergent 2nd moments). Con- sider a sequence (in t ‚àà N) of collections of random vectors {xab ‚àà Rna}ra b=1, a = 1, . . . , m, where na and xab can depend on t but m and ra are Ô¨Åxed. Suppose as t ‚Üí ‚àû, 1 na xab‚ä§xab ‚Ä≤ d ‚àí‚Üí Œ£‚àû ab,ab‚Ä≤ for some deterministic PSD matrix Œ£‚àû = {Œ£‚àû ab,a‚Ä≤b‚Ä≤}a,b,a‚Ä≤,b‚Ä≤. If va ‚àº N (0, œÉ2 aI) is sampled indepen- dently for each a, and independently from {xab}a,b, then {va‚ä§xab/ ‚àöna}a,b d ‚àí‚Üí N (0, Œ£) where the covariance Œ£ = {Œ£ab,a‚Ä≤b‚Ä≤}a,b,a‚Ä≤,b‚Ä≤ has Œ£ab,a‚Ä≤b‚Ä≤ = {œÉ2 aŒ£‚àû ab,ab‚Ä≤ if a = a ‚Ä≤ 0 else. Proof. WLOG, we assume œÉa = 1 for all a = 1, . . . , m. Let f : R‚àë a ra ‚Üí R be a bounded continuous function. We need to show that E f ({va‚ä§xab}a,b) ‚Üí E Z‚àºN (0,Œ£) f (Z) Note that if we deÔ¨Åne the PSD matrix ÀÜŒ£ by ÀÜŒ£ab,ab‚Ä≤ = 1 na xab‚ä§xab ‚Ä≤ and ÀÜŒ£ab,a‚Ä≤b‚Ä≤ = 0 if a Ã∏= a ‚Ä≤, then E f ({va‚ä§xab}a,b) = E ÀÜŒ£ E Z‚àºN (0, ÀÜŒ£) f (Z) 56 where the distribution over ÀÜŒ£ is induced by the distribution over {xab}. The function Àúf ( ÀÜŒ£) := EZ‚àºN (0, ÀÜŒ£) f (Z) is bounded because f is bounded. Thus, as ÀÜŒ£ d ‚àí‚Üí Œ£‚àû by assumption, we have E f ({va‚ä§xab}a,b) = E ÀÜŒ£ Àúf ( ÀÜŒ£) ‚Üí Àúf (Œ£ ‚àû) = E Z‚àºN (0,Œ£‚àû) f (Z) as t ‚Üí ‚àû. G.2 Review of Moore-Penrose Pseudoinverse We recall Moore-Penrose pseudoinverse and some properties of it. DeÔ¨Ånition G.5. For A ‚àà Rn√óm, a pseudoinverse of A is deÔ¨Åned as a matrix A+ ‚àà Rm√ón that satisÔ¨Åes all of the following criteria ‚Ä¢ AA+A = A ‚Ä¢ A+AA+ = A+ ‚Ä¢ (AA+) ‚ä§ = AA+ ‚Ä¢ (A+A) ‚ä§ = A+A The following facts are standard ‚Ä¢ if A has real entries, then so does A+. ‚Ä¢ The pseudoinverse always exists and is unique. ‚Ä¢ When A is invertible, A+ = A‚àí1. ‚Ä¢ (A‚ä§) + = (A+)‚ä§, which we denote as A+‚ä§. ‚Ä¢ A+ = (A‚ä§A)+A‚ä§ = A‚ä§(AA‚ä§) +. ‚Ä¢ AA+ is the orthogonal projector to the column space of A; I ‚àí A+A is the orthogonal project to the null space of A. ‚Ä¢ if A has singular value decomposition A = U ŒõV where U and V are orthogonal and Œõ has the singular values on its diagonal, then A+ = V ‚ä§Œõ +U ‚ä§ where Œõ+ inverts all nonzero entries of Œõ. ‚Ä¢ For any collection of vectors {vi} n i=1 in a Hilbert space, w 7‚Üí ‚àën i,j=1 vi(Œ£+)ij‚ü®vj, w‚ü©, where Œ£ij = ‚ü®vi, vj‚ü©, is the projection operator to the linear span of {vi}n i=1. G.3 Gaussian Conditioning Trick The Gaussian conditioning trick was introduced by Bolthausen [5] for solving the TAP equation in statistical physics. Later, this idea was used in Bayati and Montanari [3] to study the Approximate Message Passing algorithm in compressed sensing. We present a slightly more general versions of lemmas from Bayati and Montanari [3] that deal with singular matrices. Lemma G.6. Let z ‚àà Rn be a random vector with i.i.d. N (0, œÉ2) entries and let D ‚àà Rm√ón be a linear operator. Then for any constant vector b ‚àà Rn the distribution of z conditioned on Dz = b satisÔ¨Åes: z d =Dz=b D+b + Œ†Àúz where D+ is the (Moore-Penrose) pseudoinverse, Œ† is the orthogonal projection onto subspace {z : Dz = 0}, and Àúz is a random vector of i.i.d. N (0, œÉ2). Proof. When D = [Im√óm|0m√ón‚àím], this claim is immediate. By rotational symmetry, this shows that, for any vector space V and vector v orthogonal to it, conditioning z on V + v yields a Gaussian centered on v with covariance determined by Œ†V z. Then the lemma in the general case is implied by noting that {z : Dz = b} can be decomposed as {z : Dz = 0} + D+b. 57 Lemma G.7. Let A ‚àà Rn√óm be a matrix with random Gaussian entries, Aij ‚àº N (0, œÉ2). Consider Ô¨Åxed matrices Q ‚àà Rm√óq, Y ‚àà Rn√óq, P ‚àà Rn√óp, X ‚àà Rm√óp. Suppose there exists a solution in A to the equations Y = AQ and X = A‚ä§P . Then the distribution of A conditioned on Y = AQ and X = A‚ä§P is A d =Y =AQ,X=A‚ä§P E + Œ†‚ä• P ÀúAŒ†‚ä• Q where E = Y Q+ + P +‚ä§X ‚ä§ ‚àí P +‚ä§P ‚ä§Y Q+, ÀúA is an iid copy of A, and Œ†‚ä• P = I ‚àí Œ†P and Œ†‚ä• Q = I ‚àí Œ†Q in which Œ†P = P P + and Œ†Q = QQ + are the orthogonal projection to the space spanned by the column spaces of P and Q respectively. Proof. We apply Lemma G.6 to D : A 7‚Üí (AQ, P ‚ä§A). The pseudoinverse of D applied to (Y, X ‚ä§) can be formulated as the unique solution of argmin A { ‚à•A‚à• 2 F : AQ = Y, P ‚ä§A = X ‚ä§} where ‚à• ‚àí ‚à•F denotes Frobenius norm. We check that E is a 1) a solution to AQ = Y, P ‚ä§A = X ‚ä§ and 2) the minimal norm solution. We have EQ = Y Q+Q + P +‚ä§X ‚ä§Q ‚àí P +‚ä§P ‚ä§Y Q+Q. Note that Y Q+Q = Y because Y = AQ =‚áí Y Q+Q = AQQ +Q = AQ = Y . So EQ = Y + P +T (X ‚ä§Q ‚àí P ‚ä§Y ). But X ‚ä§Q = P ‚ä§AQ = P ‚ä§Y , so EQ = Y as desired. A similar, but easier reasoning, gives P ‚ä§E = X ‚ä§. This veriÔ¨Åes that E is a solution. To check that E is minimal norm, we show that it satisÔ¨Åes the stationarity of the Lagrangian L(A, Œò, Œì) = ‚à•A‚à• 2 F + ‚ü®Œò, Y ‚àí AQ‚ü© + ‚ü®Œì, X ‚àí A‚ä§P ‚ü©. So ‚àÇL ‚àÇA = 0 =‚áí 2A = ŒòQ ‚ä§ + P Œì‚ä§ for some choices of Œò ‚àà Rn√óq and Œì ‚àà Rm√óp. For Œò = 2Y (Q ‚ä§Q) + and Œì ‚ä§ = 2(P ‚ä§P ) +[X ‚ä§ ‚àí P ‚ä§Y Q‚ä§], we can check that ŒòQ ‚ä§ + P Œì‚ä§ = 2Y (Q ‚ä§Q)+Q ‚ä§ + 2P (P ‚ä§P ) +[X ‚ä§ ‚àí P ‚ä§Y Q+] = 2Y Q+ + 2P +‚ä§X ‚ä§ ‚àí 2P +‚ä§P ‚ä§Y Q+ = 2E as desired. G.4 Œ±-Controlled Functions We generalize DeÔ¨Ånition 5.3 slightly as follows. DeÔ¨Ånition G.8 (Œ±-controlled). For Œ± > 0, a function œï : Rk ‚Üí R is said to be Œ±-controlled if for some C, c > 0, |œï(x)| ‚â§ eC ‚àëk i=1 |xi|Œ±+c for all x ‚àà Rk. We present a few helper lemmas to facilitate our reasoning with Œ±-controlled functions. The next lemma is easy to show using the equivalence of norms in Ô¨Ånite dimensional Euclidean space. Lemma G.9. Let œï : Rk ‚Üí R. The following are equivalent 1. œï is Œ±-controlled 2. For some p ‚â• 1 and some g(x) = o‚à•x‚à•p‚Üí‚àû(‚à•x‚à•Œ± p ), C, c > 0, |œï(x)| ‚â§ eC‚à•x‚à•Œ± p +g(x) 3. For all p ‚â• 1, there is some C, c > 0, |œï(x)| ‚â§ e C‚à•x‚à•Œ± p +c Lemma G.10. Let C k Œ± : R‚â•0 ‚Üí R, c 7‚Üí Ez‚àºN (0,Ik) ec‚à•z‚à•Œ± 2 . Then 1. C k Œ± < ‚àû iff Œ± < 2 2. for Œ± ‚â• 1, E z‚àºN (¬µ,Œ£) eC‚à•z‚à•Œ± 2 ‚â§ eC‚à•¬µ‚à•Œ± 2 C k Œ±(CŒ±‚à•Œ£‚à•Œ±/2 2 ) where ‚à•Œ£‚à•2 denotes the spectral norm of Œ£. 58 3. for any Œ±-controlled œï : Rk ‚Üí R with Œ± ‚â• 1, there is C > 0 such that for all ¬µ ‚àà Rk and k √ó k PSD matrix Œ£, E z‚àºN (¬µ,Œ£) |œï(z)| ‚â§ Ce C‚à•¬µ‚à•Œ± 2 C k Œ±(CŒ±‚à•Œ£‚à•Œ±/2 2 ) where ‚à•Œ£‚à•2 denotes the spectral norm of Œ£. Note that the RHS is a montonic function in ‚à•¬µ‚à•2 and ‚à•Œ£‚à•2, in the sense that if ‚à•¬µ‚à•2 and ‚à•Œ£‚à•2 don‚Äôt decrease, then the RHS will not decrease either. Proof. The Ô¨Årst claim is obvious and the third follows from the second easily. For the second, E z‚àºN (¬µ,Œ£) eC‚à•z‚à•Œ± 2 ‚â§ E z‚àºN (0,I) eC‚à•‚àöŒ£z+¬µ‚à•Œ± 2 ‚â§ E z‚àºN (0,I) eCŒ±(‚à•‚àöŒ£z‚à•Œ± 2 +‚à•¬µ‚à•Œ± 2 ) ‚â§ eC‚à•¬µ‚à• Œ± 2 E z‚àºN (0,I) eCŒ±‚à•Œ£‚à•Œ±/2 2 ‚à•z‚à•Œ± 2 = eC‚à•¬µ‚à•Œ± 2 C k Œ±(CŒ±‚à•Œ£‚à• Œ±/2 2 ). H Proof of NETSOR Master Theorem In this section, we prove Theorem E.3, i.e. the Master Theorem for programs without LinComb. By Remark E.2, this would also show Theorem 5.4. A Bit of Notation and Terminology Note that, for each n, the randomness of our program speciÔ¨Åed by Theorem 5.4 comes from the sampling of the input variables. Let U be the product space obtained from multiplying together the corresponding probability space for each n. Each sample from this product probability space thus correspond to a sequence {S(n)}n of instantiatiations of input variables. Below, when we say ‚Äúalmost surely‚Äù (often abbreviated ‚Äúa.s.‚Äù), we mean ‚Äúalmost surely over the probability of U.‚Äù We will also often make statements of the form almost surely (or, a.s.), for all large n, A(n) is true where A(n) is a claim parametrized by n. This means that for all but a U-probability-zero set of sequences {S(n)}n of input variable instantiations, A(n) is true for large enough n. Note that the order of the qualiÔ¨Åers is very important here. We induct, but on what? A natural way of going about proving Theorem 5.4 is by inducting on the number of variables in a program. It turns out this is not enough to prove our claim in its full generality (see below), and it would be more fruitful to perform a simultaneous induction on our claim (Moments) along with another statement, parametrized by m, Moments(m) For any controlled œà : Rm ‚Üí R, as n ‚Üí ‚àû, 1 n n‚àë Œ±=1 œà(g1 Œ±, . . . , gm Œ± ) a.s. ‚àí‚àí‚Üí E Z‚àºN (¬µ,Œ£) œà(Z). CoreSet(m) There exists a ‚Äúcore set‚Äù M ‚äÜ [m] such that, Basis(m) almost surely, for large enough n, for every i ‚àà [m], there exist unique constants (not depending on n) {aj}j‚ààM such that gi = ‚àë j‚ààM ajgj. Note the uniqueness implies that {gi}i‚ààM is linearly independent. NullAvoid(m) for every triangular array of Lesbegue measure zero sets {AnŒ± ‚àà RM}n‚ààN,Œ±‚àà[n], almost surely for all large enough n, for all Œ± ‚àà [n], we have {gi Œ±}i‚ààM Ã∏‚àà AnŒ±. In other words, the values {gi Œ±}Œ±‚ààM of the core set ‚Äúavoid‚Äù Lebesgue measure zero sets asymptotically. Intuitively, this says that the distribution of these values are not singular. (Note the LHS depends on n although we are suppressing it notationally) 59 Let us explain in brief why we need to consider CoreSet satisfying Basis and NullAvoid. ‚Ä¢ Basis reduces the consideration of Moments to only the core set G-vars, since every other G-var is asymptotically a linear combination of them. ‚Ä¢ When we apply the Gaussian conditioning technique Proposition G.2, we need to reason about the pseudo-inverse Œõ + of some submatrix Œõ of a covariance matrix. Each entry of Œõ is of the form 1 n ‚àën Œ±=1 œïi(g1 Œ±, . . . , gm‚àí1 Œ± )œïj(g1 Œ±, . . . , gm‚àí1 Œ± ) for a collection of controlled scalar functions {œïi}i. This Œõ will be a random variable which converges a.s. to a deter- minstic limit ÀöŒõ as n ‚Üí ‚àû. It should be generically true that Œõ+ a.s. ‚àí‚àí‚Üí ÀöŒõ + as well, which is essential to make the Gaussian conditioning argument go through. But in general, this is guaranteed only if Œõ‚Äôs rank doesn‚Äôt drop suddenly in the n ‚Üí ‚àû limit. We thus need to guard against the possibility that g1, . . . , gm, in the limit, suddenly concentrate on a small set on which {œïi(g1, . . . , gm)}i are linearly dependent. This is where NullAvoid comes in. It tells us that g1, . . . , gm will avoid any such small set asymptotically, so that indeed the rank of Œõ will not drop in the limit. Proof organization We will show that Moments and CoreSet are true for input variables, as the base case, and Moments(m ‚àí 1) and CoreSet(m ‚àí 1) =‚áí Moments(m) and CoreSet(m) as the inductive step. By induction, we obtain Moments(M ), which is Theorem 5.4. The base cases are easy and we will dispatch with them immediately after this in Appendix H.1, but the inductive step is much more complicated, and we will need to set up notation in Appendix H.2. During this setup, we prove some basic limit theorems using the induction hypothesis. However, the full generality of these claims requires some consequences of CoreSet, which we call ‚Äúrank stability‚Äù and ‚Äúzero stability‚Äù (related to Assumption C.3). These notions are introduced and proved in Appendix H.3. We would then Ô¨Ånally be able to handle the inductive steps at this point. We Ô¨Årst prove Moments(m ‚àí 1) and CoreSet(m ‚àí 1) =‚áí CoreSet(m) in Appendix H.4 because it is easier. Then we prove Moments(m ‚àí 1) and CoreSet(m ‚àí 1) =‚áí Moments(m) in Appendix H.5. H.1 Base Cases: Moments and CoreSet for Input Variables Base case: Moments(input vars) Suppose the input variables are x1, . . . , xk : G(n) (so that ¬µ in ‚àà Rk, Œ£in ‚àà Rk√ók). We need to show that for any controlled function œà : Rk ‚Üí R, 1 n n‚àë Œ±=1 œà(x1 Œ±, . . . , xk Œ±) a.s. ‚àí‚àí‚Üí E Z‚àºN (¬µ,Œ£) œà(Z), where œà on the RHS ignores all coordinates corresponding to non-input G-vars. Since ¬µ and Œ£ restricted to input variables are just ¬µ in and Œ£in (see Eq. (2)), the RHS expectation is just E Z‚àºN (¬µ,Œ£) œà(Z) = E Zin‚àºN (¬µin,Œ£in) œà(Z in) and the almost sure convergence we desire is just a result of the law of large numbers. Base Case: CoreSet(input vars) Let x1, . . . , xk be the input G-vars as above. Pick the core set M to be any subset of [k] such that rank Œ£ in|M = rank Œ£ in. Then it‚Äôs straightforward to verify Basis and NullAvoid. 60 H.2 Inductive Case: Setup We now assume Moments(m ‚àí 1) and CoreSet(m ‚àí 1) and want to reason about gm to show Moments(m) and CoreSet(m). Suppose gm := Ah where A : A(n, n) and h : H(n) was introduced by h := œï(g1, . . . , gm‚àí1) (WLOG padding coordinates if necessary; if h = gi is a G-var, then pretend œï just projects to the ith coordinate). For brevity, we will just write g = gm. Consider all previous instances where A is used: ÀÜgi := AÀÜh i, i = 1, . . . , r. DeÔ¨Åne ÀÜG def = [ÀÜg1| . . . |ÀÜgr] ‚àà Rn√ór, ÀÜH def = [ÀÜh 1| . . . |ÀÜh r]. (57) We will also use ÀÜG to denote the set of G-vars {ÀÜg1, . . . , ÀÜgr} when we later write expressions like Œ£( ÀÜG, ÀÜG). Let B be the œÉ-algebra spanned by all previous G-vars g1, . . . , gm‚àí1 (and hence also all previous H-vars). Conditioning on B, A is constrained by ÀÜG = A ÀÜH, and we have by Lemma G.7, g d =B ( ÀÜG ÀÜH + + ÀúAŒ†‚ä• ÀÜH )h where ÀúA is an independent copy of A and Œ† ÀÜH = ÀÜH ÀÜH + = ÀÜH( ÀÜH ‚ä§ ÀÜH) + ÀÜH ‚ä§ is the projection to the column space of ÀÜH. If we deÔ¨Åne œâ def = ÀÜG ÀÜH +h, œÉ def = œÉA‚àö ‚à•Œ†‚ä• ÀÜH h‚à•2/n (58) then g d =B œâ + œÉy, with y ‚àº N (0, In) (59) For brevity, we will deÔ¨Åne the following matrices and vectors of Ô¨Åxed dimension ÀÜŒõ def = ÀÜH ‚ä§ ÀÜH/n ‚àà Rr√ór, ÀÜŒ∑ def = ÀÜH ‚ä§h/n ‚àà Rr. (60) Suppose ÀÜh i was introduced by ÀÜh i := ÀÜœïi(g1, . . . , gM ), where ÀÜœïi depends at most on g1, . . . , gm‚àí1. By induction hypothesis Moments(m ‚àí 1), ÀÜŒõ and ÀÜŒ∑ all converge a.s. to corresponding limit values ÀöÀÜŒõ and ÀöÀÜŒ∑, since their entries are moments of Z 1, . . . , Z m‚àí1: ÀÜŒõij a.s. ‚àí‚àí‚Üí ÀöÀÜŒõij def = E ÀÜœïi(Z) ÀÜœïj(Z) = (œÉA) ‚àí2Œ£(ÀÜgi, ÀÜgj) ÀÜŒ∑i a.s. ‚àí‚àí‚Üí ÀöÀÜŒ∑i def = E ÀÜœïi(Z)œï(Z) = (œÉA)‚àí2Œ£(ÀÜgi, g). It turns out that, as a consequence of Lemma H.4 below, a.s. for all large enough n, rank ÀÜŒõ = rank ÀöÀÜŒõ. Therefore, as pseudoinverse is continuous on matrices of Ô¨Åxed rank, we get the following proposition Proposition H.1. ÀÜŒõ + a.s. ‚àí‚àí‚Üí ÀöÀÜŒõ +. Using this proposition, we compute the limits of the conditional mean œâ and variance œÉ2. Lemma H.2. œÉ2 a.s. ‚àí‚àí‚Üí ÀöœÉ2 def = Œ£(g, g) ‚àí Œ£(g, ÀÜG)Œ£( ÀÜG, ÀÜG) +Œ£( ÀÜG, g) Proof. Note that œÉ2 = œÉ2 A n (h ‚ä§h ‚àí h ‚ä§Œ† ÀÜH h) = œÉ2 A n (h ‚ä§h ‚àí h‚ä§ ÀÜH( ÀÜH ‚ä§ ÀÜH) + ÀÜH ‚ä§h) = œÉ2 A n (h ‚ä§h ‚àí ÀÜŒ∑‚ä§ ÀÜŒõ+ ÀÜŒ∑). Because œï is polynomially-bounded, so is œï(z) 2 as well. By induction hypothesis, 1 n h ‚ä§h = 1 n n‚àë Œ±=1 œï(g1 Œ±, . . . , gm‚àí1 Œ± ) 2 a.s. ‚àí‚àí‚Üí E Z‚àºN (¬µ,Œ£) œï(Z)2 = œÉ‚àí2 A Œ£(g, g). Likewise, ÀÜŒ∑ a.s. ‚àí‚àí‚Üí ÀöÀÜŒ∑ and ÀÜŒõ a.s. ‚àí‚àí‚Üí ÀöÀÜŒõ. By Proposition H.1, ÀÜŒõ+ a.s. ‚àí‚àí‚Üí ÀöÀÜŒõ +. Combining all of these limits together yields the desired claim. 61 Lemma H.3. Let v def = ÀÜŒõ+ ÀÜŒ∑, so that v a.s. ‚àí‚àí‚Üí Àöv def = ÀöÀÜŒõ +ÀöÀÜŒ∑. Then for some vector ÀÜŒµ ‚àà Rr that go to 0 a.s. with n, œâ = Eh = ÀÜG(Àöv + ÀÜŒµ) Proof. Using Eq. (60), we can re-express œâ as œâ = ÀÜGÀÜŒõ+ ÀÜŒ∑. By Proposition H.1, ÀÜŒõ+ a.s. ‚àí‚àí‚Üí ÀöÀÜŒõ+, so that setting ÀÜŒµ def = v ‚àí Àöv, we get ÀÜŒµ a.s. ‚àí‚àí‚Üí 0. Thus, œâ = ÀÜG(Àöv + ÀÜŒµ) as desired. H.3 Rank Stability and Zero Stability In this section, we prove the following consequence of CoreSet(m ‚àí 1) and Moments(m ‚àí 1). Lemma H.4 (Rank Stability). For any collection of controlled functions {œàj : Rm‚àí1 ‚Üí R} l j=1, let K ‚àà Rl√ól be the random matrix (depending on n) deÔ¨Åned by Kij = 1 n n‚àë Œ±=1 œài(g1 Œ±, . . . , gm‚àí1 Œ± )œàj(g1 Œ±, . . . , gm‚àí1 Œ± ). By Moments(m ‚àí 1), K a.s. ‚àí‚àí‚Üí ÀöK for some matrix ÀöK ‚àà Rl√ól. 1. Then, almost surely, for large enough n, ker K = ker ÀöK, im K = im ÀöK, and rank K = rank ÀöK. Here ker denotes null space and im denotes image space. 2. Suppose I ‚äÜ [l] is any subset such that ÀöK|I , the restriction of ÀöK to rows and columns corresponding to I, satisÔ¨Åes |I| = rank ÀöK|I = rank ÀöK. There are unique coefÔ¨Åcients {Fij}i‚àà[l],j‚ààI that expresses each row of ÀöK as linear combina- tions of rows corresponding to I: ‚àÄi ‚àà [l], ÀöKi = ‚àë j‚ààI Fij ÀöKj. Then, a.s. for all large n, for all Œ± ‚àà [n], œài(g1 Œ±, . . . , gm‚àí1 Œ± ) = ‚àë j‚ààI Fijœàj(g1 Œ±, . . . , gm‚àí1 Œ± ). This will be primarily a corollary of the following Lemma H.5. Lemma H.5 (Zero Stability). If œà : Rm‚àí1 ‚Üí R‚â•0 is a nonnegative function such that 1 n n‚àë Œ±=1 œà(g1 Œ±, . . . , gm‚àí1 Œ± ) a.s. ‚àí‚àí‚Üí 0 then, almost surely, for large enough n, œà(g1 Œ±, . . . , gm‚àí1 Œ± ) = 0 for all Œ± ‚àà [n]. We give the proof of Lemma H.4 now, assuming Lemma H.5. Proof. Let v ‚àà Rl be in the null space of ÀöK, i.e. v‚ä§ ÀöKv = 0. Then we also have v‚ä§Kv a.s. ‚àí‚àí‚Üí v‚ä§ ÀöKv = 0. But v‚ä§Kv = 1 n n‚àë Œ±=1 Œ®(g1 Œ±, . . . , gm‚àí1 Œ± ), where Œ®(g1 Œ±, . . . , gm‚àí1 Œ± ) def = (‚àë i=1 viœài(g1 Œ±, . . . , gm‚àí1 Œ± ) )2 62 and Œ® is a nonnegative function. By Lemma H.5, we have that: almost surely, for large enough n, Œ®(g1 Œ±, . . . , gm‚àí1 Œ± ) = 0 for all Œ± ‚àà [n] =‚áí v‚ä§Kv = 0 Claim 1. If we apply this argument to a basis {v1, . . . , vt} of ker ÀöK, then we get, a.s. for all large n, ker ÀöK ‚äÜ ker K, so that a.s. for all large n, rank ÀöK ‚â• rank K. Because the rank function is lower semicontinuous (i.e. the rank can drop suddenly, but cannot increase suddenly), and K a.s. ‚àí‚àí‚Üí ÀöK, we also have a.s. for all large n, rank ÀöK ‚â§ rank K. Combined with the above, this gives the desired result on rank. The equality of null space then follows from the equality of rank, and the equality of image space follows immediately, as the image space is the orthogonal complement of the null space. Claim 2. If we apply the above argument to each vi deÔ¨Åned by inner product as ‚àÄx ‚àà Rl, x ‚ä§vi def = xi ‚àí ‚àë j‚ààI Fijxj, (note that only for i Ã∏‚àà I is vi nonzero), then we have, a.s. for large n, vi‚ä§Kvi = 0, or œài(g1 Œ±, . . . , gm‚àí1 Œ± ) = ‚àë j‚ààI Fijœàj(g1 Œ±, . . . , gm‚àí1 Œ± ). In the rest of this section, we prove Lemma H.5. It helps to Ô¨Årst show that the linear relations given in Basis carries over to the n ‚Üí ‚àû limit. Proposition H.6. Let Œ£|M be the submatrix of Œ£ with rows and columns corresponding to {gi : i ‚àà M}. Then rank Œ£ = rank Œ£|M = |M|. Furthermore, if Z = (Z 1, . . . , Z m‚àí1) ‚àº N (¬µ|m‚àí1, Œ£|m‚àí1), where ¬µ|m‚àí1, Œ£|m‚àí1 are the restrictions of ¬µ, Œ£ to g1, . . . , gm‚àí1, then Z i d = ‚àë j‚ààM ajZ j where {aj}j‚ààM are the coefÔ¨Åcients corresponding to gi given in Basis. Proof. By Basis property, each gi, i ‚àà M, has a set of unique constants {aj}j‚ààM (independent of n) such that, almost surely, for large enough n, gi = ‚àë j‚ààM ajgj. Let œà(x1, . . . , xm‚àí1) def = (xi ‚àí ‚àë j‚ààM ajxj) 2. Then by Basis(m ‚àí 1) and Moments(m ‚àí 1), 1 n n‚àë Œ±=1 œà(g1 Œ±, . . . , gm‚àí1 Œ± ) a.s. ‚àí‚àí‚Üí E Z‚àºN (¬µ|m‚àí1,Œ£|m‚àí1) œà(Z) = 0. where ¬µ|m‚àí1, Œ£|m‚àí1 are the restrictions of ¬µ, Œ£ to g1, . . . , gm‚àí1. This implies that for Z = (Z 1, . . . , Z m‚àí1) ‚àº N (¬µ|m‚àí1, Œ£|m‚àí1), Z i d = ‚àë j‚ààM ajZ j. Repeating this argument for all i ‚àà [m ‚àí 1] implies that {Z j}j‚ààM is a ‚Äúspanning set‚Äù of Z 1, . . . , Z m‚àí1. Furthermore, by the uniqueness of the coefÔ¨Åcients, we also have that {Z j}j‚ààM is linearly independent as well. This then implies the rank consequence we want. 63 Now we show Lemma H.5. Proof of Lemma H.5. By Moments(m ‚àí 1), 1 n n‚àë Œ±=1 œà(g1 Œ±, . . . , gm‚àí1 Œ± ) ‚Üí E Z‚àºN (¬µ|m‚àí1,Œ£|m‚àí1) œà(Z). By Proposition H.6, if Z ‚àº N (¬µ|m‚àí1, Œ£|m‚àí1) and Z|M is the part of Z corresponding to M, then Z|M has density. The law of Z|M (namely N (¬µ|M, Œ£|M), where ¬µ|M, Œ£|M are the restriction of ¬µ and Œ£ to M) is absolutely continuous against the Lebesgue measure of RM and vice versa, so that a set of Lebesgue measure zero is measure zero under N (¬µ|M, Œ£|M), and vice versa; and Z|M is basis of Z. Basis yields a linear function Œª such that Œª({gj Œ±}j‚ààM) = {gi Œ±} m‚àí1 i=1 for all Œ±, almost surely asymptotically, and Œª(Z|M) d = Z, so that E Z‚àºN (¬µ|m‚àí1,Œ£|m‚àí1) œà(Z) = E Z‚Ä≤‚àºN (¬µ|M,Œ£|M) œà ‚ó¶ Œª(Z ‚Ä≤). This expectation is 0 by our premise. Because œà, and thus œà ‚ó¶ Œª, is a nonnegative function, the nullity of the expectation implies that, other than a set U of N (¬µ|M, Œ£|M)-measure zero, œà ‚ó¶ Œª is 0. This set U also has Lebesgue measure zero as Z|M has density, by our reasoning above. If in NullAvoid, we set AnŒ± = U for all n and all Œ± ‚àà [n], then we get that: almost surely, for all large enough n, for all Œ± ‚àà [n], {gi Œ±}i‚ààM Ã∏‚àà U ‚áê‚áí œà ‚ó¶ Œª({gi Œ±}i‚ààM) = 0 ‚áê‚áí œà(g1 Œ±, . . . , gm‚àí1 Œ± ) = 0, as desired. H.4 Inductive Step: CoreSet(m) In this section, we show Moments(m ‚àí 1) and CoreSet(m ‚àí 1) =‚áí CoreSet(m). More explicitly, we need to think about whether to add m to the core set M of [m ‚àí 1] in order to maintain the Basis and NullAvoid properties. We proceed by casework on whether ÀöœÉ = 0. H.4.1 If ÀöœÉ = 0 We will show that the core set properties are maintained if we don‚Äôt add m to the core set. Consider the space L def = L 2(N (¬µ|M, Œ£|M)) of square-integrable real functions against the measure N (¬µ|M, Œ£|M) deÔ¨Åned on RM. Let ‚ü®œï, œà‚ü© = EY ‚àºN (¬µ|M,Œ£|M) œï(Y )œà(Y ) be the inner product of this space. Just like in a Ô¨Ånite-dimensional inner product space, given a Ô¨Ånite collection of functions S = {œài} k i=1, the orthogonal projection operator Œ†S to the span of S (inside L) is given by Œ†Sœï = k‚àë i=1 aiœài, for any œï ‚àà L, where a = Œõ +b ‚àà Rk, bj = ‚ü®œàj, œï‚ü©, b ‚àà Rk, Œõij = ‚ü®œài, œàj‚ü©, Œõ ‚àà Rk√ók. 64 Recall that g = Ah where h was introduced by h := œï(g1, . . . , gm‚àí1), for some controlled œï, and likewise ÀÜgi = AÀÜh i where ÀÜh i = ÀÜœïi(g1, . . . , gm‚àí1), for each i ‚àà [r]. By Basis, we know that, a.s. for large enough n, each of g1, . . . , gm‚àí1 is a (unique, constant-in-n) linear combination of {gj}j‚ààM. Therefore, we can express h = œï({gj}j‚ààM), and ‚àÄi ‚àà [r], ÀÜh i = ÀÜœïi({gj}j‚ààM) for some functions œï, ÀÜœïi ‚àà L. For convenience, set S def = { ÀÜœïi}i. One can see then, as in the proof of Lemma H.2, ÀöœÉ2 = œÉ2 A(E œï(Z) 2 ‚àí ÀöÀÜŒ∑‚ä§ÀöÀÜŒõ+ÀöÀÜŒ∑) = œÉ2 A(‚ü®œï, œï‚ü© ‚àí ‚ü®œï, Œ†Sœï‚ü©) by expanding the deÔ¨Ånition of ÀöÀÜŒ∑ and ÀöÀÜŒõ. Therefore, ÀöœÉ = 0 implies that ‚ü®œï, œï‚ü© = ‚ü®œï, Œ†Sœï‚ü© so that: after changing its values on a set U of measure zero under N (¬µ|M, Œ£|M) (and thus also under Lebesgue measure by Lemma H.4), œï is a linear combination of { ÀÜœïi}r i=1, i.e. ‚àÄx Ã∏‚àà U, œï(x) = ‚àë i‚àà[r] ci ÀÜœïi(x) for some coefÔ¨Åcients {ci}i‚àà[r]. By NullAvoid applied to AnŒ± = U for all n and Œ± ‚àà [n], we also have that: a.s. for large enough n, œï(g1, . . . , gŒ±) = œï({gj}j‚ààM) = ‚àë i‚àà[r] ci ÀÜœïi({gj}j‚ààM) = ‚àë i‚àà[r] ci ÀÜœïi(g1, . . . , gŒ±), and therefore, under the same condition, (recall A is the matrix giving rise to g in g := Ah) g = Aœï(g1, . . . , gŒ±) = ‚àë i‚àà[r] ciA ÀÜœïi(g1, . . . , gŒ±) = ‚àë i‚àà[r] ciÀÜgi. This shows that, if we keep the core set as M, then Basis is still satisÔ¨Åed. Since the core set is not changing, NullAvoid just follows from the induction hypothesis. For usage later in the proof of Moments(m), we record our observation here as follows Lemma H.7. If ÀöœÉ = 0, then there are coefÔ¨Åcients {ci} r i=1 such that a.s. for large enough n, g = ‚àë i‚àà[r] ciÀÜgi. H.4.2 If ÀöœÉ > 0 It‚Äôs clear that g cannot be in the linear span of {ÀÜgi}i‚àà[r] asymptotically, so we will add g to the core set, and the Basis property follows immediately. In the below, we shall write M for the old core set, and M ‚Ä≤ def = M ‚à™ {g} for the new one. It remains to show NullAvoid for M‚Ä≤. Because the conditional variance of gm Œ± given g1, . . . , gm‚àí1 is œÉ2, and because ÀöœÉ > 0, this assumption implies that, a.s. for all large enough n, gm Œ± |g1, . . . , gm‚àí1 has density for all Œ± ‚àà [n]. (61) By ‚Äúhas density‚Äù here, we in particular mean that any Lesbegue measure zero set in R has zero probability under the conditional distribution of gm Œ± given g1, . . . , gm‚àí1. Now, to prove NullAvoid holds for M ‚Ä≤: Let {AnŒ± ‚äÜ RM‚Ä≤}n‚ààN,Œ±‚àà[n] be a triangular array of Lesbegue measure zero sets. For each AnŒ±, deÔ¨Åne BnŒ± def = {x ‚àà RM : Œª(AnŒ±|x) Ã∏= 0}, where AnŒ±|x = {y ‚àà R : (x, y) ‚àà AnŒ± ‚äÜ RM √ó R} is the ‚Äúslice‚Äù of AnŒ± at x, and Œª is the 1-dimensional Lebesgue measure. Because each AnŒ± has measure zero in RM ‚Ä≤, necessarily each BnŒ± also has measure zero in RM. Applying NullAvoid to the triangular array {BnŒ± ‚äÜ RM}n‚ààN,Œ±‚àà[n], we get that: a.s. for large enough n, ‚àÄŒ± ‚àà [n], {gi Œ±}i‚ààM Ã∏‚àà BnŒ±. Therefore, by Eq. (61), a.s. for large enough n, ‚àÄŒ± ‚àà [n], {gi Œ±}i‚ààM‚Ä≤ Ã∏‚àà AnŒ±. This Ô¨Ånishes the proof of NullAvoid for M ‚Ä≤, and also CoreSet(m). 65 Lemma H.8. Assume Moments(m ‚àí 1). Suppose œà : Rm‚àí1 ‚Üí R is controlled. Then as n ‚Üí ‚àû, 1 np max Œ±‚àà[n] |œà(g1 Œ±, . . . , gm‚àí1 Œ± )| a.s. ‚àí‚àí‚Üí 0 for any p > 0. Proof. For any q > 0, we have the elementary bound max Œ±‚àà[n] |œà(g1 Œ±, . . . , gm‚àí1 Œ± )| ‚â§ q ‚àö ‚àë Œ±‚àà[n] |œà(g1 Œ±, . . . , gm‚àí1 Œ± )|q. Thus, for any q > 0, 1 np max Œ±‚àà[n] |œà(g1 Œ±, . . . , gm‚àí1 Œ± )| ‚â§ 1 np‚àí1/q q v u u ‚àö 1 n ‚àë Œ±‚àà[n] |œà(g1 Œ±, . . . , gm‚àí1 Œ± )|q. Because, by Moments(m ‚àí 1), 1 n ‚àë Œ±‚àà[n] |œà(g1 Œ±, . . . , gm‚àí1 Œ± )|q a.s. ‚àí‚àí‚Üí C for some constant C as n ‚Üí ‚àû, the RHS above converges a.s. to 0 as soon as we take q > 1/p, and therefore so does the LHS. H.5 Inductive Step: Moments(m) In this section, we show Moments(m ‚àí 1) and CoreSet(m ‚àí 1) =‚áí Moments(m). More speciÔ¨Åcally, we will show that for any controlled œà : Rm ‚Üí R, 1 n n‚àë Œ±=1 œà(g1 Œ±, . . . , gm Œ± ) a.s. ‚àí‚àí‚Üí E Z‚àºN (¬µ,Œ£) œà(Z) where again on the RHS œà ignores all coordinates Z m+1, . . . , Z M (corresponding to gm+1, . . . , gM ). By Lemma H.7, if ÀöœÉ = 0, then almost surely, for large enough n, g = gm is just a (Ô¨Åxed) linear combination of g1, . . . , gm‚àí1, so Moments is trivially true. Therefore, in the below, we assume ÀöœÉ > 0. (‚ãÜ) This assumption will be crucial for our arguments involving smoothness induced by Gaussian averaging. To clarify notation in the following, we will write EX [expression] to denote the expectation over only the randomization in X, and E [ expression| B] to denote the expectation taken over all randomness except those in B. Proof Plan Note that\f \f \f \f \f 1 n n‚àë Œ±=1 œà(g1 Œ±, . . . , gm Œ± ) ‚àí E Z‚àºN (¬µ,Œ£) œà(Z) \f \f \f \f \f ‚â§ A + B + C (62) where A def = \f \f \f \f \f 1 n n‚àë Œ±=1 œà(g1 Œ±, . . . , gm Œ± ) ‚àí E z œà (g1 Œ±, . . . , gm‚àí1 Œ± , œâŒ± + œÉz)\f \f \f \f \f B def = \f \f \f \f \f 1 n n‚àë Œ±=1 E z œà (g1 Œ±, . . . , gm‚àí1 Œ± , œâŒ± + œÉz) ‚àí E z œà ( g1 Œ±, . . . , gm‚àí1 Œ± , r‚àë i=1 ÀöviÀÜgi Œ± + ÀöœÉz )\f \f \f \f \f C def = \f \f \f \f \f 1 n n‚àë Œ±=1 E z œà ( g1 Œ±, . . . , gm‚àí1 Œ± , r‚àë i=1 ÀöviÀÜgi Œ± + ÀöœÉz ) ‚àí E Z‚àºN (¬µ,Œ£) œà(Z) \f \f \f \f \f 66 with z ‚àº N (0, 1). Note that B and C are random variables in B. We will show that each of A, B, C goes to 0 almost surely, which would Ô¨Ånish the proof of Theorem 5.4. Roughly speaking, A a.s. ‚àí‚àí‚Üí 0 because of a law of large number, B a.s. ‚àí‚àí‚Üí 0 because of the smoothness in Ez œà induced by Gaussian averaging, and C a.s. ‚àí‚àí‚Üí 0 by induction hypothesis. We start with the last item, since it‚Äôs the easiest. H.5.1 C Converges Almost Surely to 0 In this section we show that C a.s. ‚àí‚àí‚Üí 0 by a straightforward reduction to the inductive hypothesis. Let ÀÜZ 1, . . . , ÀÜZ r be the components of Z ‚àº N (¬µ, Œ£) corresponding to ÀÜg1, . . . , ÀÜgr, and let ÀÜZ be the col- umn vector with these entries. Note that, by Proposition G.2, Z m (corresponding to gm), conditioned on Z 1, . . . , Z m‚àí1, is distributed as a Gaussian with mean Œ£(g, ÀÜG)Œ£( ÀÜG, ÀÜG) + ÀÜZ = ÀöÀÜŒ∑‚ä§ÀöÀÜŒõ + ÀÜZ = Àöv‚ä§ ÀÜZ and variance Œ£(g, g) ‚àí Œ£(g, ÀÜG)Œ£( ÀÜG, ÀÜG) +Œ£( ÀÜG, g) = ÀöœÉ. Thus E Z œà(Z) = E Z1,...,Zm‚àí1 E[œà(Z)|Z 1, . . . , Z m‚àí1] = E Z1,...,Zm‚àí1 E z‚àºN (0,1) œà(Z 1, . . . , Z m‚àí1,Àöv‚ä§ ÀÜZ + ÀöœÉz) = E Z1,...,Zm‚àí1 Œ®(Z 1, . . . , Z m‚àí1) where we have set Œ®(Z 1, . . . , Z m‚àí1) def = Ez‚àºN (0,1) œà(Z 1, . . . , Z m‚àí1,Àöv‚ä§ ÀÜZ +ÀöœÉz). Œ® is a controlled function since œà is. Applying the induction hypothesis to Œ®, we obtain 1 n n‚àë Œ±=1 E z œà ( g1 Œ±, . . . , gm‚àí1 Œ± , r‚àë i=1 ÀöviÀÜgi Œ± + ÀöœÉz ) = 1 n n‚àë Œ±=1 Œ® (g1 Œ±, . . . , gm‚àí1 Œ± ) a.s. ‚àí‚àí‚Üí E Z1,...,Zm‚àí1 Œ®(Z 1, . . . , Z m‚àí1) by induction hypothesis = E Z1,...,Zm‚àí1 E z‚àºN (0,1) œà(Z 1, . . . , Z m‚àí1,Àöv‚ä§ ÀÜZ + ÀöœÉz) = E Z œà(Z) as desired. H.5.2 A Converges Almost Surely to 0 In this section we show A a.s. ‚àí‚àí‚Üí 0 by a bounding moments of A and then Ô¨Ånishing with Lemma G.1. For each Œ± ‚àà [n], let œàŒ±(x) def = œà(g1 Œ±, . . . , gm‚àí1 Œ± , œâŒ± + œÉx), with œâ and œÉ deÔ¨Åned in Eq. (58). This is a random function depending on the randomness of g1 Œ±, . . . , gm‚àí1 Œ± , and it changes with n as well. Note by Eq. (59), A d =B 1 n n‚àë Œ±=1 œàŒ±(ŒæŒ±) ‚àí E Œæ‚Ä≤ œàŒ±(Œæ‚Ä≤ Œ±) where Œæ, Œæ‚Ä≤ ‚àº N (0, I). Now the 2k-moment of A for any integer k ‚â• 1 satisÔ¨Åes E[A 2k | B] = 1 n2k E [ n‚àë Œ±=1 ( œàŒ±(ŒæŒ±) ‚àí E Œæ‚Ä≤ œàŒ±(Œæ‚Ä≤ Œ±) )2k + ¬∑ ¬∑ ¬∑ \f \f \f \f B ] where the ¬∑ ¬∑ ¬∑ include only terms that involve only powers of œàŒ±(ŒæŒ±) ‚àí EŒæ‚Ä≤ œàŒ±(Œæ‚Ä≤ Œ±) greater than 1 for each Œ±. Indeed, other terms are killed by the conditional mean, since each œàŒ±(ŒæŒ±) ‚àí EŒæ‚Ä≤ œàŒ±(Œæ‚Ä≤ Œ±) has 67 zero (conditional) mean and is independent from others when conditioned on B. We can push the conditional mean operator inside each product by conditional independence. Then, applying power mean inequality and AM-GM to bound each mixed moment with linear combinations of the 2kth powers, we get E[A 2k | B] ‚â§ Dn 2k‚àí1 n2k ¬∑ 1 n n‚àë Œ±=1 E [(œàŒ±(ŒæŒ±) ‚àí E Œæ‚Ä≤ œàŒ±(Œæ‚Ä≤ Œ±) )2k \f \f \f \f B ] def = D n U (63) where D is some absolute constant. Thus, to show A a.s. ‚àí‚àí‚Üí 0, it sufÔ¨Åces to bound U and then apply Lemma G.1. This is equivalent to bounding the uncentered moments Ez‚àºN (0,1) |œàŒ±(x)|q for q = 2k. Suppose œà is Œª-controlled and satisÔ¨Åes |œà(x)| ‚â§ eC ‚àë i |xi| Œª+c for some C, c > 0 and Œª < 2. (64) We have E z‚àºN (0,1) |œàŒ±(z)| q ‚â§ E z [ eCq(|œâŒ±+œÉz| Œª+ ‚àëm‚àí1 i=1 |gi Œ±| Œª)+cq] ‚â§ E z [ eCq2 Œª(|œâŒ±| Œª+|œÉz| Œª+ ‚àëm‚àí1 i=1 |gi Œ±|Œª)+cq] = eCq2 Œª(|œâŒ±| Œª+ ‚àëm‚àí1 i=1 |gi Œ±|Œª)+cq E z [ eCq2ŒªœÉŒª|z| Œª] = eCq2 Œª(|œâŒ±| Œª+ ‚àëm‚àí1 i=1 |gi Œ±|Œª)+cqR where R = C 1 Œª(Cq2ŒªœÉŒª) > 0 is deterministic and C k Œª is as deÔ¨Åned in Lemma G.10. Now, |œâŒ±| Œª = \f \f \f \f \f r‚àë i=1 viÀÜgi Œ± \f \f \f \f \f Œª ‚â§ rŒª r‚àë i=1 |vi| Œª|ÀÜgi Œ±|Œª. Additionally, almost surely, |vi| < |Àövi| + 1, for all i ‚àà [r] simultaneously, for large enough n because vi a.s. ‚àí‚àí‚Üí Àövi. Let L = Cq2ŒªrŒª max r i=1(|Àövi| + 1) and L ‚Ä≤ = cq, where C, c are as in Eq. (64). Then, almost surely, for large enough n, for all z1, . . . , zm‚àí1 ‚àà R, e Cq2 Œª(| ‚àër i=1 vizi|Œª+ ‚àëm‚àí1 i=1 |zi| Œª)+cq ‚â§ eL‚Ä≤+L ‚àëm‚àí1 i=1 |zi| Œª def = ÀÜœà(z1, . . . , zm‚àí1). Obviously ÀÜœà is Œª-controlled. Then, again a.s. for large enough n, simultaneously for all Œ±, E z‚àºN (0,1) |œàŒ±(z)| q ‚â§ R ÀÜœà(g1 Œ±, . . . , gm‚àí1 Œ± ), so that 1 n n‚àë Œ±=1 E z‚àºN (0,1) |œàŒ±(z)| q ‚â§ R 1 n n‚àë Œ±=1 ÀÜœà(g1 Œ±, . . . , gm‚àí1 Œ± ) a.s. ‚àí‚àí‚Üí R E Z ÀÜœà(Z) as n ‚Üí ‚àû, by induction hypothesis, where Z ‚àº N (¬µ, Œ£). Consequently, almost surely, the U in Eq. (63) (as a function of g1, . . . , gm‚àí1) is uniformly bounded in n. Applying Lemma G.1 for large enough q yields the result. H.5.3 B Converges Almost Surely to 0 In this section we show B a.s. ‚àí‚àí‚Üí 0. The main insight here is integrating a function against Gaussian induces smoothness in the function. We will assume that ÀöœÉ > 0, so that œÉ > 0 almost surely for large enough n. This is because ÀöœÉ = 0 implies that gm is in the linear span of {g1, . . . , gm‚àí1} almost surely by Lemma H.4, and Moments(m) then holds trivially. For each Œ± ‚àà [n], w ‚àà R, œÑ ‚â• 0, let Œ®Œ±(w; œÑ 2) def = E z‚àºN (0,1) œà (g1 Œ±, . . . , gm‚àí1 Œ± , w + œÑ z) . (Here and in all that follows, œÑ 2 is the square of œÑ , and the 2 is not an index). This is a random function, with randomness induced by g1, . . . , gm‚àí1. By Lemma G.3, Œ®Œ± is differentiable in w, and ‚àÇwŒ®Œ±(w; œÑ 2) = œÑ ‚àí1 E z‚àºN (0,1) zœà(g1 Œ±, . . . , gm‚àí1 Œ± , w + œÑ z). We can obtain the following smoothness condition on Œ®Œ±. 68 Lemma H.9. For any w, œÑ, œµ ‚àà R with œµ, œÑ > 0, |Œ®Œ±(w; œÑ 2) ‚àí Œ®Œ±(w + œµ; œÑ 2)| ‚â§ |œµ|œÑ ‚àí1R(œÑ ) ÀÜŒ®(g1 Œ±, . . . , gm‚àí1 Œ± )eC4 Œª(|w| Œª+|œµ| Œª), where ÀÜŒ®(g1 Œ±, . . . , gm‚àí1 Œ± ) def = eC2 Œª ‚àëm‚àí1 i=1 |gi Œ±| Œª+c and R(œÑ ) def = Ez |z|eC2 ŒªœÑ Œª|z| Œª. Proof. Clearly, with z ‚àº N (0, 1), |‚àÇwŒ®Œ±(w; œÑ 2)| ‚â§ œÑ ‚àí1 E z |zœà(g1 Œ±, . . . , gm‚àí1 Œ± , w + œÑ z)| ‚â§ œÑ ‚àí1 E z |z|eC(|w+œÑ z| Œª+ ‚àëm‚àí1 i=1 |gi Œ±| Œª)+c ‚â§ œÑ ‚àí1 E z |z|eC2Œª(|w| Œª+œÑ Œª|z|Œª+ ‚àëm‚àí1 i=1 |gi Œ±| Œª)+c = œÑ ‚àí1 ÀÜŒ®(g1 Œ±, . . . , gm‚àí1 Œ± )R(œÑ )eC2 Œª|w| Œª. Then |Œ®Œ±(w; œÑ 2) ‚àí Œ®Œ±(w + œµ; œÑ 2)| ‚â§ \f \f \f \f ‚à´ w+œµ w dŒæ ‚àÇŒæŒ®Œ±(Œæ; œÑ 2) \f \f \f \f ‚â§ œÑ ‚àí1R(œÑ ) ÀÜŒ®(g1 Œ±, . . . , gm‚àí1 Œ± ) ‚à´ w+œµ w dŒæ e C2 Œª|Œæ| Œª = œÑ ‚àí1R(œÑ ) ÀÜŒ®(g1 Œ±, . . . , gm‚àí1 Œ± ) ‚à´ œµ 0 dŒæ e C2 Œª|w+Œæ| Œª ‚â§ œÑ ‚àí1R(œÑ ) ÀÜŒ®(g1 Œ±, . . . , gm‚àí1 Œ± ) ‚à´ œµ 0 dŒæ e C4 Œª|w| ŒªeC4 Œª|Œæ| Œª = œÑ ‚àí1R(œÑ ) ÀÜŒ®(g1 Œ±, . . . , gm‚àí1 Œ± )e C4 Œª|w|Œª|œµ|eC4 Œª|œµ| Œª. Therefore, with z ‚àº N (0, 1), \f \f \f \f \fE z [ œà(g1 Œ±, . . . , gm‚àí1 Œ± , r‚àë i=1 viÀÜgi Œ± + œÉz) ] ‚àí E z [ œà(g1 Œ±, . . . , gm‚àí1 Œ± , r‚àë i=1 ÀöviÀÜgi Œ± + œÉz) ]\f \f \f \f \f = \f \f \f \f \fŒ®Œ± ( r‚àë i=1 viÀÜgi Œ±; œÉ2) ‚àí Œ®Œ± ( r‚àë i=1 ÀöviÀÜgi Œ±; œÉ2)\f \f \f \f \f ‚â§ œÉ‚àí1R(œÉ) ÀÜŒ®(g1 Œ±, . . . , gm‚àí1 Œ± )e C4 Œª(| ‚àër i=1 Àövi ÀÜgi Œ±| Œª+|œµŒ±| Œª)|œµŒ±|, where œµŒ± def = ‚àër i=1(vi ‚àí Àövi)ÀÜgi Œ±. Because ÀÜŒ® is Œª-controlled, 1 n n‚àë Œ±=1 ÀÜŒ®(g1 Œ±, . . . , gm‚àí1 Œ± ) converges almost surely to a deterministic limit. At the same time, since vi a.s. ‚àí‚àí‚Üí Àövi, we also have œµŒ± a.s. ‚àí‚àí‚Üí 0 as n ‚Üí ‚àû, so that 1 n n‚àë Œ±=1 \f \f \f \f \fE z [ œà(g1 Œ±, . . . , gm‚àí1 Œ± , r‚àë i=1 viÀÜgi Œ± + œÉz) ] ‚àí E z [ œà(g1 Œ±, . . . , gm‚àí1 Œ± , r‚àë i=1 ÀöviÀÜgi Œ± + œÉz) ]\f \f \f \f \f a.s. ‚àí‚àí‚Üí 0. A similar argument shows that we can replace œÉ with ÀöœÉ: 1 n n‚àë Œ±=1 \f \f \f \f \fE z [ œà(g1 Œ±, . . . , gm‚àí1 Œ± , r‚àë i=1 ÀöviÀÜgi Œ± + œÉz) ] ‚àí E z [ œà(g1 Œ±, . . . , gm‚àí1 Œ± , r‚àë i=1 ÀöviÀÜgi Œ± + ÀöœÉz) ]\f \f \f \f \f a.s. ‚àí‚àí‚Üí 0. By triangular inequality, these limits show that B a.s. ‚àí‚àí‚Üí 0 as desired. 69 I Proof of NETSOR+ Master Theorem In this section we describe how to augment the proof of Theorem E.3 given in Appendix H to yield the proof of Theorem C.4. The key points to note here are 1) the presence of LinComb rules in NETSOR+ but not in NETSOR‚àí , 2) the rank stability assumption Assumption C.3 used in Theorem C.4, and 3) an additional term in Eq. (62) due to Ô¨Çuctuations in the parameter Œò. I.1 LinComb As remarked in Remark E.2, any usage of LinComb in a NETSOR+ program can be absorbed into downstream nonlinearities or be expressed as Nonlin + rule. So WLOG, we can assume that the NETSOR+ program has no applications of LinComb. I.2 Rank Stability By Remark C.6, we see that rank stability assumption is necessary for the NETSOR+ Master Theorem. Whereas in Appendix H, we had to intricately weave together an induction on rank stability (more generally, CoreSet) and an induction on moment convergence (Moments), here to show Theorem C.4, we just need 1) to induct on Moments and 2) to invoke Assumption C.3 whenever we need to use Lemma H.4, which is when we need to show that pseudo-inverse commutes with almost surely limit, such as in Proposition H.1, and when we need to ensure either œÉ is almost surely 0 or is almost surely positive, as in Appendix H.5.3. I.3 Fluctuation of the Parameters When we have parameters in nonlinearities, Eq. (62) needs to be modiÔ¨Åed to contain an additional term D: \f \f \f \f \f 1 n n‚àë Œ±=1 œà(g1 Œ±, . . . , gm Œ± ; Œò) ‚àí E Z‚àºN (¬µ,Œ£) œà(Z; ÀöŒò) \f \f \f \f \f ‚â§ D + A + B + C where D def = \f \f \f \f \f 1 n n‚àë Œ±=1 œà(g1 Œ±, . . . , gm Œ± ; Œò) ‚àí œà(g1 Œ±, . . . , gm Œ± ; ÀöŒò) \f \f \f \f \f and A, B, C are as in Eq. (62) but replacing œà(‚àí) there with œà(‚àí; ÀöŒò). Because œà(‚àí; ‚àí) is parameter- controlled at ÀöŒò by assumption, œà(‚àí; ÀöŒò) is controlled, and A, B, C a.s. ‚àí‚àí‚Üí 0 with the same arguments as before (except using rank stability assumption Assumption C.3 where appropriate, instead of CoreSet). Now, by the other property of parameter-control, we have D ‚â§ 1 n n‚àë Œ±=1 \f \f \fœà(g1 Œ±, . . . , gm Œ± ; Œò) ‚àí œà(g1 Œ±, . . . , gm Œ± ; ÀöŒò) \f \f \f ‚â§ 1 n n‚àë Œ±=1 f (Œò) ¬Øœà(g1 Œ±, . . . , gm Œ± ) = f (Œò) 1 n n‚àë Œ±=1 ¬Øœà(g1 Œ±, . . . , gm Œ± ) for some controlled ¬Øœà : Rm ‚Üí R and some f : Rl ‚Üí R‚â•0 ‚à™ {‚àû} that is continuous at ÀöŒò and has f (ÀöŒò) = 0 (where ¬Øœà and f can both depend on ÀöŒò). Since Œò a.s. ‚àí‚àí‚Üí ÀöŒò, we have f (Œò) a.s. ‚àí‚àí‚Üí 0. In addition, by Moments, 1 n ‚àën Œ±=1 ¬Øœà(g1 Œ±, . . . , gm Œ± ) converges a.s. as well to a Ô¨Ånite constant. Therefore, D a.s. ‚àí‚àí‚Üí 0 as desired. 70 program ::= stmt* stmt ::= Input var :: type | var := expr :: type expr ::= MatMul (var, var ) | fun( var* ) var ::= ‚ü® id ‚ü© fun ::= ‚ü® function Rk ‚Üí R for some k ‚â• 0 ‚ü© type ::= G(nat) | H(nat) | A(nat, nat) nat ::= ‚ü® any integer ‚â• 1 ‚ü© Figure 4: NETSOR‚àí Grammar; see DeÔ¨Ånition E.1. expr : type var := expr :: type var : type a : A(n1, n2) h : H(n2) MatMul(a, h) : G(n1) g1, . . . , gk : G(n) f : Rk ‚Üí R f(g1, . . . , gk) : H(n) Figure 5: NETSOR‚àí Inference Rules I.4 Summary The proof of Theorem C.4, WLOG for programs without LinComb, would proceed as follows: We induct on Moments with the same setup as Appendix H.2, except using Assumption C.3 for Proposition H.1. Then we prove the inductive step for Moments as in Appendix H.5. We modify Eq. (62) to add a term D as in Appendix I.3, which goes to 0 a.s. as argued there. The same arguments for A, B, C a.s. ‚àí‚àí‚Üí 0, exhibited in Appendix H.5 still hold, except that in the proof of B a.s. ‚àí‚àí‚Üí 0, we apply Assumption C.3 (instead of Lemma H.4) to allow us to assume ÀöœÉ > 0 and œÉ > 0 almost surely. J Formal SpeciÔ¨Åcation of Tensor Programs In the main text, we have adopted an informal approach to specifying the NETSOR language and its siblings, in order to make the material accessible to a wide audience. Here we give the formal speciÔ¨Åcations for NETSOR‚àí (Figs. 4 to 6), NETSOR (Figs. 7 to 9), and self-parametrized NETSOR+ (Figs. 10 to 12). For ease of presentation, we have represented matrix multiplication explicitly via an operation MatMul (likewise for Moment in self-parametrized NETSOR+ ), and have we used double colon :: instead of single colon : for type annotation. JaK = W ‚àà Rn1√ón2 JhK = v ‚àà Rn2 JMatMul(a, h)K = W v ‚àÄi ‚àà [k], JgiK = vi ‚àà Rn JfK = f : Rk ‚Üí R Jf(g1, . . . , gk)K = u ‚àà Rn with uŒ± = f (v1Œ±, . . . , vkŒ±) Figure 6: NETSOR‚àí Semantics 71 program ::= stmt* stmt ::= Input var :: type | var := expr :: type expr ::= MatMul (var, var ) | fun( var* ) | var (+ var) + var ::= ‚ü® id ‚ü© fun ::= ‚ü® function Rk ‚Üí R for some k ‚â• 0 ‚ü© type ::= G(nat) | H(nat) | A(nat, nat) nat ::= ‚ü® any integer ‚â• 1 ‚ü© Figure 7: NETSOR Grammar; see DeÔ¨Ånition 4.1. Compared to NETSOR‚àí grammar, the only new item is LinComb in expr. expr : type var := expr :: type var : type a : A(n1, n2) h : H(n2) MatMul(a, h) : G(n1) g1, . . . , gk : G(n) f : Rk ‚Üí R f(g1, . . . , gk) : H(n) g1, . . . , gk : G(n) g1 + ¬∑ ¬∑ ¬∑ + gk : G(n) Figure 8: NETSOR Inference Rules JaK = W ‚àà Rn1√ón2 JhK = v ‚àà Rn2 JMatMul(a, h)K = W v ‚àÄi ‚àà [k], JgiK = vi ‚àà Rn JfK = f : Rk ‚Üí R Jf(g1, . . . , gk)K = u ‚àà Rn with uŒ± = f (v1Œ±, . . . , vkŒ±) ‚àÄi ‚àà [k], JgiK = vi ‚àà Rn Jg1 + ¬∑ ¬∑ ¬∑ + gkK = v1 + ¬∑ ¬∑ ¬∑ + vk ‚àà Rn Figure 9: NETSOR Semantics 72 program ::= stmt* stmt ::= Input var :: type | var := expr :: type expr ::= MatMul (var, var ) | fun( var* ; var*) | var (+ var) + | Moment(fun; var*; var*) var ::= ‚ü® id ‚ü© fun ::= ‚ü® parametrized function Rk √ó Rl ‚Üí R for some k, l ‚â• 0 ‚ü© type ::= C | G(nat) | H(nat) | A(nat, nat) nat ::= ‚ü® any integer ‚â• 1 ‚ü© Figure 10: Self-Parametrized NETSOR+ Grammar; see DeÔ¨Ånition C.8. Compared to NETSOR grammar, we have added a new type C and a new expression Moment. expr : type var := expr :: type var : type a : A(n1, n2) h : H(n2) MatMul(a, h) : G(n1) g1, . . . , gk : G(n) g1 + ¬∑ ¬∑ ¬∑ + gk : G(n) g1, . . . , gk : G(n) c1, . . . , cl : C f : Rk √ó Rl ‚Üí R f(g1, . . . , gk; c1, . . . , cl) : H(n) g1, . . . , gk : G(n) c1, . . . , cl : C f : Rk √ó Rl ‚Üí R Moment(f; g1, . . . , gk; c1, . . . , cl) : C Figure 11: Self-Parametrized NETSOR+ Inference Rules JaK = W ‚àà Rn1√ón2 JhK = v ‚àà Rn2 JMatMul(a, h)K = W v ‚àÄi ‚àà [k], JgiK = vi ‚àà Rn Jg1 + ¬∑ ¬∑ ¬∑ + gkK = v1 + ¬∑ ¬∑ ¬∑ + vk ‚àà Rn ‚àÄi ‚àà [k], JgiK = vi ‚àà Rn ‚àÄj ‚àà [l], JcjK = cj ‚àà R JfK = f : Rk √ó Rl ‚Üí R Jf(g1, . . . , gk; c1, . . . , cl)K = u ‚àà Rn with uŒ± = f (v1Œ±, . . . , vkŒ±; c1, . . . , cl) ‚àÄi ‚àà [k], JgiK = vi ‚àà Rn ‚àÄj ‚àà [l], JcjK = cj ‚àà R JfK = f : Rk √ó Rl ‚Üí R JMoment(f; g1, . . . , gk; c1, . . . , cl)K = 1 n n‚àë Œ±=1 f (v1Œ±, . . . , vkŒ±; c1, . . . , cl) Figure 12: Self-Parametrized NETSOR+ Semantics 73","libVersion":"0.2.3","langs":""}