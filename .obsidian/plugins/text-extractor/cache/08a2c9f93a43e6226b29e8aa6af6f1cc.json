{"path":"2-仓库/4-工程/PL-Theory-of-Computation.pdf","text":"This is an electronic version of the print textbook. Due to electronic rights restrictions, some third party content may be suppressed. Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. The publisher reserves the right to remove content from this title at any time if subsequent rights restrictions require it. For valuable information on pricing, previous editions, changes to current editions, and alternate formats, please visit www.cengage.com/highered to search by ISBN#, author, title, or keyword for materials in your areas of interest. Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. Intro duction to the Theory of omputatioC N t h i r d E d i t i o N m i C h a E l s i p s E r Australia • Brazil • Japan • Korea • Mexico • Singapore • Spain • United Kingdom • United States Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. Introduction to the Theory of Computation, Third Edition Michael Sipser Editor-in-Chief: Marie Lee Senior Product Manager: Alyssa Pratt Associate Product Manager: Stephanie Lorenz Content Project Manager: Jennifer Feltri-George Art Director: GEX Publishing Services Associate Marketing Manager: Shanna Shelton Cover Designer: Wing-ip Ngan, Ink design, inc Cover Image Credit: @Superstock © 2013 Cengage Learning ALL RIGHTS RESERVED. No part of this work covered by the copy- right herein may be reproduced, transmitted, stored or used in any form or by any means graphic, electronic, or mechanical, including but not limited to photocopying, recording, scanning, digitizing, tap- ing, Web distribution, information networks, or information storage and retrieval systems, except as permitted under Section 107 or 108 of the 1976 United States Copyright Act, without the prior written permission of the publisher.States Copyright Act, without the prior written permission of the publisher. Library of Congress Control Number: 2012938665 ISBN-13: 978-1-133-18779-0 ISBN-10: 1-133-18779-X Cengage Learning 20 Channel Center Street Boston, MA 02210 USA Cengage Learning is a leading provider of customized learning solu- tions with office locations around the globe, including Singapore, the United Kingdom, Australia, Mexico, Brazil, and Japan. Locate your local office at: international.cengage.com/region Cengage Learning products are represented in Canada by Nelson Education, Ltd. For your lifelong learning solutions, visit www.cengage.com Cengage Learning reserves the right to revise this publication and make changes from time to time in its content without notice. The programs in this book are for instructional purposes only. They have been tested with care, but are not guaranteed for any particular intent beyond educational purposes. The author and the publisher do not offer any warranties or representations, nor do they accept any liabilities with respect to the programs. Printed in the United States of America 1 2 3 4 5 6 7 8 16 15 14 13 12 For product information and technology assistance, contact us at Cengage Learning Customer & Sales Support, 1-800-354-9706 For permission to use material from this text or product, submit all requests online at cengage.com/permissions Further permissions questions can be emailed to permissionrequest@cengage.com Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. To Ina, Rachel, and Aaron Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. vi CONTENTS Part One: Automata and Languages 29 1 Regular Languages 31 1.1 Finite Automata . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 Formal deﬁnition of a ﬁnite automaton . . . . . . . . . . . . . 35 Examples of ﬁnite automata . . . . . . . . . . . . . . . . . . . . 37 Formal deﬁnition of computation . . . . . . . . . . . . . . . . 40 Designing ﬁnite automata . . . . . . . . . . . . . . . . . . . . . 41 The regular operations . . . . . . . . . . . . . . . . . . . . . . 44 1.2 Nondeterminism . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 Formal deﬁnition of a nondeterministic ﬁnite automaton . . . . 53 Equivalence of NFAs and DFAs . . . . . . . . . . . . . . . . . 54 Closure under the regular operations . . . . . . . . . . . . . . . 58 1.3 Regular Expressions . . . . . . . . . . . . . . . . . . . . . . . . . 63 Formal deﬁnition of a regular expression . . . . . . . . . . . . 64 Equivalence with ﬁnite automata . . . . . . . . . . . . . . . . . 66 1.4 Nonregular Languages . . . . . . . . . . . . . . . . . . . . . . . . 77 The pumping lemma for regular languages . . . . . . . . . . . 77 Exercises, Problems, and Solutions . . . . . . . . . . . . . . . . . . . 82 2 Context-Free Languages 101 2.1 Context-Free Grammars . . . . . . . . . . . . . . . . . . . . . . . 102 Formal deﬁnition of a context-free grammar . . . . . . . . . . 104 Examples of context-free grammars . . . . . . . . . . . . . . . 105 Designing context-free grammars . . . . . . . . . . . . . . . . 106 Ambiguity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107 Chomsky normal form . . . . . . . . . . . . . . . . . . . . . . 108 2.2 Pushdown Automata . . . . . . . . . . . . . . . . . . . . . . . . . 111 Formal deﬁnition of a pushdown automaton . . . . . . . . . . . 113 Examples of pushdown automata . . . . . . . . . . . . . . . . . 114 Equivalence with context-free grammars . . . . . . . . . . . . . 117 2.3 Non-Context-Free Languages . . . . . . . . . . . . . . . . . . . . 125 The pumping lemma for context-free languages . . . . . . . . . 125 2.4 Deterministic Context-Free Languages . . . . . . . . . . . . . . . 130 Properties of DCFLs . . . . . . . . . . . . . . . . . . . . . . . 133 Deterministic context-free grammars . . . . . . . . . . . . . . 135 Relationship of DPDAs and DCFGs . . . . . . . . . . . . . . . 146 Parsing and LR(k) Grammars . . . . . . . . . . . . . . . . . . . 151 Exercises, Problems, and Solutions . . . . . . . . . . . . . . . . . . . 154 Part Two: Computability Theory 163 3 The Church–Turing Thesis 165 3.1 Turing Machines . . . . . . . . . . . . . . . . . . . . . . . . . . . 165 Formal deﬁnition of a Turing machine . . . . . . . . . . . . . . 167 Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. CONTENTS vii Examples of Turing machines . . . . . . . . . . . . . . . . . . . 170 3.2 Variants of Turing Machines . . . . . . . . . . . . . . . . . . . . . 176 Multitape Turing machines . . . . . . . . . . . . . . . . . . . . 176 Nondeterministic Turing machines . . . . . . . . . . . . . . . . 178 Enumerators . . . . . . . . . . . . . . . . . . . . . . . . . . . . 180 Equivalence with other models . . . . . . . . . . . . . . . . . . 181 3.3 The Deﬁnition of Algorithm . . . . . . . . . . . . . . . . . . . . 182 Hilbert’s problems . . . . . . . . . . . . . . . . . . . . . . . . . 182 Terminology for describing Turing machines . . . . . . . . . . 184 Exercises, Problems, and Solutions . . . . . . . . . . . . . . . . . . . 187 4 Decidability 193 4.1 Decidable Languages . . . . . . . . . . . . . . . . . . . . . . . . . 194 Decidable problems concerning regular languages . . . . . . . 194 Decidable problems concerning context-free languages . . . . . 198 4.2 Undecidability . . . . . . . . . . . . . . . . . . . . . . . . . . . . 201 The diagonalization method . . . . . . . . . . . . . . . . . . . 202 An undecidable language . . . . . . . . . . . . . . . . . . . . . 207 A Turing-unrecognizable language . . . . . . . . . . . . . . . . 209 Exercises, Problems, and Solutions . . . . . . . . . . . . . . . . . . . 210 5 Reducibility 215 5.1 Undecidable Problems from Language Theory . . . . . . . . . . 216 Reductions via computation histories . . . . . . . . . . . . . . . 220 5.2 A Simple Undecidable Problem . . . . . . . . . . . . . . . . . . . 227 5.3 Mapping Reducibility . . . . . . . . . . . . . . . . . . . . . . . . 234 Computable functions . . . . . . . . . . . . . . . . . . . . . . . 234 Formal deﬁnition of mapping reducibility . . . . . . . . . . . . 235 Exercises, Problems, and Solutions . . . . . . . . . . . . . . . . . . . 239 6 Advanced Topics in Computability Theory 245 6.1 The Recursion Theorem . . . . . . . . . . . . . . . . . . . . . . . 245 Self-reference . . . . . . . . . . . . . . . . . . . . . . . . . . . 246 Terminology for the recursion theorem . . . . . . . . . . . . . 249 Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . 250 6.2 Decidability of logical theories . . . . . . . . . . . . . . . . . . . 252 A decidable theory . . . . . . . . . . . . . . . . . . . . . . . . . 255 An undecidable theory . . . . . . . . . . . . . . . . . . . . . . . 257 6.3 Turing Reducibility . . . . . . . . . . . . . . . . . . . . . . . . . . 260 6.4 A Deﬁnition of Information . . . . . . . . . . . . . . . . . . . . . 261 Minimal length descriptions . . . . . . . . . . . . . . . . . . . 262 Optimality of the deﬁnition . . . . . . . . . . . . . . . . . . . . 266 Incompressible strings and randomness . . . . . . . . . . . . . 267 Exercises, Problems, and Solutions . . . . . . . . . . . . . . . . . . . 270 Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. viii CONTENTS Part Three: Complexity Theory 273 7 Time Complexity 275 7.1 Measuring Complexity . . . . . . . . . . . . . . . . . . . . . . . . 275 Big-O and small-o notation . . . . . . . . . . . . . . . . . . . . 276 Analyzing algorithms . . . . . . . . . . . . . . . . . . . . . . . 279 Complexity relationships among models . . . . . . . . . . . . . 282 7.2 The Class P . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 284 Polynomial time . . . . . . . . . . . . . . . . . . . . . . . . . . 284 Examples of problems in P . . . . . . . . . . . . . . . . . . . . 286 7.3 The Class NP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 292 Examples of problems in NP . . . . . . . . . . . . . . . . . . . 295 The P versus NP question . . . . . . . . . . . . . . . . . . . . 297 7.4 NP-completeness . . . . . . . . . . . . . . . . . . . . . . . . . . . 299 Polynomial time reducibility . . . . . . . . . . . . . . . . . . . 300 Deﬁnition of NP-completeness . . . . . . . . . . . . . . . . . . 304 The Cook–Levin Theorem . . . . . . . . . . . . . . . . . . . . 304 7.5 Additional NP-complete Problems . . . . . . . . . . . . . . . . . 311 The vertex cover problem . . . . . . . . . . . . . . . . . . . . . 312 The Hamiltonian path problem . . . . . . . . . . . . . . . . . 314 The subset sum problem . . . . . . . . . . . . . . . . . . . . . 319 Exercises, Problems, and Solutions . . . . . . . . . . . . . . . . . . . 322 8 Space Complexity 331 8.1 Savitch’s Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . 333 8.2 The Class PSPACE . . . . . . . . . . . . . . . . . . . . . . . . . 336 8.3 PSPACE-completeness . . . . . . . . . . . . . . . . . . . . . . . 337 The TQBF problem . . . . . . . . . . . . . . . . . . . . . . . . 338 Winning strategies for games . . . . . . . . . . . . . . . . . . . 341 Generalized geography . . . . . . . . . . . . . . . . . . . . . . 343 8.4 The Classes L and NL . . . . . . . . . . . . . . . . . . . . . . . . 348 8.5 NL-completeness . . . . . . . . . . . . . . . . . . . . . . . . . . 351 Searching in graphs . . . . . . . . . . . . . . . . . . . . . . . . 353 8.6 NL equals coNL . . . . . . . . . . . . . . . . . . . . . . . . . . . 354 Exercises, Problems, and Solutions . . . . . . . . . . . . . . . . . . . 356 9 Intractability 363 9.1 Hierarchy Theorems . . . . . . . . . . . . . . . . . . . . . . . . . 364 Exponential space completeness . . . . . . . . . . . . . . . . . 371 9.2 Relativization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 376 Limits of the diagonalization method . . . . . . . . . . . . . . 377 9.3 Circuit Complexity . . . . . . . . . . . . . . . . . . . . . . . . . . 379 Exercises, Problems, and Solutions . . . . . . . . . . . . . . . . . . . 388 10 Advanced Topics in Complexity Theory 393 10.1 Approximation Algorithms . . . . . . . . . . . . . . . . . . . . . 393 Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. CONTENTS ix 10.2 Probabilistic Algorithms . . . . . . . . . . . . . . . . . . . . . . . 396 The class BPP . . . . . . . . . . . . . . . . . . . . . . . . . . . 396 Primality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 399 Read-once branching programs . . . . . . . . . . . . . . . . . . 404 10.3 Alternation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 408 Alternating time and space . . . . . . . . . . . . . . . . . . . . 410 The Polynomial time hierarchy . . . . . . . . . . . . . . . . . . 414 10.4 Interactive Proof Systems . . . . . . . . . . . . . . . . . . . . . . 415 Graph nonisomorphism . . . . . . . . . . . . . . . . . . . . . . 415 Deﬁnition of the model . . . . . . . . . . . . . . . . . . . . . . 416 IP = PSPACE . . . . . . . . . . . . . . . . . . . . . . . . . . . 418 10.5 Parallel Computation . . . . . . . . . . . . . . . . . . . . . . . . 427 Uniform Boolean circuits . . . . . . . . . . . . . . . . . . . . . 428 The class NC . . . . . . . . . . . . . . . . . . . . . . . . . . . 430 P-completeness . . . . . . . . . . . . . . . . . . . . . . . . . . 432 10.6 Cryptography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 433 Secret keys . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 433 Public-key cryptosystems . . . . . . . . . . . . . . . . . . . . . 435 One-way functions . . . . . . . . . . . . . . . . . . . . . . . . . 435 Trapdoor functions . . . . . . . . . . . . . . . . . . . . . . . . 437 Exercises, Problems, and Solutions . . . . . . . . . . . . . . . . . . . 439 Selected Bibliography 443 Index 448 Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. xii PREFACE TO THE FIRST EDITION Theory is relevant to practice. It provides conceptual tools that practition- ers use in computer engineering. Designing a new programming language for a specialized application? What you learned about grammars in this course comes in handy. Dealing with string searching and pattern matching? Remember ﬁnite automata and regular expressions. Confronted with a problem that seems to re- quire more computer time than you can afford? Think back to what you learned about NP-completeness. Various application areas, such as modern cryptographic protocols, rely on theoretical principles that you will learn here. Theory also is relevant to you because it shows you a new, simpler, and more elegant side of computers, which we normally consider to be complicated ma- chines. The best computer designs and applications are conceived with elegance in mind. A theoretical course can heighten your aesthetic sense and help you build more beautiful systems. Finally, theory is good for you because studying it expands your mind. Com- puter technology changes quickly. Speciﬁc technical knowledge, though useful today, becomes outdated in just a few years. Consider instead the abilities to think, to express yourself clearly and precisely, to solve problems, and to know when you haven’t solved a problem. These abilities have lasting value. Studying theory trains you in these areas. Practical considerations aside, nearly everyone working with computers is cu- rious about these amazing creations, their capabilities, and their limitations. A whole new branch of mathematics has grown up in the past 30 years to answer certain basic questions. Here’s a big one that remains unsolved: If I give you a large number—say, with 500 digits—can you ﬁnd its factors (the numbers that divide it evenly) in a reasonable amount of time? Even using a supercomputer, no one presently knows how to do that in all cases within the lifetime of the universe! The factoring problem is connected to certain secret codes in modern cryptosys- tems. Find a fast way to factor, and fame is yours! TO THE EDUCATOR This book is intended as an upper-level undergraduate or introductory gradu- ate text in computer science theory. It contains a mathematical treatment of the subject, designed around theorems and proofs. I have made some effort to accommodate students with little prior experience in proving theorems, though more experienced students will have an easier time. My primary goal in presenting the material has been to make it clear and interesting. In so doing, I have emphasized intuition and “the big picture” in the subject over some lower level details. For example, even though I present the method of proof by induction in Chapter 0 along with other mathematical preliminaries, it doesn’t play an im- portant role subsequently. Generally, I do not present the usual induction proofs of the correctness of various constructions concerning automata. If presented clearly, these constructions convince and do not need further argument. An in- duction may confuse rather than enlighten because induction itself is a rather sophisticated technique that many ﬁnd mysterious. Belaboring the obvious with Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. PREFACE TO THE FIRST EDITION xiii an induction risks teaching students that a mathematical proof is a formal ma- nipulation instead of teaching them what is and what is not a cogent argument. A second example occurs in Parts Two and Three, where I describe algorithms in prose instead of pseudocode. I don’t spend much time programming Turing machines (or any other formal model). Students today come with a program- ming background and ﬁnd the Church–Turing thesis to be self-evident. Hence I don’t present lengthy simulations of one model by another to establish their equivalence. Besides giving extra intuition and suppressing some details, I give what might be called a classical presentation of the subject material. Most theorists will ﬁnd the choice of material, terminology, and order of presentation consistent with that of other widely used textbooks. I have introduced original terminology in only a few places, when I found the standard terminology particularly obscure or confusing. For example, I introduce the term mapping reducibility instead of many–one reducibility. Practice through solving problems is essential to learning any mathemati- cal subject. In this book, the problems are organized into two main categories called Exercises and Problems. The Exercises review deﬁnitions and concepts. The Problems require some ingenuity. Problems marked with a star are more difﬁcult. I have tried to make the Exercises and Problems interesting challenges. THE FIRST EDITION Introduction to the Theory of Computation ﬁrst appeared as a Preliminary Edition in paperback. The ﬁrst edition differs from the Preliminary Edition in several substantial ways. The ﬁnal three chapters are new: Chapter 8 on space complex- ity; Chapter 9 on provable intractability; and Chapter 10 on advanced topics in complexity theory. Chapter 6 was expanded to include several advanced topics in computability theory. Other chapters were improved through the inclusion of additional examples and exercises. Comments from instructors and students who used the Preliminary Edition were helpful in polishing Chapters 0–7. Of course, the errors they reported have been corrected in this edition. Chapters 6 and 10 give a survey of several more advanced topics in com- putability and complexity theories. They are not intended to comprise a cohesive unit in the way that the remaining chapters are. These chapters are included to allow the instructor to select optional topics that may be of interest to the serious student. The topics themselves range widely. Some, such as Turing reducibility and alternation, are direct extensions of other concepts in the book. Others, such as decidable logical theories and cryptography, are brief introductions to large ﬁelds. FEEDBACK TO THE AUTHOR The internet provides new opportunities for interaction between authors and readers. I have received much e-mail offering suggestions, praise, and criticism, and reporting errors for the Preliminary Edition. Please continue to correspond! Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. xiv PREFACE TO THE FIRST EDITION I try to respond to each message personally, as time permits. The e-mail address for correspondence related to this book is sipserbook@math.mit.edu . A web site that contains a list of errata is maintained. Other material may be added to that site to assist instructors and students. Let me know what you would like to see there. The location for that site is http://math.mit.edu/~sipser/book.html . ACKNOWLEDGMENTS I could not have written this book without the help of many friends, colleagues, and my family. I wish to thank the teachers who helped shape my scientiﬁc viewpoint and educational style. Five of them stand out. My thesis advisor, Manuel Blum, is due a special note for his unique way of inspiring students through clarity of thought, enthusiasm, and caring. He is a model for me and for many others. I am grateful to Richard Karp for introducing me to complexity theory, to John Addison for teaching me logic and assigning those wonderful homework sets, to Juris Hartmanis for introducing me to the theory of computation, and to my father for introducing me to mathematics, computers, and the art of teaching. This book grew out of notes from a course that I have taught at MIT for the past 15 years. Students in my classes took these notes from my lectures. I hope they will forgive me for not listing them all. My teaching assistants over the years—Avrim Blum, Thang Bui, Benny Chor, Andrew Chou, Stavros Cos- madakis, Aditi Dhagat, Wayne Goddard, Parry Husbands, Dina Kravets, Jakov Kuˇcan, Brian O’Neill, Ioana Popescu, and Alex Russell—helped me to edit and expand these notes and provided some of the homework problems. Nearly three years ago, Tom Leighton persuaded me to write a textbook on the theory of computation. I had been thinking of doing so for some time, but it took Tom’s persuasion to turn theory into practice. I appreciate his generous advice on book writing and on many other things. I wish to thank Eric Bach, Peter Beebee, Cris Calude, Marek Chrobak, Anna Chefter, Guang-Ien Cheng, Elias Dahlhaus, Michael Fischer, Steve Fisk, Lance Fortnow, Henry J. Friedman, Jack Fu, Seymour Ginsburg, Oded Goldreich, Brian Grossman, David Harel, Micha Hofri, Dung T. Huynh, Neil Jones, H. Chad Lane, Kevin Lin, Michael Loui, Silvio Micali, Tadao Murata, Chris- tos Papadimitriou, Vaughan Pratt, Daniel Rosenband, Brian Scassellati, Ashish Sharma, Nir Shavit, Alexander Shen, Ilya Shlyakhter, Matt Stallmann, Perry Susskind, Y. C. Tay, Joseph Traub, Osamu Watanabe, Peter Widmayer, David Williamson, Derick Wood, and Charles Yang for comments, suggestions, and assistance as the writing progressed. The following people provided additional comments that have improved this book: Isam M. Abdelhameed, Eric Allender, Shay Artzi, Michelle Ather- ton, Rolfe Blodgett, Al Briggs, Brian E. Brooks, Jonathan Buss, Jin Yi Cai, Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. PREFACE TO THE FIRST EDITION xv Steve Chapel, David Chow, Michael Ehrlich, Yaakov Eisenberg, Farzan Fallah, Shaun Flisakowski, Hjalmtyr Hafsteinsson, C. R. Hale, Maurice Herlihy, Vegard Holmedahl, Sandy Irani, Kevin Jiang, Rhys Price Jones, James M. Jowdy, David M. Martin Jr., Manrique Mata-Montero, Ryota Matsuura, Thomas Minka, Farooq Mohammed, Tadao Murata, Jason Murray, Hideo Nagahashi, Kazuo Ohta, Constantine Papageorgiou, Joseph Raj, Rick Regan, Rhonda A. Reumann, Michael Rintzler, Arnold L. Rosenberg, Larry Roske, Max Rozenoer, Walter L. Ruzzo, Sanatan Sahgal, Leonard Schulman, Steve Seiden, Joel Seiferas, Ambuj Singh, David J. Stucki, Jayram S. Thathachar, H. Venkateswaran, Tom Whaley, Christopher Van Wyk, Kyle Young, and Kyoung Hwan Yun. Robert Sloan used an early version of the manuscript for this book in a class that he taught and provided me with invaluable commentary and ideas from his experience with it. Mark Herschberg, Kazuo Ohta, and Latanya Sweeney read over parts of the manuscript and suggested extensive improvements. Shaﬁ Goldwasser helped me with material in Chapter 10. I received expert technical support from William Baxter at Superscript, who wrote the LATEX macro package implementing the interior design, and from Larry Nolan at the MIT mathematics department, who keeps things running. It has been a pleasure to work with the folks at PWS Publishing in creat- ing the ﬁnal product. I mention Michael Sugarman, David Dietz, Elise Kaiser, Monique Calello, Susan Garland and Tanja Brull because I have had the most contact with them, but I know that many others have been involved, too. Thanks to Jerry Moore for the copy editing, to Diane Levy for the cover design, and to Catherine Hawkes for the interior design. I am grateful to the National Science Foundation for support provided under grant CCR-9503322. My father, Kenneth Sipser, and sister, Laura Sipser, converted the book di- agrams into electronic form. My other sister, Karen Fisch, saved us in various computer emergencies, and my mother, Justine Sipser, helped out with motherly advice. I thank them for contributing under difﬁcult circumstances, including insane deadlines and recalcitrant software. Finally, my love goes to my wife, Ina, and my daughter, Rachel. Thanks for putting up with all of this. Cambridge, Massachusetts Michael Sipser October, 1996 Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. xviii PREFACE TO THE SECOND EDITION use it mechanically without really understanding what is going on. Using reduc- tions instead, for proving undecidability, gives more valuable preparation for the reductions that appear in complexity theory. I am indebted to my teaching assistants—Ilya Baran, Sergi Elizalde, Rui Fan, Jonathan Feldman, Venkatesan Guruswami, Prahladh Harsha, Christos Kapout- sis, Julia Khodor, Adam Klivans, Kevin Matulef, Ioana Popescu, April Rasala, Sofya Raskhodnikova, and Iuliu Vasilescu—who helped me to craft some of the new problems and solutions. Ching Law, Edmond Kayi Lee, and Zulﬁkar Ramzan also contributed to the solutions. I thank Victor Shoup for coming up with a simple way to repair the gap in the analysis of the probabilistic primality algorithm that appears in the ﬁrst edition. I appreciate the efforts of the people at Course Technology in pushing me and the other parts of this project along, especially Alyssa Pratt and Aimee Poirier. Many thanks to Gerald Eisman, Weizhen Mao, Rupak Majumdar, Chris Umans, and Christopher Wilson for their reviews. I’m indebted to Jerry Moore for his superb job copy editing and to Laura Segel of ByteGraphics (lauras@bytegraphics.com) for her beautiful rendition of the ﬁgures. The volume of email I’ve received has been more than I expected. Hearing from so many of you from so many places has been absolutely delightful, and I’ve tried to respond to all eventually—my apologies for those I missed. I’ve listed here the people who made suggestions that speciﬁcally affected this edition, but I thank everyone for their correspondence: Luca Aceto, Arash Afkanpour, Rostom Aghanian, Eric Allender, Karun Bak- shi, Brad Ballinger, Ray Bartkus, Louis Barton, Arnold Beckmann, Mihir Bel- lare, Kevin Trent Bergeson, Matthew Berman, Rajesh Bhatt, Somenath Biswas, Lenore Blum, Mauro A. Bonatti, Paul Bondin, Nicholas Bone, Ian Bratt, Gene Browder, Doug Burke, Sam Buss, Vladimir Bychkovsky, Bruce Carneal, Soma Chaudhuri, Rong-Jaye Chen, Samir Chopra, Benny Chor, John Clausen, Alli- son Coates, Anne Condon, Jeffrey Considine, John J. Crashell, Claude Crepeau, Shaun Cutts, Susheel M. Daswani, Geoff Davis, Scott Dexter, Peter Drake, Jeff Edmonds, Yaakov Eisenberg, Kurtcebe Eroglu, Georg Essl, Alexander T. Fader, Farzan Fallah, Faith Fich, Joseph E. Fitzgerald, Perry Fizzano, David Ford, Jeannie Fromer, Kevin Fu, Atsushi Fujioka, Michel Galley, K. Gane- san, Simson Garﬁnkel, Travis Gebhardt, Peymann Gohari, Ganesh Gopalakr- ishnan, Steven Greenberg, Larry Grifﬁth, Jerry Grossman, Rudolf de Haan, Michael Halper, Nick Harvey, Mack Hendricks, Laurie Hiyakumoto, Steve Hockema, Michael Hoehle, Shahadat Hossain, Dave Isecke, Ghaith Issa, Raj D. Iyer, Christian Jacobi, Thomas Janzen, Mike D. Jones, Max Kanovitch, Aaron Kaufman, Roger Khazan, Sarfraz Khurshid, Kevin Killourhy, Seungjoo Kim, Victor Kuncak, Kanata Kuroda, Thomas Lasko, Suk Y. Lee, Edward D. Leg- enski, Li-Wei Lehman, Kong Lei, Zsolt Lengvarszky, Jeffrey Levetin, Baekjun Lim, Karen Livescu, Stephen Louie, TzerHung Low, Wolfgang Maass, Arash Madani, Michael Manapat, Wojciech Marchewka, David M. Martin Jr., Anders Martinson, Lyle McGeoch, Alberto Medina, Kurt Mehlhorn, Nihar Mehta, Al- bert R. Meyer, Thomas Minka, Mariya Minkova, Daichi Mizuguchi, G. Allen Morris III, Damon Mosk-Aoyama, Xiaolong Mou, Paul Muir, German Muller, Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. PREFACE TO THE SECOND EDITION xix Donald Nelson, Gabriel Nivasch, Mary Obelnicki, Kazuo Ohta, Thomas M. Oleson, Jr., Curtis Oliver, Owen Ozier, Rene Peralta, Alexander Perlis, Holger Petersen, Detlef Plump, Robert Prince, David Pritchard, Bina Reed, Nicholas Riley, Ronald Rivest, Robert Robinson, Christi Rockwell, Phil Rogaway, Max Rozenoer, John Rupf, Teodor Rus, Larry Ruzzo, Brian Sanders, Cem Say, Kim Schioett, Joel Seiferas, Joao Carlos Setubal, Geoff Lee Seyon, Mark Skandera, Bob Sloan, Geoff Smith, Marc L. Smith, Stephen Smith, Alex C. Snoeren, Guy St-Denis, Larry Stockmeyer, Radu Stoleru, David Stucki, Hisham M. Sueyllam, Kenneth Tam, Elizabeth Thompson, Michel Toulouse, Eric Tria, Chittaranjan Tripathy, Dan Trubow, Hiroki Ueda, Giora Unger, Kurt L. Van Etten, Jesir Vargas, Bienvenido Velez-Rivera, Kobus Vos, Alex Vrenios, Sven Waibel, Marc Waldman, Tom Whaley, Anthony Widjaja, Sean Williams, Joseph N. Wilson, Chris Van Wyk, Guangming Xing, Vee Voon Yee, Cheng Yongxi, Neal Young, Timothy Yuen, Kyle Yung, Jinghua Zhang, Lilla Zollei. I thank Suzanne Balik, Matthew Kane, Kurt L. Van Etten, Nancy Lynch, Gregory Roberts, and Cem Say for pointing out errata in the ﬁrst printing. Most of all, I thank my family—Ina, Rachel, and Aaron—for their patience, understanding, and love as I sat for endless hours here in front of my computer screen. Cambridge, Massachusetts Michael Sipser December, 2004 Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. xxii PREFACE TO THE THIRD EDITION may prefer to designate it as supplementary reading. Later chapters do not de- pend on this material. Many people helped directly or indirectly in developing this edition. I’m in- debted to reviewers Christos Kapoutsis and Cem Say who read a draft of the new section and provided valuable feedback. Several individuals at Cengage Learning assisted with the production, notably Alyssa Pratt and Jennifer Feltri-George. Suzanne Huizenga copyedited the text and Laura Segel of ByteGraphics created the new ﬁgures and modiﬁed some of the older ﬁgures. I wish to thank my teaching assistants at MIT, Victor Chen, Andy Drucker, Michael Forbes, Elena Grigorescu, Brendan Juba, Christos Kapoutsis, Jon Kel- ner, Swastik Kopparty, Kevin Matulef, Amanda Redlich, Zack Remscrim, Ben Rossman, Shubhangi Saraf, and Oren Weimann. Each of them helped me by discussing new problems and their solutions, and by providing insight into how well our students understood the course content. I’ve greatly enjoyed working with such talented and enthusiastic young people. It has been gratifying to receive email from around the globe. Thanks to all for your suggestions, questions, and ideas. Here is a list of those correspondents whose comments affected this edition: Djihed Aﬁﬁ, Steve Aldrich, Eirik Bakke, Suzanne Balik, Victor Bandur, Paul Beame, Elazar Birnbaum, Goutam Biswas, Rob Bittner, Marina Blanton, Rod- ney Bliss, Promita Chakraborty, Lewis Collier, Jonathan Deber, Simon Dex- ter, Matt Diephouse, Peter Dillinger, Peter Drake, Zhidian Du, Peter Fe- jer, Margaret Fleck, Atsushi Fujioka, Valerio Genovese, Evangelos Georgiadis, Joshua Grochow, Jerry Grossman, Andreas Guelzow, Hjalmtyr Hafsteinsson, Arthur Hall III, Cihat Imamoglu, Chinawat Isradisaikul, Kayla Jacobs, Flem- ming Jensen, Barbara Kaiser, Matthew Kane, Christos Kapoutsis, Ali Durlov Khan, Edwin Sze Lun Khoo, Yongwook Kim, Akash Kumar, Eleazar Leal, Zsolt Lengvarszky, Cheng-Chung Li, Xiangdong Liang, Vladimir Lifschitz, Ryan Lortie, Jonathan Low, Nancy Lynch, Alexis Maciel, Kevin Matulef, Nelson Max, Hans-Rudolf Metz, Mladen Mikˆsa, Sara Miner More, Rajagopal Nagara- jan, Marvin Nakayama, Jonas Nyrup, Gregory Roberts, Ryan Romero, Santhosh Samarthyam, Cem Say, Joel Seiferas, John Sieg, Marc Smith, John Steinberger, Nuri Tas¸demir, Tamir Tassa, Mark Testa, Jesse Tjang, John Trammell, Hi- roki Ueda, Jeroen Vaelen, Kurt L. Van Etten, Guillermo V´azquez, Phanisekhar Botlaguduru Venkata, Benjamin Bing-Yi Wang, Lutz Warnke, David Warren, Thomas Watson, Joseph Wilson, David Wittenberg, Brian Wongchaowart, Kis- han Yerubandi, Dai Yi. Above all, I thank my family—my wife, Ina, and our children, Rachel and Aaron. Time is ﬁnite and ﬂeeting. Your love is everything. Cambridge, Massachusetts Michael Sipser April, 2012 Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 2 CHAPTER 0 / INTRODUCTION separate part of this book. Here, we introduce these parts in reverse order be- cause by starting from the end you can better understand the reason for the beginning. COMPLEXITY THEORY Computer problems come in different varieties; some are easy, and some are hard. For example, the sorting problem is an easy one. Say that you need to arrange a list of numbers in ascending order. Even a small computer can sort a million numbers rather quickly. Compare that to a scheduling problem. Say that you must ﬁnd a schedule of classes for the entire university to satisfy some reasonable constraints, such as that no two classes take place in the same room at the same time. The scheduling problem seems to be much harder than the sorting problem. If you have just a thousand classes, ﬁnding the best schedule may require centuries, even with a supercomputer. What makes some problems computationally hard and others easy? This is the central question of complexity theory. Remarkably, we don’t know the answer to it, though it has been intensively researched for over 40 years. Later, we explore this fascinating question and some of its ramiﬁcations. In one important achievement of complexity theory thus far, researchers have discovered an elegant scheme for classifying problems according to their com- putational difﬁculty. It is analogous to the periodic table for classifying elements according to their chemical properties. Using this scheme, we can demonstrate a method for giving evidence that certain problems are computationally hard, even if we are unable to prove that they are. You have several options when you confront a problem that appears to be computationally hard. First, by understanding which aspect of the problem is at the root of the difﬁculty, you may be able to alter it so that the problem is more easily solvable. Second, you may be able to settle for less than a perfect solution to the problem. In certain cases, ﬁnding solutions that only approximate the perfect one is relatively easy. Third, some problems are hard only in the worst case situation, but easy most of the time. Depending on the application, you may be satisﬁed with a procedure that occasionally is slow but usually runs quickly. Finally, you may consider alternative types of computation, such as randomized computation, that can speed up certain tasks. One applied area that has been affected directly by complexity theory is the ancient ﬁeld of cryptography. In most ﬁelds, an easy computational problem is preferable to a hard one because easy ones are cheaper to solve. Cryptography is unusual because it speciﬁcally requires computational problems that are hard, rather than easy. Secret codes should be hard to break without the secret key or password. Complexity theory has pointed cryptographers in the direction of computationally hard problems around which they have designed revolutionary new codes. Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 0.2 MATHEMATICAL NOTIONS AND TERMINOLOGY 3 COMPUTABILITY THEORY During the ﬁrst half of the twentieth century, mathematicians such as Kurt G ¨odel, Alan Turing, and Alonzo Church discovered that certain basic problems cannot be solved by computers. One example of this phenomenon is the prob- lem of determining whether a mathematical statement is true or false. This task is the bread and butter of mathematicians. It seems like a natural for solution by computer because it lies strictly within the realm of mathematics. But no computer algorithm can perform this task. Among the consequences of this profound result was the development of ideas concerning theoretical models of computers that eventually would help lead to the construction of actual computers. The theories of computability and complexity are closely related. In com- plexity theory, the objective is to classify problems as easy ones and hard ones; whereas in computability theory, the classiﬁcation of problems is by those that are solvable and those that are not. Computability theory introduces several of the concepts used in complexity theory. AUTOMATA THEORY Automata theory deals with the deﬁnitions and properties of mathematical mod- els of computation. These models play a role in several applied areas of computer science. One model, called the ﬁnite automaton, is used in text processing, com- pilers, and hardware design. Another model, called the context-free grammar, is used in programming languages and artiﬁcial intelligence. Automata theory is an excellent place to begin the study of the theory of computation. The theories of computability and complexity require a precise deﬁnition of a computer. Automata theory allows practice with formal deﬁnitions of computation as it introduces concepts relevant to other nontheoretical areas of computer science. 4 CHAPTER 0 / INTRODUCTION One way is by listing a set’s elements inside braces. Thus the set S = {7, 21, 57} contains the elements 7, 21, and 57. The symbols ∈ and ̸∈ denote set member- ship and nonmembership. We write 7 ∈ {7, 21, 57} and 8 ̸∈ {7, 21, 57}. For two sets A and B, we say that A is a subset of B, written A ⊆ B, if every member of A also is a member of B. We say that A is a proper subset of B, written A ⊊ B, if A is a subset of B and not equal to B. The order of describing a set doesn’t matter, nor does repetition of its mem- bers. We get the same set S by writing {57, 7, 7, 7, 21}. If we do want to take the number of occurrences of members into account, we call the group a multiset instead of a set. Thus {7} and {7, 7} are different as multisets but identical as sets. An inﬁnite set contains inﬁnitely many elements. We cannot write a list of all the elements of an inﬁnite set, so we sometimes use the “. . .” notation to mean “continue the sequence forever.” Thus we write the set of natural numbers N as {1, 2, 3, . . . }. The set of integers Z is written as { . . . , −2, −1, 0, 1, 2, . . . }. The set with zero members is called the empty set and is written ∅. A set with one member is sometimes called a singleton set, and a set with two members is called an unordered pair. When we want to describe a set containing elements according to some rule, we write {n| rule about n}. Thus {n| n = m2 for some m ∈ N } means the set of perfect squares. If we have two sets A and B, the union of A and B, written A∪B, is the set we get by combining all the elements in A and B into a single set. The intersection of A and B, written A ∩ B, is the set of elements that are in both A and B. The complement of A, written 0.2 MATHEMATICAL NOTIONS AND TERMINOLOGY 5 FIGURE 0.1 Venn diagram for the set of English words starting with “t” Similarly, we represent the set END-z of English words that end with “z” in the following ﬁgure. FIGURE 0.2 Venn diagram for the set of English words ending with “z” To represent both sets in the same Venn diagram, we must draw them so that they overlap, indicating that they share some elements, as shown in the following ﬁgure. For example, the word topaz is in both sets. The ﬁgure also contains a circle for the set START-j. It doesn’t overlap the circle for START-t because no word lies in both sets. FIGURE 0.3 Overlapping circles indicate common elements Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 6 CHAPTER 0 / INTRODUCTION The next two Venn diagrams depict the union and intersection of sets A and B. FIGURE 0.4 Diagrams for (a) A ∪ B and (b) A ∩ B SEQUENCES AND TUPLES A sequence of objects is a list of these objects in some order. We usually designate a sequence by writing the list within parentheses. For example, the sequence 7, 21, 57 would be written (7, 21, 57). The order doesn’t matter in a set, but in a sequence it does. Hence (7, 21, 57) is not the same as (57, 7, 21). Similarly, repetition does matter in a sequence, but it doesn’t matter in a set. Thus (7, 7, 21, 57) is different from both of the other sequences, whereas the set {7, 21, 57} is identical to the set {7, 7, 21, 57}. As with sets, sequences may be ﬁnite or inﬁnite. Finite sequences often are called tuples. A sequence with k elements is a k-tuple. Thus (7, 21, 57) is a 3-tuple. A 2-tuple is also called an ordered pair. Sets and sequences may appear as elements of other sets and sequences. For example, the power set of A is the set of all subsets of A. If A is the set {0, 1}, the power set of A is the set { ∅, {0}, {1}, {0, 1} }. The set of all ordered pairs whose elements are 0s and 1s is { (0, 0), (0, 1), (1, 0), (1, 1) }. If A and B are two sets, the Cartesian product or cross product of A and B, written A × B, is the set of all ordered pairs wherein the ﬁrst element is a member of A and the second element is a member of B. EXAMPLE 0.5 0.2 MATHEMATICAL NOTIONS AND TERMINOLOGY 7 EXAMPLE 0.6 8 CHAPTER 0 / INTRODUCTION We may describe a speciﬁc function in several ways. One way is with a pro- cedure for computing an output from a speciﬁed input. Another way is with a table that lists all possible inputs and gives the output for each input. EXAMPLE 0.8 0.2 MATHEMATICAL NOTIONS AND TERMINOLOGY 9 A predicate or property is a function whose range is {TRUE, FALSE}. For example, let even be a property that is TRUE if its input is an even number and FALSE if its input is an odd number. Thus even(4) = TRUE and even(5) = FALSE. A property whose domain is a set of k-tuples A × · · · × A is called a relation, a k-ary relation, or a k-ary relation on A. A common case is a 2-ary relation, called a binary relation. When writing an expression involving a binary rela- tion, we customarily use inﬁx notation. For example, “less than” is a relation usually written with the inﬁx operation symbol <. “Equality”, written with the = symbol, is another familiar relation. If R is a binary relation, the statement aRb means that aRb = TRUE. Similarly, if R is a k-ary relation, the statement R(a1, . . . , ak) means that R(a1, . . . , ak) = TRUE. EXAMPLE 0.10 10 CHAPTER 0 / INTRODUCTION EXAMPLE 0.11 0.2 MATHEMATICAL NOTIONS AND TERMINOLOGY 11 and a formal description of the graph in Figure 0.12(b) is ( {1, 2, 3, 4}, {(1, 2), (1, 3), (1, 4), (2, 3), (2, 4), (3, 4)}) . Graphs frequently are used to represent data. Nodes might be cities and edges the connecting highways, or nodes might be people and edges the friendships between them. Sometimes, for convenience, we label the nodes and/or edges of a graph, which then is called a labeled graph. Figure 0.13 depicts a graph whose nodes are cities and whose edges are labeled with the dollar cost of the cheapest nonstop airfare for travel between those cities if ﬂying nonstop between them is possible. FIGURE 0.13 Cheapest nonstop airfares between various cities We say that graph G is a subgraph of graph H if the nodes of G are a subset of the nodes of H, and the edges of G are the edges of H on the corresponding nodes. The following ﬁgure shows a graph H and a subgraph G. FIGURE 0.14 Graph G (shown darker) is a subgraph of H Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 12 CHAPTER 0 / INTRODUCTION A path in a graph is a sequence of nodes connected by edges. A simple path is a path that doesn’t repeat any nodes. A graph is connected if every two nodes have a path between them. A path is a cycle if it starts and ends in the same node. A simple cycle is one that contains at least three nodes and repeats only the ﬁrst and last nodes. A graph is a tree if it is connected and has no simple cycles, as shown in Figure 0.15. A tree may contain a specially designated node called the root. The nodes of degree 1 in a tree, other than the root, are called the leaves of the tree. FIGURE 0.15 (a) A path in a graph, (b) a cycle in a graph, and (c) a tree A directed graph has arrows instead of lines, as shown in the following ﬁgure. The number of arrows pointing from a particular node is the outdegree of that node, and the number of arrows pointing to a particular node is the indegree. FIGURE 0.16 A directed graph Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 0.2 MATHEMATICAL NOTIONS AND TERMINOLOGY 13 In a directed graph, we represent an edge from i to j as a pair (i, j). The formal description of a directed graph G is (V, E), where V is the set of nodes and E is the set of edges. The formal description of the graph in Figure 0.16 is ( {1,2,3,4,5,6}, {(1,2), (1,5), (2,1), (2,4), (5,4), (5,6), (6,1), (6,3)}) . A path in which all the arrows point in the same direction as its steps is called a directed path. A directed graph is strongly connected if a directed path connects every two nodes. Directed graphs are a handy way of depicting binary relations. If R is a binary relation whose domain is D × D, a labeled graph G = (D, E) represents R, where E = {(x, y)| xRy}. EXAMPLE 0.17 14 CHAPTER 0 / INTRODUCTION A string over an alphabet is a ﬁnite sequence of symbols from that alphabet, usually written next to one another and not separated by commas. If Σ1 = {0,1}, then 01001 is a string over Σ1. If Σ2 = {a, b, c, . . . , z}, then abracadabra is a string over Σ2. If w is a string over Σ, the length of w, written |w|, is the number of symbols that it contains. The string of length zero is called the empty string and is written ε. The empty string plays the role of 0 in a number system. If w has length n, we can write w = w1w2 · · · wn where each wi ∈ Σ. The reverse of w, written wR, is the string obtained by writing w in the opposite order (i.e., wnwn−1 · · · w1). String z is a substring of w if z appears consecutively within w. For example, cad is a substring of abracadabra. If we have string x of length m and string y of length n, the concatenation of x and y, written xy, is the string obtained by appending y to the end of x, as in x1 · · · xmy1 · · · yn. To concatenate a string with itself many times, we use the superscript notation x k to mean k z 0.2 MATHEMATICAL NOTIONS AND TERMINOLOGY 15 tion of two Boolean values is 1 if either of those values is 1. We summarize this information as follows. 0 ∧ 0 = 0 0 ∨ 0 = 0 ¬0 = 1 0 ∧ 1 = 0 0 ∨ 1 = 1 ¬1 = 0 1 ∧ 0 = 0 1 ∨ 0 = 1 1 ∧ 1 = 1 1 ∨ 1 = 1 We use Boolean operations for combining simple statements into more com- plex Boolean expressions, just as we use the arithmetic operations + and × to construct complex arithmetic expressions. For example, if P is the Boolean value representing the truth of the statement “the sun is shining” and Q represents the truth of the statement “today is Monday”, we may write P ∧ Q to represent the truth value of the statement “the sun is shining and today is Monday” and sim- ilarly for P ∨ Q with and replaced by or. The values P and Q are called the operands of the operation. Several other Boolean operations occasionally appear. The exclusive or, or XOR, operation is designated by the ⊕ symbol and is 1 if either but not both of its two operands is 1. The equality operation, written with the symbol ↔, is 1 if both of its operands have the same value. Finally, the implication operation is designated by the symbol → and is 0 if its ﬁrst operand is 1 and its second operand is 0; otherwise, → is 1. We summarize this information as follows. 0 ⊕ 0 = 0 0 ↔ 0 = 1 0 → 0 = 1 0 ⊕ 1 = 1 0 ↔ 1 = 0 0 → 1 = 1 1 ⊕ 0 = 1 1 ↔ 0 = 0 1 → 0 = 0 1 ⊕ 1 = 0 1 ↔ 1 = 1 1 → 1 = 1 We can establish various relationships among these operations. In fact, we can express all Boolean operations in terms of the AND and NOT operations, as the following identities show. The two expressions in each row are equivalent. Each row expresses the operation in the left-hand column in terms of operations above it and AND and NOT. P ∨ Q ¬(¬P ∧ ¬Q) P → Q ¬P ∨ Q P ↔ Q (P → Q) ∧ (Q → P ) P ⊕ Q ¬(P ↔ Q) The distributive law for AND and OR comes in handy when we manipulate Boolean expressions. It is similar to the distributive law for addition and multi- plication, which states that a × (b + c) = (a × b) + (a × c). The Boolean version comes in two forms: • P ∧ (Q ∨ R) equals (P ∧ Q) ∨ (P ∧ R), and its dual • P ∨ (Q ∧ R) equals (P ∨ Q) ∧ (P ∨ R). Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 16 CHAPTER 0 / INTRODUCTION SUMMARY OF MATHEMATICAL TERMS Alphabet A ﬁnite, nonempty set of objects called symbols Argument An input to a function Binary relation A relation whose domain is a set of pairs Boolean operation An operation on Boolean values Boolean value The values TRUE or FALSE, often represented by 1 or 0 Cartesian product An operation on sets forming a set of all tuples of elements from respective sets Complement An operation on a set, forming the set of all elements not present Concatenation An operation that joins strings together Conjunction Boolean AND operation Connected graph A graph with paths connecting every two nodes Cycle A path that starts and ends in the same node Directed graph A collection of points and arrows connecting some pairs of points Disjunction Boolean OR operation Domain The set of possible inputs to a function Edge A line in a graph Element An object in a set Empty set The set with no members Empty string The string of length zero Equivalence relation A binary relation that is reﬂexive, symmetric, and transitive Function An operation that translates inputs into outputs Graph A collection of points and lines connecting some pairs of points Intersection An operation on sets forming the set of common elements k-tuple A list of k objects Language A set of strings Member An object in a set Node A point in a graph Ordered pair A list of two elements Path A sequence of nodes in a graph connected by edges Predicate A function whose range is {TRUE, FALSE} Property A predicate Range The set from which outputs of a function are drawn Relation A predicate, most typically when the domain is a set of k-tuples Sequence A list of objects Set A group of objects Simple path A path without repetition Singleton set A set with one member String A ﬁnite list of symbols from an alphabet Symbol A member of an alphabet Tree A connected graph without simple cycles Union An operation on sets combining all elements into a single set Unordered pair A set with two members Vertex A point in a graph Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 0.3 DEFINITIONS, THEOREMS, AND PROOFS 1718 CHAPTER 0 / INTRODUCTION Sometimes the parts of a multipart statement are not immediately evident. One frequently occurring type of multipart statement has the form “P if and only if Q”, often written “P iff Q”, where both P and Q are mathematical state- ments. This notation is shorthand for a two-part statement. The ﬁrst part is “P only if Q,” which means: If P is true, then Q is true, written P ⇒ Q. The second is “P if Q,” which means: If Q is true, then P is true, written P ⇐ Q. The ﬁrst of these parts is the forward direction of the original statement and the second is the reverse direction. We write “P if and only if Q” as P ⇐⇒ Q. To prove a statement of this form, you must prove each of the two directions. Often, one of these directions is easier to prove than the other. Another type of multipart statement states that two sets A and B are equal. The ﬁrst part states that A is a subset of B, and the second part states that B is a subset of A. Thus one common way to prove that A = B is to prove that every member of A also is a member of B, and that every member of B also is a member of A. Next, when you want to prove a statement or part thereof, try to get an in- tuitive, “gut” feeling of why it should be true. Experimenting with examples is especially helpful. Thus if the statement says that all objects of a certain type have a particular property, pick a few objects of that type and observe that they actually do have that property. After doing so, try to ﬁnd an object that fails to have the property, called a counterexample. If the statement actually is true, you will not be able to ﬁnd a counterexample. Seeing where you run into difﬁculty when you attempt to ﬁnd a counterexample can help you understand why the statement is true. EXAMPLE 0.19 0.3 DEFINITIONS, THEOREMS, AND PROOFS 19 Can you now begin to see why the statement is true and how to prove it? 20 CHAPTER 0 / INTRODUCTION For practice, let’s prove one of DeMorgan’s laws. THEOREM 0.20 0.4 TYPES OF PROOF 2122 CHAPTER 0 / INTRODUCTION EXAMPLE 0.23 0.4 TYPES OF PROOF 23 every assignment to its variables, or that a program works correctly at all steps or for all inputs. To illustrate how proof by induction works, let’s take the inﬁnite set to be the natural numbers, N = {1, 2, 3, . . . }, and say that the property is called P. Our goal is to prove that P(k) is true for each natural number k. In other words, we want to prove that P(1) is true, as well as P(2), P(3), P(4), and so on. Every proof by induction consists of two parts, the basis and the induction step. Each part is an individual proof on its own. The basis proves that P(1) is true. The induction step proves that for each i ≥ 1, if P(i) is true, then so is P(i + 1). When we have proven both of these parts, the desired result follows—namely, that P(i) is true for each i. Why? First, we know that P(1) is true because the basis alone proves it. Second, we know that P(2) is true because the induction step proves that if P(1) is true then P(2) is true, and we already know that P(1) is true. Third, we know that P(3) is true because the induction step proves that if P(2) is true then P(3) is true, and we already know that P(2) is true. This process continues for all natural numbers, showing that P(4) is true, P(5) is true, and so on. Once you understand the preceding paragraph, you can easily understand variations and generalizations of the same idea. For example, the basis doesn’t necessarily need to start with 1; it may start with any value b. In that case, the induction proof shows that P(k) is true for every k that is at least b. In the induction step, the assumption that P(i) is true is called the induction hypothesis. Sometimes having the stronger induction hypothesis that P(j) is true for every j ≤ i is useful. The induction proof still works because when we want to prove that P(i + 1) is true, we have already proved that P(j) is true for every j ≤ i. The format for writing down a proof by induction is as follows. Basis: Prove that P(1) is true. ... Induction step: For each i ≥ 1, assume that P(i) is true and use this assumption to show that P(i + 1) is true. ... Now, let’s prove by induction the correctness of the formula used to calculate the size of monthly payments of home mortgages. When buying a home, many people borrow some of the money needed for the purchase and repay this loan over a certain number of years. Typically, the terms of such repayments stipulate that a ﬁxed amount of money is paid each month to cover the interest, as well as part of the original sum, so that the total is repaid in 30 years. The formula for calculating the size of the monthly payments is shrouded in mystery, but actually is quite simple. It touches many people’s lives, so you should ﬁnd it interesting. We use induction to prove that it works, making it a good illustration of that technique. Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 24 CHAPTER 0 / INTRODUCTION First, we set up the names and meanings of several variables. Let P be the principal, the amount of the original loan. Let I > 0 be the yearly interest rate of the loan, where I = 0.06 indicates a 6% rate of interest. Let Y be the monthly payment. For convenience, we use I to deﬁne another variable M , the monthly multiplier. It is the rate at which the loan changes each month because of the interest on it. Following standard banking practice, the monthly interest rate is one-twelfth of the annual rate so M = 1 + I/12, and interest is paid monthly (monthly compounding). Two things happen each month. First, the amount of the loan tends to in- crease because of the monthly multiplier. Second, the amount tends to decrease because of the monthly payment. Let Pt be the amount of the loan outstand- ing after the tth month. Then P0 = P is the amount of the original loan, P1 = M P0 − Y is the amount of the loan after one month, P2 = M P1 − Y is the amount of the loan after two months, and so on. Now we are ready to state and prove a theorem by induction on t that gives a formula for the value of Pt. THEOREM 0.25 EXERCISES 25 We do so with the following steps. First, from the deﬁnition of Pk+1 from Pk, we know that Pk+1 = PkM − Y. Therefore, using the induction hypothesis to calculate Pk, Pk+1 = [P M k − Y ( M k − 1 26 CHAPTER 0 / INTRODUCTION 0.3 Let A be the set {x, y, z} and B be the set {x, y}. a. Is A a subset of B? b. Is B a subset of A? c. What is A ∪ B? d. What is A ∩ B? e. What is A × B? f. What is the power set of B? 0.4 If A has a elements and B has b elements, how many elements are in A × B? Explain your answer. 0.5 If C is a set with c elements, how many elements are in the power set of C? Explain your answer. 0.6 Let X be the set {1, 2, 3, 4, 5} and Y be the set {6, 7, 8, 9, 10}. The unary function f : X−→ Y and the binary function g : X × Y −→Y are described in the following tables. n PROBLEMS 27 0.9 Write a formal description of the following graph. 28 CHAPTER 0 / INTRODUCTION A⋆0.14 Ramsey’s theorem. Let G be a graph. A clique in G is a subgraph in which every two nodes are connected by an edge. An anti-clique, also called an independent set, is a subgraph in which every two nodes are not connected by an edge. Show that every graph with n nodes contains either a clique or an anti-clique with at least 1 P A R T O N E Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 32 CHAPTER 1 / REGULAR LANGUAGES in front to detect the presence of a person about to walk through the doorway. Another pad is located to the rear of the doorway so that the controller can hold the door open long enough for the person to pass all the way through and also so that the door does not strike someone standing behind it as it opens. This conﬁguration is shown in the following ﬁgure. FIGURE 1.1 Top view of an automatic door The controller is in either of two states: “OPEN” or “CLOSED,” representing the corresponding condition of the door. As shown in the following ﬁgures, there are four possible input conditions: “FRONT” (meaning that a person is standing on the pad in front of the doorway), “REAR” (meaning that a person is standing on the pad to the rear of the doorway), “BOTH” (meaning that people are standing on both pads), and “NEITHER” (meaning that no one is standing on either pad). FIGURE 1.2 State diagram for an automatic door controller Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 1.1 FINITE AUTOMATA 33 input signal 34 CHAPTER 1 / REGULAR LANGUAGES In beginning to describe the mathematical theory of ﬁnite automata, we do so in the abstract, without reference to any particular application. The following ﬁgure depicts a ﬁnite automaton called M1. FIGURE 1.4 A ﬁnite automaton called M1 that has three states Figure 1.4 is called the state diagram of M1. It has three states, labeled q1, q2, and q3. The start state, q1, is indicated by the arrow pointing at it from nowhere. The accept state, q2, is the one with a double circle. The arrows going from one state to another are called transitions. When this automaton receives an input string such as 1101, it processes that string and produces an output. The output is either accept or reject. We will consider only this yes/no type of output for now to keep things simple. The processing begins in M1’s start state. The automaton receives the symbols from the input string one by one from left to right. After reading each symbol, M1 moves from one state to another along the transition that has that symbol as its label. When it reads the last symbol, M1 produces its output. The output is accept if M1 is now in an accept state and reject if it is not. For example, when we feed the input string 1101 into the machine M1 in Figure 1.4, the processing proceeds as follows: 1. Start in state q1. 2. Read 1, follow transition from q1 to q2. 3. Read 1, follow transition from q2 to q2. 4. Read 0, follow transition from q2 to q3. 5. Read 1, follow transition from q3 to q2. 6. Accept because M1 is in an accept state q2 at the end of the input. Experimenting with this machine on a variety of input strings reveals that it accepts the strings 1, 01, 11, and 0101010101. In fact, M1 accepts any string that ends with a 1, as it goes to its accept state q2 whenever it reads the symbol 1. In addition, it accepts strings 100, 0100, 110000, and 0101000000, and any string that ends with an even number of 0s following the last 1. It rejects other strings, such as 0, 10, 101000. Can you describe the language consisting of all strings that M1 accepts? We will do so shortly. Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 1.1 FINITE AUTOMATA 35 FORMAL DEFINITION OF A FINITE AUTOMATON In the preceding section, we used state diagrams to introduce ﬁnite automata. Now we deﬁne ﬁnite automata formally. Although state diagrams are easier to grasp intuitively, we need the formal deﬁnition, too, for two speciﬁc reasons. First, a formal deﬁnition is precise. It resolves any uncertainties about what is allowed in a ﬁnite automaton. If you were uncertain about whether ﬁnite automata were allowed to have 0 accept states or whether they must have ex- actly one transition exiting every state for each possible input symbol, you could consult the formal deﬁnition and verify that the answer is yes in both cases. Sec- ond, a formal deﬁnition provides notation. Good notation helps you think and express your thoughts clearly. The language of a formal deﬁnition is somewhat arcane, having some simi- larity to the language of a legal document. Both need to be precise, and every detail must be spelled out. A ﬁnite automaton has several parts. It has a set of states and rules for going from one state to another, depending on the input symbol. It has an input al- phabet that indicates the allowed input symbols. It has a start state and a set of accept states. The formal deﬁnition says that a ﬁnite automaton is a list of those ﬁve objects: set of states, input alphabet, rules for moving, start state, and accept states. In mathematical language, a list of ﬁve elements is often called a 5-tuple. Hence we deﬁne a ﬁnite automaton to be a 5-tuple consisting of these ﬁve parts. We use something called a transition function, frequently denoted δ, to de- ﬁne the rules for moving. If the ﬁnite automaton has an arrow from a state x to a state y labeled with the input symbol 1, that means that if the automaton is in state x when it reads a 1, it then moves to state y. We can indicate the same thing with the transition function by saying that δ(x, 1) = y. This notation is a kind of mathematical shorthand. Putting it all together, we arrive at the formal deﬁnition of ﬁnite automata. 36 CHAPTER 1 / REGULAR LANGUAGES The formal deﬁnition precisely describes what we mean by a ﬁnite automa- ton. For example, returning to the earlier question of whether 0 accept states is allowable, you can see that setting F to be the empty set ∅ yields 0 accept states, which is allowable. Furthermore, the transition function δ speciﬁes exactly one next state for each possible combination of a state and an input symbol. That an- swers our other question afﬁrmatively, showing that exactly one transition arrow exits every state for each possible input symbol. We can use the notation of the formal deﬁnition to describe individual ﬁnite automata by specifying each of the ﬁve parts listed in Deﬁnition 1.5. For exam- ple, let’s return to the ﬁnite automaton M1 we discussed earlier, redrawn here for convenience. FIGURE 1.6 The ﬁnite automaton M1 We can describe M1 formally by writing M1 = (Q, Σ, δ, q1, F ), where 1. Q = {q1, q2, q3}, 2. Σ = {0,1}, 3. δ is described as 1.1 FINITE AUTOMATA 37 In our example, let A = {w| w contains at least one 1 and an even number of 0s follow the last 1}. Then L(M1) = A, or equivalently, M1 recognizes A. EXAMPLES OF FINITE AUTOMATA EXAMPLE 1.7 38 CHAPTER 1 / REGULAR LANGUAGES EXAMPLE 1.9 1.1 FINITE AUTOMATA 39 Machine M4 has two accept states, q1 and r1, and operates over the alphabet Σ = {a, b}. Some experimentation shows that it accepts strings a, b, aa, bb, and bab, but not strings ab, ba, or bbba. This machine begins in state s, and after it reads the ﬁrst symbol in the input, it goes either left into the q states or right into the r states. In both cases, it can never return to the start state (in contrast to the previous examples), as it has no way to get from any other state back to s. If the ﬁrst symbol in the input string is a, then it goes left and accepts when the string ends with an a. Similarly, if the ﬁrst symbol is a b, the machine goes right and accepts when the string ends in b. So M4 accepts all strings that start and end with a or that start and end with b. In other words, M4 accepts strings that start and end with the same symbol. 40 CHAPTER 1 / REGULAR LANGUAGES EXAMPLE 1.15 1.1 FINITE AUTOMATA 41 EXAMPLE 1.17 42 CHAPTER 1 / REGULAR LANGUAGES an input string of 0s and 1s symbol by symbol. Do you need to remember the entire string seen so far in order to determine whether the number of 1s is odd? Of course not. Simply remember whether the number of 1s seen so far is even or odd and keep track of this information as you read new symbols. If you read a 1, ﬂip the answer; but if you read a 0, leave the answer as is. But how does this help you design E1? Once you have determined the neces- sary information to remember about the string as it is being read, you represent this information as a ﬁnite list of possibilities. In this instance, the possibilities would be 1. even so far, and 2. odd so far. Then you assign a state to each of the possibilities. These are the states of E1, as shown here. FIGURE 1.18 The two states qeven and qodd Next, you assign the transitions by seeing how to go from one possibility to another upon reading a symbol. So, if state qeven represents the even possibility and state qodd represents the odd possibility, you would set the transitions to ﬂip state on a 1 and stay put on a 0, as shown here. FIGURE 1.19 Transitions telling how the possibilities rearrange Next, you set the start state to be the state corresponding to the possibility associated with having seen 0 symbols so far (the empty string ε). In this case, the start state corresponds to state qeven because 0 is an even number. Last, set the accept states to be those corresponding to possibilities where you want to accept the input string. Set qodd to be an accept state because you want to accept Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 1.1 FINITE AUTOMATA 43 when you have seen an odd number of 1s. These additions are shown in the following ﬁgure. FIGURE 1.20 Adding the start and accept states EXAMPLE 1.21 44 CHAPTER 1 / REGULAR LANGUAGES FIGURE 1.22 Accepts strings containing 001 1.1 FINITE AUTOMATA 45 “any number” includes 0 as a possibility, the empty string ε is always a member of A ∗, no matter what A is. EXAMPLE 1.24 46 CHAPTER 1 / REGULAR LANGUAGES Pretend that you are M . As the input symbols arrive one by one, you simulate both M1 and M2 simultaneously. That way, only one pass through the input is necessary. But can you keep track of both simulations with ﬁnite memory? All you need to remember is the state that each machine would be in if it had read up to this point in the input. Therefore, you need to remember a pair of states. How many possible pairs are there? If M1 has k1 states and M2 has k2 states, the number of pairs of states, one from M1 and the other from M2, is the product k1 × k2. This product will be the number of states in M , one for each pair. The transitions of M go from pair to pair, updating the current state for both M1 and M2. The accept states of M are those pairs wherein either M1 or M2 is in an accept state. PROOF Let M1 recognize A1, where M1 = (Q1, Σ, δ1, q1, F1), and M2 recognize A2, where M2 = (Q2, Σ, δ2, q2, F2). Construct M to recognize A1 ∪ A2, where M = (Q, Σ, δ, q0, F ). 1. Q = {(r1, r2)| r1 ∈ Q1 and r2 ∈ Q2}. This set is the Cartesian product of sets Q1 and Q2 and is written Q1 × Q2. It is the set of all pairs of states, the ﬁrst from Q1 and the second from Q2. 2. Σ, the alphabet, is the same as in M1 and M2. In this theorem and in all subsequent similar theorems, we assume for simplicity that both M1 and M2 have the same input alphabet Σ. The theorem remains true if they have different alphabets, Σ1 and Σ2. We would then modify the proof to let Σ = Σ1 ∪ Σ2. 3. δ, the transition function, is deﬁned as follows. For each (r1, r2) ∈ Q and each a ∈ Σ, let δ( (r1, r2), a) = ( δ1(r1, a), δ2(r2, a) ) . Hence δ gets a state of M (which actually is a pair of states from M1 and M2), together with an input symbol, and returns M ’s next state. 4. q0 is the pair (q1, q2). 5. F is the set of pairs in which either member is an accept state of M1 or M2. We can write it as F = {(r1, r2)| r1 ∈ F1 or r2 ∈ F2}. This expression is the same as F = (F1 × Q2) ∪ (Q1 × F2). (Note that it is not the same as F = F1 × F2. What would that give us instead?3) 1.2 NONDETERMINISM 47 This concludes the construction of the ﬁnite automaton M that recognizes the union of A1 and A2. This construction is fairly simple, and thus its correct- ness is evident from the strategy described in the proof idea. More complicated constructions require additional discussion to prove correctness. A formal cor- rectness proof for a construction of this type usually proceeds by induction. For an example of a construction proved correct, see the proof of Theorem 1.54. Most of the constructions that you will encounter in this course are fairly simple and so do not require a formal correctness proof. 48 CHAPTER 1 / REGULAR LANGUAGES FIGURE 1.27 The nondeterministic ﬁnite automaton N1 The difference between a deterministic ﬁnite automaton, abbreviated DFA, and a nondeterministic ﬁnite automaton, abbreviated NFA, is immediately ap- parent. First, every state of a DFA always has exactly one exiting transition arrow for each symbol in the alphabet. The NFA shown in Figure 1.27 violates that rule. State q1 has one exiting arrow for 0, but it has two for 1; q2 has one arrow for 0, but it has none for 1. In an NFA, a state may have zero, one, or many exiting arrows for each alphabet symbol. Second, in a DFA, labels on the transition arrows are symbols from the alpha- bet. This NFA has an arrow with the label ε. In general, an NFA may have arrows labeled with members of the alphabet or ε. Zero, one, or many arrows may exit from each state with the label ε. How does an NFA compute? Suppose that we are running an NFA on an input string and come to a state with multiple ways to proceed. For example, say that we are in state q1 in NFA N1 and that the next input symbol is a 1. After reading that symbol, the machine splits into multiple copies of itself and follows all the possibilities in parallel. Each copy of the machine takes one of the possible ways to proceed and continues as before. If there are subsequent choices, the machine splits again. If the next input symbol doesn’t appear on any of the arrows exiting the state occupied by a copy of the machine, that copy of the machine dies, along with the branch of the computation associated with it. Finally, if any one of these copies of the machine is in an accept state at the end of the input, the NFA accepts the input string. If a state with an ε symbol on an exiting arrow is encountered, something similar happens. Without reading any input, the machine splits into multiple copies, one following each of the exiting ε-labeled arrows and one staying at the current state. Then the machine proceeds nondeterministically as before. Nondeterminism may be viewed as a kind of parallel computation wherein multiple independent “processes” or “threads” can be running concurrently. When the NFA splits to follow several choices, that corresponds to a process “forking” into several children, each proceeding separately. If at least one of these processes accepts, then the entire computation accepts. Another way to think of a nondeterministic computation is as a tree of possi- bilities. The root of the tree corresponds to the start of the computation. Every branching point in the tree corresponds to a point in the computation at which the machine has multiple choices. The machine accepts if at least one of the computation branches ends in an accept state, as shown in Figure 1.28. Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 1.2 NONDETERMINISM 49 FIGURE 1.28 Deterministic and nondeterministic computations with an accepting branch Let’s consider some sample runs of the NFA N1 shown in Figure 1.27. The computation of N1 on input 010110 is depicted in the following ﬁgure. FIGURE 1.29 The computation of N1 on input 010110 Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 50 CHAPTER 1 / REGULAR LANGUAGES On input 010110, start in the start state q1 and read the ﬁrst symbol 0. From q1 there is only one place to go on a 0—namely, back to q1—so remain there. Next, read the second symbol 1. In q1 on a 1 there are two choices: either stay in q1 or move to q2. Nondeterministically, the machine splits in two to follow each choice. Keep track of the possibilities by placing a ﬁnger on each state where a machine could be. So you now have ﬁngers on states q1 and q2. An ε arrow exits state q2 so the machine splits again; keep one ﬁnger on q2, and move the other to q3. You now have ﬁngers on q1, q2, and q3. When the third symbol 0 is read, take each ﬁnger in turn. Keep the ﬁnger on q1 in place, move the ﬁnger on q2 to q3, and remove the ﬁnger that has been on q3. That last ﬁnger had no 0 arrow to follow and corresponds to a process that simply “dies.” At this point, you have ﬁngers on states q1 and q3. When the fourth symbol 1 is read, split the ﬁnger on q1 into ﬁngers on states q1 and q2, then further split the ﬁnger on q2 to follow the ε arrow to q3, and move the ﬁnger that was on q3 to q4. You now have a ﬁnger on each of the four states. When the ﬁfth symbol 1 is read, the ﬁngers on q1 and q3 result in ﬁngers on states q1, q2, q3, and q4, as you saw with the fourth symbol. The ﬁnger on state q2 is removed. The ﬁnger that was on q4 stays on q4. Now you have two ﬁngers on q4, so remove one because you only need to remember that q4 is a possible state at this point, not that it is possible for multiple reasons. When the sixth and ﬁnal symbol 0 is read, keep the ﬁnger on q1 in place, move the one on q2 to q3, remove the one that was on q3, and leave the one on q4 in place. You are now at the end of the string, and you accept if some ﬁnger is on an accept state. You have ﬁngers on states q1, q3, and q4; and as q4 is an accept state, N1 accepts this string. What does N1 do on input 010? Start with a ﬁnger on q1. After reading the 0, you still have a ﬁnger only on q1; but after the 1 there are ﬁngers on q1, q2, and q3 (don’t forget the ε arrow). After the third symbol 0, remove the ﬁnger on q3, move the ﬁnger on q2 to q3, and leave the ﬁnger on q1 where it is. At this point you are at the end of the input; and as no ﬁnger is on an accept state, N1 rejects this input. By continuing to experiment in this way, you will see that N1 accepts all strings that contain either 101 or 11 as a substring. Nondeterministic ﬁnite automata are useful in several respects. As we will show, every NFA can be converted into an equivalent DFA, and constructing NFAs is sometimes easier than directly constructing DFAs. An NFA may be much smaller than its deterministic counterpart, or its functioning may be easier to understand. Nondeterminism in ﬁnite automata is also a good introduction to nondeterminism in more powerful computational models because ﬁnite au- tomata are especially easy to understand. Now we turn to several examples of NFAs. Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 1.2 NONDETERMINISM 51 EXAMPLE 1.30 52 CHAPTER 1 / REGULAR LANGUAGES EXAMPLE 1.33 1.2 NONDETERMINISM 53 FIGURE 1.36 The NFA N4 54 CHAPTER 1 / REGULAR LANGUAGES EXAMPLE 1.38 1.2 NONDETERMINISM 55 THEOREM 1.39 56 CHAPTER 1 / REGULAR LANGUAGES Now we need to consider the ε arrows. To do so, we set up an extra bit of notation. For any state R of M , we deﬁne E(R) to be the collection of states that can be reached from members of R by going only along ε arrows, including the members of R themselves. Formally, for R ⊆ Q let E(R) = {q| q can be reached from R by traveling along 0 or more ε arrows}. Then we modify the transition function of M to place additional ﬁngers on all states that can be reached by going along ε arrows after every step. Replacing δ(r, a) by E(δ(r, a)) achieves this effect. Thus δ′(R, a) = {q ∈ Q| q ∈ E(δ(r, a)) for some r ∈ R}. Additionally, we need to modify the start state of M to move the ﬁngers ini- tially to all possible states that can be reached from the start state of N along the ε arrows. Changing q0′ to be E({q0}) achieves this effect. We have now completed the construction of the DFA M that simulates the NFA N . The construction of M obviously works correctly. At every step in the com- putation of M on an input, it clearly enters a state that corresponds to the subset of states that N could be in at that point. Thus our proof is complete. 1.2 NONDETERMINISM 57 To construct a DFA D that is equivalent to N4, we ﬁrst determine D’s states. N4 has three states, {1, 2, 3}, so we construct D with eight states, one for each subset of N4’s states. We label each of D’s states with the corresponding subset. Thus D’s state set is {∅, {1}, {2}, {3}, {1,2}, {1,3}, {2,3}, {1,2,3}} . FIGURE 1.42 The NFA N4 Next, we determine the start and accept states of D. The start state is E({1}), the set of states that are reachable from 1 by traveling along ε arrows, plus 1 itself. An ε arrow goes from 1 to 3, so E({1}) = {1, 3}. The new accept states are those containing N4’s accept state; thus { {1}, {1,2}, {1,3}, {1,2,3}} . Finally, we determine D’s transition function. Each of D’s states goes to one place on input a and one place on input b. We illustrate the process of deter- mining the placement of D’s transition arrows with a few examples. In D, state {2} goes to {2,3} on input a because in N4, state 2 goes to both 2 and 3 on input a and we can’t go farther from 2 or 3 along ε arrows. State {2} goes to state {3} on input b because in N4, state 2 goes only to state 3 on input b and we can’t go farther from 3 along ε arrows. State {1} goes to ∅ on a because no a arrows exit it. It goes to {2} on b. Note that the procedure in Theorem 1.39 speciﬁes that we follow the ε arrows after each input symbol is read. An alternative procedure based on following the ε arrows before reading each input symbol works equally well, but that method is not illustrated in this example. State {3} goes to {1,3} on a because in N4, state 3 goes to 1 on a and 1 in turn goes to 3 with an ε arrow. State {3} on b goes to ∅. State {1,2} on a goes to {2,3} because 1 points at no states with a arrows, 2 points at both 2 and 3 with a arrows, and neither points anywhere with ε ar- rows. State {1,2} on b goes to {2,3}. Continuing in this way, we obtain the diagram for D in Figure 1.43. Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 58 CHAPTER 1 / REGULAR LANGUAGES FIGURE 1.43 A DFA D that is equivalent to the NFA N4 We may simplify this machine by observing that no arrows point at states {1} and {1, 2}, so they may be removed without affecting the performance of the machine. Doing so yields the following ﬁgure. FIGURE 1.44 DFA D after removing unnecessary states 1.2 NONDETERMINISM 59 technique of nondeterminism. Reviewing the ﬁrst proof, appearing on page 45, may be worthwhile to see how much easier and more intuitive the new proof is. THEOREM 1.45 60 CHAPTER 1 / REGULAR LANGUAGES PROOF Let N1 = (Q1, Σ, δ1, q1, F1) recognize A1, and N2 = (Q2, Σ, δ2, q2, F2) recognize A2. Construct N = (Q, Σ, δ, q0, F ) to recognize A1 ∪ A2. 1. Q = {q0} ∪ Q1 ∪ Q2. The states of N are all the states of N1 and N2, with the addition of a new start state q0. 2. The state q0 is the start state of N . 3. The set of accept states F = F1 ∪ F2. The accept states of N are all the accept states of N1 and N2. That way, N accepts if either N1 accepts or N2 accepts. 4. Deﬁne δ so that for any q ∈ Q and any a ∈ Σε, δ(q, a) =  ||| ||| δ1(q, a) q ∈ Q1 δ2(q, a) q ∈ Q2 {q1, q2} q = q0 and a = ε ∅ q = q0 and a ̸= ε. 1.2 NONDETERMINISM 61 FIGURE 1.48 Construction of N to recognize A1 ◦ A2 PROOF Let N1 = (Q1, Σ, δ1, q1, F1) recognize A1, and N2 = (Q2, Σ, δ2, q2, F2) recognize A2. Construct N = (Q, Σ, δ, q1, F2) to recognize A1 ◦ A2. 1. Q = Q1 ∪ Q2. The states of N are all the states of N1 and N2. 2. The state q1 is the same as the start state of N1. 3. The accept states F2 are the same as the accept states of N2. 4. Deﬁne δ so that for any q ∈ Q and any a ∈ Σε, δ(q, a) =  ||| ||| δ1(q, a) q ∈ Q1 and q ̸∈ F1 δ1(q, a) q ∈ F1 and a ̸= ε δ1(q, a) ∪ {q2} q ∈ F1 and a = ε δ2(q, a) q ∈ Q2. 62 CHAPTER 1 / REGULAR LANGUAGES THEOREM 1.49 1.3 REGULAR EXPRESSIONS 63 4. Deﬁne δ so that for any q ∈ Q and any a ∈ Σε, δ(q, a) =  |||||| |||||| δ1(q, a) q ∈ Q1 and q ̸∈ F1 δ1(q, a) q ∈ F1 and a ̸= ε δ1(q, a) ∪ {q1} q ∈ F1 and a = ε {q1} q = q0 and a = ε ∅ q = q0 and a ̸= ε. 64 CHAPTER 1 / REGULAR LANGUAGES EXAMPLE 1.51 1.3 REGULAR EXPRESSIONS 65 Seemingly, we are in danger of deﬁning the notion of a regular expression in terms of itself. If true, we would have a circular deﬁnition, which would be invalid. However, R1 and R2 always are smaller than R. Thus we actually are deﬁning regular expressions in terms of smaller regular expressions and thereby avoiding circularity. A deﬁnition of this type is called an inductive deﬁnition. Parentheses in an expression may be omitted. If they are, evaluation is done in the precedence order: star, then concatenation, then union. For convenience, we let R+ be shorthand for RR∗. In other words, whereas R∗ has all strings that are 0 or more concatenations of strings from R, the lan- guage R+ has all strings that are 1 or more concatenations of strings from R. So R+ ∪ ε = R∗. In addition, we let Rk be shorthand for the concatenation of k R’s with each other. When we want to distinguish between a regular expression R and the lan- guage that it describes, we write L(R) to be the language of R. EXAMPLE 1.53 66 CHAPTER 1 / REGULAR LANGUAGES If we let R be any regular expression, we have the following identities. They are good tests of whether you understand the deﬁnition. R ∪ ∅ = R. Adding the empty language to any other language will not change it. R ◦ ε = R. Joining the empty string to any string will not change it. However, exchanging ∅ and ε in the preceding identities may cause the equalities to fail. R ∪ ε may not equal R. For example, if R = 0, then L(R) = {0} but L(R ∪ ε) = {0, ε}. R ◦ ∅ may not equal R. For example, if R = 0, then L(R) = {0} but L(R ◦ ∅) = ∅. Regular expressions are useful tools in the design of compilers for program- ming languages. Elemental objects in a programming language, called tokens, such as the variable names and constants, may be described with regular ex- pressions. For example, a numerical constant that may include a fractional part and/or a sign may be described as a member of the language ( + ∪ - ∪ ε) (D+ ∪ D+.D∗ ∪ D∗.D+) where D = {0, 1, 2, 3, 4, 5, 6, 7, 8, 9} is the alphabet of decimal digits. Examples of generated strings are: 72, 3.14159, +7., and -.01. Once the syntax of a programming language has been described with a regular expression in terms of its tokens, automatic systems can generate the lexical analyzer, the part of a compiler that initially processes the input program. EQUIVALENCE WITH FINITE AUTOMATA Regular expressions and ﬁnite automata are equivalent in their descriptive power. This fact is surprising because ﬁnite automata and regular expressions superﬁcially appear to be rather different. However, any regular expression can be converted into a ﬁnite automaton that recognizes the language it describes, and vice versa. Recall that a regular language is one that is recognized by some ﬁnite automaton. THEOREM 1.54 1.3 REGULAR EXPRESSIONS 67 LEMMA 1.55 68 CHAPTER 1 / REGULAR LANGUAGES That ends the ﬁrst part of the proof of Theorem 1.54, giving the easier di- rection of the if and only if condition. Before going on to the other direction, let’s consider some examples whereby we use this procedure to convert a regular expression to an NFA. EXAMPLE 1.56 1.3 REGULAR EXPRESSIONS 69 EXAMPLE 1.58 70 CHAPTER 1 / REGULAR LANGUAGES We break this procedure into two parts, using a new type of ﬁnite automaton called a generalized nondeterministic ﬁnite automaton, GNFA. First we show how to convert DFAs into GNFAs, and then GNFAs into regular expressions. Generalized nondeterministic ﬁnite automata are simply nondeterministic ﬁ- nite automata wherein the transition arrows may have any regular expressions as labels, instead of only members of the alphabet or ε. The GNFA reads blocks of symbols from the input, not necessarily just one symbol at a time as in an ordi- nary NFA. The GNFA moves along a transition arrow connecting two states by reading a block of symbols from the input, which themselves constitute a string described by the regular expression on that arrow. A GNFA is nondeterministic and so may have several different ways to process the same input string. It ac- cepts its input if its processing can cause the GNFA to be in an accept state at the end of the input. The following ﬁgure presents an example of a GNFA. FIGURE 1.61 A generalized nondeterministic ﬁnite automaton For convenience, we require that GNFAs always have a special form that meets the following conditions. • The start state has transition arrows going to every other state but no arrows coming in from any other state. • There is only a single accept state, and it has arrows coming in from every other state but no arrows going to any other state. Furthermore, the accept state is not the same as the start state. • Except for the start and accept states, one arrow goes from every state to every other state and also from each state to itself. Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 1.3 REGULAR EXPRESSIONS 71 We can easily convert a DFA into a GNFA in the special form. We simply add a new start state with an ε arrow to the old start state and a new accept state with ε arrows from the old accept states. If any arrows have multiple labels (or if there are multiple arrows going between the same two states in the same direction), we replace each with a single arrow whose label is the union of the previous labels. Finally, we add arrows labeled ∅ between states that had no arrows. This last step won’t change the language recognized because a transition labeled with ∅ can never be used. From here on we assume that all GNFAs are in the special form. Now we show how to convert a GNFA into a regular expression. Say that the GNFA has k states. Then, because a GNFA must have a start and an accept state and they must be different from each other, we know that k ≥ 2. If k > 2, we construct an equivalent GNFA with k − 1 states. This step can be repeated on the new GNFA until it is reduced to two states. If k = 2, the GNFA has a single arrow that goes from the start state to the accept state. The label of this arrow is the equivalent regular expression. For example, the stages in converting a DFA with three states to an equivalent regular expression are shown in the following ﬁgure. FIGURE 1.62 Typical stages in converting a DFA to a regular expression The crucial step is constructing an equivalent GNFA with one fewer state when k > 2. We do so by selecting a state, ripping it out of the machine, and repairing the remainder so that the same language is still recognized. Any state will do, provided that it is not the start or accept state. We are guaranteed that such a state will exist because k > 2. Let’s call the removed state qrip. After removing qrip we repair the machine by altering the regular expressions that label each of the remaining arrows. The new labels compensate for the absence of qrip by adding back the lost computations. The new label going from a state qi to a state qj is a regular expression that describes all strings that would Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 72 CHAPTER 1 / REGULAR LANGUAGES take the machine from qi to qj either directly or via qrip. We illustrate this approach in Figure 1.63. FIGURE 1.63 Constructing an equivalent GNFA with one fewer state In the old machine, if 1. qi goes to qrip with an arrow labeled R1, 2. qrip goes to itself with an arrow labeled R2, 3. qrip goes to qj with an arrow labeled R3, and 4. qi goes to qj with an arrow labeled R4, then in the new machine, the arrow from qi to qj gets the label (R1)(R2) ∗(R3) ∪ (R4). We make this change for each arrow going from any state qi to any state qj, including the case where qi = qj. The new machine recognizes the original language. PROOF Let’s now carry out this idea formally. First, to facilitate the proof, we formally deﬁne the new type of automaton introduced. A GNFA is similar to a nondeterministic ﬁnite automaton except for the transition function, which has the form δ : ( Q − {qaccept}) × ( Q − {qstart}) −→R. The symbol R is the collection of all regular expressions over the alphabet Σ, and qstart and qaccept are the start and accept states. If δ(qi, qj) = R, the arrow from state qi to state qj has the regular expression R as its label. The domain of the transition function is ( Q − {qaccept}) × ( Q − {qstart}) because an arrow connects every state to every other state, except that no arrows are coming from qaccept or going to qstart. Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 1.3 REGULAR EXPRESSIONS 7374 CHAPTER 1 / REGULAR LANGUAGES Next we prove that CONVERT returns a correct value. CLAIM 1.65 1.3 REGULAR EXPRESSIONS 75 labeled ∅, even though they are present. Note that we replace the label a, b on the self-loop at state 2 on the DFA with the label a ∪ b at the corresponding point on the GNFA. We do so because the DFA’s label represents two transitions, one for a and the other for b, whereas the GNFA may have only a single transition going from 2 to itself. In Figure 1.67(c), we remove state 2 and update the remaining arrow labels. In this case, the only label that changes is the one from 1 to a. In part (b) it was ∅, but in part (c) it is b(a ∪ b) ∗. We obtain this result by following step 3 of the CONVERT procedure. State qi is state 1, state qj is a, and qrip is 2, so R1 = b, R2 = a ∪ b, R3 = ε, and R4 = ∅. Therefore, the new label on the arrow from 1 to a is (b)(a ∪ b) ∗(ε) ∪ ∅. We simplify this regular expression to b(a ∪ b) ∗. In Figure 1.67(d), we remove state 1 from part (c) and follow the same pro- cedure. Because only the start and accept states remain, the label on the arrow joining them is the regular expression that is equivalent to the original DFA. FIGURE 1.67 Converting a two-state DFA to an equivalent regular expression 76 CHAPTER 1 / REGULAR LANGUAGES EXAMPLE 1.68 1.4 NONREGULAR LANGUAGES 7778 CHAPTER 1 / REGULAR LANGUAGES THEOREM 1.70 1.4 NONREGULAR LANGUAGES 79 piece z is the remaining part of s, coming after the second occurrence of q9. So x takes M from the state q1 to q9, y takes M from q9 back to q9, and z takes M from q9 to the accept state q13, as shown in the following ﬁgure. FIGURE 1.72 Example showing how the strings x, y, and z affect M Let’s see why this division of s satisﬁes the three conditions. Suppose that we run M on input xyyz. We know that x takes M from q1 to q9, and then the ﬁrst y takes it from q9 back to q9, as does the second y, and then z takes it to q13. With q13 being an accept state, M accepts input xyyz. Similarly, it will accept xyiz for any i > 0. For the case i = 0, xyiz = xz, which is accepted for similar reasons. That establishes condition 1. Checking condition 2, we see that |y| > 0, as it was the part of s that occurred between two different occurrences of state q9. In order to get condition 3, we make sure that q9 is the ﬁrst repetition in the sequence. By the pigeonhole principle, the ﬁrst p + 1 states in the sequence must contain a repetition. Therefore, |xy| ≤ p. PROOF Let M = (Q, Σ, δ, q1, F ) be a DFA recognizing A and p be the number of states of M . Let s = s1s2 · · · sn be a string in A of length n, where n ≥ p. Let r1, . . . , rn+1 be the sequence of states that M enters while processing s, so ri+1 = δ(ri, si) for 1 ≤ i ≤ n. This sequence has length n + 1, which is at least p + 1. Among the ﬁrst p + 1 elements in the sequence, two must be the same state, by the pigeonhole principle. We call the ﬁrst of these rj and the second rl. Because rl occurs among the ﬁrst p + 1 places in a sequence starting at r1, we have l ≤ p + 1. Now let x = s1 · · · sj−1, y = sj · · · sl−1, and z = sl · · · sn. As x takes M from r1 to rj , y takes M from rj to rj, and z takes M from rj to rn+1, which is an accept state, M must accept xyiz for i ≥ 0. We know that j ̸= l, so |y| > 0; and l ≤ p + 1, so |xy| ≤ p. Thus we have satisﬁed all conditions of the pumping lemma. 80 CHAPTER 1 / REGULAR LANGUAGES To use the pumping lemma to prove that a language B is not regular, ﬁrst as- sume that B is regular in order to obtain a contradiction. Then use the pumping lemma to guarantee the existence of a pumping length p such that all strings of length p or greater in B can be pumped. Next, ﬁnd a string s in B that has length p or greater but that cannot be pumped. Finally, demonstrate that s cannot be pumped by considering all ways of dividing s into x, y, and z (taking condition 3 of the pumping lemma into account if convenient) and, for each such division, ﬁnding a value i where xyiz ̸∈ B. This ﬁnal step often involves grouping the various ways of dividing s into several cases and analyzing them individually. The existence of s contradicts the pumping lemma if B were regular. Hence B cannot be regular. Finding s sometimes takes a bit of creative thinking. You may need to hunt through several candidates for s before you discover one that works. Try mem- bers of B that seem to exhibit the “essence” of B’s nonregularity. We further discuss the task of ﬁnding s in some of the following examples. EXAMPLE 1.73 1.4 NONREGULAR LANGUAGES 81 Assume to the contrary that C is regular. Let p be the pumping length given by the pumping lemma. As in Example 1.73, let s be the string 0 p1 p. With s being a member of C and having length more than p, the pumping lemma guarantees that s can be split into three pieces, s = xyz, where for any i ≥ 0 the string xyiz is in C. We would like to show that this outcome is impossible. But wait, it is possible! If we let x and z be the empty string and y be the string 0 p1 p, then xyiz always has an equal number of 0s and 1s and hence is in C. So it seems that s can be pumped. Here condition 3 in the pumping lemma is useful. It stipulates that when pumping s, it must be divided so that |xy| ≤ p. That restriction on the way that s may be divided makes it easier to show that the string s = 0p1p we selected cannot be pumped. If |xy| ≤ p, then y must consist only of 0s, so xyyz ̸∈ C. Therefore, s cannot be pumped. That gives us the desired contradiction. Selecting the string s in this example required more care than in Exam- ple 1.73. If we had chosen s = (01) p instead, we would have run into trouble because we need a string that cannot be pumped and that string can be pumped, even taking condition 3 into account. Can you see how to pump it? One way to do so sets x = ε, y = 01, and z = (01) p−1. Then xyiz ∈ C for every value of i. If you fail on your ﬁrst attempt to ﬁnd a string that cannot be pumped, don’t despair. Try another one! An alternative method of proving that C is nonregular follows from our knowledge that B is nonregular. If C were regular, C ∩ 0∗1∗ also would be regular. The reasons are that the language 0∗1∗ is regular and that the class of regular languages is closed under intersection, which we proved in footnote 3 (page 46). But C ∩ 0∗1∗ equals B, and we know that B is nonregular from Example 1.73. 82 CHAPTER 1 / REGULAR LANGUAGES EXAMPLE 1.76 EXERCISES 83 EXERCISES A1.1 The following are the state diagrams of two DFAs, M1 and M2. Answer the follow- ing questions about each of these machines. a. What is the start state? b. What is the set of accept states? c. What sequence of states does the machine go through on input aabb? d. Does the machine accept the string aabb? e. Does the machine accept the string ε? A1.2 Give the formal description of the machines M1 and M2 pictured in Exercise 1.1. 1.3 The formal description of a DFA M is ({q1, q2, q3, q4, q5}, {u, d}, δ, q3, {q3} ), where δ is given by the following table. Give the state diagram of this machine. 84 CHAPTER 1 / REGULAR LANGUAGES 1.5 Each of the following languages is the complement of a simpler language. In each part, construct a DFA for the simpler language, then use it to give the state diagram of a DFA for the language given. In all parts, Σ = {a, b}. Aa. {w| w does not contain the substring ab} Ab. {w| w does not contain the substring baba} c. {w| w contains neither the substrings ab nor ba} d. {w| w is any string not in a ∗b∗} e. {w| w is any string not in (ab +)∗} f. {w| w is any string not in a ∗ ∪ b ∗} g. {w| w is any string that doesn’t contain exactly two a’s} h. {w| w is any string except a and b} 1.6 Give state diagrams of DFAs recognizing the following languages. In all parts, the alphabet is {0,1}. a. {w| w begins with a 1 and ends with a 0} b. {w| w contains at least three 1s} c. {w| w contains the substring 0101 (i.e., w = x0101y for some x and y)} d. {w| w has length at least 3 and its third symbol is a 0} e. {w| w starts with 0 and has odd length, or starts with 1 and has even length} f. {w| w doesn’t contain the substring 110} g. {w| the length of w is at most 5} h. {w| w is any string except 11 and 111} i. {w| every odd position of w is a 1} j. {w| w contains at least two 0s and at most one 1} k. {ε, 0} l. {w| w contains an even number of 0s, or contains exactly two 1s} m. The empty set n. All strings except the empty string 1.7 Give state diagrams of NFAs with the speciﬁed number of states recognizing each of the following languages. In all parts, the alphabet is {0,1}. Aa. The language {w| w ends with 00} with three states b. The language of Exercise 1.6c with ﬁve states c. The language of Exercise 1.6l with six states d. The language {0} with two states e. The language 0∗1 ∗0 + with three states Af. The language 1∗(001 +)∗ with three states g. The language {ε} with one state h. The language 0∗ with one state 1.8 Use the construction in the proof of Theorem 1.45 to give the state diagrams of NFAs recognizing the union of the languages described in a. Exercises 1.6a and 1.6b. b. Exercises 1.6c and 1.6f. Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. EXERCISES 85 1.9 Use the construction in the proof of Theorem 1.47 to give the state diagrams of NFAs recognizing the concatenation of the languages described in a. Exercises 1.6g and 1.6i. b. Exercises 1.6b and 1.6m. 1.10 Use the construction in the proof of Theorem 1.49 to give the state diagrams of NFAs recognizing the star of the languages described in a. Exercise 1.6b. b. Exercise 1.6j. c. Exercise 1.6m. A1.11 Prove that every NFA can be converted to an equivalent one that has a single accept state. 1.12 Let D = {w| w contains an even number of a’s and an odd number of b’s and does not contain the substring ab}. Give a DFA with ﬁve states that recognizes D and a regular expression that generates D. (Suggestion: Describe D more simply.) 1.13 Let F be the language of all strings over {0,1} that do not contain a pair of 1s that are separated by an odd number of symbols. Give the state diagram of a DFA with ﬁve states that recognizes F . (You may ﬁnd it helpful ﬁrst to ﬁnd a 4-state NFA for the complement of F .) 1.14 a. Show that if M is a DFA that recognizes language B, swapping the accept and nonaccept states in M yields a new DFA recognizing the complement of B. Conclude that the class of regular languages is closed under complement. b. Show by giving an example that if M is an NFA that recognizes language C, swapping the accept and nonaccept states in M doesn’t necessarily yield a new NFA that recognizes the complement of C. Is the class of languages recognized by NFAs closed under complement? Explain your answer. 1.15 Give a counterexample to show that the following construction fails to prove The- orem 1.49, the closure of the class of regular languages under the star operation.7 Let N1 = (Q1, Σ, δ1, q1, F1) recognize A1. Construct N = (Q1, Σ, δ, q1, F ) as follows. N is supposed to recognize A∗ 1. a. The states of N are the states of N1. b. The start state of N is the same as the start state of N1. c. F = {q1} ∪ F1. The accept states F are the old accept states plus its start state. d. Deﬁne δ so that for any q ∈ Q1 and any a ∈ Σε, δ(q, a) = { δ1(q, a) q ̸∈ F1 or a ̸= ε δ1(q, a) ∪ {q1} q ∈ F1 and a = ε. (Suggestion: Show this construction graphically, as in Figure 1.50.) 86 CHAPTER 1 / REGULAR LANGUAGES 1.16 Use the construction given in Theorem 1.39 to convert the following two nonde- terministic ﬁnite automata to equivalent deterministic ﬁnite automata. 1.17 a. Give an NFA recognizing the language (01 ∪ 001 ∪ 010)∗. b. Convert this NFA to an equivalent DFA. Give only the portion of the DFA that is reachable from the start state. 1.18 Give regular expressions generating the languages of Exercise 1.6. 1.19 Use the procedure described in Lemma 1.55 to convert the following regular ex- pressions to nondeterministic ﬁnite automata. a. (0 ∪ 1)∗000(0 ∪ 1)∗ b. (((00)∗(11)) ∪ 01)∗ c. ∅ ∗ 1.20 For each of the following languages, give two strings that are members and two strings that are not members—a total of four strings for each part. Assume the alphabet Σ = {a,b} in all parts. a. a∗b ∗ b. a(ba)∗b c. a∗ ∪ b ∗ d. (aaa)∗ e. Σ ∗aΣ ∗bΣ ∗aΣ ∗ f. aba ∪ bab g. (ε ∪ a)b h. (a ∪ ba ∪ bb)Σ ∗ 1.21 Use the procedure described in Lemma 1.60 to convert the following ﬁnite au- tomata to regular expressions. Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. EXERCISES 87 1.22 In certain programming languages, comments appear between delimiters such as /# and #/. Let C be the language of all valid delimited comment strings. A mem- ber of C must begin with /# and end with #/ but have no intervening #/. For simplicity, assume that the alphabet for C is Σ = {a, b, /, #}. a. Give a DFA that recognizes C. b. Give a regular expression that generates C. A1.23 Let B be any language over the alphabet Σ. Prove that B = B+ iff BB ⊆ B. 1.24 A ﬁnite state transducer (FST) is a type of deterministic ﬁnite automaton whose output is a string and not just accept or reject . The following are state diagrams of ﬁnite state transducers T1 and T2. Each transition of an FST is labeled with two symbols, one designating the input symbol for that transition and the other designating the output symbol. The two symbols are written with a slash, /, separating them. In T1, the transition from q1 to q2 has input symbol 2 and output symbol 1. Some transitions may have multiple input–output pairs, such as the transition in T1 from q1 to itself. When an FST computes on an input string w, it takes the input symbols w1 · · · wn one by one and, starting at the start state, follows the transitions by matching the input labels with the sequence of symbols w1 · · · wn = w. Every time it goes along a transition, it outputs the corresponding output symbol. For example, on input 2212011, machine T1 enters the sequence of states q1, q2, q2, q2, q2, q1, q1, q1 and produces output 1111000. On input abbb, T2 outputs 1011. Give the sequence of states entered and the output produced in each of the following parts. a. T1 on input 011 b. T1 on input 211 c. T1 on input 121 d. T1 on input 0202 e. T2 on input b f. T2 on input bbab g. T2 on input bbbbbb h. T2 on input ε 1.25 Read the informal deﬁnition of the ﬁnite state transducer given in Exercise 1.24. Give a formal deﬁnition of this model, following the pattern in Deﬁnition 1.5 (page 35). Assume that an FST has an input alphabet Σ and an output alphabet Γ but not a set of accept states. Include a formal deﬁnition of the computation of an FST. (Hint: An FST is a 5-tuple. Its transition function is of the form δ : Q×Σ−→Q×Γ.) 1.26 Using the solution you gave to Exercise 1.25, give a formal description of the ma- chines T1 and T2 depicted in Exercise 1.24. Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 88 CHAPTER 1 / REGULAR LANGUAGES 1.27 Read the informal deﬁnition of the ﬁnite state transducer given in Exercise 1.24. Give the state diagram of an FST with the following behavior. Its input and output alphabets are {0,1}. Its output string is identical to the input string on the even positions but inverted on the odd positions. For example, on input 0000111 it should output 1010010. 1.28 Convert the following regular expressions to NFAs using the procedure given in Theorem 1.54. In all parts, Σ = {a, b}. a. a(abb)∗ ∪ b b. a+ ∪ (ab)+ c. (a ∪ b +)a +b + 1.29 Use the pumping lemma to show that the following languages are not regular. Aa. A1 = {0n1 n2 n| n ≥ 0} b. A2 = {www| w ∈ {a, b} ∗} Ac. A3 = {a2 n | n ≥ 0} (Here, a 2 n means a string of 2 n a’s.) 1.30 Describe the error in the following “proof” that 0∗1 ∗ is not a regular language. (An error must exist because 0 ∗1 ∗ is regular.) The proof is by contradiction. Assume that 0 ∗1 ∗ is regular. Let p be the pumping length for 0 ∗1 ∗ given by the pumping lemma. Choose s to be the string 0 p1 p. You know that s is a member of 0∗1 ∗, but Example 1.73 shows that s cannot be pumped. Thus you have a contradiction. So 0 ∗1∗ is not regular. PROBLEMS 89 1.33 Let Σ2 = {[ 0 0 ], [ 0 1 ], [ 1 0 ], [ 1 1 ]} . Here, Σ2 contains all columns of 0s and 1s of height two. A string of symbols in Σ2 gives two rows of 0s and 1s. Consider each row to be a binary number and let C = {w ∈ Σ ∗ 2| the bottom row of w is three times the top row}. For example, [ 0 0 ][ 0 1 ][ 1 1 ][ 0 0 ] ∈ C, but [ 0 1 ][ 0 1 ][ 1 0 ] ̸∈ C. Show that C is regular. (You may assume the result claimed in Problem 1.31.) 1.34 Let Σ2 be the same as in Problem 1.33. Consider each row to be a binary number and let D = {w ∈ Σ ∗ 2| the top row of w is a larger number than is the bottom row}. For example, [ 0 0 ][ 1 0 ][ 1 1 ][ 0 0 ] ∈ D, but [ 0 0 ][ 0 1 ][ 1 1 ][ 0 0 ] ̸∈ D. Show that D is regular. 1.35 Let Σ2 be the same as in Problem 1.33. Consider the top and bottom rows to be strings of 0s and 1s, and let E = {w ∈ Σ ∗ 2| the bottom row of w is the reverse of the top row of w}. Show that E is not regular. 1.36 Let Bn = {a k| k is a multiple of n}. Show that for each n ≥ 1, the language Bn is regular. 1.37 Let Cn = {x| x is a binary number that is a multiple of n}. Show that for each n ≥ 1, the language Cn is regular. 1.38 An all-NFA M is a 5-tuple (Q, Σ, δ, q0, F ) that accepts x ∈ Σ ∗ if every possible state that M could be in after reading input x is a state from F . Note, in contrast, that an ordinary NFA accepts a string if some state among these possible states is an accept state. Prove that all-NFAs recognize the class of regular languages. 1.39 The construction in Theorem 1.54 shows that every GNFA is equivalent to a GNFA with only two states. We can show that an opposite phenomenon occurs for DFAs. Prove that for every k > 1, a language Ak ⊆ {0,1} ∗ exists that is recognized by a DFA with k states but not by one with only k − 1 states. 1.40 Recall that string x is a preﬁx of string y if a string z exists where xz = y, and that x is a proper preﬁx of y if in addition x ̸= y. In each of the following parts, we deﬁne an operation on a language A. Show that the class of regular languages is closed under that operation. Aa. NOPREFIX (A) = {w ∈ A| no proper preﬁx of w is a member of A}. b. NOEXTEND(A) = {w ∈ A| w is not the proper preﬁx of any string in A}. 1.41 For languages A and B, let the perfect shufﬂe of A and B be the language {w| w = a1b1 · · · akbk, where a1 · · · ak ∈ A and b1 · · · bk ∈ B, each ai, bi ∈ Σ}. Show that the class of regular languages is closed under perfect shufﬂe. 1.42 For languages A and B, let the shufﬂe of A and B be the language {w| w = a1b1 · · · akbk, where a1 · · · ak ∈ A and b1 · · · bk ∈ B, each ai, bi ∈ Σ ∗}. Show that the class of regular languages is closed under shufﬂe. Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 90 CHAPTER 1 / REGULAR LANGUAGES 1.43 Let A be any language. Deﬁne DROP-OUT (A) to be the language containing all strings that can be obtained by removing one symbol from a string in A. Thus, DROP-OUT (A) = {xz| xyz ∈ A where x, z ∈ Σ ∗, y ∈ Σ}. Show that the class of regular languages is closed under the DROP-OUT operation. Give both a proof by picture and a more formal proof by construction as in Theorem 1.47. A1.44 Let B and C be languages over Σ = {0, 1}. Deﬁne B 1 ← C = {w ∈ B| for some y ∈ C, strings w and y contain equal numbers of 1s}. Show that the class of regular languages is closed under the 1 ← operation. ⋆1.45 Let A/B = {w| wx ∈ A for some x ∈ B}. Show that if A is regular and B is any language, then A/B is regular. 1.46 Prove that the following languages are not regular. You may use the pumping lemma and the closure of the class of regular languages under union, intersection, and complement. a. {0n1 m0n| m, n ≥ 0} Ab. {0m1 n| m ̸= n} c. {w| w ∈ {0,1} ∗ is not a palindrome}8 ⋆d. {wtw| w, t ∈ {0,1} +} 1.47 Let Σ = {1, #} and let Y = {w| w = x1#x2# · · · #xk for k ≥ 0, each xi ∈ 1 ∗, and xi ̸= xj for i ̸= j}. Prove that Y is not regular. 1.48 Let Σ = {0,1} and let D = {w| w contains an equal number of occurrences of the substrings 01 and 10}. Thus 101 ∈ D because 101 contains a single 01 and a single 10, but 1010 ̸∈ D because 1010 contains two 10s and one 01. Show that D is a regular language. 1.49 a. Let B = {1 ky| y ∈ {0, 1} ∗ and y contains at least k 1s, for k ≥ 1}. Show that B is a regular language. b. Let C = {1ky| y ∈ {0, 1} ∗ and y contains at most k 1s, for k ≥ 1}. Show that C isn’t a regular language. A1.50 Read the informal deﬁnition of the ﬁnite state transducer given in Exercise 1.24. Prove that no FST can output wR for every input w if the input and output alpha- bets are {0,1}. 1.51 Let x and y be strings and let L be any language. We say that x and y are distin- guishable by L if some string z exists whereby exactly one of the strings xz and yz is a member of L; otherwise, for every string z, we have xz ∈ L whenever yz ∈ L and we say that x and y are indistinguishable by L. If x and y are indistinguishable by L, we write x ≡L y. Show that ≡L is an equivalence relation. PROBLEMS 91 A⋆1.52 Myhill–Nerode theorem. Refer to Problem 1.51. Let L be a language and let X be a set of strings. Say that X is pairwise distinguishable by L if every two distinct strings in X are distinguishable by L. Deﬁne the index of L to be the maximum number of elements in any set that is pairwise distinguishable by L. The index of L may be ﬁnite or inﬁnite. a. Show that if L is recognized by a DFA with k states, L has index at most k. b. Show that if the index of L is a ﬁnite number k, it is recognized by a DFA with k states. c. Conclude that L is regular iff it has ﬁnite index. Moreover, its index is the size of the smallest DFA recognizing it. 1.53 Let Σ = {0, 1, +, =} and ADD = {x=y+z| x, y, z are binary integers, and x is the sum of y and z}. Show that ADD is not regular. 1.54 Consider the language F = {aibjc k| i, j, k ≥ 0 and if i = 1 then j = k}. a. Show that F is not regular. b. Show that F acts like a regular language in the pumping lemma. In other words, give a pumping length p and demonstrate that F satisﬁes the three conditions of the pumping lemma for this value of p. c. Explain why parts (a) and (b) do not contradict the pumping lemma. 1.55 The pumping lemma says that every regular language has a pumping length p, such that every string in the language can be pumped if it has length p or more. If p is a pumping length for language A, so is any length p′ ≥ p. The minimum pumping length for A is the smallest p that is a pumping length for A. For example, if A = 01 ∗, the minimum pumping length is 2. The reason is that the string s = 0 is in A and has length 1 yet s cannot be pumped; but any string in A of length 2 or more contains a 1 and hence can be pumped by dividing it so that x = 0, y = 1, and z is the rest. For each of the following languages, give the minimum pumping length and justify your answer. Aa. 0001∗ Ab. 0 ∗1 ∗ c. 001 ∪ 0 ∗1 ∗ Ad. 0 ∗1 +0 +1 ∗ ∪ 10∗1 e. (01)∗ f. ε g. 1 ∗01 ∗01∗ h. 10(11 ∗0)∗0 i. 1011 j. Σ ∗ ⋆1.56 If A is a set of natural numbers and k is a natural number greater than 1, let Bk(A) = {w| w is the representation in base k of some number in A}. Here, we do not allow leading 0s in the representation of a number. For example, B2({3, 5}) = {11, 101} and B3({3, 5}) = {10, 12}. Give an example of a set A for which B2(A) is regular but B3(A) is not regular. Prove that your example works. Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 92 CHAPTER 1 / REGULAR LANGUAGES ⋆1.57 If A is any language, let A 1 PROBLEMS 93 ⋆1.65 Prove that for each n > 0, a language Bn exists where a. Bn is recognizable by an NFA that has n states, and b. if Bn = A1 ∪ · · · ∪ Ak, for regular languages Ai, then at least one of the Ai requires a DFA with exponentially many states. 1.66 A homomorphism is a function f : Σ−→Γ∗ from one alphabet to strings over another alphabet. We can extend f to operate on strings by deﬁning f (w) = f (w1)f (w2) · · · f (wn), where w = w1w2 · · · wn and each wi ∈ Σ. We further extend f to operate on languages by deﬁning f (A) = {f (w)| w ∈ A}, for any language A. a. Show, by giving a formal construction, that the class of regular languages is closed under homomorphism. In other words, given a DFA M that rec- ognizes B and a homomorphism f , construct a ﬁnite automaton M ′ that recognizes f (B). Consider the machine M ′ that you constructed. Is it a DFA in every case? b. Show, by giving an example, that the class of non-regular languages is not closed under homomorphism. ⋆1.67 Let the rotational closure of language A be RC(A) = {yx| xy ∈ A}. a. Show that for any language A, we have RC(A) = RC(RC(A)). b. Show that the class of regular languages is closed under rotational closure. ⋆1.68 In the traditional method for cutting a deck of playing cards, the deck is arbitrarily split two parts, which are exchanged before reassembling the deck. In a more complex cut, called Scarne’s cut, the deck is broken into three parts and the middle part in placed ﬁrst in the reassembly. We’ll take Scarne’s cut as the inspiration for an operation on languages. For a language A, let CUT(A) = {yxz| xyz ∈ A}. a. Exhibit a language B for which CUT(B) ̸= CUT(CUT(B)). b. Show that the class of regular languages is closed under CUT. 1.69 Let Σ = {0,1}. Let WWk = {ww| w ∈ Σ ∗ and w is of length k}. a. Show that for each k, no DFA can recognize WWk with fewer than 2 k states. b. Describe a much smaller NFA for 94 CHAPTER 1 / REGULAR LANGUAGES SELECTED SOLUTIONS 95 (d) These are DFAs for the two languages {w| w has an even number of a’s} and {w| each a in w is followed by at least one b}. Combining them using the intersection construction gives the following DFA. Though the problem doesn’t request you to simplify the DFA, certain states can be combined to give the following DFA. Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 96 CHAPTER 1 / REGULAR LANGUAGES 1.5 (a) The left-hand DFA recognizes {w| w contains ab}. The right-hand DFA recog- nizes its complement, {w| w doesn’t contain ab}. (b) This DFA recognizes {w| w contains baba}. This DFA recognizes {w| w does not contain baba}. 1.7 (a) (f) 1.11 Let N = (Q, Σ, δ, q0, F ) be any NFA. Construct an NFA N ′ with a single accept state that recognizes the same language as N . Informally, N ′ is exactly like N except it has ε-transitions from the states corresponding to the accept states of N , to a new accept state, qaccept. State qaccept has no emerging transitions. More formally, N ′ = (Q ∪ {qaccept}, Σ, δ′, q0, {qaccept}), where for each q ∈ Q and a ∈ Σε δ′(q, a) = {δ(q, a) if a ̸= ε or q ̸∈ F δ(q, a) ∪ {qaccept} if a = ε and q ∈ F and δ′(qaccept, a) = ∅ for each a ∈ Σε. 1.23 We prove both directions of the “iff.” (→) Assume that B = B+ and show that BB ⊆ B. For every language BB ⊆ B+ holds, so if B = B+, then BB ⊆ B. (←) Assume that BB ⊆ B and show that B = B+. For every language B ⊆ B+, so we need to show only B+ ⊆ B. If w ∈ B+, then w = x1x2 · · · xk where each xi ∈ B and k ≥ 1. Because x1, x2 ∈ B and BB ⊆ B, we have x1x2 ∈ B. Similarly, because x1x2 is in B and x3 is in B, we have x1x2x3 ∈ B. Continuing in this way, x1 · · · xk ∈ B. Hence w ∈ B, and so we may conclude that B+ ⊆ B. Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. SELECTED SOLUTIONS 97 The latter argument may be written formally as the following proof by induction. Assume that BB ⊆ B. Claim: For each k ≥ 1, if x1, . . . , xk ∈ B, then x1 · · · xk ∈ B. Basis: Prove for k = 1. This statement is obviously true. Induction step: For each k ≥ 1, assume that the claim is true for k and prove it to be true for k + 1. If x1, . . . , xk, xk+1 ∈ B, then by the induction assumption, x1 · · · xk ∈ B. There- fore, x1 · · · xkxk+1 ∈ BB, but BB ⊆ B, so x1 · · · xk+1 ∈ B. That proves the induction step and the claim. The claim implies that if BB ⊆ B, then B+ ⊆ B. 1.29 (a) Assume that A1 = {0 n1 n2n| n ≥ 0} is regular. Let p be the pumping length given by the pumping lemma. Choose s to be the string 0 p1 p2 p. Because s is a member of A1 and s is longer than p, the pumping lemma guarantees that s can be split into three pieces, s = xyz, where for any i ≥ 0 the string xyiz is in A1. Consider two possibilities: 1. The string y consists only of 0s, only of 1s, or only of 2s. In these cases, the string xyyz will not have equal numbers of 0s, 1s, and 2s. Hence xyyz is not a member of A1, a contradiction. 2. The string y consists of more than one kind of symbol. In this case, xyyz will have the 0s, 1s, or 2s out of order. Hence xyyz is not a member of A1, a contradiction. Either way we arrive at a contradiction. Therefore, A1 is not regular. (c) Assume that A3 = {a 2 n | n ≥ 0} is regular. Let p be the pumping length given by the pumping lemma. Choose s to be the string a 2 p . Because s is a member of A3 and s is longer than p, the pumping lemma guarantees that s can be split into three pieces, s = xyz, satisfying the three conditions of the pumping lemma. The third condition tells us that |xy| ≤ p. Furthermore, p < 2 p and so |y| < 2 p. Therefore, |xyyz| = |xyz| + |y| < 2 p + 2 p = 2 p+1. The second condition requires |y| > 0 so 2 p < |xyyz| < 2 p+1. The length of xyyz cannot be a power of 2. Hence xyyz is not a member of A3, a contradiction. Therefore, A3 is not regular. 1.40 (a) Let M = (Q, Σ, δ, q0, F ) be a DFA recognizing A, where A is some regular language. Construct M ′ = (Q′, Σ, δ′, q0′, F ′) recognizing NOPREFIX (A) as follows: 1. Q′ = Q. 2. For r ∈ Q′ and a ∈ Σ, deﬁne δ′(r, a) = {{δ(r, a)} if r /∈ F ∅ if r ∈ F. 3. q0′ = q0. 4. F ′ = F . Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 98 CHAPTER 1 / REGULAR LANGUAGES 1.44 Let MB = (QB, Σ, δB, qB, FB) and MC = (QC, Σ, δC , qC , FC ) be DFAs recog- nizing B and C, respectively. Construct NFA M = (Q, Σ, δ, q0, F ) that recognizes B 1 ← C as follows. To decide whether its input w is in B 1 ← C, the machine M checks that w ∈ B, and in parallel nondeterministically guesses a string y that contains the same number of 1s as contained in w and checks that y ∈ C. 1. Q = QB × QC. 2. For (q, r) ∈ Q and a ∈ Σε, deﬁne δ((q, r), a) =  | | {(δB(q, 0), r)} if a = 0 {(δB(q, 1), δC (r, 1))} if a = 1 {(q, δC(r, 0))} if a = ε. 3. q0 = (qB, qC ). 4. F = FB × FC . 1.46 (b) Let B = {0 m1 n| m ̸= n}. Observe that SELECTED SOLUTIONS 99 (c) Suppose that L is regular and let k be the number of states in a DFA recognizing L. Then from part (a), L has index at most k. Conversely, if L has index k, then by part (b) it is recognized by a DFA with k states and thus is regular. To show that the index of L is the size of the smallest DFA accepting it, suppose that L’s index is exactly k. Then, by part (b), there is a k-state DFA accepting L. That is the smallest such DFA because if it were any smaller, then we could show by part (a) that the index of L is less than k. 1.55 (a) The minimum pumping length is 4. The string 000 is in the language but cannot be pumped, so 3 is not a pumping length for this language. If s has length 4 or more, it contains 1s. By dividing s into xyz, where x is 000 and y is the ﬁrst 1 and z is everything afterward, we satisfy the pumping lemma’s three conditions. (b) The minimum pumping length is 1. The pumping length cannot be 0 because the string ε is in the language and it cannot be pumped. Every nonempty string in the language can be divided into xyz, where x, y, and z are ε, the ﬁrst character, and the remainder, respectively. This division satisﬁes the three conditions. (d) The minimum pumping length is 3. The pumping length cannot be 2 because the string 11 is in the language and it cannot be pumped. Let s be a string in the language of length at least 3. If s is generated by 0 ∗1 +0+1 ∗ and s begins either 0 or 11, write s = xyz where x = ε, y is the ﬁrst symbol, and z is the remainder of s. If s is generated by 0 ∗1+0 +1 ∗ and s begins 10, write s = xyz where x = 10, y is the next symbol, and z is the remainder of s. Breaking s up in this way shows that it can be pumped. If s is generated by 10∗1, we can write it as xyz where x = 1, y = 0, and z is the remainder of s. This division gives a way to pump s. Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 102 CHAPTER 2 / CONTEXT-FREE LANGUAGES The collection of languages associated with context-free grammars are called the context-free languages. They include all the regular languages and many additional languages. In this chapter, we give a formal deﬁnition of context-free grammars and study the properties of context-free languages. We also introduce pushdown automata, a class of machines recognizing the context-free languages. Pushdown automata are useful because they allow us to gain additional insight into the power of context-free grammars. 2.1 CONTEXT-FREE GRAMMARS 103 FIGURE 2.1 Parse tree for 000#111 in grammar G1 All strings generated in this way constitute the language of the grammar. We write L(G1) for the language of grammar G1. Some experimentation with the grammar G1 shows us that L(G1) is {0 n#1 n| n ≥ 0}. Any language that can be generated by some context-free grammar is called a context-free language (CFL). For convenience when presenting a context-free grammar, we abbreviate several rules with the same left-hand variable, such as A → 0A1 and A → B, into a single line A → 0A1 | B, using the symbol “ | ” as an “or”. The following is a second example of a context-free grammar, called G2, which describes a fragment of the English language. ⟨SENTENCE⟩ → ⟨NOUN-PHRASE⟩⟨VERB-PHRASE⟩ ⟨NOUN-PHRASE⟩ → ⟨CMPLX-NOUN⟩ | ⟨CMPLX-NOUN⟩⟨PREP-PHRASE⟩ ⟨VERB-PHRASE⟩ → ⟨CMPLX-VERB⟩ | ⟨CMPLX-VERB⟩⟨PREP-PHRASE⟩ ⟨PREP-PHRASE⟩ → ⟨PREP⟩⟨CMPLX-NOUN⟩ ⟨CMPLX-NOUN⟩ → ⟨ARTICLE⟩⟨NOUN⟩ ⟨CMPLX-VERB⟩ → ⟨VERB⟩ | ⟨VERB⟩⟨NOUN-PHRASE⟩ ⟨ARTICLE⟩ → a | the ⟨NOUN⟩ → boy | girl | flower ⟨VERB⟩ → touches | likes | sees ⟨PREP⟩ → with Grammar G2 has 10 variables (the capitalized grammatical terms written in- side brackets); 27 terminals (the standard English alphabet plus a space charac- ter); and 18 rules. Strings in L(G2) include: a boy sees the boy sees a flower a girl with a flower likes the boy Each of these strings has a derivation in grammar G2. The following is a deriva- tion of the ﬁrst string on this list. Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 104 CHAPTER 2 / CONTEXT-FREE LANGUAGES ⟨SENTENCE⟩ ⇒ ⟨NOUN-PHRASE⟩⟨VERB-PHRASE⟩ ⇒ ⟨CMPLX-NOUN⟩⟨VERB-PHRASE⟩ ⇒ ⟨ARTICLE⟩⟨NOUN⟩⟨VERB-PHRASE⟩ ⇒ a ⟨NOUN⟩⟨VERB-PHRASE⟩ ⇒ a boy ⟨VERB-PHRASE⟩ ⇒ a boy ⟨CMPLX-VERB⟩ ⇒ a boy ⟨VERB⟩ ⇒ a boy sees FORMAL DEFINITION OF A CONTEXT-FREE GRAMMAR Let’s formalize our notion of a context-free grammar (CFG). 2.1 CONTEXT-FREE GRAMMARS 105 EXAMPLES OF CONTEXT-FREE GRAMMARS EXAMPLE 2.3 106 CHAPTER 2 / CONTEXT-FREE LANGUAGES the meaning of the code to be compiled in a process called parsing. One rep- resentation of this meaning is the parse tree for the code, in the context-free grammar for the programming language. We discuss an algorithm that parses context-free languages later in Theorem 7.16 and in Problem 7.45. Grammar G4 describes a fragment of a programming language concerned with arithmetic expressions. Observe how the parse trees in Figure 2.5 “group” the operations. The tree for a+a xa groups the x operator and its operands (the second two a’s) as one operand of the + operator. In the tree for (a+a) xa, the grouping is reversed. These groupings ﬁt the standard precedence of mul- tiplication before addition and the use of parentheses to override the standard precedence. Grammar G4 is designed to capture these precedence relations. 2.1 CONTEXT-FREE GRAMMARS 107 Second, constructing a CFG for a language that happens to be regular is easy if you can ﬁrst construct a DFA for that language. You can convert any DFA into an equivalent CFG as follows. Make a variable Ri for each state qi of the DFA. Add the rule Ri → aRj to the CFG if δ(qi, a) = qj is a transition in the DFA. Add the rule Ri → ε if qi is an accept state of the DFA. Make R0 the start variable of the grammar, where q0 is the start state of the machine. Verify on your own that the resulting CFG generates the same language that the DFA recognizes. Third, certain context-free languages contain strings with two substrings that are “linked” in the sense that a machine for such a language would need to re- member an unbounded amount of information about one of the substrings to verify that it corresponds properly to the other substring. This situation occurs in the language {0 n1n| n ≥ 0} because a machine would need to remember the number of 0s in order to verify that it equals the number of 1s. You can construct a CFG to handle this situation by using a rule of the form R → uRv, which gen- erates strings wherein the portion containing the u’s corresponds to the portion containing the v’s. Finally, in more complex languages, the strings may contain certain structures that appear recursively as part of other (or the same) structures. That situation occurs in the grammar that generates arithmetic expressions in Example 2.4. Any time the symbol a appears, an entire parenthesized expression might appear recursively instead. To achieve this effect, place the variable symbol generating the structure in the location of the rules corresponding to where that structure may recursively appear. AMBIGUITY Sometimes a grammar can generate the same string in several different ways. Such a string will have several different parse trees and thus several different meanings. This result may be undesirable for certain applications, such as pro- gramming languages, where a program should have a unique interpretation. If a grammar generates the same string in several different ways, we say that the string is derived ambiguously in that grammar. If a grammar generates some string ambiguously, we say that the grammar is ambiguous. For example, consider grammar G5: ⟨EXPR⟩ → ⟨EXPR⟩+⟨EXPR⟩ | ⟨EXPR⟩x⟨EXPR⟩ | ( ⟨EXPR⟩ ) | a This grammar generates the string a+a xa ambiguously. The following ﬁgure shows the two different parse trees. Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 108 CHAPTER 2 / CONTEXT-FREE LANGUAGES FIGURE 2.6 The two parse trees for the string a+a xa in grammar G5 This grammar doesn’t capture the usual precedence relations and so may group the + before the × or vice versa. In contrast, grammar G4 generates exactly the same language, but every generated string has a unique parse tree. Hence G4 is unambiguous, whereas G5 is ambiguous. Grammar G2 (page 103) is another example of an ambiguous grammar. The sentence the girl touches the boy with the flower has two different derivations. In Exercise 2.8 you are asked to give the two parse trees and observe their correspondence with the two different ways to read that sentence. Now we formalize the notion of ambiguity. When we say that a grammar generates a string ambiguously, we mean that the string has two different parse trees, not two different derivations. Two derivations may differ merely in the order in which they replace variables yet not in their overall structure. To con- centrate on structure, we deﬁne a type of derivation that replaces variables in a ﬁxed order. A derivation of a string w in a grammar G is a leftmost derivation if at every step the leftmost remaining variable is the one replaced. The derivation preceding Deﬁnition 2.2 (page 104) is a leftmost derivation. 2.1 CONTEXT-FREE GRAMMARS 109 Chomsky normal form. Chomsky normal form is useful in giving algorithms for working with context-free grammars, as we do in Chapters 4 and 7. 110 CHAPTER 2 / CONTEXT-FREE LANGUAGES with the rules A → u1A1, A1 → u2A2, A2 → u3A3, . . . , and Ak−2 → uk−1uk. The Ai’s are new variables. We replace any terminal ui in the preceding rule(s) with the new variable Ui and add the rule Ui → ui. 2.2 PUSHDOWN AUTOMATA 111 4. Convert the remaining rules into the proper form by adding additional vari- ables and rules. The ﬁnal grammar in Chomsky normal form is equivalent to G6. (Actually the procedure given in Theorem 2.9 produces several variables Ui and several rules Ui → a. We simpliﬁed the resulting grammar by using a single variable U and rule U → a.) S0 → AA1 | U B | a | SA | AS S → AA1 | U B | a | SA | AS A → b | AA1 | U B | a | SA | AS A1 → SA U → a B → b 112 CHAPTER 2 / CONTEXT-FREE LANGUAGES With the addition of a stack component we obtain a schematic representation of a pushdown automaton, as shown in the following ﬁgure. FIGURE 2.12 Schematic of a pushdown automaton A pushdown automaton (PDA) can write symbols on the stack and read them back later. Writing a symbol “pushes down” all the other symbols on the stack. At any time the symbol on the top of the stack can be read and removed. The remaining symbols then move back up. Writing a symbol on the stack is of- ten referred to as pushing the symbol, and removing a symbol is referred to as popping it. Note that all access to the stack, for both reading and writing, may be done only at the top. In other words a stack is a “last in, ﬁrst out” storage device. If certain information is written on the stack and additional information is written afterward, the earlier information becomes inaccessible until the later information is removed. Plates on a cafeteria serving counter illustrate a stack. The stack of plates rests on a spring so that when a new plate is placed on top of the stack, the plates below it move down. The stack on a pushdown automaton is like a stack of plates, with each plate having a symbol written on it. A stack is valuable because it can hold an unlimited amount of information. Recall that a ﬁnite automaton is unable to recognize the language {0 n1 n| n ≥ 0} because it cannot store very large numbers in its ﬁnite memory. A PDA is able to recognize this language because it can use its stack to store the number of 0s it has seen. Thus the unlimited nature of a stack allows the PDA to store numbers of unbounded size. The following informal description shows how the automaton for this language works. Read symbols from the input. As each 0 is read, push it onto the stack. As soon as 1s are seen, pop a 0 off the stack for each 1 read. If reading the input is ﬁnished exactly when the stack becomes empty of 0s, accept the input. If the stack becomes empty while 1s remain or if the 1s are ﬁnished while the stack still contains 0s or if any 0s appear in the input following 1s, reject the input. As mentioned earlier, pushdown automata may be nondeterministic. Deter- ministic and nondeterministic pushdown automata are not equivalent in power. Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 2.2 PUSHDOWN AUTOMATA 113 Nondeterministic pushdown automata recognize certain languages that no de- terministic pushdown automata can recognize, as we will see in Section 2.4. We give languages requiring nondeterminism in Examples 2.16 and 2.18. Recall that deterministic and nondeterministic ﬁnite automata do recognize the same class of languages, so the pushdown automata situation is different. We focus on nondeterministic pushdown automata because these automata are equivalent in power to context-free grammars. FORMAL DEFINITION OF A PUSHDOWN AUTOMATON The formal deﬁnition of a pushdown automaton is similar to that of a ﬁnite automaton, except for the stack. The stack is a device containing symbols drawn from some alphabet. The machine may use different alphabets for its input and its stack, so now we specify both an input alphabet Σ and a stack alphabet Γ. At the heart of any formal deﬁnition of an automaton is the transition func- tion, which describes its behavior. Recall that Σε = Σ ∪ {ε} and Γε = Γ ∪ {ε}. The domain of the transition function is Q × Σε × Γε. Thus the current state, next input symbol read, and top symbol of the stack determine the next move of a pushdown automaton. Either symbol may be ε, causing the machine to move without reading a symbol from the input or without reading a symbol from the stack. For the range of the transition function we need to consider what to allow the automaton to do when it is in a particular situation. It may enter some new state and possibly write a symbol on the top of the stack. The function δ can indicate this action by returning a member of Q together with a member of Γε, that is, a member of Q × Γε. Because we allow nondeterminism in this model, a situation may have several legal next moves. The transition function incorporates nondeterminism in the usual way, by returning a set of members of Q × Γε, that is, a member of P(Q × Γε). Putting it all together, our transition function δ takes the form δ : Q × Σε × Γε−→P(Q × Γε). 114 CHAPTER 2 / CONTEXT-FREE LANGUAGES A pushdown automaton M = (Q, Σ, Γ, δ, q0, F ) computes as follows. It ac- cepts input w if w can be written as w = w1w2 · · · wm, where each wi ∈ Σε and sequences of states r0, r1, . . . , rm ∈ Q and strings s0, s1, . . . , sm ∈ Γ∗ exist that satisfy the following three conditions. The strings si represent the sequence of stack contents that M has on the accepting branch of the computation. 1. r0 = q0 and s0 = ε. This condition signiﬁes that M starts out properly, in the start state and with an empty stack. 2. For i = 0, . . . , m − 1, we have (ri+1, b) ∈ δ(ri, wi+1, a), where si = at and si+1 = bt for some a, b ∈ Γε and t ∈ Γ∗. This condition states that M moves properly according to the state, stack, and next input symbol. 3. rm ∈ F . This condition states that an accept state occurs at the input end. EXAMPLES OF PUSHDOWN AUTOMATA EXAMPLE 2.14 2.2 PUSHDOWN AUTOMATA 115 FIGURE 2.15 State diagram for the PDA M1 that recognizes {0 n1 n| n ≥ 0} 116 CHAPTER 2 / CONTEXT-FREE LANGUAGES FIGURE 2.17 State diagram for PDA M2 that recognizes {a ib jck| i, j, k ≥ 0 and i = j or i = k} 2.2 PUSHDOWN AUTOMATA 117 EQUIVALENCE WITH CONTEXT-FREE GRAMMARS In this section we show that context-free grammars and pushdown automata are equivalent in power. Both are capable of describing the class of context-free languages. We show how to convert any context-free grammar into a pushdown automaton that recognizes the same language and vice versa. Recalling that we deﬁned a context-free language to be any language that can be described with a context-free grammar, our objective is the following theorem. THEOREM 2.20 118 CHAPTER 2 / CONTEXT-FREE LANGUAGES symbol on the stack and that may be a terminal symbol instead of a variable. The way around this problem is to keep only part of the intermediate string on the stack: the symbols starting with the ﬁrst variable in the intermediate string. Any terminal symbols appearing before the ﬁrst variable are matched immediately with symbols in the input string. The following ﬁgure shows the PDA P . FIGURE 2.22 P representing the intermediate string 01A1A0 The following is an informal description of P . 1. Place the marker symbol $ and the start variable on the stack. 2. Repeat the following steps forever. a. If the top of stack is a variable symbol A, nondeterministically select one of the rules for A and substitute A by the string on the right-hand side of the rule. b. If the top of stack is a terminal symbol a, read the next symbol from the input and compare it to a. If they match, repeat. If they do not match, reject on this branch of the nondeterminism. c. If the top of stack is the symbol $, enter the accept state. Doing so accepts the input if it has all been read. PROOF We now give the formal details of the construction of the pushdown automaton P = (Q, Σ, Γ, δ, qstart, F ). To make the construction clearer, we use shorthand notation for the transition function. This notation provides a way to write an entire string on the stack in one step of the machine. We can simulate this action by introducing additional states to write the string one symbol at a time, as implemented in the following formal construction. Let q and r be states of the PDA and let a be in Σε and s be in Γε. Say that we want the PDA to go from q to r when it reads a and pops s. Furthermore, we want it to push the entire string u = u1 · · · ul on the stack at the same time. We can implement this action by introducing new states q1, . . . , ql−1 and setting the Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 2.2 PUSHDOWN AUTOMATA 119 transition function as follows: δ(q, a, s) to contain (q1, ul), δ(q1, ε, ε) = {(q2, ul−1)}, δ(q2, ε, ε) = {(q3, ul−2)}, ... δ(ql−1, ε, ε) = {(r, u1)}. We use the notation (r, u) ∈ δ(q, a, s) to mean that when q is the state of the automaton, a is the next input symbol, and s is the symbol on the top of the stack, the PDA may read the a and pop the s, then push the string u onto the stack and go on to the state r. The following ﬁgure shows this implementation. a s xyz a s z y x FIGURE 2.23 Implementing the shorthand (r, xyz) ∈ δ(q, a, s) The states of P are Q = {qstart, qloop, qaccept} ∪ E, where E is the set of states we need for implementing the shorthand just described. The start state is qstart. The only accept state is qaccept. The transition function is deﬁned as follows. We begin by initializing the stack to contain the symbols $ and S, implementing step 1 in the informal de- scription: δ(qstart, ε, ε) = {(qloop, S$)}. Then we put in transitions for the main loop of step 2. First, we handle case (a) wherein the top of the stack contains a variable. Let δ(qloop, ε, A) = {(qloop, w)| where A → w is a rule in R}. Second, we handle case (b) wherein the top of the stack contains a terminal. Let δ(qloop, a, a) = {(qloop, ε)}. Finally, we handle case (c) wherein the empty stack marker $ is on the top of the stack. Let δ(qloop, ε, $) = {(qaccept, ε)}. The state diagram is shown in Figure 2.24. Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 120 CHAPTER 2 / CONTEXT-FREE LANGUAGES FIGURE 2.24 State diagram of P That completes the proof of Lemma 2.21. 2.2 PUSHDOWN AUTOMATA 121 Now we prove the reverse direction of Theorem 2.20. For the forward di- rection, we gave a procedure for converting a CFG into a PDA. The main idea was to design the automaton so that it simulates the grammar. Now we want to give a procedure for going the other way: converting a PDA into a CFG. We design the grammar to simulate the automaton. This task is challenging because “programming” an automaton is easier than “programming” a grammar. LEMMA 2.27 122 CHAPTER 2 / CONTEXT-FREE LANGUAGES PROOF Say that P = (Q, Σ, Γ, δ, q0, {qaccept}) and construct G. The variables of G are {Apq| p, q ∈ Q}. The start variable is Aq0,qaccept . Now we describe G’s rules in three parts. 1. For each p, q, r, s ∈ Q, u ∈ Γ, and a, b ∈ Σε, if δ(p, a, ε) contains (r, u) and δ(s, b, u) contains (q, ε), put the rule Apq → aArsb in G. 2. For each p, q, r ∈ Q, put the rule Apq → AprArq in G. 3. Finally, for each p ∈ Q, put the rule App → ε in G. You may gain some insight for this construction from the following ﬁgures. FIGURE 2.28 PDA computation corresponding to the rule Apq → AprArq FIGURE 2.29 PDA computation corresponding to the rule Apq → aArsb Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 2.2 PUSHDOWN AUTOMATA 123 Now we prove that this construction works by demonstrating that Apq gener- ates x if and only if (iff) x can bring P from p with empty stack to q with empty stack. We consider each direction of the iff as a separate claim. CLAIM 2.30 124 CHAPTER 2 / CONTEXT-FREE LANGUAGES Basis: The computation has 0 steps. If a computation has 0 steps, it starts and ends at the same state—say, p. So we must show that App ∗ ⇒ x. In 0 steps, P cannot read any characters, so x = ε. By construction, G has the rule App → ε, so the basis is proved. Induction step: Assume true for computations of length at most k, where k ≥ 0, and prove true for computations of length k + 1. Suppose that P has a computation wherein x brings p to q with empty stacks in k + 1 steps. Either the stack is empty only at the beginning and end of this computation, or it becomes empty elsewhere, too. In the ﬁrst case, the symbol that is pushed at the ﬁrst move must be the same as the symbol that is popped at the last move. Call this symbol u. Let a be the input read in the ﬁrst move, b be the input read in the last move, r be the state after the ﬁrst move, and s be the state before the last move. Then δ(p, a, ε) contains (r, u) and δ(s, b, u) contains (q, ε), and so rule Apq → aArsb is in G. Let y be the portion of x without a and b, so x = ayb. Input y can bring P from r to s without touching the symbol u that is on the stack and so P can go from r with an empty stack to s with an empty stack on input y. We have removed the ﬁrst and last steps of the k + 1 steps in the original computation on x so the computation on y has (k + 1) − 2 = k − 1 steps. Thus the induction hypothesis tells us that Ars ∗ ⇒ y. Hence Apq ∗ ⇒ x. In the second case, let r be a state where the stack becomes empty other than at the beginning or end of the computation on x. Then the portions of the computation from p to r and from r to q each contain at most k steps. Say that y is the input read during the ﬁrst portion and z is the input read during the second portion. The induction hypothesis tells us that Apr ∗ ⇒ y and Arq ∗ ⇒ z. Because rule Apq → AprArq is in G, Apq ∗ ⇒ x, and the proof is complete. That completes the proof of Lemma 2.27 and of Theorem 2.20. 2.3 NON-CONTEXT-FREE LANGUAGES 125 FIGURE 2.33 Relationship of the regular and context-free languages 126 CHAPTER 2 / CONTEXT-FREE LANGUAGES states that the pieces v, x, and y together have length at most p. This technical condition sometimes is useful in proving that certain languages are not context free. PROOF IDEA Let A be a CFL and let G be a CFG that generates it. We must show that any sufﬁciently long string s in A can be pumped and remain in A. The idea behind this approach is simple. Let s be a very long string in A. (We make clear later what we mean by “very long.”) Because s is in A, it is derivable from G and so has a parse tree. The parse tree for s must be very tall because s is very long. That is, the parse tree must contain some long path from the start variable at the root of the tree to one of the terminal symbols at a leaf. On this long path, some variable symbol R must repeat because of the pigeonhole principle. As the following ﬁgure shows, this repetition allows us to replace the subtree under the second occurrence of R with the subtree under the ﬁrst occurrence of R and still get a legal parse tree. Therefore, we may cut s into ﬁve pieces uvxyz as the ﬁgure indicates, and we may repeat the second and fourth pieces and obtain a string still in the language. In other words, uvixyiz is in A for any i ≥ 0. FIGURE 2.35 Surgery on parse trees Let’s now turn to the details to obtain all three conditions of the pumping lemma. We also show how to calculate the pumping length p. Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 2.3 NON-CONTEXT-FREE LANGUAGES 127 PROOF Let G be a CFG for CFL A. Let b be the maximum number of symbols in the right-hand side of a rule (assume at least 2). In any parse tree using this grammar, we know that a node can have no more than b children. In other words, at most b leaves are 1 step from the start variable; at most b2 leaves are within 2 steps of the start variable; and at most bh leaves are within h steps of the start variable. So, if the height of the parse tree is at most h, the length of the string generated is at most bh. Conversely, if a generated string is at least bh + 1 long, each of its parse trees must be at least h + 1 high. Say |V | is the number of variables in G. We set p, the pumping length, to be b|V |+1. Now if s is a string in A and its length is p or more, its parse tree must be at least |V | + 1 high, because b|V |+1 ≥ b|V | + 1. To see how to pump any such string s, let τ be one of its parse trees. If s has several parse trees, choose τ to be a parse tree that has the smallest number of nodes. We know that τ must be at least |V | + 1 high, so its longest path from the root to a leaf has length at least |V | + 1. That path has at least |V | + 2 nodes; one at a terminal, the others at variables. Hence that path has at least |V | + 1 variables. With G having only |V | variables, some variable R appears more than once on that path. For convenience later, we select R to be a variable that repeats among the lowest |V | + 1 variables on this path. We divide s into uvxyz according to Figure 2.35. Each occurrence of R has a subtree under it, generating a part of the string s. The upper occurrence of R has a larger subtree and generates vxy, whereas the lower occurrence generates just x with a smaller subtree. Both of these subtrees are generated by the same variable, so we may substitute one for the other and still obtain a valid parse tree. Replacing the smaller by the larger repeatedly gives parse trees for the strings uvixyiz at each i > 1. Replacing the larger by the smaller generates the string uxz. That establishes condition 1 of the lemma. We now turn to conditions 2 and 3. To get condition 2, we must be sure that v and y are not both ε. If they were, the parse tree obtained by substituting the smaller subtree for the larger would have fewer nodes than τ does and would still generate s. This result isn’t possible because we had already chosen τ to be a parse tree for s with the smallest number of nodes. That is the reason for selecting τ in this way. In order to get condition 3, we need to be sure that vxy has length at most p. In the parse tree for s the upper occurrence of R generates vxy. We chose R so that both occurrences fall within the bottom |V | + 1 variables on the path, and we chose the longest path in the parse tree, so the subtree where R generates vxy is at most |V | + 1 high. A tree of this height can generate a string of length at most b|V |+1 = p. 128 CHAPTER 2 / CONTEXT-FREE LANGUAGES EXAMPLE 2.36 2.3 NON-CONTEXT-FREE LANGUAGES 129 a. The a’s do not appear. Then we try pumping down to obtain the string uv0xy0z = uxz. That contains the same number of a’s as s does, but it contains fewer b’s or fewer c’s. Therefore, it is not a member of C, and a contradiction occurs. b. The b’s do not appear. Then either a’s or c’s must appear in v or y be- cause both can’t be the empty string. If a’s appear, the string uv2xy2z contains more a’s than b’s, so it is not in C. If c’s appear, the string uv0xy0z contains more b’s than c’s, so it is not in C. Either way, a contradiction occurs. c. The c’s do not appear. Then the string uv2xy2z contains more a’s or more b’s than c’s, so it is not in C, and a contradiction occurs. 2. When either v or y contains more than one type of symbol, uv2xy2z will not contain the symbols in the correct order. Hence it cannot be a member of C, and a contradiction occurs. Thus we have shown that s cannot be pumped in violation of the pumping lemma and that C is not context free. 130 CHAPTER 2 / CONTEXT-FREE LANGUAGES2.4 DETERMINISTIC CONTEXT-FREE LANGUAGES 131 The transition function may output either a single move of the form (r, y) or it may indicate no action by outputting ∅. To illustrate these possibilities, let’s consider an example. Suppose a DPDA M with transition function δ is in state q, has a as its next input symbol, and has symbol x on the top of its stack. If δ(q, a, x) = (r, y) then M reads a, pops x off the stack, enters state r, and pushes y on the stack. Alternatively, if δ(q, a, x) = ∅ then when M is in state q, it has no move that reads a and pops x. In that case, the condition on δ requires that one of δ(q, ε, x), δ(q, a, ε), or δ(q, ε, ε) is nonempty, and then M moves accordingly. The condition enforces deterministic behavior by preventing the DPDA from taking two different actions in the same situation, such as would be the case if both δ(q, a, x) ̸= ∅ and δ(q, a, ε) ̸= ∅. A DPDA has exactly one legal move in every situation where its stack is nonempty. If the stack is empty, a DPDA can move only if the transition function speciﬁes a move that pops ε. Otherwise the DPDA has no legal move and it rejects without reading the rest of the input. Acceptance for DPDAs works in the same way it does for PDAs. If a DPDA enters an accept state after it has read the last input symbol of an input string, it accepts that string. In all other cases, it rejects that string. Rejection occurs if the DPDA reads the entire input but doesn’t enter an accept state when it is at the end, or if the DPDA fails to read the entire input string. The latter case may arise if the DPDA tries to pop an empty stack or if the DPDA makes an endless sequence of ε-input moves without reading the input past a certain point. The language of a DPDA is called a deterministic context-free language. EXAMPLE 2.40 132 CHAPTER 2 / CONTEXT-FREE LANGUAGES LEMMA 2.41 2.4 DETERMINISTIC CONTEXT-FREE LANGUAGES 133 PROPERTIES OF DCFLS We’ll explore closure and nonclosure properties of the class of DCFLs, and use these to exhibit a CFL that is not a DCFL. THEOREM 2.42 134 CHAPTER 2 / CONTEXT-FREE LANGUAGES Problem 2.53 asks you to show that the class of DCFLs isn’t closed under other familiar operations such as union, intersection, star, and reversal. To simplify arguments, we will occasionally consider endmarked inputs whereby the special endmarker symbol ⊣⊣⊣ is appended to the input string. Here we add ⊣⊣⊣ to the DPDA’s input alphabet. As we show in the next theorem, adding endmarkers doesn’t change the power of DPDAs. However, designing DPDAs on endmarked inputs is often easier because we can take advantage of knowing when the input string ends. For any language A, we write the endmarked language A⊣⊣⊣ to be the collection of strings w⊣⊣⊣ where w ∈ A. THEOREM 2.43 2.4 DETERMINISTIC CONTEXT-FREE LANGUAGES 135 Initially, P ′ pushes the set R0 on the stack, where R0 contains every state q such that when P is started in q with an empty stack, it eventually accepts without reading any input symbols. Then P ′ begins simulating P . To simulate a pop move, P ′ ﬁrst pops and discards the set of states that appears as the top stack symbol, then it pops again to obtain the symbol that P would have popped at this point, and uses it to determine the next move of P . Simulating a push move δ(q, ε, ε) = (r, x), where P pushes x as it goes from state q to state r, goes as follows. First P ′ examines the set of states R on the top of its stack, and then it pushes x and after that the set S, where q ∈ S if q ∈ F or if δ(q, ε, x) = (r, ε) and r ∈ R. In other words, S is the set of states that are either accepting immediately, or that would lead to a state in R after popping x. Lastly, P ′ simulates a read move δ(q, a, ε) = (r, ε), by examining the set R on the top of the stack and entering an accept state if r ∈ R. If P ′ is at the end of the input string when it enters this state, it will accept the input. If it is not at the end of the input string, it will continue simulating P , so this accept state must also record P ’s state. Thus we create this state as a second copy of P ’s original state, marking it as an accept state in P ′. 136 CHAPTER 2 / CONTEXT-FREE LANGUAGES More formally, if u and v are strings of variables and terminals, write u ↣ v to mean that v can be obtained from u by a reduce step. In other words, u ↣ v means the same as v ⇒ u. A reduction from u to v is a sequence u = u1 ↣ u2 ↣ . . . ↣ uk = v and we say that u is reducible to v, written u ∗ ↣ v. Thus u ∗ ↣ v whenever v ∗ ⇒ u. A reduction from u is a reduction from u to the start variable. In a leftmost reduction, each reducing string is reduced only after all other reducing strings that lie entirely to its left. With a little thought we can see that a leftmost reduction is a rightmost derivation in reverse. Here’s the idea behind determinism in CFGs. In a CFG with start variable S and string w in its language, say that a leftmost reduction of w is w = u1 ↣ u2 ↣ . . . ↣ uk = S. First, we stipulate that every ui determines the next reduce step and hence ui+1. Thus w determines its entire leftmost reduction. This requirement implies only that the grammar is unambiguous. To get determinism, we need to go further. In each ui, the next reduce step must be uniquely determined by the preﬁx of ui up through and including the reducing string h of that reduce step. In other words, the leftmost reduce step in ui doesn’t depend on the symbols in ui to the right of its reducing string. Introducing terminology will help us make this idea precise. Let w be a string in the language of CFG G, and let ui appear in a leftmost reduction of w. In the reduce step ui ↣ ui+1, say that rule T → h was applied in reverse. That means we can write ui = xhy and ui+1 = xT y, where h is the reducing string, x is the part of ui that appears leftward of h, and y is the part of ui that appears rightward of h. Pictorially, ui = x z 2.4 DETERMINISTIC CONTEXT-FREE LANGUAGES 137 therefore the leftmost reductions, and hence the handles, are also unique. In that case, we may refer to the handle of a valid string. Observe that y, the portion of ui following a handle, is always a string of ter- minals because the reduction is leftmost. Otherwise, y would contain a variable symbol and that could arise only from a previous reduce step whose reducing string was completely to the right of h. But then the leftmost reduction should have reduced the handle at an earlier step. EXAMPLE 2.45 138 CHAPTER 2 / CONTEXT-FREE LANGUAGES EXAMPLE 2.46 2.4 DETERMINISTIC CONTEXT-FREE LANGUAGES 139140 CHAPTER 2 / CONTEXT-FREE LANGUAGES of its progress through the chosen right-hand side. We represent this progress by placing a dot in the corresponding point in the rule, yielding a dotted rule, also called an item in some other treatments of this material. Thus for each rule B → u1u2 · · · uk with k symbols on the right-hand side, we get k + 1 dotted rules: B → .u1u2 · · · uk B → u1.u2 · · · uk ... B → u1u2 · · · .uk B → u1u2 · · · uk. Each of these dotted rules corresponds to one state of J. We indicate the state associated with the dotted rule B → u.v with a box around it, \u0000 2.4 DETERMINISTIC CONTEXT-FREE LANGUAGES 141 LEMMA 2.48 142 CHAPTER 2 / CONTEXT-FREE LANGUAGES To obtain a valid string, fully expand all variables that appear in y′ until each variable derives some string of terminals, and call the resulting string y. The string xuvy is valid because it occurs in a leftmost reduction of w ∈ L(G), a string of terminals obtained by fully expanding all variables in xuvy. As is evident from the ﬁgure below, uv is the handle in the reduction and its reducing rule is T → uv. … … S1 S2 S3 T x yu v… x FIGURE 2.50 Parse tree leading to valid string xuvy with handle uv Now we prove the reverse direction of the lemma. Assume that string xuvy is a valid string with handle uv and reducing rule T → uv. Show that K on input xu may enter state \u0000 2.4 DETERMINISTIC CONTEXT-FREE LANGUAGES 143 K contains the following path from its start state to state \u0000 144 CHAPTER 2 / CONTEXT-FREE LANGUAGES Conversely, if the DK-test fails because an accept state has two completed rules, extend the associated string to two valid strings with differing handles at that point. Similarly, if it has a completed rule and a dotted rule with a terminal following the dot, employ Lemma 2.48 to get two valid extensions with differing handles at that point. Constructing the valid extension corresponding to the second rule is a bit delicate. PROOF Start with the forward direction. Assume that G isn’t deterministic and show that it fails the DK-test. Take a valid string xhy that has an unforced handle h. Hence some valid string xhy′ has a different handle ˆh ̸= h, where y′ is a string of terminals. We can thus write xhy′ as xhy′ = ˆxˆhˆy. If xh = ˆxˆh, the reducing rules differ because h and ˆh aren’t the same handle. Therefore, input xh sends DK to a state that contains two completed rules, a violation of the DK-test. If xh ̸= ˆxˆh, one of these extends the other. Assume that xh is the proper preﬁx of ˆxˆh. The argument is the same with the strings interchanged and y in place of y′, if ˆxˆh is the shorter string. Let q be the state that DK enters on input xh. State q must be accepting because h is a handle of xhy. A transition arrow must exit q because ˆxˆh sends DK to an accept state via q. Furthermore, that transition arrow is labeled with a terminal symbol, because y′ ∈ Σ+. Here y′ ̸= ε because ˆxˆh extends xh. Hence q contains a dotted rule with a terminal symbol immediately following the dot, violating the DK-test. To prove the reverse direction, assume G fails the DK -test at some accept state q, and show that G isn’t deterministic by exhibiting an unforced handle. Because q is accepting, it has a completed rule T → h.. Let z be a string that leads DK to q. Then z = xh where some valid string xhy has handle h with reducing rule T → h, for y ∈ Σ∗. Now we consider two cases, depending on how the DK -test fails. First, say q has another completed rule B → ˆh.. Then some valid string xhy′ must have a different handle ˆh with reducing rule B → ˆh. Therefore, h isn’t a forced handle. Second, say q contains a rule B → u.av where a ∈ Σ. Because xh takes DK to q, we have xh = ˆxu, where ˆxuav ˆy is valid and has a handle uav with reducing rule B → uav, for some ˆy ∈ Σ∗. To show that h is unforced, fully expand all variables in v to get the result v′ ∈ Σ∗, then let y′ = av′ ˆy and notice that y′ ∈ Σ∗. The following leftmost reduction shows that xhy′ is a valid string and h is not the handle. xhy′ = xhav′ ˆy = ˆx uav′ ˆy ∗ ↣ ˆx uav ˆy ↣ ˆx B ˆy ∗ ↣ S where S is the start variable. We know that ˆx uav ˆy is valid and we can obtain ˆx uav′ ˆy from it by using a rightmost derivation so ˆx uav′ ˆy is also valid. More- over, the handle of ˆx uav′ ˆy either lies inside v′ (if v ̸= v′) or is uav (if v = v′). In either case, the handle includes a or follows a and thus cannot be h because h fully precedes a. Hence h isn’t a forced handle. 2.4 DETERMINISTIC CONTEXT-FREE LANGUAGES 145 When building the DFA DK in practice, a direct construction may be faster than ﬁrst constructing the NFA K. Begin by adding a dot at the initial point in all rules involving the start variable and place these now-dotted rules into DK’s start state. If a dot precedes a variable C in any of these rules, place dots at the initial position in all rules that have C on the left-hand side and add these rules to the state, continuing this process until no new dotted rules are obtained. For any symbol c that follows a dot, add an outgoing edge labeled c to a new state containing the dotted rules obtained by shifting the dot across the c in any of the dotted rules where the dot precedes the c, and add rules corresponding to the rules where a dot precedes a variable as before. EXAMPLE 2.53 146 CHAPTER 2 / CONTEXT-FREE LANGUAGES EXAMPLE 2.55 2.4 DETERMINISTIC CONTEXT-FREE LANGUAGES 147 We have two directions to prove. First we will show that every DCFG has an equivalent DPDA. Then we will show that every DPDA that recognizes an end- marked language has an equivalent DCFG. We handle these two directions in separate lemmas. LEMMA 2.58 148 CHAPTER 2 / CONTEXT-FREE LANGUAGES Here P and G are deterministic. In the proof idea for Lemma 2.27, we altered P to empty its stack and enter a speciﬁc accept state qaccept when it accepts. A PDA cannot directly determine that it is at the end of its input, so P uses its nondeterminism to guess that it is in that situation. We don’t want to introduce nondeterminism in constructing DPDA P . Instead we use the assumption that L(P ) is endmarked. We modify P to empty its stack and enter qaccept when it enters one of its original accept states after it has read the endmarker ⊣⊣⊣. Next we apply the grammar construction to obtain G. Simply applying the original construction to a DPDA produces a nearly deterministic grammar be- cause the CFG’s derivations closely correspond to the DPDA’s computations. That grammar fails to be deterministic in one minor, ﬁxable way. The original construction introduces rules of the form Apq → AprArq and these may cause ambiguity. These rules cover the case where Apq generates a string that takes P from state p to state q with its stack empty at both ends, and the stack empties midway. The substitution corresponds to dividing the computation at that point. But if the stack empties several times, several divisions are possible. Each of these divisions yields different parse trees, so the resulting grammar is ambiguous. We ﬁx this problem by modifying the grammar to divide the computation only at the very last point where the stack empties midway, thereby removing this ambiguity. For illustration, a similar but simpler situation occurs in the ambiguous grammar S → T ⊣⊣⊣ T → T T | (T ) | ε which is equivalent to the unambiguous, and deterministic, grammar S → T ⊣⊣⊣ T → T (T ) | ε. Next we show the modiﬁed grammar is deterministic by using the DK-test. The grammar is designed to simulate the DPDA. As we proved in Lemma 2.27, Apq generates exactly those strings on which P goes from state p on empty stack to state q on empty stack. We’ll prove G’s determinism using P ’s determinism so we will ﬁnd it useful to deﬁne P ’s computation on valid strings to observe its action on handles. Then we can use P ’s deterministic behavior to show that handles are forced. PROOF Say that P = (Q, Σ, Γ, δ, q0, {qaccept}) and construct G. The start variable is Aq0,qaccept . The construction on page 121 contains parts 1, 2, and 3, repeated here for convenience. Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 2.4 DETERMINISTIC CONTEXT-FREE LANGUAGES 149 1. For each p, q, r, s ∈ Q, u ∈ Γ, and a, b ∈ Σε, if δ(p, a, ε) contains (r, u) and δ(s, b, u) contains (q, ε), put the rule Apq → aArsb in G. 2. For each p, q, r ∈ Q, put the rule Apq → AprArq in G. 3. For each p ∈ Q, put the rule App → ε in G. We modify the construction to avoid introducing ambiguity, by combining rules of types 1 and 2 into a single type 1-2 rule that achieves the same effect. 1-2. For each p, q, r, s, t ∈ Q, u ∈ Γ, and a, b ∈ Σε, if δ(r, a, ε) = (s, u) and δ(t, b, u) = (q, ε), put the rule Apq → ApraAstb in G. To see that the modiﬁed grammar generates the same language, consider any derivation in the original grammar. For each substitution due to a type 2 rule Apq → AprArq, we can assume that r is P ’s state when it is at the rightmost point where the stack becomes empty midway by modifying the proof of Claim 2.31 on page 123 to select r in this way. Then the subsequent substitution of Arq must expand it using a type 1 rule Arq → aAstb. We can combine these two substitutions into a single type 1-2 rule Apq → ApraAstb. Conversely, in a derivation using the modiﬁed grammar, if we replace each type 1-2 rule Apq → ApraAstb by the type 2 rule Apq → AprArq followed by the type 1 rule Arq → aAstb, we get the same result. Now we use the DK-test to show that G is deterministic. To do that, we’ll analyze how P operates on valid strings by extending its input alphabet and tran- sition function to process variable symbols in addition to terminal symbols. We add all symbols Apq to P ’s input alphabet and we extend its transition function δ by deﬁning δ(p, Apq, ε) = (q, ε). Set all other transitions involving Apq to ∅. To preserve P ’s deterministic behavior, if P reads Apq from the input then disallow an ε-input move. The following claim applies to a derivation of any string w in L(G) such as Aq0,qaccept = v0 ⇒ v1 ⇒ · · · ⇒ vi ⇒ · · · ⇒ vk = w. CLAIM 2.60 150 CHAPTER 2 / CONTEXT-FREE LANGUAGES 1. Apq → ApraAstb or 2. App → ε. Thus either vi+1 = xApraAstb y or vi+1 = x y, depending on which type of rule was used. In the ﬁrst case, when P reads ApraAstb in vi+1, we know it starts in state p, because it has just ﬁnished reading x. As P reads ApraAstb in vi+1, it enters the sequence of states r, s, t, and q, due to the substitution rule’s construction. Therefore, it enters state p just prior to reading Apr and it enters state s just prior to reading Ast, thereby establishing the claim for these two occurrences of variables. The claim holds on occurrences of variables in the y part because, after P reads b it enters state q and then it reads string y. On input vi, it also enters q just before reading y, so the computations agree on the y parts of vi and vi+1. Obviously, the computations agree on the x parts. Therefore, the claim holds for vi+1. In the second case, no new variables are introduced, so we only need to observe that the computations agree on the x and y parts of vi and vi+1. This proves the claim. CLAIM 2.61 2.4 DETERMINISTIC CONTEXT-FREE LANGUAGES 151 exception occurs at DK’s start state, where this dot may occur at the beginning of the rule, but this accept state cannot be the start state because it contains a completed type 1-2 rule.) In G, that means T derives from a type 1-2 dotted rule where the dot precedes the second variable. From G’s construction a push occurs just before the dot. This implies that P does a push move at the very end of z, contradicting our previous statement. Thus the completed ε-rule T cannot exist. Either way, a second completed rule of either type cannot occur in this accept state. Case 2a. Here R is a completed ε-rule App → .. We show that no other com- pleted ε-rule Aqq → . can coexist with R. If it does, the preceding claim shows that P must be in p after reading z and it must also be in q after reading z. Hence p = q and therefore the two completed ε-rules are the same. Case 1b. Here R is a completed type 1-2 rule. From Case 1a, we know that P pops its stack at the end of z. Suppose the accept state also contains a dotted rule T where a terminal symbol immediately follows the dot. From T we know that P doesn’t pop its stack at the end of z. This contradiction shows that this situation cannot arise. Case 2b. Here R is a completed ε-rule. Assume that the accept state also contains a dotted rule T where a terminal symbol immediately follows the dot. Because T is of type 1-2, a variable symbol immediately precedes the dot, and thus z ends with that variable symbol. Moreover, after P reads z it is prepared to read a non-ε input symbol because a terminal follows the dot. As in Case 1a, the completed ε-rule R derives from a type 1-2 dotted rule S where the dot immediately precedes the second variable. (Again this accept state cannot be DK’s start state because the dot doesn’t occur at the beginning of T .) Thus some symbol ˆa ∈ Σε immediately precedes the dot in S and so z ends with ˆa. Either ˆa ∈ Σ or ˆa = ε, but because z ends with a variable symbol, ˆa ̸∈ Σ so ˆa = ε. Therefore, after P reads z but before it makes the ε-input move to process ˆa, it is prepared to read an ε input. We also showed above that P is prepared to read a non-ε input symbol at this point. But a DPDA isn’t allowed to make both an ε-input move and a move that reads a non-ε input symbol at a given state and stack, so the above situation is impossible. Thus this situation cannot occur. 152 CHAPTER 2 / CONTEXT-FREE LANGUAGES Fortunately, a broader class of grammars called the LR(k) grammars gives us the best of both worlds. They are close enough to DCFGs to allow direct conver- sion into DPDAs. Yet they are expressive enough for many applications. Algorithms for LR(k) grammars introduce lookahead. In a DCFG, all handles are forced. A handle depends only on the symbols in a valid string up through and including the handle, but not on terminal symbols that follow the handle. In an LR(k) grammar, a handle may also depend on symbols that follow the handle, but only on the ﬁrst k of these. The acronym LR(k) stands for: L 2.4 DETERMINISTIC CONTEXT-FREE LANGUAGES 153 Let R1 be a completed rule with lookahead symbol a1, and let R2 be a dotted rule with lookahead symbol a2. Say that R1 and R2 are consistent if 1. R2 is completed and a1 = a2, or 2. R2 is not completed and a1 immediately follows its dot. Now we are ready to describe the DK1-test. Construct the DFA DK 1. The test stipulates that every accept state must not contain any two consistent dotted rules. THEOREM 2.63 154 CHAPTER 2 / CONTEXT-FREE LANGUAGES THEOREM 2.66 EXERCISES 155 A2.3 Answer each part for the following context-free grammar G. R → XRX | S S → aT b | bT a T → XT X | X | ε X → a | b a. What are the variables of G? b. What are the terminals of G? c. Which is the start variable of G? d. Give three strings in L(G). e. Give three strings not in L(G). f. True or False: T ⇒ aba. g. True or False: T ∗ ⇒ aba. h. True or False: T ⇒ T . i. True or False: T ∗ ⇒ T . j. True or False: XXX ∗ ⇒ aba. k. True or False: X ∗ ⇒ aba. l. True or False: T ∗ ⇒ XX. m. True or False: T ∗ ⇒ XXX. n. True or False: S ∗ ⇒ ε. o. Give a description in English of L(G). 2.4 Give context-free grammars that generate the following languages. In all parts, the alphabet Σ is {0,1}. Aa. {w| w contains at least three 1s} b. {w| w starts and ends with the same symbol} c. {w| the length of w is odd} Ad. {w| the length of w is odd and its middle symbol is a 0} e. {w| w = wR, that is, w is a palindrome} f. The empty set 2.5 Give informal descriptions and state diagrams of pushdown automata for the lan- guages in Exercise 2.4. 2.6 Give context-free grammars generating the following languages. Aa. The set of strings over the alphabet {a,b} with more a’s than b’s b. The complement of the language {anb n| n ≥ 0} Ac. {w#x| wR is a substring of x for w, x ∈ {0,1} ∗} d. {x1#x2# · · · #xk| k ≥ 1, each xi ∈ {a, b} ∗, and for some i and j, xi = xR j } A2.7 Give informal English descriptions of PDAs for the languages in Exercise 2.6. A2.8 Show that the string the girl touches the boy with the flower has two different leftmost derivations in grammar G2 on page 103. Describe in English the two different meanings of this sentence. 2.9 Give a context-free grammar that generates the language A = {aib j c k| i = j or j = k where i, j, k ≥ 0}. Is your grammar ambiguous? Why or why not? 2.10 Give an informal description of a pushdown automaton that recognizes the lan- guage A in Exercise 2.9. 2.11 Convert the CFG G4 given in Exercise 2.1 to an equivalent PDA, using the proce- dure given in Theorem 2.20. Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 156 CHAPTER 2 / CONTEXT-FREE LANGUAGES 2.12 Convert the CFG G given in Exercise 2.3 to an equivalent PDA, using the procedure given in Theorem 2.20. 2.13 Let G = (V, Σ, R, S) be the following grammar. V = {S, T, U }; Σ = {0, #}; and R is the set of rules: S → T T | U T → 0T | T 0 | # U → 0U 00 | # a. Describe L(G) in English. b. Prove that L(G) is not regular. 2.14 Convert the following CFG into an equivalent CFG in Chomsky normal form, using the procedure given in Theorem 2.9. A → BAB | B | ε B → 00 | ε 2.15 Give a counterexample to show that the following construction fails to prove that the class of context-free languages is closed under star. Let A be a CFL that is generated by the CFG G = (V, Σ, R, S). Add the new rule S → SS and call the resulting grammar G ′. This grammar is supposed to generate A∗. 2.16 Show that the class of context-free languages is closed under the regular operations, union, concatenation, and star. 2.17 Use the results of Exercise 2.16 to give another proof that every regular language is context free, by showing how to convert a regular expression directly to an equiv- alent context-free grammar. PROBLEMS 157 ⋆2.23 Let D = {xy|x, y ∈ {0,1} ∗ and |x| = |y| but x ̸= y}. Show that D is a context-free language. ⋆2.24 Let E = {aib j | i ̸= j and 2i ̸= j}. Show that E is a context-free language. 2.25 For any language A, let SUFFIX (A) = {v| uv ∈ A for some string u}. Show that the class of context-free languages is closed under the SUFFIX operation. 2.26 Show that if G is a CFG in Chomsky normal form, then for any string w ∈ L(G) of length n ≥ 1, exactly 2n − 1 steps are required for any derivation of w. ⋆2.27 Let G = (V, Σ, R, ⟨STMT⟩) be the following grammar. ⟨STMT⟩ → ⟨ASSIGN⟩ | ⟨IF-THEN⟩ | ⟨IF-THEN-ELSE⟩ ⟨IF-THEN⟩ → if condition then ⟨STMT⟩ ⟨IF-THEN-ELSE⟩ → if condition then ⟨STMT⟩ else ⟨STMT⟩ ⟨ASSIGN⟩ → a:=1 Σ = {if, condition, then, else, a:=1} V = {⟨STMT⟩, ⟨IF-THEN⟩, ⟨IF-THEN-ELSE⟩, ⟨ASSIGN⟩} G is a natural-looking grammar for a fragment of a programming language, but G is ambiguous. a. Show that G is ambiguous. b. Give a new unambiguous grammar for the same language. ⋆2.28 Give unambiguous CFGs for the following languages. a. {w| in every preﬁx of w the number of a’s is at least the number of b’s} b. {w| the number of a’s and the number of b’s in w are equal} c. {w| the number of a’s is at least the number of b’s in w} ⋆2.29 Show that the language A in Exercise 2.9 is inherently ambiguous. 2.30 Use the pumping lemma to show that the following languages are not context free. a. {0n1 n0 n1 n| n ≥ 0} Ab. {0n#02n#0 3n| n ≥ 0} Ac. {w#t| w is a substring of t, where w, t ∈ {a, b} ∗} d. {t1#t2# · · · #tk| k ≥ 2, each ti ∈ {a, b} ∗, and ti = tj for some i ̸= j} 2.31 Let B be the language of all palindromes over {0,1} containing equal numbers of 0s and 1s. Show that B is not context free. 2.32 Let Σ = {1, 2, 3, 4} and C = {w ∈ Σ ∗| in w, the number of 1s equals the number of 2s, and the number of 3s equals the number of 4s}. Show that C is not context free. ⋆2.33 Show that F = {a ib j | i = kj for some positive integer k} is not context free. 2.34 Consider the language B = L(G), where G is the grammar given in Exercise 2.13. The pumping lemma for context-free languages, Theorem 2.34, states the exis- tence of a pumping length p for B. What is the minimum value of p that works in the pumping lemma? Justify your answer. 2.35 Let G be a CFG in Chomsky normal form that contains b variables. Show that if G generates some string with a derivation having at least 2 b steps, L(G) is inﬁnite. Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 158 CHAPTER 2 / CONTEXT-FREE LANGUAGES 2.36 Give an example of a language that is not context free but that acts like a CFL in the pumping lemma. Prove that your example works. (See the analogous example for regular languages in Problem 1.54.) ⋆2.37 Prove the following stronger form of the pumping lemma, wherein both pieces v and y must be nonempty when the string s is broken up. If A is a context-free language, then there is a number k where, if s is any string in A of length at least k, then s may be divided into ﬁve pieces, s = uvxyz, satisfying the conditions: a. for each i ≥ 0, uvixyiz ∈ A, b. v ̸= ε and y ̸= ε, and c. |vxy| ≤ k. A2.38 Refer to Problem 1.41 for the deﬁnition of the perfect shufﬂe operation. Show that the class of context-free languages is not closed under perfect shufﬂe. 2.39 Refer to Problem 1.42 for the deﬁnition of the shufﬂe operation. Show that the class of context-free languages is not closed under shufﬂe. ⋆2.40 Say that a language is preﬁx-closed if all preﬁxes of every string in the language are also in the language. Let C be an inﬁnite, preﬁx-closed, context-free language. Show that C contains an inﬁnite regular subset. ⋆2.41 Read the deﬁnitions of NOPREFIX (A) and NOEXTEND(A) in Problem 1.40. a. Show that the class of CFLs is not closed under NOPREFIX . b. Show that the class of CFLs is not closed under NOEXTEND. ⋆2.42 Let Y = {w| w = t1#t2# · · · #tk for k ≥ 0, each ti ∈ 1∗, and ti ̸= tj whenever i ̸= j}. Here Σ = {1, #}. Prove that Y is not context free. 2.43 For strings w and t, write w ⊜ t if the symbols of w are a permutation of the symbols of t. In other words, w ⊜ t if t and w have the same symbols in the same quantities, but possibly in a different order. For any string w, deﬁne SCRAMBLE(w) = {t| t ⊜ w}. For any language A, let SCRAMBLE(A) = {t| t ∈ SCRAMBLE(w) for some w ∈ A}. a. Show that if Σ = {0,1}, then the SCRAMBLE of a regular language is con- text free. b. What happens in part (a) if Σ contains three or more symbols? Prove your answer. 2.44 If A and B are languages, deﬁne A ⋄ B = {xy| x ∈ A and y ∈ B and |x| = |y|}. Show that if A and B are regular languages, then A ⋄ B is a CFL. ⋆2.45 Let A = {wtwR| w, t ∈ {0,1} ∗ and |w| = |t|}. Prove that A is not a CFL. 2.46 Consider the following CFG G: S → SS | T T → aT b | ab Describe L(G) and show that G is ambiguous. Give an unambiguous grammar H where L(H) = L(G) and sketch a proof that H is unambiguous. Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. PROBLEMS 159 2.47 Let Σ = {0,1} and let B be the collection of strings that contain at least one 1 in their second half. In other words, B = {uv| u ∈ Σ ∗, v ∈ Σ ∗1Σ ∗ and |u| ≥ |v|}. a. Give a PDA that recognizes B. b. Give a CFG that generates B. 2.48 Let Σ = {0,1}. Let C1 be the language of all strings that contain a 1 in their middle third. Let C2 be the language of all strings that contain two 1s in their middle third. So C1 = {xyz| x, z ∈ Σ ∗ and y ∈ Σ ∗1Σ ∗, where |x| = |z| ≥ |y|} and C2 = {xyz| x, z ∈ Σ ∗ and y ∈ Σ ∗1Σ ∗1Σ ∗, where |x| = |z| ≥ |y|}. a. Show that C1 is a CFL. b. Show that C2 is not a CFL. ⋆2.49 We deﬁned the rotational closure of language A to be RC(A) = {yx| xy ∈ A}. Show that the class of CFLs is closed under rotational closure. ⋆2.50 We deﬁned the CUT of language A to be CUT(A) = {yxz| xyz ∈ A}. Show that the class of CFLs is not closed under CUT. 2.51 Show that every DCFG is an unambiguous CFG. A⋆2.52 Show that every DCFG generates a preﬁx-free language. ⋆2.53 Show that the class of DCFLs is not closed under the following operations: a. Union b. Intersection c. Concatenation d. Star e. Reversal 2.54 Let G be the following grammar: S → T⊣⊣⊣ T → T aT b | T bT a | ε a. Show that L(G) = {w⊣⊣⊣| w contains equal numbers of a’s and b’s}. Use a proof by induction on the length of w. b. Use the DK -test to show that G is a DCFG. c. Describe a DPDA that recognizes L(G). 2.55 Let G1 be the following grammar that we introduced in Example 2.45. Use the DK-test to show that G1 is not a DCFG. R → S | T S → aSb | ab T → aT bb | abb Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 160 CHAPTER 2 / CONTEXT-FREE LANGUAGES ⋆2.56 Let A = L(G1) where G1 is deﬁned in Problem 2.55. Show that A is not a DCFL. (Hint: Assume that A is a DCFL and consider its DPDA P . Modify P so that its input alphabet is {a, b, c}. When it ﬁrst enters an accept state, it pretends that c’s are b’s in the input from that point on. What language would the modiﬁed P accept?) ⋆2.57 Let B = {aibjc k| i, j, k ≥ 0 and i = j or i = k}. Prove that B is not a DCFL. ⋆2.58 Let C = {wwR| w ∈ {0,1} ∗}. Prove that C is not a DCFL. (Hint: Suppose that when some DPDA P is started in state q with symbol x on the top of its stack, P never pops its stack below x, no matter what input string P reads from that point on. In that case, the contents of P ’s stack at that point cannot affect its subsequent behavior, so P ’s subsequent behavior can depend only on q and x.) ⋆2.59 If we disallow ε-rules in CFGs, we can simplify the DK-test. In the simpliﬁed test, we only need to check that each of DK’s accept states has a single rule. Prove that a CFG without ε-rules passes the simpliﬁed DK-test iff it is a DCFG. SELECTED SOLUTIONS 161 2.8 Here is one derivation: ⟨SENTENCE⟩ → ⟨NOUN-PHRASE⟩⟨VERB-PHRASE⟩ → ⟨CMPLX-NOUN⟩⟨VERB-PHRASE⟩ → ⟨ARTICLE⟩⟨NOUN⟩⟨VERB-PHRASE⟩ → The ⟨NOUN⟩⟨VERB-PHRASE⟩ → The girl ⟨VERB-PHRASE⟩ → The girl ⟨CMPLX-VERB⟩⟨PREP-PHRASE⟩ → The girl ⟨VERB⟩⟨NOUN-PHRASE⟩⟨PREP-PHRASE⟩ → The girl touches ⟨NOUN-PHRASE⟩⟨PREP-PHRASE⟩ → The girl touches ⟨CMPLX-NOUN⟩⟨PREP-PHRASE⟩ → The girl touches ⟨ARTICLE⟩⟨NOUN⟩⟨PREP-PHRASE⟩ → The girl touches the ⟨NOUN⟩⟨PREP-PHRASE⟩ → The girl touches the boy ⟨PREP-PHRASE⟩ → The girl touches the boy ⟨PREP⟩⟨CMPLX-NOUN⟩ → The girl touches the boy with ⟨CMPLX-NOUN⟩ → The girl touches the boy with ⟨ARTICLE⟩⟨NOUN⟩ → The girl touches the boy with the ⟨NOUN⟩ → The girl touches the boy with the flower Here is another leftmost derivation: ⟨SENTENCE⟩ → ⟨NOUN-PHRASE⟩⟨VERB-PHRASE⟩ → ⟨CMPLX-NOUN⟩⟨VERB-PHRASE⟩ → ⟨ARTICLE⟩⟨NOUN⟩⟨VERB-PHRASE⟩ → The ⟨NOUN⟩⟨VERB-PHRASE⟩ → The girl ⟨VERB-PHRASE⟩ → The girl ⟨CMPLX-VERB⟩ → The girl ⟨VERB⟩⟨NOUN-PHRASE⟩ → The girl touches ⟨NOUN-PHRASE⟩ → The girl touches ⟨CMPLX-NOUN⟩⟨PREP-PHRASE⟩ → The girl touches ⟨ARTICLE⟩⟨NOUN⟩⟨PREP-PHRASE⟩ → The girl touches the ⟨NOUN⟩⟨PREP-PHRASE⟩ → The girl touches the boy ⟨PREP-PHRASE⟩ → The girl touches the boy ⟨PREP⟩⟨CMPLX-NOUN⟩ → The girl touches the boy with ⟨CMPLX-NOUN⟩ → The girl touches the boy with ⟨ARTICLE⟩⟨NOUN⟩ → The girl touches the boy with the ⟨NOUN⟩ → The girl touches the boy with the flower Each of these derivations corresponds to a different English meaning. In the ﬁrst derivation, the sentence means that the girl used the ﬂower to touch the boy. In the second derivation, the boy is holding the ﬂower when the girl touches her. 2.18 (a) Let C be a context-free language and R be a regular language. Let P be the PDA that recognizes C, and D be the DFA that recognizes R. If Q is the set of states of P and Q′ is the set of states of D, we construct a PDA P ′ that recognizes C ∩ R with the set of states Q × Q′. P ′ will do what P does and also keep track of the states of D. It accepts a string w if and only if it stops at a state q ∈ FP × FD, where FP is the set of accept states of P and FD is the set of accept states of D. Since C ∩ R is recognized by P ′, it is context free. (b) Let R be the regular language a ∗b ∗c ∗. If A were a CFL then A ∩ R would be a CFL by part (a). However, A ∩ R = {a nb nc n| n ≥ 0}, and Example 2.36 proves that A ∩ R is not context free. Thus A is not a CFL. Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 162 CHAPTER 2 / CONTEXT-FREE LANGUAGES 2.30 (b) Let B = {0n#0 2n#0 3n| n ≥ 0}. Let p be the pumping length given by the pumping lemma. Let s = 0p#0 2p#0 3p. We show that s = uvxyz cannot be pumped. Neither v nor y can contain #, otherwise uv2xy2z contains more than two #s. Therefore, if we divide s into three segments by #’s: 0 p, 0 2p, and 03p, at least one of the segments is not contained within either v or y. Hence uv2xy2z is not in B because the 1 : 2 : 3 length ratio of the segments is not maintained. (c) Let C = {w#t| w is a substring of t, where w, t ∈ {a, b} ∗}. Let p be the pumping length given by the pumping lemma. Let s = a pbp#a pbp. We show that the string s = uvxyz cannot be pumped. Neither v nor y can contain #, otherwise uv0xy0z does not contain # and therefore is not in C. If both v and y occur on the left-hand side of the #, the string uv2xy2z cannot be in C because it is longer on the left-hand side of the #. Similarly, if both strings occur on the right-hand side of the #, the string uv0xy0z cannot be in C because it is again longer on the left-hand side of the #. If one of v and y is empty (both cannot be empty), treat them as if both occurred on the same side of the # as above. The only remaining case is where both v and y are nonempty and straddle the #. But then v consists of b’s and y consists of a’s because of the third pumping lemma condition |vxy| ≤ p. Hence, uv2xy2z contains more b’s on the left-hand side of the #, so it cannot be a member of C. 2.38 Let A be the language {0k1 k| k ≥ 0} and let B be the language {akb 3k| k ≥ 0}. The perfect shufﬂe of A and B is the language C = {(0a)k(0b)k(1b)2k| k ≥ 0}. Languages A and B are easily seen to be CFLs, but C is not a CFL, as follows. If C were a CFL, let p be the pumping length given by the pumping lemma, and let s be the string (0a)p(0b)p(1b)2p. Because s is longer than p and s ∈ C, we can divide s = uvxyz satisfying the pumping lemma’s three conditions. Strings in C are exactly one-fourth 1s and one-eighth a’s. In order for uv2xy2z to have that property, the string vxy must contain both 1s and a’s. But that is impossible, because the 1s and a’s are separated by 2p symbols in s yet the third condition says that |vxy| ≤ p. Hence C is not context free. 2.52 We use a proof by contradiction. Assume that w and wz are two unequal strings in L(G), where G is a DCFG. Both are valid strings so both have handles, and these handles must agree because we can write w = xhy and wz = xhyz = xhˆy where h is the handle of w. Hence, the ﬁrst reduce steps of w and wz produce valid strings u and uz, respectively. We can continue this process until we obtain S1 and S1z where S1 is the start variable. However, S1 does not appear on the right-hand side of any rule so we cannot reduce S1z. That gives a contradiction. Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. P A R T T W O Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 166 CHAPTER 3 / THE CHURCH---TURING THESIS Initially the tape contains only the input string and is blank everywhere else. If the machine needs to store information, it may write this information on the tape. To read the information that it has written, the machine can move its head back over it. The machine continues computing until it decides to produce an output. The outputs accept and reject are obtained by entering designated accepting and rejecting states. If it doesn’t enter an accepting or a rejecting state, it will go on forever, never halting. FIGURE 3.1 Schematic of a Turing machine The following list summarizes the differences between ﬁnite automata and Turing machines. 1. A Turing machine can both write on the tape and read from it. 2. The read–write head can move both to the left and to the right. 3. The tape is inﬁnite. 4. The special states for rejecting and accepting take effect immediately. Let’s introduce a Turing machine M1 for testing membership in the language B = {w#w| w ∈ {0,1}∗}. We want M1 to accept if its input is a member of B and to reject otherwise. To understand M1 better, put yourself in its place by imagining that you are standing on a mile-long input consisting of millions of characters. Your goal is to determine whether the input is a member of B—that is, whether the input comprises two identical strings separated by a # symbol. The input is too long for you to remember it all, but you are allowed to move back and forth over the input and make marks on it. The obvious strategy is to zig-zag to the corresponding places on the two sides of the # and determine whether they match. Place marks on the tape to keep track of which places correspond. We design M1 to work in that way. It makes multiple passes over the input string with the read–write head. On each pass it matches one of the characters on each side of the # symbol. To keep track of which symbols have been checked already, M1 crosses off each symbol as it is examined. If it crosses off all the symbols, that means that everything matched successfully, and M1 goes into an accept state. If it discovers a mismatch, it enters a reject state. In summary, M1’s algorithm is as follows. Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 3.1 TURING MACHINES 167 M1 = “On input string w: 1. Zig-zag across the tape to corresponding positions on either side of the # symbol to check whether these positions contain the same symbol. If they do not, or if no # is found, reject . Cross off symbols as they are checked to keep track of which symbols correspond. 2. When all symbols to the left of the # have been crossed off, check for any remaining symbols to the right of the #. If any symbols remain, reject ; otherwise, accept .” The following ﬁgure contains several nonconsecutive snapshots of M1’s tape after it is started on input 011000#011000. FIGURE 3.2 Snapshots of Turing machine M1 computing on input 011000#011000 This description of Turing machine M1 sketches the way it functions but does not give all its details. We can describe Turing machines in complete detail by giving formal descriptions analogous to those introduced for ﬁnite and push- down automata. The formal descriptions specify each of the parts of the formal deﬁnition of the Turing machine model to be presented shortly. In actuality, we almost never give formal descriptions of Turing machines because they tend to be very big. FORMAL DEFINITION OF A TURING MACHINE The heart of the deﬁnition of a Turing machine is the transition function δ be- cause it tells us how the machine gets from one step to the next. For a Turing machine, δ takes the form: Q×Γ −→ Q×Γ×{L, R}. That is, when the machine Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 168 CHAPTER 3 / THE CHURCH---TURING THESIS is in a certain state q and the head is over a tape square containing a symbol a, and if δ(q, a) = (r, b, L), the machine writes the symbol b replacing the a, and goes to state r. The third component is either L or R and indicates whether the head moves to the left or right after writing. In this case, the L indicates a move to the left. 3.1 TURING MACHINES 169 FIGURE 3.4 A Turing machine with conﬁguration 1011q701111 Here we formalize our intuitive understanding of the way that a Turing ma- chine computes. Say that conﬁguration C1 yields conﬁguration C2 if the Turing machine can legally go from C1 to C2 in a single step. We deﬁne this notion formally as follows. Suppose that we have a, b, and c in Γ, as well as u and v in Γ∗ and states qi and qj. In that case, ua qi bv and u qj acv are two conﬁgurations. Say that ua qi bv yields u qj acv if in the transition function δ(qi, b) = (qj, c, L). That handles the case where the Turing machine moves leftward. For a rightward move, say that ua qi bv yields uac qj v if δ(qi, b) = (qj, c, R). Special cases occur when the head is at one of the ends of the conﬁguration. For the left-hand end, the conﬁguration qi bv yields qj cv if the transition is left- moving (because we prevent the machine from going off the left-hand end of the tape), and it yields c qjv for the right-moving transition. For the right-hand end, the conﬁguration ua qi is equivalent to ua qi ␣ because we assume that blanks follow the part of the tape represented in the conﬁguration. Thus we can handle this case as before, with the head no longer at the right-hand end. The start conﬁguration of M on input w is the conﬁguration q0 w, which indicates that the machine is in the start state q0 with its head at the leftmost position on the tape. In an accepting conﬁguration, the state of the conﬁguration is qaccept. In a rejecting conﬁguration, the state of the conﬁguration is qreject. Accepting and rejecting conﬁgurations are halting conﬁgurations and do not yield further conﬁgurations. Because the machine is deﬁned to halt when in the states qaccept and qreject, we equivalently could have deﬁned the transition function to have the more complicated form δ : Q′ × Γ−→ Q × Γ × {L, R}, where Q′ is Q without qaccept and qreject. A Turing machine M accepts input w if a sequence of conﬁgurations C1, C2, . . . , Ck exists, where 1. C1 is the start conﬁguration of M on input w, 2. each Ci yields Ci+1, and 3. Ck is an accepting conﬁguration. Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 170 CHAPTER 3 / THE CHURCH---TURING THESIS The collection of strings that M accepts is the language of M , or the lan- guage recognized by M , denoted L(M ). 3.1 TURING MACHINES 171 will give only higher level descriptions because they are precise enough for our purposes and are much easier to understand. Nevertheless, it is important to remember that every higher level description is actually just shorthand for its formal counterpart. With patience and care we could describe any of the Turing machines in this book in complete formal detail. To help you make the connection between the formal descriptions and the higher level descriptions, we give state diagrams in the next two examples. You may skip over them if you already feel comfortable with this connection. EXAMPLE 3.7 172 CHAPTER 3 / THE CHURCH---TURING THESIS FIGURE 3.8 State diagram for Turing machine M2 In this state diagram, the label 0→␣,R appears on the transition from q1 to q2. This label signiﬁes that when in state q1 with the head reading 0, the machine goes to state q2, writes ␣, and moves the head to the right. In other words, δ(q1,0) = (q2,␣,R). For clarity we use the shorthand 0→R in the transition from q3 to q4, to mean that the machine moves to the right when reading 0 in state q3 but doesn’t alter the tape, so δ(q3,0) = (q4,0,R). This machine begins by writing a blank symbol over the leftmost 0 on the tape so that it can ﬁnd the left-hand end of the tape in stage 4. Whereas we would normally use a more suggestive symbol such as # for the left-hand end delimiter, we use a blank here to keep the tape alphabet, and hence the state diagram, small. Example 3.11 gives another method of ﬁnding the left-hand end of the tape. Next we give a sample run of this machine on input 0000. The starting con- ﬁguration is q10000. The sequence of conﬁgurations the machine enters appears as follows; read down the columns and left to right. q10000 ␣q5x0x␣ ␣xq5xx␣ ␣q2000 q5␣x0x␣ ␣q5xxx␣ ␣xq300 ␣q2x0x␣ q5␣xxx␣ ␣x0q40 ␣xq20x␣ ␣q2xxx␣ ␣x0xq3␣ ␣xxq3x␣ ␣xq2xx␣ ␣x0q5x␣ ␣xxxq3␣ ␣xxq2x␣ ␣xq50x␣ ␣xxq5x␣ ␣xxxq2␣ ␣xxx␣qaccept 3.1 TURING MACHINES 173 EXAMPLE 3.9 174 CHAPTER 3 / THE CHURCH---TURING THESIS EXAMPLE 3.11 3.1 TURING MACHINES 175 EXAMPLE 3.12 176 CHAPTER 3 / THE CHURCH---TURING THESIS3.2 VARIANTS OF TURING MACHINES 177 THEOREM 3.13 178 CHAPTER 3 / THE CHURCH---TURING THESIS COROLLARY 3.15 3.2 VARIANTS OF TURING MACHINES 179 PROOF The simulating deterministic TM D has three tapes. By Theo- rem 3.13, this arrangement is equivalent to having a single tape. The machine D uses its three tapes in a particular way, as illustrated in the following ﬁgure. Tape 1 always contains the input string and is never altered. Tape 2 maintains a copy of N ’s tape on some branch of its nondeterministic computation. Tape 3 keeps track of D’s location in N ’s nondeterministic computation tree. FIGURE 3.17 Deterministic TM D simulating nondeterministic TM N Let’s ﬁrst consider the data representation on tape 3. Every node in the tree can have at most b children, where b is the size of the largest set of possible choices given by N ’s transition function. To every node in the tree we assign an address that is a string over the alphabet Γb = {1, 2, . . . , b}. We assign the address 231 to the node we arrive at by starting at the root, going to its 2nd child, going to that node’s 3rd child, and ﬁnally going to that node’s 1st child. Each symbol in the string tells us which choice to make next when simulating a step in one branch in N ’s nondeterministic computation. Sometimes a symbol may not correspond to any choice if too few choices are available for a conﬁguration. In that case, the address is invalid and doesn’t correspond to any node. Tape 3 contains a string over Γb. It represents the branch of N ’s computation from the root to the node addressed by that string unless the address is invalid. The empty string is the address of the root of the tree. Now we are ready to describe D. 1. Initially, tape 1 contains the input w, and tapes 2 and 3 are empty. 2. Copy tape 1 to tape 2 and initialize the string on tape 3 to be ε. 3. Use tape 2 to simulate N with input w on one branch of its nondeterminis- tic computation. Before each step of N , consult the next symbol on tape 3 to determine which choice to make among those allowed by N ’s transition function. If no more symbols remain on tape 3 or if this nondeterministic choice is invalid, abort this branch by going to stage 4. Also go to stage 4 if a rejecting conﬁguration is encountered. If an accepting conﬁguration is encountered, accept the input. 4. Replace the string on tape 3 with the next string in the string ordering. Simulate the next branch of N ’s computation by going to stage 2. 180 CHAPTER 3 / THE CHURCH---TURING THESIS COROLLARY 3.18 3.2 VARIANTS OF TURING MACHINES 181 An enumerator E starts with a blank input on its work tape. If the enumerator doesn’t halt, it may print an inﬁnite list of strings. The language enumerated by E is the collection of all the strings that it eventually prints out. Moreover, E may generate the strings of the language in any order, possibly with repetitions. Now we are ready to develop the connection between enumerators and Turing- recognizable languages. THEOREM 3.21 182 CHAPTER 3 / THE CHURCH---TURING THESIS To understand this phenomenon, consider the analogous situation for pro- gramming languages. Many, such as Pascal and LISP, look quite different from one another in style and structure. Can some algorithm be programmed in one of them and not the others? Of course not—we can compile LISP into Pascal and Pascal into LISP, which means that the two languages describe exactly the same class of algorithms. So do all other reasonable programming languages. The widespread equivalence of computational models holds for precisely the same reason. Any two computational models that satisfy certain reasonable re- quirements can simulate one another and hence are equivalent in power. This equivalence phenomenon has an important philosophical corollary. Even though we can imagine many different computational models, the class of algorithms that they describe remains the same. Whereas each individual computational model has a certain arbitrariness to its deﬁnition, the underlying class of algorithms that it describes is natural because the other models arrive at the same, unique class. This phenomenon has had profound implications for mathematics, as we show in the next section. 3.3 THE DEFINITION OF ALGORITHM 183 constant, called a coefﬁcient. For example, 6 · x · x · x · y · z · z = 6x 3yz2 is a term with coefﬁcient 6, and 6x 3yz2 + 3xy2 − x 3 − 10 is a polynomial with four terms, over the variables x, y, and z. For this discus- sion, we consider only coefﬁcients that are integers. A root of a polynomial is an assignment of values to its variables so that the value of the polynomial is 0. This polynomial has a root at x = 5, y = 3, and z = 0. This root is an integral root because all the variables are assigned integer values. Some polynomials have an integral root and some do not. Hilbert’s tenth problem was to devise an algorithm that tests whether a poly- nomial has an integral root. He did not use the term algorithm but rather “a process according to which it can be determined by a ﬁnite number of oper- ations.”4 Interestingly, in the way he phrased this problem, Hilbert explicitly asked that an algorithm be “devised.” Thus he apparently assumed that such an algorithm must exist—someone need only ﬁnd it. As we now know, no algorithm exists for this task; it is algorithmically unsolv- able. For mathematicians of that period to come to this conclusion with their intuitive concept of algorithm would have been virtually impossible. The intu- itive concept may have been adequate for giving algorithms for certain tasks, but it was useless for showing that no algorithm exists for a particular task. Proving that an algorithm does not exist requires having a clear deﬁnition of algorithm. Progress on the tenth problem had to wait for that deﬁnition. The deﬁnition came in the 1936 papers of Alonzo Church and Alan Tur- ing. Church used a notational system called the λ-calculus to deﬁne algorithms. Turing did it with his “machines.” These two deﬁnitions were shown to be equivalent. This connection between the informal notion of algorithm and the precise deﬁnition has come to be called the Church–Turing thesis. The Church–Turing thesis provides the deﬁnition of algorithm necessary to resolve Hilbert’s tenth problem. In 1970, Yuri Matijasevi˘c, building on the work of Martin Davis, Hilary Putnam, and Julia Robinson, showed that no algorithm exists for testing whether a polynomial has integral roots. In Chapter 4 we de- velop the techniques that form the basis for proving that this and other problems are algorithmically unsolvable. 184 CHAPTER 3 / THE CHURCH---TURING THESIS Let’s phrase Hilbert’s tenth problem in our terminology. Doing so helps to introduce some themes that we explore in Chapters 4 and 5. Let D = {p| p is a polynomial with an integral root}. Hilbert’s tenth problem asks in essence whether the set D is decidable. The answer is negative. In contrast, we can show that D is Turing-recognizable. Before doing so, let’s consider a simpler problem. It is an analog of Hilbert’s tenth problem for polynomials that have only a single variable, such as 4x 3 − 2x 2 + x − 7. Let D1 = {p| p is a polynomial over x with an integral root}. Here is a TM M1 that recognizes D1: M1 = “On input ⟨p⟩: where p is a polynomial over the variable x. 1. Evaluate p with x set successively to the values 0, 1, −1, 2, −2, 3, −3, . . . . If at any point the polynomial evaluates to 0, accept .” If p has an integral root, M1 eventually will ﬁnd it and accept. If p does not have an integral root, M1 will run forever. For the multivariable case, we can present a similar TM M that recognizes D. Here, M goes through all possible settings of its variables to integral values. Both M1 and M are recognizers but not deciders. We can convert M1 to be a decider for D1 because we can calculate bounds within which the roots of a single variable polynomial must lie and restrict the search to these bounds. In Problem 3.21 you are asked to show that the roots of such a polynomial must lie between the values ± k cmax 3.3 THE DEFINITION OF ALGORITHM 185 such algorithms? Students commonly ask this question, especially when prepar- ing solutions to exercises and problems. Let’s entertain three possibilities. The ﬁrst is the formal description that spells out in full the Turing machine’s states, transition function, and so on. It is the lowest, most detailed level of description. The second is a higher level of description, called the implementation descrip- tion, in which we use English prose to describe the way that the Turing machine moves its head and the way that it stores data on its tape. At this level we do not give details of states or transition function. The third is the high-level description, wherein we use English prose to describe an algorithm, ignoring the implemen- tation details. At this level we do not need to mention how the machine manages its tape or head. In this chapter, we have given formal and implementation-level descriptions of various examples of Turing machines. Practicing with lower level Turing ma- chine descriptions helps you understand Turing machines and gain conﬁdence in using them. Once you feel conﬁdent, high-level descriptions are sufﬁcient. We now set up a format and notation for describing Turing machines. The in- put to a Turing machine is always a string. If we want to provide an object other than a string as input, we must ﬁrst represent that object as a string. Strings can easily represent polynomials, graphs, grammars, automata, and any combi- nation of those objects. A Turing machine may be programmed to decode the representation so that it can be interpreted in the way we intend. Our nota- tion for the encoding of an object O into its representation as a string is ⟨O⟩. If we have several objects O1, O2, . . . , Ok, we denote their encoding into a single string ⟨O1, O2, . . . , Ok⟩. The encoding itself can be done in many reasonable ways. It doesn’t matter which one we pick because a Turing machine can always translate one such encoding into another. In our format, we describe Turing machine algorithms with an indented seg- ment of text within quotes. We break the algorithm into stages, each usually involving many individual steps of the Turing machine’s computation. We indi- cate the block structure of the algorithm with further indentation. The ﬁrst line of the algorithm describes the input to the machine. If the input description is simply w, the input is taken to be a string. If the input description is the encod- ing of an object as in ⟨A⟩, the Turing machine ﬁrst implicitly tests whether the input properly encodes an object of the desired form and rejects it if it doesn’t. EXAMPLE 3.23 186 CHAPTER 3 / THE CHURCH---TURING THESIS The following is a high-level description of a TM M that decides A. M = “On input ⟨G⟩, the encoding of a graph G: 1. Select the ﬁrst node of G and mark it. 2. Repeat the following stage until no new nodes are marked: 3. For each node in G, mark it if it is attached by an edge to a node that is already marked. 4. Scan all the nodes of G to determine whether they all are marked. If they are, accept ; otherwise, reject .” For additional practice, let’s examine some implementation-level details of Turing machine M . Usually we won’t give this level of detail in the future and you won’t need to either, unless speciﬁcally requested to do so in an exercise. First, we must understand how ⟨G⟩ encodes the graph G as a string. Consider an encoding that is a list of the nodes of G followed by a list of the edges of G. Each node is a decimal number, and each edge is the pair of decimal numbers that represent the nodes at the two endpoints of the edge. The following ﬁgure depicts such a graph and its encoding. FIGURE 3.24 A graph G and its encoding ⟨G⟩ When M receives the input ⟨G⟩, it ﬁrst checks to determine whether the input is the proper encoding of some graph. To do so, M scans the tape to be sure that there are two lists and that they are in the proper form. The ﬁrst list should be a list of distinct decimal numbers, and the second should be a list of pairs of decimal numbers. Then M checks several things. First, the node list should contain no repetitions; and second, every node appearing on the edge list should also appear on the node list. For the ﬁrst, we can use the procedure given in Example 3.12 for TM M4 that checks element distinctness. A similar method works for the second check. If the input passes these checks, it is the encoding of some graph G. This veriﬁcation completes the input check, and M goes on to stage 1. For stage 1, M marks the ﬁrst node with a dot on the leftmost digit. For stage 2, M scans the list of nodes to ﬁnd an undotted node n1 and ﬂags it by marking it differently—say, by underlining the ﬁrst symbol. Then M scans the list again to ﬁnd a dotted node n2 and underlines it, too. Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. EXERCISES 187 Now M scans the list of edges. For each edge, M tests whether the two underlined nodes n1 and n2 are the ones appearing in that edge. If they are, M dots n1, removes the underlines, and goes on from the beginning of stage 2. If they aren’t, M checks the next edge on the list. If there are no more edges, {n1, n2} is not an edge of G. Then M moves the underline on n2 to the next dotted node and now calls this node n2. It repeats the steps in this paragraph to check, as before, whether the new pair {n1, n2} is an edge. If there are no more dotted nodes, n1 is not attached to any dotted nodes. Then M sets the underlines so that n1 is the next undotted node and n2 is the ﬁrst dotted node and repeats the steps in this paragraph. If there are no more undotted nodes, M has not been able to ﬁnd any new nodes to dot, so it moves on to stage 4. For stage 4, M scans the list of nodes to determine whether all are dotted. If they are, it enters the accept state; otherwise, it enters the reject state. This completes the description of TM M . 188 CHAPTER 3 / THE CHURCH---TURING THESIS A3.5 Examine the formal deﬁnition of a Turing machine to answer the following ques- tions, and explain your reasoning. a. Can a Turing machine ever write the blank symbol ␣ on its tape? b. Can the tape alphabet Γ be the same as the input alphabet Σ? c. Can a Turing machine’s head ever be in the same location in two successive steps? d. Can a Turing machine contain just a single state? 3.6 In Theorem 3.21, we showed that a language is Turing-recognizable iff some enu- merator enumerates it. Why didn’t we use the following simpler algorithm for the forward direction of the proof? As before, s1, s2, . . . is a list of all strings in Σ ∗. E = “Ignore the input. 1. Repeat the following for i = 1, 2, 3, . . . . 2. Run M on si. 3. If it accepts, print out si.” 3.7 Explain why the following is not a description of a legitimate Turing machine. Mbad = “On input ⟨p⟩, a polynomial over variables x1, . . . , xk: 1. Try all possible settings of x1, . . . , xk to integer values. 2. Evaluate p on all of these settings. 3. If any of these settings evaluates to 0, accept ; otherwise, reject .” 3.8 Give implementation-level descriptions of Turing machines that decide the follow- ing languages over the alphabet {0,1}. Aa. {w| w contains an equal number of 0s and 1s} b. {w| w contains twice as many 0s as 1s} c. {w| w does not contain twice as many 0s as 1s} PROBLEMS 189 3.11 A Turing machine with doubly inﬁnite tape is similar to an ordinary Turing ma- chine, but its tape is inﬁnite to the left as well as to the right. The tape is initially ﬁlled with blanks except for the portion that contains the input. Computation is deﬁned as usual except that the head never encounters an end to the tape as it moves leftward. Show that this type of Turing machine recognizes the class of Turing-recognizable languages. 3.12 A Turing machine with left reset is similar to an ordinary Turing machine, but the transition function has the form δ : Q × Γ−→ Q × Γ × {R, RESET}. If δ(q, a) = (r, b, RESET), when the machine is in state q reading an a, the ma- chine’s head jumps to the left-hand end of the tape after it writes b on the tape and enters state r. Note that these machines do not have the usual ability to move the head one symbol left. Show that Turing machines with left reset recognize the class of Turing-recognizable languages. 3.13 A Turing machine with stay put instead of left is similar to an ordinary Turing machine, but the transition function has the form δ : Q × Γ−→ Q × Γ × {R, S}. At each point, the machine can move its head right or let it stay in the same posi- tion. Show that this Turing machine variant is not equivalent to the usual version. What class of languages do these machines recognize? 3.14 A queue automaton is like a push-down automaton except that the stack is replaced by a queue. A queue is a tape allowing symbols to be written only on the left-hand end and read only at the right-hand end. Each write operation (we’ll call it a push) adds a symbol to the left-hand end of the queue and each read operation (we’ll call it a pull) reads and removes a symbol at the right-hand end. As with a PDA, the input is placed on a separate read-only input tape, and the head on the input tape can move only from left to right. The input tape contains a cell with a blank symbol following the input, so that the end of the input can be detected. A queue automaton accepts its input by entering a special accept state at any time. Show that a language can be recognized by a deterministic queue automaton iff the language is Turing-recognizable. 3.15 Show that the collection of decidable languages is closed under the operation of Aa. union. b. concatenation. c. star. d. complementation. e. intersection. 3.16 Show that the collection of Turing-recognizable languages is closed under the op- eration of Aa. union. b. concatenation. c. star. d. intersection. e. homomorphism. ⋆3.17 Let B = {⟨M1⟩, ⟨M2⟩, . . .} be a Turing-recognizable language consisting of TM descriptions. Show that there is a decidable language C consisting of TM descrip- tions such that every machine described in B has an equivalent machine in C and vice versa. Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 190 CHAPTER 3 / THE CHURCH---TURING THESIS ⋆3.18 Show that a language is decidable iff some enumerator enumerates the language in the standard string order. ⋆3.19 Show that every inﬁnite Turing-recognizable language has an inﬁnite decidable subset. ⋆3.20 Show that single-tape TMs that cannot write on the portion of the tape containing the input string recognize only regular languages. 3.21 Let c1xn + c2xn−1 + · · · + cnx + cn+1 be a polynomial with a root at x = x0. Let cmax be the largest absolute value of a ci. Show that |x0| < (n + 1) cmax SELECTED SOLUTIONS 191 3.8 (a) “On input string w: 1. Scan the tape and mark the ﬁrst 0 that has not been marked. If no unmarked 0 is found, go to stage 4. Otherwise, move the head back to the front of the tape. 2. Scan the tape and mark the ﬁrst 1 that has not been marked. If no unmarked 1 is found, reject . 3. Move the head back to the front of the tape and go to stage 1. 4. Move the head back to the front of the tape. Scan the tape to see if any unmarked 1s remain. If none are found, accept ; otherwise, reject .” 3.10 We ﬁrst simulate an ordinary Turing machine by a write-twice Turing machine. The write-twice machine simulates a single step of the original machine by copying the entire tape over to a fresh portion of the tape to the right-hand side of the currently used portion. The copying procedure operates character by character, marking a character as it is copied. This procedure alters each tape square twice: once to write the character for the ﬁrst time, and again to mark that it has been copied. The position of the original Turing machine’s tape head is marked on the tape. When copying the cells at or adjacent to the marked position, the tape content is updated according to the rules of the original Turing machine. To carry out the simulation with a write-once machine, operate as before, except that each cell of the previous tape is now represented by two cells. The ﬁrst of these contains the original machine’s tape symbol and the second is for the mark used in the copying procedure. The input is not presented to the machine in the format with two cells per symbol, so the very ﬁrst time the tape is copied, the copying marks are put directly over the input symbols. 3.15 (a) For any two decidable languages L1 and L2, let M1 and M2 be the TMs that decide them. We construct a TM M ′ that decides the union of L1 and L2: “On input w: 1. Run M1 on w. If it accepts, accept . 2. Run M2 on w. If it accepts, accept . Otherwise, reject .” M ′ accepts w if either M1 or M2 accepts it. If both reject, M ′ rejects. 3.16 (a) For any two Turing-recognizable languages L1 and L2, let M1 and M2 be the TMs that recognize them. We construct a TM M ′ that recognizes the union of L1 and L2: “On input w: 1. Run M1 and M2 alternately on w step by step. If either accepts, accept . If both halt and reject, reject .” If either M1 or M2 accepts w, M ′ accepts w because the accepting TM arrives to its accepting state after a ﬁnite number of steps. Note that if both M1 and M2 reject and either of them does so by looping, then M ′ will loop. 3.22 The language A is one of the two languages {0} or {1}. In either case, the language is ﬁnite and hence decidable. If you aren’t able to determine which of these two languages is A, you won’t be able to describe the decider for A. However, you can give two Turing machines, one of which is A’s decider. Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 194 CHAPTER 4 / DECIDABILITY4.1 DECIDABLE LANGUAGES 195 PROOF IDEA We simply need to present a TM M that decides ADFA. M = “On input ⟨B, w⟩, where B is a DFA and w is a string: 1. Simulate B on input w. 2. If the simulation ends in an accept state, accept . If it ends in a nonaccepting state, reject .” PROOF We mention just a few implementation details of this proof. For those of you familiar with writing programs in any standard programming lan- guage, imagine how you would write a program to carry out the simulation. First, let’s examine the input ⟨B, w⟩. It is a representation of a DFA B together with a string w. One reasonable representation of B is simply a list of its ﬁve components: Q, Σ, δ, q0, and F . When M receives its input, M ﬁrst determines whether it properly represents a DFA B and a string w. If not, M rejects. Then M carries out the simulation directly. It keeps track of B’s current state and B’s current position in the input w by writing this information down on its tape. Initially, B’s current state is q0 and B’s current input position is the leftmost symbol of w. The states and position are updated according to the speciﬁed transition function δ. When M ﬁnishes processing the last symbol of w, M accepts the input if B is in an accepting state; M rejects the input if B is in a nonaccepting state. 196 CHAPTER 4 / DECIDABILITY Similarly, we can determine whether a regular expression generates a given string. Let AREX = {⟨R, w⟩| R is a regular expression that generates string w}. THEOREM 4.3 4.1 DECIDABLE LANGUAGES 197 The next theorem states that determining whether two DFAs recognize the same language is decidable. Let EQ DFA = {⟨A, B⟩| A and B are DFAs and L(A) = L(B)}. THEOREM 4.5 198 CHAPTER 4 / DECIDABILITY DECIDABLE PROBLEMS CONCERNING CONTEXT-FREE LANGUAGES Here, we describe algorithms to determine whether a CFG generates a particular string and to determine whether the language of a CFG is empty. Let ACFG = {⟨G, w⟩| G is a CFG that generates string w}. THEOREM 4.7 4.1 DECIDABLE LANGUAGES 199 Recall that we have given procedures for converting back and forth between CFGs and PDAs in Theorem 2.20. Hence everything we say about the decidability of problems concerning CFGs applies equally well to PDAs. Let’s turn now to the emptiness testing problem for the language of a CFG. As we did for DFAs, we can show that the problem of determining whether a CFG generates any strings at all is decidable. Let ECFG = {⟨G⟩| G is a CFG and L(G) = ∅}. THEOREM 4.8 200 CHAPTER 4 / DECIDABILITY Next, we consider the problem of determining whether two context-free grammars generate the same language. Let EQ CFG = {⟨G, H⟩| G and H are CFGs and L(G) = L(H)}. Theorem 4.5 gave an algorithm that decides the analogous language EQ DFA for ﬁnite automata. We used the decision procedure for EDFA to prove that EQ DFA is decidable. Because ECFG also is decidable, you might think that we can use a similar strategy to prove that EQ CFG is decidable. But something is wrong with this idea! The class of context-free languages is not closed under comple- mentation or intersection, as you proved in Exercise 2.2. In fact, EQ CFG is not decidable. The technique for proving so is presented in Chapter 5. Now we show that context-free languages are decidable by Turing machines. THEOREM 4.9 4.2 UNDECIDABILITY 201 FIGURE 4.10 The relationship among classes of languages 202 CHAPTER 4 / DECIDABILITY given input string. We call it ATM by analogy with ADFA and ACFG. But, whereas ADFA and ACFG were decidable, ATM is not. Let ATM = {⟨M, w⟩| M is a TM and M accepts w}. THEOREM 4.11 4.2 UNDECIDABILITY 203204 CHAPTER 4 / DECIDABILITY elements of Q. Then we pair the ﬁrst element on the list with the number 1 from N , the second element on the list with the number 2 from N , and so on. We must ensure that every member of Q appears only once on the list. To get this list, we make an inﬁnite matrix containing all the positive ratio- nal numbers, as shown in Figure 4.16. The ith row contains all numbers with numerator i and the jth column has all numbers with denominator j. So the number i 4.2 UNDECIDABILITY 205 √ 206 CHAPTER 4 / DECIDABILITY n 4.2 UNDECIDABILITY 207 Thus we have shown that the set of all languages cannot be put into a corre- spondence with the set of all Turing machines. We conclude that some languages are not recognized by any Turing machine. 208 CHAPTER 4 / DECIDABILITY Let’s review the steps of this proof. Assume that a TM H decides ATM. Use H to build a TM D that takes an input ⟨M ⟩, where D accepts its input ⟨M ⟩ exactly when M does not accept its input ⟨M ⟩. Finally, run D on itself. Thus, the machines take the following actions, with the last line being the contradiction. • H accepts ⟨M, w⟩ exactly when M accepts w. • D rejects ⟨M ⟩ exactly when M accepts ⟨M ⟩. • D rejects ⟨D⟩ exactly when D accepts ⟨D⟩. Where is the diagonalization in the proof of Theorem 4.11? It becomes ap- parent when you examine tables of behavior for TMs H and D. In these tables we list all TMs down the rows, M1, M2, . . . , and all their descriptions across the columns, ⟨M1⟩, ⟨M2⟩, . . . . The entries tell whether the machine in a given row accepts the input in a given column. The entry is accept if the machine accepts the input but is blank if it rejects or loops on that input. We made up the entries in the following ﬁgure to illustrate the idea. ⟨M1⟩ ⟨M2⟩ ⟨M3⟩ ⟨M4⟩ · · · 4.2 UNDECIDABILITY 209 In the following ﬁgure, we added D to Figure 4.20. By our assumption, H is a TM and so is D. Therefore, it must occur on the list M1, M2, . . . of all TMs. Note that D computes the opposite of the diagonal entries. The contradiction occurs at the point of the question mark where the entry must be the opposite of itself. ⟨M1⟩ ⟨M2⟩ ⟨M3⟩ ⟨M4⟩ · · · ⟨D⟩ · · · 210 CHAPTER 4 / DECIDABILITY For the other direction, if both A and PROBLEMS 211 4.2 Consider the problem of determining whether a DFA and a regular expression are equivalent. Express this problem as a language and show that it is decidable. 4.3 Let ALLDFA = {⟨A⟩| A is a DFA and L(A) = Σ ∗}. Show that ALLDFA is decidable. 4.4 Let AεCFG = {⟨G⟩| G is a CFG that generates ε}. Show that AεCFG is decidable. A4.5 Let ETM = {⟨M ⟩| M is a TM and L(M ) = ∅}. Show that 212 CHAPTER 4 / DECIDABILITY ⋆4.15 Show that the problem of determining whether a CFG generates all strings in 1 ∗ is decidable. In other words, show that {⟨G⟩| G is a CFG over {0,1} and 1∗ ⊆ L(G)} is a decidable language. 4.16 Let A = {⟨R⟩| R is a regular expression describing a language containing at least one string w that has 111 as a substring (i.e., w = x111y for some x and y)}. Show that A is decidable. 4.17 Prove that EQ DFA is decidable by testing the two DFAs on all strings up to a certain size. Calculate a size that works. ⋆4.18 Let C be a language. Prove that C is Turing-recognizable iff a decidable language D exists such that C = {x| ∃y (⟨x, y⟩ ∈ D)}. ⋆4.19 Prove that the class of decidable languages is not closed under homomorphism. 4.20 Let A and B be two disjoint languages. Say that language C separates A and B if A ⊆ C and B ⊆ SELECTED SOLUTIONS 213 4.32 The proof of Lemma 2.41 says that (q, x) is a looping situation for a DPDA P if when P is started in state q with x ∈ Γ on the top of the stack, it never pops anything below x and it never reads an input symbol. Show that F is decidable, where F = {⟨P, q, x⟩| (q, x) is a looping situation for P }. 214 CHAPTER 4 / DECIDABILITY 4.12 The following TM decides A. “On input ⟨M ⟩: 1. Construct a DFA O that accepts every string containing an odd number of 1s. 2. Construct a DFA B such that L(B) = L(M ) ∩ L(O). 3. Test whether L(B) = ∅ using the EDFA decider T from Theo- rem 4.4. 4. If T accepts, accept ; if T rejects, reject .” 4.14 You showed in Problem 2.18 that if C is a context-free language and R is a regular language, then C ∩ R is context free. Therefore, 1 ∗ ∩ L(G) is context free. The following TM decides the language of this problem. “On input ⟨G⟩: 1. Construct CFG H such that L(H) = 1 ∗ ∩ L(G). 2. Test whether L(H) = ∅ using the ECFG decider R from Theo- rem 4.8. 3. If R accepts, reject ; if R rejects, accept .” 4.23 The following procedure decides AMBIGNFA. Given an NFA N , we design a DFA D that simulates N and accepts a string iff it is accepted by N along two different computational branches. Then we use a decider for EDFA to determine whether D accepts any strings. Our strategy for constructing D is similar to the NFA-to-DFA conversion in the proof of Theorem 1.39. We simulate N by keeping a pebble on each active state. We begin by putting a red pebble on the start state and on each state reachable from the start state along ε transitions. We move, add, and remove pebbles in accordance with N ’s transitions, preserving the color of the pebbles. Whenever two or more pebbles are moved to the same state, we replace its pebbles with a blue pebble. After reading the input, we accept if a blue pebble is on an accept state of N or if two different accept states of N have red pebbles on them. The DFA D has a state corresponding to each possible position of pebbles. For each state of N , three possibilities occur: It can contain a red pebble, a blue pebble, or no pebble. Thus, if N has n states, D will have 3 n states. Its start state, accept states, and transition function are deﬁned to carry out the simulation. 4.25 The language of all strings with an equal number of 0s and 1s is a context-free language, generated by the grammar S → 1S0S | 0S1S | ε. Let P be the PDA that recognizes this language. Build a TM M for BALDFA, which operates as follows. On input ⟨B⟩, where B is a DFA, use B and P to construct a new PDA R that recognizes the intersection of the languages of B and P . Then test whether R’s language is empty. If its language is empty, reject ; otherwise, accept . Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 216 CHAPTER 5 / REDUCIBILITY Reducibility also occurs in mathematical problems. For example, the problem of measuring the area of a rectangle reduces to the problem of measuring its length and width. The problem of solving a system of linear equations reduces to the problem of inverting a matrix. Reducibility plays an important role in classifying problems by decidability, and later in complexity theory as well. When A is reducible to B, solving A cannot be harder than solving B because a solution to B gives a solution to A. In terms of computability theory, if A is reducible to B and B is decidable, A also is decidable. Equivalently, if A is undecidable and reducible to B, B is undecidable. This last version is key to proving that various problems are undecidable. In short, our method for proving that a problem is undecidable will be to show that some other problem already known to be undecidable reduces to it. 5.1 UNDECIDABLE PROBLEMS FROM LANGUAGE THEORY 217 Instead, use the assumption that you have TM R that decides HALT TM. With R, you can test whether M halts on w. If R indicates that M doesn’t halt on w, reject because ⟨M, w⟩ isn’t in ATM. However, if R indicates that M does halt on w, you can do the simulation without any danger of looping. Thus, if TM R exists, we can decide ATM, but we know that ATM is unde- cidable. By virtue of this contradiction, we can conclude that R does not exist. Therefore, HALT TM is undecidable. PROOF Let’s assume for the purpose of obtaining a contradiction that TM R decides HALT TM. We construct TM S to decide ATM, with S operating as follows. S = “On input ⟨M, w⟩, an encoding of a TM M and a string w: 1. Run TM R on input ⟨M, w⟩. 2. If R rejects, reject . 3. If R accepts, simulate M on w until it halts. 4. If M has accepted, accept ; if M has rejected, reject .” Clearly, if R decides HALT TM, then S decides ATM. Because ATM is unde- cidable, HALT TM also must be undecidable. 218 CHAPTER 5 / REDUCIBILITY Instead of running R on ⟨M ⟩, we run R on a modiﬁcation of ⟨M ⟩. We modify ⟨M ⟩ to guarantee that M rejects all strings except w, but on input w it works as usual. Then we use R to determine whether the modiﬁed machine recognizes the empty language. The only string the machine can now accept is w, so its language will be nonempty iff it accepts w. If R accepts when it is fed a descrip- tion of the modiﬁed machine, we know that the modiﬁed machine doesn’t accept anything and that M doesn’t accept w. PROOF Let’s write the modiﬁed machine described in the proof idea using our standard notation. We call it M1. M1 = “On input x: 1. If x ̸= w, reject . 2. If x = w, run M on input w and accept if M does.” This machine has the string w as part of its description. It conducts the test of whether x = w in the obvious way, by scanning the input and comparing it character by character with w to determine whether they are the same. Putting all this together, we assume that TM R decides ETM and construct TM S that decides ATM as follows. S = “On input ⟨M, w⟩, an encoding of a TM M and a string w: 1. Use the description of M and w to construct the TM M1 just described. 2. Run R on input ⟨M1⟩. 3. If R accepts, reject ; if R rejects, accept .” Note that S must actually be able to compute a description of M1 from a description of M and w. It is able to do so because it only needs to add extra states to M that perform the x = w test. If R were a decider for ETM, S would be a decider for ATM. A decider for ATM cannot exist, so we know that ETM must be undecidable. 5.1 UNDECIDABLE PROBLEMS FROM LANGUAGE THEORY 219 THEOREM 5.3 220 CHAPTER 5 / REDUCIBILITY machines is an undecidable problem. We could prove it by a reduction from ATM, but we use this opportunity to give an example of an undecidability proof by reduction from ETM. Let EQ TM = {⟨M1, M2⟩| M1 and M2 are TMs and L(M1) = L(M2)}. THEOREM 5.4 5.1 UNDECIDABLE PROBLEMS FROM LANGUAGE THEORY 221222 CHAPTER 5 / REDUCIBILITY Despite their memory constraint, linear bounded automata (LBAs) are quite powerful. For example, the deciders for ADFA, ACFG, EDFA, and ECFG all are LBAs. Every CFL can be decided by an LBA. In fact, coming up with a decidable language that can’t be decided by an LBA takes some work. We develop the techniques to do so in Chapter 9. Here, ALBA is the problem of determining whether an LBA accepts its input. Even though ALBA is the same as the undecidable problem ATM where the Tur- ing machine is restricted to be an LBA, we can show that ALBA is decidable. Let ALBA = {⟨M, w⟩| M is an LBA that accepts string w}. Before proving the decidability of ALBA, we ﬁnd the following lemma useful. It says that an LBA can have only a limited number of conﬁgurations when a string of length n is the input. LEMMA 5.8 5.1 UNDECIDABLE PROBLEMS FROM LANGUAGE THEORY 223 PROOF The algorithm that decides ALBA is as follows. L = “On input ⟨M, w⟩, where M is an LBA and w is a string: 1. Simulate M on w for qngn steps or until it halts. 2. If M has halted, accept if it has accepted and reject if it has rejected. If it has not halted, reject .” If M on w has not halted within qngn steps, it must be repeating a conﬁgura- tion according to Lemma 5.8 and therefore looping. That is why our algorithm rejects in this instance. 224 CHAPTER 5 / REDUCIBILITY # | 5.1 UNDECIDABLE PROBLEMS FROM LANGUAGE THEORY 225 FIGURE 5.12 LBA B checking a TM computation history 226 CHAPTER 5 / REDUCIBILITY 1. that do not start with C1, 2. that do not end with an accepting conﬁguration, or 3. in which some Ci does not properly yield Ci+1 under the rules of M . If M does not accept w, no accepting computation history exists, so all strings fail in one way or another. Therefore, G would generate all strings, as desired. Now we get down to the actual construction of G. Instead of constructing G, we construct a PDA D. We know that we can use the construction given in Theorem 2.20 (page 117) to convert D to a CFG. We do so because, for our purposes, designing a PDA is easier than designing a CFG. In this instance, D will start by nondeterministically branching to guess which of the preceding three conditions to check. One branch checks on whether the beginning of the input string is C1 and accepts if it isn’t. Another branch checks on whether the input string ends with a conﬁguration containing the accept state, qaccept, and accepts if it isn’t. The third branch is supposed to accept if some Ci does not properly yield Ci+1. It works by scanning the input until it nondeterministically decides that it has come to Ci. Next, it pushes Ci onto the stack until it comes to the end as marked by the # symbol. Then D pops the stack to compare with Ci+1. They are supposed to match except around the head position, where the difference is dictated by the transition function of M . Finally, D accepts if it discovers a mismatch or an improper update. The problem with this idea is that when D pops Ci off the stack, it is in reverse order and not suitable for comparison with Ci+1. At this point, the twist in the proof appears: We write the accepting computation history differently. Every other conﬁguration appears in reverse order. The odd positions remain written in the forward order, but the even positions are written backward. Thus, an accepting computation history would appear as shown in the following ﬁgure. # −→| 5.2 A SIMPLE UNDECIDABLE PROBLEM 227228 CHAPTER 5 / REDUCIBILITY Before getting to the formal statement of this theorem and its proof, let’s state the problem precisely and then express it as a language. An instance of the PCP is a collection P of dominos P = {[ t1 5.2 A SIMPLE UNDECIDABLE PROBLEM 229 where Q, Σ, Γ, and δ are the state set, input alphabet, tape alphabet, and transi- tion function of M , respectively. In this case, S constructs an instance of the PCP P that has a match iff M accepts w. To do that, S ﬁrst constructs an instance P ′ of the MPCP. We de- scribe the construction in seven parts, each of which accomplishes a particular aspect of simulating M on w. To explain what we are doing, we interleave the construction with an example of the construction in action. Part 1. The construction begins in the following manner. Put [ # 230 CHAPTER 5 / REDUCIBILITY Part 4. For every a ∈ Γ, put [ a 5.2 A SIMPLE UNDECIDABLE PROBLEM 231 Part 5. Put [ # 232 CHAPTER 5 / REDUCIBILITY Part 6. For every a ∈ Γ, put [ a qaccept 5.2 A SIMPLE UNDECIDABLE PROBLEM 233 the PCP instead of the MPCP, it obviously has a match, regardless of whether M accepts w. Can you ﬁnd it? (Hint: It is very short.) We now show how to convert P ′ to P , an instance of the PCP that still sim- ulates M on w. We do so with a somewhat technical trick. The idea is to take the requirement that the match starts with the ﬁrst domino and build it directly into the problem instance itself so that it becomes enforced automatically. After that, the requirement isn’t needed. We introduce some notation to implement this idea. Let u = u1u2 · · · un be any string of length n. Deﬁne ⋆u, u⋆, and ⋆u⋆ to be the three strings ⋆u = ∗ u1 ∗ u2 ∗ u3 ∗ · · · ∗ un u⋆ = u1 ∗ u2 ∗ u3 ∗ · · · ∗ un ∗ ⋆u⋆ = ∗ u1 ∗ u2 ∗ u3 ∗ · · · ∗ un ∗ . Here, ⋆u adds the symbol ∗ before every character in u, u⋆ adds one after each character in u, and ⋆u⋆ adds one both before and after each character in u. To convert P ′ to P , an instance of the PCP, we do the following. If P ′ were the collection {[ t1 234 CHAPTER 5 / REDUCIBILITY 5.3 MAPPING REDUCIBILITY 235 The machine M ′ is a machine that recognizes the same language as M , but never attempts to move its head off the left-hand end of its tape. The function f accomplishes this task by adding several states to the description of M . The function returns ε if w is not a legal encoding of a Turing machine. 236 CHAPTER 5 / REDUCIBILITY THEOREM 5.22 5.3 MAPPING REDUCIBILITY 237 string not in HALT TM. Any string not in HALT TM will do. In general, when we describe a Turing machine that computes a reduction from A to B, improperly formed inputs are assumed to map to strings outside of B. 238 CHAPTER 5 / REDUCIBILITY COROLLARY 5.29 EXERCISES 239240 CHAPTER 5 / REDUCIBILITY 5.14 Consider the problem of determining whether a Turing machine M on an input w ever attempts to move its head left when its head is on the left-most tape cell. Formulate this problem as a language and show that it is undecidable. 5.15 Consider the problem of determining whether a Turing machine M on an input w ever attempts to move its head left at any point during its computation on w. Formulate this problem as a language and show that it is decidable. 5.16 Let Γ = {0, 1, ␣} be the tape alphabet for all TMs in this problem. Deﬁne the busy beaver function BB: N −→N as follows. For each value of k, consider all k-state TMs that halt when started with a blank tape. Let BB(k) be the maximum number of 1s that remain on the tape among all of these machines. Show that BB is not a computable function. 5.17 Show that the Post Correspondence Problem is decidable over the unary alphabet Σ = {1}. 5.18 Show that the Post Correspondence Problem is undecidable over the binary alpha- bet Σ = {0,1}. 5.19 In the silly Post Correspondence Problem, SPCP, the top string in each pair has the same length as the bottom string. Show that the SPCP is decidable. 5.20 Prove that there exists an undecidable subset of {1} ∗. 5.21 Let AMBIGCFG = {⟨G⟩| G is an ambiguous CFG}. Show that AMBIGCFG is unde- cidable. (Hint: Use a reduction from PCP. Given an instance P = {[ t1 PROBLEMS 241 5.27 A two-dimensional ﬁnite automaton (2DIM-DFA) is deﬁned as follows. The input is an m × n rectangle, for any m, n ≥ 2. The squares along the boundary of the rectangle contain the symbol # and the internal squares contain symbols over the input alphabet Σ. The transition function δ : Q × (Σ ∪ {#})−→Q × {L, R, U, D} indicates the next state and the new head position (Left, Right, Up, Down). The machine accepts when it enters one of the designated accept states. It rejects if it tries to move off the input rectangle or if it never halts. Two such machines are equivalent if they accept the same rectangles. Consider the problem of determin- ing whether two of these machines are equivalent. Formulate this problem as a language and show that it is undecidable. A⋆5.28 Rice’s theorem. Let P be any nontrivial property of the language of a Turing machine. Prove that the problem of determining whether a given Turing machine’s language has property P is undecidable. In more formal terms, let P be a language consisting of Turing machine descrip- tions where P fulﬁlls two conditions. First, P is nontrivial—it contains some, but not all, TM descriptions. Second, P is a property of the TM’s language—whenever L(M1) = L(M2), we have ⟨M1⟩ ∈ P iff ⟨M2⟩ ∈ P . Here, M1 and M2 are any TMs. Prove that P is an undecidable language. 5.29 Show that both conditions in Problem 5.28 are necessary for proving that P is undecidable. 5.30 Use Rice’s theorem, which appears in Problem 5.28, to prove the undecidability of each of the following languages. Aa. INFINITETM = {⟨M ⟩| M is a TM and L(M ) is an inﬁnite language}. b. {⟨M ⟩| M is a TM and 1011 ∈ L(M )}. c. ALLTM = {⟨M ⟩| M is a TM and L(M ) = Σ ∗}. 5.31 Let f (x) = {3x + 1 for odd x x/2 for even x for any natural number x. If you start with an integer x and iterate f , you obtain a sequence, x, f (x), f (f (x)), . . . . Stop if you ever hit 1. For example, if x = 17, you get the sequence 17, 52, 26, 13, 40, 20, 10, 5, 16, 8, 4, 2, 1. Extensive computer tests have shown that every starting point between 1 and a large positive integer gives a sequence that ends in 1. But the question of whether all positive starting points end up at 1 is unsolved; it is called the 3x + 1 problem. Suppose that ATM were decidable by a TM H. Use H to describe a TM that is guaranteed to state the answer to the 3x + 1 problem. 5.32 Prove that the following two languages are undecidable. a. OVERLAPCFG = {⟨G, H⟩| G and H are CFGs where L(G) ∩ L(H) ̸= ∅}. (Hint: Adapt the hint in Problem 5.21.) b. PREFIX-FREECFG = {⟨G⟩| G is a CFG where L(G) is preﬁx-free}. 5.33 Consider the problem of determining whether a PDA accepts some string of the form {ww| w ∈ {0,1} ∗} . Use the computation history method to show that this problem is undecidable. 5.34 Let X = {⟨M, w⟩| M is a single-tape TM that never modiﬁes the portion of the tape that contains the input w}. Is X decidable? Prove your answer. Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 242 CHAPTER 5 / REDUCIBILITY 5.35 Say that a variable A in CFG G is necessary if it appears in every derivation of some string w ∈ G. Let NECESSARY CFG = {⟨G, A⟩| A is a necessary variable in G}. a. Show that NECESSARY CFG is Turing-recognizable. b. Show that NECESSARY CFG is undecidable. ⋆5.36 Say that a CFG is minimal if none of its rules can be removed without changing the language generated. Let MIN CFG = {⟨G⟩| G is a minimal CFG}. a. Show that MIN CFG is T-recognizable. b. Show that MIN CFG is undecidable. SELECTED SOLUTIONS 243 5.10 Let B = {⟨M, w⟩| M is a two-tape TM that writes a nonblank symbol on its second tape when it is run on w}. Show that ATM reduces to B. Assume for the sake of contradiction that TM R decides B. Then construct a TM S that uses R to decide ATM. S = “On input ⟨M, w⟩: 1. Use M to construct the following two-tape TM T . T = “On input x: 1. Simulate M on x using the ﬁrst tape. 2. If the simulation shows that M accepts, write a non- blank symbol on the second tape.” 2. Run R on ⟨T, w⟩ to determine whether T on input w writes a nonblank symbol on its second tape. 3. If R accepts, M accepts w, so accept . Otherwise, reject .” 5.11 Let C = {⟨M ⟩| M is a two-tape TM that writes a nonblank symbol on its second tape when it is run on some input}. Show that ATM reduces to C. Assume for the sake of contradiction that TM R decides C. Construct a TM S that uses R to decide ATM. S = “On input ⟨M, w⟩: 1. Use M and w to construct the following two-tape TM Tw. Tw = “On any input: 1. Simulate M on w using the ﬁrst tape. 2. If the simulation shows that M accepts, write a non- blank symbol on the second tape.” 2. Run R on ⟨Tw⟩ to determine whether Tw ever writes a nonblank symbol on its second tape. 3. If R accepts, M accepts w, so accept . Otherwise, reject .” 5.28 Assume for the sake of contradiction that P is a decidable language satisfying the properties and let RP be a TM that decides P . We show how to decide ATM using RP by constructing TM S. First, let T∅ be a TM that always rejects, so L(T∅) = ∅. You may assume that ⟨T∅⟩ ̸∈ P without loss of generality because you could pro- ceed with 244 CHAPTER 5 / REDUCIBILITY 5.30 (a) INFINITETM is a language of TM descriptions. It satisﬁes the two conditions of Rice’s theorem. First, it is nontrivial because some TMs have inﬁnite languages and others do not. Second, it depends only on the language. If two TMs recognize the same language, either both have descriptions in INFINITETM or neither do. Consequently, Rice’s theorem implies that INFINITETM is undecidable. Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 246 CHAPTER 6 / ADVANCED TOPICS IN COMPUTABILITY THEORY 1. Living things are machines. 2. Living things can self-reproduce. 3. Machines cannot self-reproduce. Statement 1 is a tenet of modern biology. We believe that organisms operate in a mechanistic way. Statement 2 is obvious. The ability to self-reproduce is an essential characteristic of every biological species. For statement 3, we make the following argument that machines cannot self-reproduce. Consider a machine that constructs other machines, such as an automated factory that produces cars. Raw materials go in at one end, the manufacturing robots follow a set of instructions, and then completed vehicles come out the other end. We claim that the factory must be more complex than the cars produced, in the sense that designing the factory would be more difﬁcult than designing a car. This claim must be true because the factory itself has the car’s design within it, in addition to the design of all the manufacturing robots. The same reasoning applies to any machine A that constructs a machine B: A must be more complex than B. But a machine cannot be more complex than itself. Consequently, no machine can construct itself, and thus self-reproduction is impossible. How can we resolve this paradox? The answer is simple: Statement 3 is in- correct. Making machines that reproduce themselves is possible. The recursion theorem demonstrates how. SELF-REFERENCE Let’s begin by making a Turing machine that ignores its input and prints out a copy of its own description. We call this machine SELF . To help describe SELF , we need the following lemma. LEMMA 6.1 6.1 THE RECURSION THEOREM 247 The Turing machine SELF is in two parts: A and B. We think of A and B as being two separate procedures that go together to make up SELF . We want SELF to print out ⟨SELF ⟩ = ⟨AB⟩. Part A runs ﬁrst and upon completion passes control to B. The job of A is to print out a description of B, and conversely the job of B is to print out a description of A. The result is the desired description of SELF . The jobs are similar, but they are carried out differently. We show how to get part A ﬁrst. For A we use the machine P⟨B⟩, described by q(⟨B⟩ ) , which is the result of applying the function q to ⟨B⟩. Thus, part A is a Turing machine that prints out ⟨B⟩. Our description of A depends on having a description of B. So we can’t complete the description of A until we construct B. Now for part B. We might be tempted to deﬁne B with q( ⟨A⟩ ), but that doesn’t make sense! Doing so would deﬁne B in terms of A, which in turn is deﬁned in terms of B. That would be a circular deﬁnition of an object in terms of itself, a logical transgression. Instead, we deﬁne B so that it prints A by using a different strategy: B computes A from the output that A produces. We deﬁned ⟨A⟩ to be q( ⟨B⟩ ). Now comes the tricky part: If B can obtain ⟨B⟩, it can apply q to that and obtain ⟨A⟩. But how does B obtain ⟨B⟩? It was left on the tape when A ﬁnished! So B only needs to look at the tape to obtain ⟨B⟩. Then after B computes q( ⟨B⟩ ) = ⟨A⟩, it combines A and B into a single machine and writes its description ⟨AB⟩ = ⟨SELF ⟩ on the tape. In summary, we have: A = P⟨B⟩, and B = “On input ⟨M ⟩, where M is a portion of a TM: 1. Compute q( ⟨M ⟩ ) . 2. Combine the result with ⟨M ⟩ to make a complete TM. 3. Print the description of this TM and halt.” This completes the construction of SELF , for which a schematic diagram is presented in the following ﬁgure. FIGURE 6.2 Schematic of SELF , a TM that prints its own description Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 248 CHAPTER 6 / ADVANCED TOPICS IN COMPUTABILITY THEORY If we now run SELF , we observe the following behavior. 1. First A runs. It prints ⟨B⟩ on the tape. 2. B starts. It looks at the tape and ﬁnds its input, ⟨B⟩. 3. B calculates q( ⟨B⟩ ) = ⟨A⟩ and combines that with ⟨B⟩ into a TM description, ⟨SELF ⟩. 4. B prints this description and halts. We can easily implement this construction in any programming language to obtain a program that outputs a copy of itself. We can even do so in plain En- glish. Suppose that we want to give an English sentence that commands the reader to print a copy of the same sentence. One way to do so is to say: Print out this sentence. This sentence has the desired meaning because it directs the reader to print a copy of the sentence itself. However, it doesn’t have an obvious translation into a programming language because the self-referential word “this” in the sentence usually has no counterpart. But no self-reference is needed to make such a sen- tence. Consider the following alternative. Print out two copies of the following, the second one in quotes: “Print out two copies of the following, the second one in quotes:” In this sentence, the self-reference is replaced with the same construction used to make the TM SELF . Part B of the construction is the clause: Print out two copies of the following, the second one in quotes: Part A is the same, with quotes around it. A provides a copy of B to B so B can process that copy as the TM does. The recursion theorem provides the ability to implement the self-referential this into any programming language. With it, any program has the ability to refer to its own description, which has certain applications, as you will see. Before getting to that, we state the recursion theorem itself. The recursion theorem extends the technique we used in constructing SELF so that a program can obtain its own description and then go on to compute with it, instead of merely printing it out. THEOREM 6.3 6.1 THE RECURSION THEOREM 249 in the statement, that receives the description of the machine as an extra input. Then the recursion theorem produces a new machine R, which operates exactly as T does but with R’s description ﬁlled in automatically. PROOF The proof is similar to the construction of SELF . We construct a TM R in three parts, A, B, and T , where T is given by the statement of the theorem; a schematic diagram is presented in the following ﬁgure. FIGURE 6.4 Schematic of R Here, A is the Turing machine P⟨BT ⟩ described by q( ⟨BT ⟩ ) . To preserve the input w, we redesign q so that P⟨BT ⟩ writes its output following any string preexisting on the tape. After A runs, the tape contains w⟨BT ⟩. Again, B is a procedure that examines its tape and applies q to its contents. The result is ⟨A⟩. Then B combines A, B, and T into a single machine and ob- tains its description ⟨ABT ⟩ = ⟨R⟩. Finally, it encodes that description together with w, places the resulting string ⟨R, w⟩ on the tape, and passes control to T . 250 CHAPTER 6 / ADVANCED TOPICS IN COMPUTABILITY THEORY SELF = “On any input: 1. Obtain, via the recursion theorem, own description ⟨SELF ⟩. 2. Print ⟨SELF ⟩.” The recursion theorem shows how to implement the “obtain own descrip- tion” construct. To produce the machine SELF , we ﬁrst write the following machine T . T = “On input ⟨M, w⟩: 1. Print ⟨M ⟩ and halt.” The TM T receives a description of a TM M and a string w as input, and it prints the description of M . Then the recursion theorem shows how to obtain a TM R, which on input w operates like T on input ⟨R, w⟩. Thus, R prints the description of R—exactly what is required of the machine SELF . APPLICATIONS A computer virus is a computer program that is designed to spread itself among computers. Aptly named, it has much in common with a biological virus. Com- puter viruses are inactive when standing alone as a piece of code. But when placed appropriately in a host computer, thereby “infecting” it, they can become activated and transmit copies of themselves to other accessible machines. Vari- ous media can transmit viruses, including the Internet and transferable disks. In order to carry out its primary task of self-replication, a virus may contain the construction described in the proof of the recursion theorem. Let’s now consider three theorems whose proofs use the recursion theorem. An additional application appears in the proof of Theorem 6.17 in Section 6.2. First we return to the proof of the undecidability of ATM. Recall that we ear- lier proved it in Theorem 4.11, using Cantor’s diagonal method. The recursion theorem gives us a new and simpler proof. THEOREM 6.5 6.1 THE RECURSION THEOREM 251 The following theorem concerning minimal Turing machines is another ap- plication of the recursion theorem. 252 CHAPTER 6 / ADVANCED TOPICS IN COMPUTABILITY THEORY THEOREM 6.8 6.2 DECIDABILITY OF LOGICAL THEORIES 253 To consider whether we could automate the process of determining which of these statements are true, we treat such statements merely as strings and deﬁne a language consisting of those statements that are true. Then we ask whether this language is decidable. To make this a bit more precise, let’s describe the form of the alphabet of this language: {∧, ∨, ¬, (, ), ∀, ∃, x, R1, . . . , Rk}. The symbols ∧, ∨, and ¬ are called Boolean operations; “(” and “)” are the parentheses; the symbols ∀ and ∃ are called quantiﬁers; the symbol x is used to denote variables;2 and the symbols R1, . . . , Rk are called relations. A formula is a well-formed string over this alphabet. For completeness, we’ll sketch the technical but obvious deﬁnition of a well-formed formula here, but feel free to skip this part and go on to the next paragraph. A string of the form Ri(x1, . . . , xk) is an atomic formula. The value j is the arity of the relation symbol Ri. All appearances of the same relation symbol in a well-formed formula must have the same arity. Subject to this requirement, a string ϕ is a formula if it 1. is an atomic formula, 2. has the form ϕ1 ∧ ϕ2 or ϕ1 ∨ ϕ2 or ¬ϕ1, where ϕ1 and ϕ2 are smaller formulas, or 3. has the form ∃xi [ ϕ1 ] or ∀xi [ ϕ1 ], where ϕ1 is a smaller formula. A quantiﬁer may appear anywhere in a mathematical statement. Its scope is the fragment of the statement appearing within the matched pair of parentheses or brackets following the quantiﬁed variable. We assume that all formulas are in prenex normal form, where all quantiﬁers appear in the front of the formula. A variable that isn’t bound within the scope of a quantiﬁer is called a free variable. A formula with no free variables is called a sentence or statement. EXAMPLE 6.9 254 CHAPTER 6 / ADVANCED TOPICS IN COMPUTABILITY THEORY is an assignment of speciﬁc relations to the relation symbols. As we described in Section 0.2 (page 9), a relation is a function from k-tuples over the universe to {TRUE, FALSE}. The arity of a relation symbol must match that of its assigned relation. A universe together with an assignment of relations to relation symbols is called a model.3 Formally, we say that a model M is a tuple (U, P1, . . . , Pk), where U is the universe and P1 through Pk are the relations assigned to symbols R1 through Rk. We sometimes refer to the language of a model to be the collection of formulas that use only the relation symbols the model assigns, and that use each relation symbol with the correct arity. If ϕ is a sentence in the language of a model, ϕ is either true or false in that model. If ϕ is true in a model M, we say that M is a model of ϕ. If you feel overwhelmed by these deﬁnitions, concentrate on our objective in stating them. We want to set up a precise language of mathematical statements so that we can ask whether an algorithm can determine which are true and which are false. The following two examples should be helpful. EXAMPLE 6.10 6.2 DECIDABILITY OF LOGICAL THEORIES 255 Now we give one ﬁnal deﬁnition in preparation for the next section. If M is a model, we let the theory of M, written Th(M), be the collection of true sentences in the language of that model. A DECIDABLE THEORY Number theory is one of the oldest branches of mathematics and also one of its most difﬁcult. Many innocent-looking statements about the natural num- bers with the plus and times operations have confounded mathematicians for centuries, such as the twin prime conjecture mentioned earlier. In one of the celebrated developments in mathematical logic, Alonzo Church, building on the work of Kurt G ¨odel, showed that no algorithm can decide in general whether statements in number theory are true or false. Formally, we write (N , +, ×) to be the model whose universe is the natural numbers4 with the usual + and × relations. Church showed that Th(N , +, ×), the theory of this model, is undecidable. Before looking at this undecidable theory, let’s examine one that is decidable. Let (N , +) be the same model, without the × relation. Its theory is Th(N , +). For example, the formula ∀x ∃y [ x + x = y ] is true and is therefore a member of Th(N , +), but the formula ∃y∀x [ x + x = y ] is false and is therefore not a member. THEOREM 6.12 256 CHAPTER 6 / ADVANCED TOPICS IN COMPUTABILITY THEORY Formula ϕi has i free variables. For a1, . . . , ai ∈ N , write ϕi(a1, . . . , ai) to be the sentence obtained by substituting the constants a1, . . . , ai for the variables x1, . . . , xi in ϕi. For each i from 0 to l, the algorithm constructs a ﬁnite automaton Ai that recognizes the collection of strings representing i-tuples of numbers that make ϕi true. The algorithm begins by constructing Al directly, using a generalization of the method in the solution to Problem 1.32. Then, for each i from l down to 1, it uses Ai to construct Ai−1. Finally, once the algorithm has A0, it tests whether A0 accepts the empty string. If it does, ϕ is true and the algorithm accepts. PROOF For i > 0, deﬁne the alphabet Σi = {[ 0 ... 0 0 ] , [ 0 ... 0 1 ] , [ 0 ... 1 0 ] , [ 0 ... 1 1 ] , . . . , [ 1 ... 1 1 ]} . Hence Σi contains all size i columns of 0s and 1s. A string over Σi represents i binary integers (reading across the rows). We also deﬁne Σ0 = {[ ]}, where [ ] is a symbol. We now present an algorithm that decides Th(N , +). On input ϕ, where ϕ is a sentence, the algorithm operates as follows. Write ϕ and deﬁne ϕi for each i from 0 to l, as in the proof idea. For each such i, construct a ﬁnite automaton Ai from ϕi that accepts strings over Σi corresponding to i-tuples a1, . . . , ai whenever ϕi(a1, . . . , ai) is true, as follows. To construct the ﬁrst machine Al, observe that ϕl = ψ is a Boolean combi- nation of atomic formulas. An atomic formula in the language of Th(N , +) is a single addition. Finite automata can be constructed to compute any of these in- dividual relations corresponding to a single addition and then combined to give the automaton Al. Doing so involves the use of the regular language closure constructions for union, intersection, and complementation to compute Boolean combinations of the atomic formulas. Next, we show how to construct Ai from Ai+1. If ϕi = ∃xi+1 ϕi+1, we con- struct Ai to operate as Ai+1 operates, except that it nondeterministically guesses the value of ai+1 instead of receiving it as part of the input. More precisely, Ai contains a state for each Ai+1 state and a new start state. Every time Ai reads a symbol   b1 ... bi−1 bi   , where every bj ∈ {0,1} is a bit of the number aj, it nondeterministically guesses z ∈ {0,1} and simulates Ai+1 on the input symbol    b1 ... bi−1 bi z    . Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 6.2 DECIDABILITY OF LOGICAL THEORIES 257 Initially, Ai nondeterministically guesses the leading bits of ai+1 corresponding to suppressed leading 0s in a1 through ai by nondeterministically branching using ε-transitions from its new start state to all states that Ai+1 could reach from its start state with input strings of the symbols {[ 0 ... 0 0 ] , [ 0 ... 0 1 ]} in Σi+1. Clearly, Ai accepts its input (a1, . . . , ai) if some ai+1 exists where Ai+1 accepts (a1, . . . , ai+1). If ϕi = ∀xi+1 ϕi+1, it is equivalent to ¬∃xi+1¬ ϕi+1. Thus, we can construct the ﬁnite automaton that recognizes the complement of the language of Ai+1, then apply the preceding construction for the ∃ quantiﬁer, and ﬁnally apply com- plementation once again to obtain Ai. Finite automaton A0 accepts any input iff ϕ0 is true. So the ﬁnal step of the algorithm tests whether A0 accepts ε. If it does, ϕ is true and the algorithm accepts; otherwise, it rejects. 258 CHAPTER 6 / ADVANCED TOPICS IN COMPUTABILITY THEORY The actual construction of ϕM,w is too complicated to present here. It ex- tracts individual symbols in the computation history with the + and × operations to check that the start conﬁguration for M on w is correct, that each conﬁgura- tion legally follows from the one preceding it, and that the last conﬁguration is accepting. PROOF OF THEOREM 6.13 We give a mapping reduction from ATM to Th(N , +, ×). The reduction constructs the formula ϕM,w from the input ⟨M, w⟩ by using Lemma 6.14. Then it outputs the sentence ∃x ϕM,w. 6.2 DECIDABILITY OF LOGICAL THEORIES 259 THEOREM 6.16 260 CHAPTER 6 / ADVANCED TOPICS IN COMPUTABILITY THEORY6.4 A DEFINITION OF INFORMATION 261 If M ’s language isn’t empty, N will accept every input and, in particular, in- put 0. Hence the oracle will answer YES, and T ATM will reject. Conversely, if M ’s language is empty, T ATM will accept. Thus T ATM decides ETM. We say that ETM is decidable relative to ATM. That brings us to the deﬁnition of Turing reducibility. 262 CHAPTER 6 / ADVANCED TOPICS IN COMPUTABILITY THEORY We start with an example. Consider the information content of the following two binary sequences. A = 0101010101010101010101010101010101010101 B = 1110010110100011101010000111010011010111 Intuitively, sequence A contains little information because it is merely a repe- tition of the pattern 01 twenty times. In contrast, sequence B appears to contain more information. We can use this simple example to illustrate the idea behind the deﬁnition of information that we present. We deﬁne the quantity of information contained in an object to be the size of that object’s smallest representation or description. By a description of an object, we mean a precise and unambiguous characterization of the object so that we may recreate it from the description alone. Thus, se- quence A contains little information because it has a small description, whereas sequence B apparently contains more information because it seems to have no concise description. Why do we consider only the shortest description when determining an ob- ject’s quantity of information? We may always describe an object, such as a string, by placing a copy of the object directly into the description. Thus, we can obviously describe the preceding string B with a table that is 40 bits long containing a copy of B. This type of description is never shorter than the object itself and doesn’t tell us anything about its information quantity. However, a de- scription that is signiﬁcantly shorter than the object implies that the information contained within it can be compressed into a small volume, and so the amount of information can’t be very large. Hence the size of the shortest description determines the amount of information. Now we formalize this intuitive idea. Doing so isn’t difﬁcult, but we must do some preliminary work. First, we restrict our attention to objects that are binary strings. Other objects can be represented as binary strings, so this restriction doesn’t limit the scope of the theory. Second, we consider only descriptions that are themselves binary strings. By imposing this requirement, we may easily compare the length of the object with the length of its description. In the next section, we consider the type of description that we allow. MINIMAL LENGTH DESCRIPTIONS Many types of description language can be used to deﬁne information. Selecting which language to use affects the characteristics of the deﬁnition. Our descrip- tion language is based on algorithms. Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 6.4 A DEFINITION OF INFORMATION 263 One way to use algorithms to describe strings is to construct a Turing machine that prints out the string when it is started on a blank tape and then represent that Turing machine itself as a string. Thus, the string representing the Turing machine is a description of the original string. A drawback to this approach is that a Turing machine cannot represent a table of information concisely with its transition function. To represent a string of n bits, you might use n states and n rows in the transition function table. That would result in a description that is excessively long for our purpose. Instead, we use the following more concise description language. We describe a binary string x with a Turing machine M and a binary input w to M . The length of the description is the combined length of representing M and w. We write this description with our usual notation for encoding sev- eral objects into a single binary string ⟨M, w⟩. But here we must pay additional attention to the encoding operation ⟨· , ·⟩ because we need to produce a concise result. We deﬁne the string ⟨M, w⟩ to be ⟨M ⟩w, where we simply concatenate the binary string w onto the end of the binary encoding of M . The encoding ⟨M ⟩ of M may be done in any standard way, except for the subtlety that we de- scribe in the next paragraph. (Don’t worry about this subtle point on your ﬁrst reading of this material. For now, skip past the next paragraph and the following ﬁgure.) When concatenating w onto the end of ⟨M ⟩ to yield a description of x, you might run into trouble if the point at which ⟨M ⟩ ends and w begins is not dis- cernible from the description itself. Otherwise, several ways of partitioning the description ⟨M ⟩w into a syntactically correct TM and an input may occur, and then the description would be ambiguous and hence invalid. We avoid this prob- lem by ensuring that we can locate the separation between ⟨M ⟩ and w in ⟨M ⟩w. One way to do so is to write each bit of ⟨M ⟩ twice, writing 0 as 00 and 1 as 11, and then follow it with 01 to mark the separation point. We illustrate this idea in the following ﬁgure, depicting the description ⟨M, w⟩ of some string x. ⟨M, w⟩ = 11001111001100 · · · 1100 delimiter z}|{ 01| 264 CHAPTER 6 / ADVANCED TOPICS IN COMPUTABILITY THEORY6.4 A DEFINITION OF INFORMATION 265 THEOREM 6.25 266 CHAPTER 6 / ADVANCED TOPICS IN COMPUTABILITY THEORY Further small improvements are possible. However, as Problem 6.26 asks you to show, we cannot reach the bound K(x) + K(y) + c. OPTIMALITY OF THE DEFINITION Now that we have established some of the elementary properties of descriptive complexity and you have had a chance to develop some intuition, we discuss some features of the deﬁnitions. Our deﬁnition of K(x) has an optimality property among all possible ways of deﬁning descriptive complexity with algorithms. Suppose that we consider a general description language to be any computable function p: Σ∗−→Σ∗ and deﬁne the minimal description of x with respect to p, written dp(x), to be the ﬁrst string s where p(s) = x, in the standard string order. Thus, s is lexico- graphically ﬁrst among the shortest descriptions of x. Deﬁne Kp(x) = |dp(x)|. For example, consider a programming language such as Python (encoded into binary) as the description language. Then dPython(x) would be the minimal Python program that outputs x, and KPython(x) would be the length of the min- imal program. The following theorem shows that any description language of this type is not signiﬁcantly more concise than the language of Turing machines and inputs that we originally deﬁned. THEOREM 6.27 6.4 A DEFINITION OF INFORMATION 267 INCOMPRESSIBLE STRINGS AND RANDOMNESS Theorem 6.24 shows that a string’s minimal description is never much longer than the string itself. Of course for some strings, the minimal description may be much shorter if the information in the string appears sparsely or redundantly. Do some strings lack short descriptions? In other words, is the minimal de- scription of some strings actually as long as the string itself? We show that such strings exist. These strings can’t be described any more concisely than simply writing them out explicitly. 268 CHAPTER 6 / ADVANCED TOPICS IN COMPUTABILITY THEORY COROLLARY 6.30 6.4 A DEFINITION OF INFORMATION 269 Fix any number b > 0. Select n such that at most a 1/2b+c+1 fraction of strings of length n or less fail to have property f . All sufﬁciently large n satisfy this condition because f holds for almost all strings. Let x be a string of length n that fails to have property f . We have 2n+1 − 1 strings of length n or less, so ix ≤ 2n+1 − 1 270 CHAPTER 6 / ADVANCED TOPICS IN COMPUTABILITY THEORYPROBLEMS 271 6.13 For each m > 1 let Zm = {0, 1, 2, . . . , m − 1}, and let Fm = (Zm, +, ×) be the model whose universe is Zm and that has relations corresponding to the + and × relations computed modulo m. Show that for each m, the theory Th(Fm) is decidable. 6.14 Show that for any two languages A and B, a language J exists, where A ≤T J and B ≤T J. 6.15 Show that for any language A, a language B exists, where A ≤T B and B ̸≤T A. ⋆6.16 Prove that there exist two languages A and B that are Turing-incomparable—that is, where A ̸≤T B and B ̸≤T A. ⋆6.17 Let A and B be two disjoint languages. Say that language C separates A and B if A ⊆ C and B ⊆ 272 CHAPTER 6 / ADVANCED TOPICS IN COMPUTABILITY THEORY SELECTED SOLUTIONS 6.3 Say that M B 1 decides A and M C 2 decides B. Use an oracle TM M3, where M C 3 decides A. Machine M3 simulates M1. Every time M1 queries its oracle about some string x, machine M3 tests whether x ∈ B and provides the answer to M1. Because machine M3 doesn’t have an oracle for B and cannot perform that test directly, it simulates M2 on input x to obtain that information. Machine M3 can obtain the answer to M2’s queries directly because these two machines use the same oracle, C. 6.5 The statement ∃x ∀y [ x+y=y ] is a member of Th(N , +) because that statement is true for the standard interpretation of + over the universe N . Recall that we use N = {0, 1, 2, . . .} in this chapter and so we may use x = 0. The statement ∃x ∀y [ x+y=x ] is not a member of Th(N , +) because that statement isn’t true in this model. For any value of x, setting y = 1 causes x+y=x to fail. 6.9 Assume for the sake of contradiction that some TM X decides a property P , and P satisﬁes the conditions of Rice’s theorem. One of these conditions says that TMs A and B exist where ⟨A⟩ ∈ P and ⟨B⟩ ̸∈ P . Use A and B to construct TM R: R = “On input w: 1. Obtain own description ⟨R⟩ using the recursion theorem. 2. Run X on ⟨R⟩. 3. If X accepts ⟨R⟩, simulate B on w. If X rejects ⟨R⟩, simulate A on w.” If ⟨R⟩ ∈ P , then X accepts ⟨R⟩ and L(R) = L(B). But ⟨B⟩ ̸∈ P , contradicting ⟨R⟩ ∈ P , because P agrees on TMs that have the same language. We arrive at a similar contradiction if ⟨R⟩ ̸∈ P . Therefore, our original assumption is false. Every property satisfying the conditions of Rice’s theorem is undecidable. 6.10 The statement ϕeq gives the three conditions of an equivalence relation. A model (A, R1), where A is any universe and R1 is any equivalence relation over A, is a model of ϕeq. For example, let A be the integers Z and let R1 = {(i, i)| i ∈ Z}. 6.12 Reduce Th(N , <) to Th(N , +), which we’ve already shown to be decidable. Show how to convert a sentence ϕ1 over the language of (N , <) to a sentence ϕ2 over the language of (N , +) while preserving truth or falsity in the respective models. Replace every occurrence of i < j in ϕ1 with the formula ∃k[ (i+k=j)∧(k+k̸=k) ] in ϕ2, where k is a different new variable each time. Sentence ϕ2 is equivalent to ϕ1 because “i is less than j” means that we can add a nonzero value to i and obtain j. Putting ϕ2 into prenex-normal form, as re- quired by the algorithm for deciding Th(N , +), requires a bit of additional work. The new existential quantiﬁers are brought to the front of the sentence. To do so, these quantiﬁers must pass through Boolean operations that appear in the sen- tence. Quantiﬁers can be brought through the operations of ∧ and ∨ without change. Passing through ¬ changes ∃ to ∀ and vice versa. Thus, ¬∃k ψ becomes the equivalent expression ∀k ¬ψ, and ¬∀k ψ becomes ∃k ¬ψ. 6.28 (a) R0 is deﬁnable in Th(N , +) by ϕ0(x) = ∀y[x + y = y]. (c) R= is deﬁnable in Th(N , +) by ϕ=(u, v) = ∀x[ϕ0(x) → x+u=v]. Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. PART THREE Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 276 CHAPTER 7 / TIME COMPLEXITY the Turing machine description at a low level, including the actual head motion on the tape so that we can count the number of steps that M1 uses when it runs. M1 = “On input string w: 1. Scan across the tape and reject if a 0 is found to the right of a 1. 2. Repeat if both 0s and 1s remain on the tape: 3. Scan across the tape, crossing off a single 0 and a single 1. 4. If 0s still remain after all the 1s have been crossed off, or if 1s still remain after all the 0s have been crossed off, reject . Other- wise, if neither 0s nor 1s remain on the tape, accept .” We will analyze the algorithm for TM M1 deciding A to determine how much time it uses. First, we introduce some terminology and notation for this purpose. The number of steps that an algorithm uses on a particular input may depend on several parameters. For instance, if the input is a graph, the number of steps may depend on the number of nodes, the number of edges, and the maximum degree of the graph, or some combination of these and/or other factors. For simplicity, we compute the running time of an algorithm purely as a function of the length of the string representing the input and don’t consider any other parameters. In worst-case analysis, the form we consider here, we consider the longest running time of all inputs of a particular length. In average-case anal- ysis, we consider the average of all the running times of inputs of a particular length. 7.1 MEASURING COMPLEXITY 277 For example, the function f (n) = 6n3 + 2n2 + 20n + 45 has four terms and the highest order term is 6n3. Disregarding the coefﬁcient 6, we say that f is asymptotically at most n3. The asymptotic notation or big-O notation for describing this relationship is f (n) = O(n3). We formalize this notion in the following deﬁnition. Let R + be the set of nonnegative real numbers. 278 CHAPTER 7 / TIME COMPLEXITY Big-O notation also appears in arithmetic expressions such as the expression f (n) = O(n2) + O(n). In that case, each occurrence of the O symbol represents a different suppressed constant. Because the O(n2) term dominates the O(n) term, that expression is equivalent to f (n) = O(n2). When the O symbol occurs in an exponent, as in the expression f (n) = 2O(n), the same idea applies. This expression represents an upper bound of 2cn for some constant c. The expression f (n) = 2O(log n) occurs in some analyses. Using the identity n = 2log2 n and thus nc = 2c log2 n, we see that 2O(log n) represents an upper bound of nc for some c. The expression nO(1) represents the same bound in a different way because the expression O(1) represents a value that is never more than a ﬁxed constant. Frequently, we derive bounds of the form nc for c greater than 0. Such bounds are called polynomial bounds. Bounds of the form 2(n δ) are called exponential bounds when δ is a real number greater than 0. Big-O notation has a companion called small-o notation. Big-O notation says that one function is asymptotically no more than another. To say that one func- tion is asymptotically less than another, we use small-o notation. The difference between the big-O and small-o notations is analogous to the difference between ≤ and <. 7.1 MEASURING COMPLEXITY 279 ANALYZING ALGORITHMS Let’s analyze the TM algorithm we gave for the language A = {0 k1 k| k ≥ 0}. We repeat the algorithm here for convenience. M1 = “On input string w: 1. Scan across the tape and reject if a 0 is found to the right of a 1. 2. Repeat if both 0s and 1s remain on the tape: 3. Scan across the tape, crossing off a single 0 and a single 1. 4. If 0s still remain after all the 1s have been crossed off, or if 1s still remain after all the 0s have been crossed off, reject . Other- wise, if neither 0s nor 1s remain on the tape, accept .” To analyze M1, we consider each of its four stages separately. In stage 1, the machine scans across the tape to verify that the input is of the form 0∗1 ∗. Performing this scan uses n steps. As we mentioned earlier, we typically use n to represent the length of the input. Repositioning the head at the left-hand end of the tape uses another n steps. So the total used in this stage is 2n steps. In big-O notation, we say that this stage uses O(n) steps. Note that we didn’t mention the repositioning of the tape head in the machine description. Using asymptotic notation allows us to omit details of the machine description that affect the running time by at most a constant factor. In stages 2 and 3, the machine repeatedly scans the tape and crosses off a 0 and 1 on each scan. Each scan uses O(n) steps. Because each scan crosses off two symbols, at most n/2 scans can occur. So the total time taken by stages 2 and 3 is (n/2)O(n) = O(n2) steps. In stage 4, the machine makes a single scan to decide whether to accept or reject. The time taken in this stage is at most O(n). Thus, the total time of M1 on an input of length n is O(n) + O(n2) + O(n), or O(n2). In other words, its running time is O(n2), which completes the time analysis of this machine. Let’s set up some notation for classifying languages according to their time requirements. 280 CHAPTER 7 / TIME COMPLEXITY Is there a machine that decides A asymptotically more quickly? In other words, is A in TIME(t(n)) for t(n) = o(n2)? We can improve the running time by crossing off two 0s and two 1s on every scan instead of just one because doing so cuts the number of scans by half. But that improves the running time only by a factor of 2 and doesn’t affect the asymptotic running time. The fol- lowing machine, M2, uses a different method to decide A asymptotically faster. It shows that A ∈ TIME(n log n). M2 = “On input string w: 1. Scan across the tape and reject if a 0 is found to the right of a 1. 2. Repeat as long as some 0s and some 1s remain on the tape: 3. Scan across the tape, checking whether the total number of 0s and 1s remaining is even or odd. If it is odd, reject . 4. Scan again across the tape, crossing off every other 0 starting with the ﬁrst 0, and then crossing off every other 1 starting with the ﬁrst 1. 5. If no 0s and no 1s remain on the tape, accept . Otherwise, reject .” Before analyzing M2, let’s verify that it actually decides A. On every scan performed in stage 4, the total number of 0s remaining is cut in half and any remainder is discarded. Thus, if we started with 13 0s, after stage 4 is executed a single time, only 6 0s remain. After subsequent executions of this stage, 3, then 1, and then 0 remain. This stage has the same effect on the number of 1s. Now we examine the even/odd parity of the number of 0s and the number of 1s at each execution of stage 3. Consider again starting with 13 0s and 13 1s. The ﬁrst execution of stage 3 ﬁnds an odd number of 0s (because 13 is an odd number) and an odd number of 1s. On subsequent executions, an even number (6) occurs, then an odd number (3), and an odd number (1). We do not execute this stage on 0 0s or 0 1s because of the condition on the repeat loop speciﬁed in stage 2. For the sequence of parities found (odd, even, odd, odd), if we replace the evens with 0s and the odds with 1s and then reverse the sequence, we obtain 1101, the binary representation of 13, or the number of 0s and 1s at the beginning. The sequence of parities always gives the reverse of the binary representation. When stage 3 checks to determine that the total number of 0s and 1s re- maining is even, it actually is checking on the agreement of the parity of the 0s with the parity of the 1s. If all parities agree, the binary representations of the numbers of 0s and of 1s agree, and so the two numbers are equal. To analyze the running time of M2, we ﬁrst observe that every stage takes O(n) time. We then determine the number of times that each is executed. Stages 1 and 5 are executed once, taking a total of O(n) time. Stage 4 crosses off at least half the 0s and 1s each time it is executed, so at most 1 + log2 n iter- ations of the repeat loop occur before all get crossed off. Thus the total time of stages 2, 3, and 4 is (1 + log2 n)O(n), or O(n log n). The running time of M2 is O(n) + O(n log n) = O(n log n). Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 7.1 MEASURING COMPLEXITY 281 Earlier we showed that A ∈ TIME(n2), but now we have a better bound— namely, A ∈ TIME(n log n). This result cannot be further improved on single- tape Turing machines. In fact, any language that can be decided in o(n log n) time on a single-tape Turing machine is regular, as Problem 7.49 asks you to show. We can decide the language A in O(n) time (also called linear time) if the Turing machine has a second tape. The following two-tape TM M3 decides A in linear time. Machine M3 operates differently from the previous machines for A. It simply copies the 0s to its second tape and then matches them against the 1s. M3 = “On input string w: 1. Scan across tape 1 and reject if a 0 is found to the right of a 1. 2. Scan across the 0s on tape 1 until the ﬁrst 1. At the same time, copy the 0s onto tape 2. 3. Scan across the 1s on tape 1 until the end of the input. For each 1 read on tape 1, cross off a 0 on tape 2. If all 0s are crossed off before all the 1s are read, reject . 4. If all the 0s have now been crossed off, accept . If any 0s remain, reject .” This machine is simple to analyze. Each of the four stages uses O(n) steps, so the total running time is O(n) and thus is linear. Note that this running time is the best possible because n steps are necessary just to read the input. Let’s summarize what we have shown about the time complexity of A, the amount of time required for deciding A. We produced a single-tape TM M1 that decides A in O(n2) time and a faster single tape TM M2 that decides A in O(n log n) time. The solution to Problem 7.49 implies that no single-tape TM can do it more quickly. Then we exhibited a two-tape TM M3 that decides A in O(n) time. Hence the time complexity of A on a single-tape TM is O(n log n), and on a two-tape TM it is O(n). Note that the complexity of A depends on the model of computation selected. This discussion highlights an important difference between complexity the- ory and computability theory. In computability theory, the Church–Turing thesis implies that all reasonable models of computation are equivalent—that is, they all decide the same class of languages. In complexity theory, the choice of model affects the time complexity of languages. Languages that are decidable in, say, linear time on one model aren’t necessarily decidable in linear time on another. In complexity theory, we classify computational problems according to their time complexity. But with which model do we measure time? The same language may have different time requirements on different models. Fortunately, time requirements don’t differ greatly for typical deterministic models. So, if our classiﬁcation system isn’t very sensitive to relatively small differences in complexity, the choice of deterministic model isn’t crucial. We discuss this idea further in the next several sections. Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 282 CHAPTER 7 / TIME COMPLEXITY COMPLEXITY RELATIONSHIPS AMONG MODELS Here we examine how the choice of computational model can affect the time complexity of languages. We consider three models: the single-tape Turing ma- chine; the multitape Turing machine; and the nondeterministic Turing machine. THEOREM 7.8 7.1 MEASURING COMPLEXITY 283 uses t(n) × O(t(n)) = O(t2(n)) steps. Therefore, the entire simulation of M uses O(n) + O(t2(n)) steps. We have assumed that t(n) ≥ n (a reasonable assumption because M could not even read the entire input in less time). Therefore, the running time of S is O(t2(n)) and the proof is complete. 284 CHAPTER 7 / TIME COMPLEXITY THEOREM 7.11 7.2 THE CLASS P 285 why we chose to make this separation between polynomials and exponentials rather than between some other classes of functions. First, note the dramatic difference between the growth rate of typically oc- curring polynomials such as n3 and typically occurring exponentials such as 2n. For example, let n be 1000, the size of a reasonable input to an algorithm. In that case, n3 is 1 billion, a large but manageable number, whereas 2n is a num- ber much larger than the number of atoms in the universe. Polynomial time algorithms are fast enough for many purposes, but exponential time algorithms rarely are useful. Exponential time algorithms typically arise when we solve problems by ex- haustively searching through a space of solutions, called brute-force search. For example, one way to factor a number into its constituent primes is to search through all potential divisors. The size of the search space is exponential, so this search uses exponential time. Sometimes brute-force search may be avoided through a deeper understanding of a problem, which may reveal a polynomial time algorithm of greater utility. All reasonable deterministic computational models are polynomially equiv- alent. That is, any one of them can simulate another with only a polynomial increase in running time. When we say that all reasonable deterministic models are polynomially equivalent, we do not attempt to deﬁne reasonable. However, we have in mind a notion broad enough to include models that closely approxi- mate running times on actual computers. For example, Theorem 7.8 shows that the deterministic single-tape and multitape Turing machine models are polyno- mially equivalent. From here on we focus on aspects of time complexity theory that are unaf- fected by polynomial differences in running time. Ignoring these differences allows us to develop a theory that doesn’t depend on the selection of a partic- ular model of computation. Remember, our aim is to present the fundamental properties of computation, rather than properties of Turing machines or any other special model. You may feel that disregarding polynomial differences in running time is ab- surd. Real programmers certainly care about such differences and work hard just to make their programs run twice as quickly. However, we disregarded constant factors a while back when we introduced asymptotic notation. Now we propose to disregard the much greater polynomial differences, such as that between time n and time n3. Our decision to disregard polynomial differences doesn’t imply that we con- sider such differences unimportant. On the contrary, we certainly do consider the difference between time n and time n3 to be an important one. But some questions, such as the polynomiality or nonpolynomiality of the factoring prob- lem, do not depend on polynomial differences and are important, too. We merely choose to focus on this type of question here. Ignoring the trees to see the forest doesn’t mean that one is more important than the other—it just gives a different perspective. Now we come to an important deﬁnition in complexity theory. Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 286 CHAPTER 7 / TIME COMPLEXITY 7.2 THE CLASS P 287 runs in polynomial time because we have demonstrated that it runs for a poly- nomial number of stages, each of which can be done in polynomial time, and the composition of polynomials is a polynomial. One point that requires attention is the encoding method used for problems. We continue to use the angle-bracket notation ⟨·⟩ to indicate a reasonable en- coding of one or more objects into a string, without specifying any particular encoding method. Now, a reasonable method is one that allows for polyno- mial time encoding and decoding of objects into natural internal representations or into other reasonable encodings. Familiar encoding methods for graphs, au- tomata, and the like all are reasonable. But note that unary notation for encoding numbers (as in the number 17 encoded by the unary string 11111111111111111) isn’t reasonable because it is exponentially larger than truly reasonable encod- ings, such as base k notation for any k ≥ 2. Many computational problems you encounter in this chapter contain encod- ings of graphs. One reasonable encoding of a graph is a list of its nodes and edges. Another is the adjacency matrix, where the (i, j)th entry is 1 if there is an edge from node i to node j and 0 if not. When we analyze algorithms on graphs, the running time may be computed in terms of the number of nodes instead of the size of the graph representation. In reasonable graph represen- tations, the size of the representation is a polynomial in the number of nodes. Thus, if we analyze an algorithm and show that its running time is polynomial (or exponential) in the number of nodes, we know that it is polynomial (or expo- nential) in the size of the input. The ﬁrst problem concerns directed graphs. A directed graph G contains nodes s and t, as shown in the following ﬁgure. The PATH problem is to deter- mine whether a directed path exists from s to t. Let PATH = {⟨G, s, t⟩| G is a directed graph that has a directed path from s to t}. FIGURE 7.13 The PATH problem: Is there a path from s to t? Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 288 CHAPTER 7 / TIME COMPLEXITY THEOREM 7.14 7.2 THE CLASS P 289 both are divisible by 2. Let RELPRIME be the problem of testing whether two numbers are relatively prime. Thus RELPRIME = {⟨x, y⟩| x and y are relatively prime}. THEOREM 7.15 290 CHAPTER 7 / TIME COMPLEXITY executed, x > y. If x/2 ≥ y, then x mod y < y ≤ x/2 and x drops by at least half. If x/2 < y, then x mod y = x − y < x/2 and x drops by at least half. The values of x and y are exchanged every time stage 3 is executed, so each of the original values of x and y are reduced by at least half every other time through the loop. Thus, the maximum number of times that stages 2 and 3 are executed is the lesser of 2 log2 x and 2 log2 y. These logarithms are proportional to the lengths of the representations, giving the number of stages executed as O(n). Each stage of E uses only polynomial time, so the total running time is polynomial. 7.2 THE CLASS P 291 It uses the entries for the shorter lengths to assist in determining the entries for the longer lengths. For example, suppose that the algorithm has already determined which vari- ables generate all substrings up to length k. To determine whether a variable A generates a particular substring of length k+1, the algorithm splits that substring into two nonempty pieces in the k possible ways. For each split, the algorithm examines each rule A → BC to determine whether B generates the ﬁrst piece and C generates the second piece, using table entries previously computed. If both B and C generate the respective pieces, A generates the substring and so is added to the associated table entry. The algorithm starts the process with the strings of length 1 by examining the table for the rules A → b. PROOF The following algorithm D implements the proof idea. Let G be a CFG in Chomsky normal form generating the CFL L. Assume that S is the start variable. (Recall that the empty string is handled specially in a Chomsky normal form grammar. The algorithm handles the special case in which w = ε in stage 1.) Comments appear inside double brackets. D = “On input w = w1 · · · wn: 1. For w = ε, if S → ε is a rule, accept ; else, reject . [[ w = ε case ]] 2. For i = 1 to n: [[ examine each substring of length 1 ]] 3. For each variable A: 4. Test whether A → b is a rule, where b = wi. 5. If so, place A in table(i, i). 6. For l = 2 to n: [[ l is the length of the substring ]] 7. For i = 1 to n − l + 1: [[ i is the start position of the substring ]] 8. Let j = i + l − 1. [[ j is the end position of the substring ]] 9. For k = i to j − 1: [[ k is the split position ]] 10. For each rule A → BC: 11. If table(i, k) contains B and table(k + 1, j) contains C, put A in table(i, j). 12. If S is in table(1, n), accept ; else, reject .” Now we analyze D. Each stage is easily implemented to run in polynomial time. Stages 4 and 5 run at most nv times, where v is the number of variables in G and is a ﬁxed constant independent of n; hence these stages run O(n) times. Stage 6 runs at most n times. Each time stage 6 runs, stage 7 runs at most n times. Each time stage 7 runs, stages 8 and 9 run at most n times. Each time stage 9 runs, stage 10 runs r times, where r is the number of rules of G and is another ﬁxed constant. Thus stage 11, the inner loop of the algorithm, runs O(n3) times. Summing the total shows that D executes O(n3) stages. 292 CHAPTER 7 / TIME COMPLEXITY 7.3 THE CLASS NP 293 The HAMPATH problem has a feature called polynomial veriﬁability that is important for understanding its complexity. Even though we don’t know of a fast (i.e., polynomial time) way to determine whether a graph contains a Hamiltonian path, if such a path were discovered somehow (perhaps using the exponential time algorithm), we could easily convince someone else of its existence simply by presenting it. In other words, verifying the existence of a Hamiltonian path may be much easier than determining its existence. Another polynomially veriﬁable problem is compositeness. Recall that a nat- ural number is composite if it is the product of two integers greater than 1 (i.e., a composite number is one that is not a prime number). Let COMPOSITES = {x| x = pq, for integers p, q > 1}. We can easily verify that a number is composite—all that is needed is a divisor of that number. Recently, a polynomial time algorithm for testing whether a number is prime or composite was discovered, but it is considerably more com- plicated than the preceding method for verifying compositeness. Some problems may not be polynomially veriﬁable. For example, take 294 CHAPTER 7 / TIME COMPLEXITY 7.3 THE CLASS NP 295 N = “On input w of length n: 1. Nondeterministically select string c of length at most nk. 2. Run V on input ⟨w, c⟩. 3. If V accepts, accept ; otherwise, reject .” To prove the other direction of the theorem, assume that A is decided by a polynomial time NTM N and construct a polynomial time veriﬁer V as follows. V = “On input ⟨w, c⟩, where w and c are strings: 1. Simulate N on input w, treating each symbol of c as a descrip- tion of the nondeterministic choice to make at each step (as in the proof of Theorem 3.16). 2. If this branch of N ’s computation accepts, accept ; otherwise, reject .” 296 CHAPTER 7 / TIME COMPLEXITY FIGURE 7.23 A graph with a 5-clique The clique problem is to determine whether a graph contains a clique of a speciﬁed size. Let CLIQUE = {⟨G, k⟩| G is an undirected graph with a k-clique}. THEOREM 7.24 7.3 THE CLASS NP 297 Thus, SUBSET-SUM = {⟨S, t⟩| S = {x1, . . . , xk}, and for some {y1, . . . , yl} ⊆ {x1, . . . , xk}, we have Σyi = t}. For example, ⟨{4, 11, 16, 21, 27}, 25⟩ ∈ SUBSET-SUM because 4 + 21 = 25. Note that {x1, . . . , xk} and {y1, . . . , yl} are considered to be multisets and so allow repetition of elements. THEOREM 7.25 298 CHAPTER 7 / TIME COMPLEXITY time. P is the class of languages where membership can be tested in polyno- mial time. We summarize this information as follows, where we loosely refer to polynomial time solvable as solvable “quickly.” P = the class of languages for which membership can be decided quickly. NP = the class of languages for which membership can be veriﬁed quickly. We have presented examples of languages, such as HAMPATH and CLIQUE, that are members of NP but that are not known to be in P. The power of polyno- mial veriﬁability seems to be much greater than that of polynomial decidability. But, hard as it may be to imagine, P and NP could be equal. We are unable to prove the existence of a single language in NP that is not in P. The question of whether P = NP is one of the greatest unsolved problems in theoretical computer science and contemporary mathematics. If these classes were equal, any polynomially veriﬁable problem would be polynomially decid- able. Most researchers believe that the two classes are not equal because people have invested enormous effort to ﬁnd polynomial time algorithms for certain problems in NP, without success. Researchers also have tried proving that the classes are unequal, but that would entail showing that no fast algorithm exists to replace brute-force search. Doing so is presently beyond scientiﬁc reach. The following ﬁgure shows the two possibilities. FIGURE 7.26 One of these two possibilities is correct The best deterministic method currently known for deciding languages in NP uses exponential time. In other words, we can prove that NP ⊆ EXPTIME = ⋃ k TIME(2n k ), but we don’t know whether NP is contained in a smaller deterministic time com- plexity class. Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 7.4 NP-COMPLETENESS 299300 CHAPTER 7 / TIME COMPLEXITY THEOREM 7.27 7.4 NP-COMPLETENESS 301 FIGURE 7.30 Polynomial time function f reducing A to B As with an ordinary mapping reduction, a polynomial time reduction of A to B provides a way to convert membership testing in A to membership testing in B—but now the conversion is done efﬁciently. To test whether w ∈ A, we use the reduction f to map w to f (w) and test whether f (w) ∈ B. If one language is polynomial time reducible to a language already known to have a polynomial time solution, we obtain a polynomial time solution to the original language, as in the following theorem. THEOREM 7.31 302 CHAPTER 7 / TIME COMPLEXITY form. A literal is a Boolean variable or a negated Boolean variable, as in x or 7.4 NP-COMPLETENESS 303 FIGURE 7.33 The graph that the reduction produces from ϕ = (x1 ∨ x1 ∨ x2) ∧ ( 304 CHAPTER 7 / TIME COMPLEXITY DEFINITION OF NP-COMPLETENESS 7.4 NP-COMPLETENESS 305 PROOF IDEA Showing that SAT is in NP is easy, and we do so shortly. The hard part of the proof is showing that any language in NP is polynomial time reducible to SAT. To do so, we construct a polynomial time reduction for each language A in NP to SAT . The reduction for A takes a string w and produces a Boolean formula ϕ that simulates the NP machine for A on input w. If the machine accepts, ϕ has a satisfying assignment that corresponds to the accepting computation. If the machine doesn’t accept, no assignment satisﬁes ϕ. Therefore, w is in A if and only if ϕ is satisﬁable. Actually constructing the reduction to work in this way is a conceptually simple task, though we must cope with many details. A Boolean formula may contain the Boolean operations AND, OR, and NOT, and these operations form the basis for the circuitry used in electronic computers. Hence the fact that we can design a Boolean formula to simulate a Turing machine isn’t surprising. The details are in the implementation of this idea. PROOF First, we show that SAT is in NP. A nondeterministic polynomial time machine can guess an assignment to a given formula ϕ and accept if the assignment satisﬁes ϕ. Next, we take any language A in NP and show that A is polynomial time reducible to SAT. Let N be a nondeterministic Turing machine that decides A in nk time for some constant k. (For convenience, we actually assume that N runs in time nk − 3; but only those readers interested in details should worry about this minor point.) The following notion helps to describe the reduction. A tableau for N on w is an nk × nk table whose rows are the conﬁgurations of a branch of the computation of N on input w, as shown in the following ﬁgure. FIGURE 7.38 A tableau is an nk × nk table of conﬁgurations Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 306 CHAPTER 7 / TIME COMPLEXITY For convenience later, we assume that each conﬁguration starts and ends with a # symbol. Therefore, the ﬁrst and last columns of a tableau are all #s. The ﬁrst row of the tableau is the starting conﬁguration of N on w, and each row follows the previous one according to N ’s transition function. A tableau is accepting if any row of the tableau is an accepting conﬁguration. Every accepting tableau for N on w corresponds to an accepting computation branch of N on w. Thus, the problem of determining whether N accepts w is equivalent to the problem of determining whether an accepting tableau for N on w exists. Now we get to the description of the polynomial time reduction f from A to SAT. On input w, the reduction produces a formula ϕ. We begin by describing the variables of ϕ. Say that Q and Γ are the state set and tape alphabet of N , respectively. Let C = Q ∪ Γ ∪ {#}. For each i and j between 1 and nk and for each s in C, we have a variable, xi,j,s. Each of the (nk) 2 entries of a tableau is called a cell. The cell in row i and column j is called cell [i, j] and contains a symbol from C. We represent the contents of the cells with the variables of ϕ. If xi,j,s takes on the value 1, it means that cell [i, j] contains an s. Now we design ϕ so that a satisfying assignment to the variables does corre- spond to an accepting tableau for N on w. The formula ϕ is the AND of four parts: ϕcell ∧ ϕstart ∧ ϕmove ∧ ϕaccept. We describe each part in turn. As we mentioned previously, turning variable xi,j,s on corresponds to placing symbol s in cell [i, j]. The ﬁrst thing we must guarantee in order to obtain a cor- respondence between an assignment and a tableau is that the assignment turns on exactly one variable for each cell. Formula ϕcell ensures this requirement by expressing it in terms of Boolean operations: ϕcell = ⋀ 1≤i,j≤nk [( ⋁ s∈C xi,j,s) ∧ ( ⋀ s,t∈C s̸=t ( 7.4 NP-COMPLETENESS 307 The ﬁrst part of ϕcell inside the brackets stipulates that at least one variable that is associated with each cell is on, whereas the second part stipulates that no more than one variable is on for each cell. Any assignment to the variables that satisﬁes ϕ (and therefore ϕcell) must have exactly one variable on for every cell. Thus, any satisfying assignment speciﬁes one symbol in each cell of the table. Parts ϕstart, ϕmove, and ϕaccept ensure that these symbols actually correspond to an accepting tableau as follows. Formula ϕstart ensures that the ﬁrst row of the table is the starting conﬁgu- ration of N on w by explicitly stipulating that the corresponding variables are on: ϕstart = x1,1,# ∧ x1,2,q0 ∧ x1,3,w1 ∧ x1,4,w2 ∧ . . . ∧ x1,n+2,wn ∧ x1,n+3,␣ ∧ . . . ∧ x1,nk−1,␣ ∧ x1,nk,# . Formula ϕaccept guarantees that an accepting conﬁguration occurs in the tableau. It ensures that qaccept, the symbol for the accept state, appears in one of the cells of the tableau by stipulating that one of the corresponding variables is on: ϕaccept = ⋁ 1≤i,j≤nk xi,j,qaccept . Finally, formula ϕmove guarantees that each row of the tableau corresponds to a conﬁguration that legally follows the preceding row’s conﬁguration according to N ’s rules. It does so by ensuring that each 2 × 3 window of cells is legal. We say that a 2 × 3 window is legal if that window does not violate the actions speciﬁed by N ’s transition function. In other words, a window is legal if it might appear when one conﬁguration correctly follows another.3 For example, say that a, b, and c are members of the tape alphabet, and q1 and q2 are states of N . Assume that when in state q1 with the head reading an a, N writes a b, stays in state q1, and moves right; and that when in state q1 with the head reading a b, N nondeterministically either 1. writes a c, enters q2, and moves to the left, or 2. writes an a, enters q2, and moves to the right. Expressed formally, δ(q1, a) = {(q1,b,R)} and δ(q1, b) = {(q2,c,L), (q2,a,R)}. Examples of legal windows for this machine are shown in Figure 7.39. 308 CHAPTER 7 / TIME COMPLEXITY7.4 NP-COMPLETENESS 309 We prove this claim by considering any two adjacent conﬁgurations in the tableau, called the upper conﬁguration and the lower conﬁguration. In the up- per conﬁguration, every cell that contains a tape symbol and isn’t adjacent to a state symbol is the center top cell in a window whose top row contains no states. Therefore, that symbol must appear unchanged in the center bottom of the window. Hence it appears in the same position in the bottom conﬁguration. The window containing the state symbol in the center top cell guarantees that the corresponding three positions are updated consistently with the transition function. Therefore, if the upper conﬁguration is a legal conﬁguration, so is the lower conﬁguration, and the lower one follows the upper one according to N ’s rules. Note that this proof, though straightforward, depends crucially on our choice of a 2 × 3 window size, as Problem 7.41 shows. Now we return to the construction of ϕmove. It stipulates that all the windows in the tableau are legal. Each window contains six cells, which may be set in a ﬁxed number of ways to yield a legal window. Formula ϕmove says that the settings of those six cells must be one of these ways, or ϕmove = ⋀ 1≤i<nk, 1<j<nk ( the (i, j)-window is legal ) . The (i, j)-window has cell [i, j] as the upper central position. We replace the text “the (i, j)-window is legal” in this formula with the following formula. We write the contents of six cells of a window as a1, . . . , a6. ⋁ a1,...,a6 is a legal window ( xi,j−1,a1 ∧ xi,j,a2 ∧ xi,j+1,a3 ∧ xi+1,j−1,a4 ∧ xi+1,j,a5 ∧ xi+1,j+1,a6 ) Next, we analyze the complexity of the reduction to show that it operates in polynomial time. To do so, we examine the size of ϕ. First, we estimate the number of variables it has. Recall that the tableau is an nk × nk table, so it contains n2k cells. Each cell has l variables associated with it, where l is the number of symbols in C. Because l depends only on the TM N and not on the length of the input n, the total number of variables is O(n2k). We estimate the size of each of the parts of ϕ. Formula ϕcell contains a ﬁxed- size fragment of the formula for each cell of the tableau, so its size is O(n2k). Formula ϕstart has a fragment for each cell in the top row, so its size is O(nk). Formulas ϕmove and ϕaccept each contain a ﬁxed-size fragment of the formula for each cell of the tableau, so their size is O(n2k). Thus, ϕ’s total size is O(n2k). That bound is sufﬁcient for our purposes because it shows that the size of ϕ is polynomial in n. If it were more than polynomial, the reduction wouldn’t have any chance of generating it in polynomial time. (Actually, our estimates are low by a factor of O(log n) because each variable has indices that can range up to nk and so may require O(log n) symbols to write into the formula, but this additional factor doesn’t change the polynomiality of the result.) To see that we can generate the formula in polynomial time, observe its highly repetitive nature. Each component of the formula is composed of many nearly Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 310 CHAPTER 7 / TIME COMPLEXITY identical fragments, which differ only at the indices in a simple way. Therefore, we may easily construct a reduction that produces ϕ in polynomial time from the input w. 7.5 ADDITIONAL NP-COMPLETE PROBLEMS 311 variable. If some setting of the ai’s satisﬁes the original clause, we can ﬁnd some setting of z so that the two new clauses are satisﬁed and vice versa. In general, if the clause contains l literals, (a1 ∨ a2 ∨ · · · ∨ al), we can replace it with the l − 2 clauses (a1 ∨ a2 ∨ z1) ∧ ( 312 CHAPTER 7 / TIME COMPLEXITY THE VERTEX COVER PROBLEM If G is an undirected graph, a vertex cover of G is a subset of the nodes where every edge of G touches one of those nodes. The vertex cover problem asks whether a graph contains a vertex cover of a speciﬁed size: VERTEX-COVER = {⟨G, k⟩| G is an undirected graph that has a k-node vertex cover}. THEOREM 7.44 7.5 ADDITIONAL NP-COMPLETE PROBLEMS 313 The gadgets for the clauses are a bit more complex. Each clause gadget is a triple of nodes that are labeled with the three literals of the clause. These three nodes are connected to each other and to the nodes in the variable gadgets that have the identical labels. Thus, the total number of nodes that appear in G is 2m + 3l, where ϕ has m variables and l clauses. Let k be m + 2l. For example, if ϕ = (x1 ∨ x1 ∨ x2) ∧ ( 314 CHAPTER 7 / TIME COMPLEXITY THE HAMILTONIAN PATH PROBLEM Recall that the Hamiltonian path problem asks whether the input graph contains a path from s to t that goes through every node exactly once. THEOREM 7.46 7.5 ADDITIONAL NP-COMPLETE PROBLEMS 315 We represent each clause of ϕ as a single node, as follows. FIGURE 7.48 Representing the clause cj as a node The following ﬁgure depicts the global structure of G. It shows all the ele- ments of G and their relationships, except the edges that represent the relation- ship of the variables to the clauses that contain them. FIGURE 7.49 The high-level structure of G Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 316 CHAPTER 7 / TIME COMPLEXITY Next, we show how to connect the diamonds representing the variables to the nodes representing the clauses. Each diamond structure contains a horizontal row of nodes connected by edges running in both directions. The horizontal row contains 3k + 1 nodes in addition to the two nodes on the ends belonging to the diamond. These nodes are grouped into adjacent pairs, one for each clause, with extra separator nodes next to the pairs, as shown in the following ﬁgure. FIGURE 7.50 The horizontal nodes in a diamond structure If variable xi appears in clause cj, we add the following two edges from the jth pair in the ith diamond to the jth clause node. FIGURE 7.51 The additional edges when clause cj contains xi If 7.5 ADDITIONAL NP-COMPLETE PROBLEMS 317 FIGURE 7.52 The additional edges when clause cj contains 318 CHAPTER 7 / TIME COMPLEXITY in the correct order to allow a detour and return. (Note that each true literal in a clause provides an option of a detour to hit the clause node. As a result, if several literals in a clause are true, only one detour is taken.) Thus, we have constructed the desired Hamiltonian path. For the reverse direction, if G has a Hamiltonian path from s to t, we demon- strate a satisfying assignment for ϕ. If the Hamiltonian path is normal—that is, it goes through the diamonds in order from the top one to the bottom one, except for the detours to the clause nodes—we can easily obtain the satisfying assign- ment. If the path zig-zags through the diamond, we assign the corresponding variable TRUE; and if it zag-zigs, we assign FALSE. Because each clause node ap- pears on the path, by observing how the detour to it is taken, we may determine which of the literals in the corresponding clause is TRUE. All that remains to be shown is that a Hamiltonian path must be normal. Normality may fail only if the path enters a clause from one diamond but returns to another, as in the following ﬁgure. FIGURE 7.54 This situation cannot occur The path goes from node a1 to c; but instead of returning to a2 in the same diamond, it returns to b2 in a different diamond. If that occurs, either a2 or a3 must be a separator node. If a2 were a separator node, the only edges entering a2 would be from a1 and a3. If a3 were a separator node, a1 and a2 would be in the same clause pair, and hence the only edges entering a2 would be from a1, a3, and c. In either case, the path could not contain node a2. The path cannot enter a2 from c or a1 because the path goes elsewhere from these nodes. The path cannot enter a2 from a3 because a3 is the only available node that a2 points at, so the path must exit a2 via a3. Hence a Hamiltonian path must be normal. This reduction obviously operates in polynomial time and the proof is complete. 7.5 ADDITIONAL NP-COMPLETE PROBLEMS 319 Next, we consider an undirected version of the Hamiltonian path problem, called UHAMPATH. To show that UHAMPATH is NP-complete, we give a polynomial time reduction from the directed version of the problem. THEOREM 7.55 320 CHAPTER 7 / TIME COMPLEXITY THEOREM 7.56 7.5 ADDITIONAL NP-COMPLETE PROBLEMS 321 Finally, the target number t, the bottom row of the table, consists of l 1s followed by k 3s. 1 2 3 4 · · · l 322 CHAPTER 7 / TIME COMPLEXITY Now we make the satisfying assignment. If the subset contains yi, we assign xi TRUE; otherwise, we assign it FALSE. This assignment must satisfy ϕ because in each of the ﬁnal k columns, the sum is always 3. In column cj, at most 2 can come from gj and hj, so at least 1 in this column must come from some yi or zi in the subset. If it is yi, then xi appears in cj and is assigned TRUE, so cj is satisﬁed. If it is zi, then PROBLEMS 323 7.8 Let CONNECTED = {⟨G⟩| G is a connected undirected graph}. Analyze the algorithm given on page 185 to show that this language is in P. 7.9 A triangle in an undirected graph is a 3-clique. Show that TRIANGLE ∈ P, where TRIANGLE = {⟨G⟩| G contains a triangle}. 7.10 Show that ALLDFA is in P. 7.11 In both parts, provide an analysis of the time complexity of your algorithm. a. Show that EQ DFA ∈ P. b. Say that a language A is star-closed if A = A∗. Give a polynomial time algorithm to test whether a DFA recognizes a star-closed language. (Note that EQ NFA is not known to be in P.) 7.12 Call graphs G and H isomorphic if the nodes of G may be reordered so that it is identical to H. Let ISO = {⟨G, H⟩| G and H are isomorphic graphs}. Show that ISO ∈ NP. 324 CHAPTER 7 / TIME COMPLEXITY ⋆7.19 Show that PRIMES = {m| m is a prime number in binary} ∈ NP. (Hint: For p > 1, the multiplicative group Z∗ p = {x| x is relatively prime to p and 1 ≤ x < p} is both cyclic and of order p − 1 iff p is prime. You may use this fact without justifying it. The stronger statement PRIMES ∈ P is now known to be true, but it is more difﬁcult to prove.) 7.20 We generally believe that PATH is not NP-complete. Explain the reason behind this belief. Show that proving PATH is not NP-complete would prove P ̸= NP. 7.21 Let G represent an undirected graph. Also let SPATH = {⟨G, a, b, k⟩| G contains a simple path of length at most k from a to b}, and LPATH = {⟨G, a, b, k⟩| G contains a simple path of length at least k from a to b}. a. Show that SPATH ∈ P. b. Show that LPATH is NP-complete. 7.22 Let DOUBLE-SAT = {⟨ϕ⟩| ϕ has at least two satisfying assignments}. Show that DOUBLE-SAT is NP-complete. A7.23 Let HALF-CLIQUE = {⟨G⟩| G is an undirected graph having a complete sub- graph with at least m/2 nodes, where m is the number of nodes in G}. Show that HALF-CLIQUE is NP-complete. 7.24 Let CNFk = {⟨ϕ⟩| ϕ is a satisﬁable cnf-formula where each variable appears in at most k places}. a. Show that CNF2 ∈ P. b. Show that CNF3 is NP-complete. 7.25 Let CNFH = {⟨ϕ⟩| ϕ is a satisﬁable cnf-formula where each clause contains any number of literals, but at most one negated literal}. Show that CNFH ∈ P. 7.26 Let ϕ be a 3cnf-formula. An ̸=-assignment to the variables of ϕ is one where each clause contains two literals with unequal truth values. In other words, an ̸=-assignment satisﬁes ϕ without assigning three true literals in any clause. a. Show that the negation of any ̸=-assignment to ϕ is also an ̸=-assignment. b. Let ̸=SAT be the collection of 3cnf-formulas that have an ̸=-assignment. Show that we obtain a polynomial time reduction from 3SAT to ̸=SAT by replacing each clause ci (y1 ∨ y2 ∨ y3) with the two clauses (y1 ∨ y2 ∨ zi) and ( PROBLEMS 325 7.27 A cut in an undirected graph is a separation of the vertices V into two disjoint subsets S and T . The size of a cut is the number of edges that have one endpoint in S and the other in T . Let MAX-CUT = {⟨G, k⟩| G has a cut of size k or more}. Show that MAX-CUT is NP-complete. You may assume the result of Prob- lem 7.26. (Hint: Show that ̸=SAT ≤P MAX-CUT. The variable gadget for variable x is a collection of 3c nodes labeled with x and another 3c nodes labeled with 326 CHAPTER 7 / TIME COMPLEXITY 7.30 Let SET-SPLITTING = {⟨S, C⟩| S is a ﬁnite set and C = {C1, . . . , Ck} is a collection of subsets of S, for some k > 0, such that elements of S can be colored red or blue so that no Ci has all its elements colored with the same color}. Show that SET-SPLITTING is NP-complete. 7.31 Consider the following scheduling problem. You are given a list of ﬁnal exams F1, . . . , Fk to be scheduled, and a list of students S1, . . . , Sl. Each student is taking some speciﬁed subset of these exams. You must schedule these exams into slots so that no student is required to take two exams in the same slot. The problem is to determine if such a schedule exists that uses only h slots. Formulate this problem as a language and show that this language is NP-complete. 7.32 This problem is inspired by the single-player game Minesweeper, generalized to an arbitrary graph. Let G be an undirected graph, where each node either contains a single, hidden mine or is empty. The player chooses nodes, one by one. If the player chooses a node containing a mine, the player loses. If the player chooses an empty node, the player learns the number of neighboring nodes containing mines. (A neighboring node is one connected to the chosen node by an edge.) The player wins if and when all empty nodes have been so chosen. In the mine consistency problem, you are given a graph G along with numbers labeling some of G’s nodes. You must determine whether a placement of mines on the remaining nodes is possible, so that any node v that is labeled m has exactly m neighboring nodes containing mines. Formulate this problem as a language and show that it is NP-complete. A7.33 In the following solitaire game, you are given an m × m board. On each of its m2 positions lies either a blue stone, a red stone, or nothing at all. You play by removing stones from the board until each column contains only stones of a sin- gle color and each row contains at least one stone. You win if you achieve this objective. Winning may or may not be possible, depending upon the initial con- ﬁguration. Let SOLITAIRE = {⟨G⟩| G is a winnable game conﬁguration}. Prove that SOLITAIRE is NP-complete. 7.34 Recall, in our discussion of the Church–Turing thesis, that we introduced the lan- guage D = {⟨p⟩| p is a polynomial in several variables having an integral root}. We stated, but didn’t prove, that D is undecidable. In this problem, you are to prove a different property of D—namely, that D is NP-hard. A problem is NP-hard if all problems in NP are polynomial time reducible to it, even though it may not be in NP itself. So you must show that all problems in NP are polynomial time reducible to D. 7.35 A subset of the nodes of a graph G is a dominating set if every other node of G is adjacent to some node in the subset. Let DOMINATING-SET = {⟨G, k⟩| G has a dominating set with k nodes}. Show that it is NP-complete by giving a reduction from VERTEX-COVER. ⋆7.36 Show that the following problem is NP-complete. You are given a set of states Q = {q0, q1, . . . , ql} and a collection of pairs {(s1, r1), . . . , (sk, rk)} where the si are distinct strings over Σ = {0, 1}, and the ri are (not necessarily distinct) members of Q. Determine whether a DFA M = (Q, Σ, δ, q0, F ) exists where δ(q0, si) = ri for each i. Here, δ(q, s) is the state that M enters after reading s, starting at state q. (Note that F is irrelevant here.) Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. PROBLEMS 327 7.37 Let U = {⟨M, x, #t⟩| NTM M accepts x within t steps on at least one branch}. Note that M isn’t required to halt on all branches. Show that U is NP-complete. ⋆7.38 Show that if P = NP, a polynomial time algorithm exists that produces a satisfying assignment when given a satisﬁable Boolean formula. (Note: The algorithm you are asked to provide computes a function; but NP contains languages, not func- tions. The P = NP assumption implies that SAT is in P, so testing satisﬁability is solvable in polynomial time. But the assumption doesn’t say how this test is done, and the test may not reveal satisfying assignments. You must show that you can ﬁnd them anyway. Hint: Use the satisﬁability tester repeatedly to ﬁnd the assignment bit-by-bit.) ⋆7.39 Show that if P = NP, you can factor integers in polynomial time. (See the note in Problem 7.38.) A⋆7.40 Show that if P = NP, a polynomial time algorithm exists that takes an undirected graph as input and ﬁnds a largest clique contained in that graph. (See the note in Problem 7.38.) 7.41 In the proof of the Cook–Levin theorem, a window is a 2 × 3 rectangle of cells. Show why the proof would have failed if we had used 2 × 2 windows instead. ⋆7.42 Consider the algorithm MINIMIZE, which takes a DFA M as input and outputs DFA M ′. MINIMIZE = “On input ⟨M ⟩, where M = (Q, Σ, δ, q0, A) is a DFA: 1. Remove all states of M that are unreachable from the start state. 2. Construct the following undirected graph G whose nodes are the states of M . 3. Place an edge in G connecting every accept state with every nonaccept state. Add additional edges as follows. 4. Repeat until no new edges are added to G: 5. For every pair of distinct states q and r of M and every a ∈ Σ: 6. Add the edge (q, r) to G if (δ(q, a), δ(r, a)) is an edge of G. 7. For each state q, let [q] be the collection of states [q] = {r ∈ Q| no edge joins q and r in G}. 8. Form a new DFA M ′ = (Q′, Σ, δ′, q0′, A′) where Q′ = {[q]| q ∈ Q} (if [q] = [r], only one of them is in Q′), δ′([q], a) = [δ(q, a)] for every q ∈ Q and a ∈ Σ, q0′ = [q0], and A′ = {[q]| q ∈ A}. 9. Output ⟨M ′⟩.” a. Show that M and M ′ are equivalent. b. Show that M ′ is minimal—that is, no DFA with fewer states recognizes the same language. You may use the result of Problem 1.52 without proof. c. Show that MINIMIZE operates in polynomial time. 7.43 For a cnf-formula ϕ with m variables and c clauses, show that you can construct in polynomial time an NFA with O(cm) states that accepts all nonsatisfying assign- ments, represented as Boolean strings of length m. Conclude that P ̸= NP implies that NFAs cannot be minimized in polynomial time. ⋆7.44 A 2cnf-formula is an AND of clauses, where each clause is an OR of at most two literals. Let 2SAT = {⟨ϕ⟩| ϕ is a satisﬁable 2cnf-formula}. Show that 2SAT ∈ P. Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 328 CHAPTER 7 / TIME COMPLEXITY 7.45 Modify the algorithm for context-free language recognition in the proof of The- orem 7.16 to give a polynomial time algorithm that produces a parse tree for a string, given the string and a CFG, if that grammar generates the string. 7.46 Say that two Boolean formulas are equivalent if they have the same set of variables and are true on the same set of assignments to those variables (i.e., they describe the same Boolean function). A Boolean formula is minimal if no shorter Boolean formula is equivalent to it. Let MIN-FORMULA be the collection of minimal Boolean formulas. Show that if P = NP, then MIN-FORMULA ∈ P. 7.47 The difference hierarchy DiP is deﬁned recursively as a. D1P = NP and b. DiP = {A| A = B \\ C for B in NP and C in Di−1P}. (Here B \\ C = B ∩ SELECTED SOLUTIONS 329 7.54 In a directed graph, the indegree of a node is the number of incoming edges and the outdegree is the number of outgoing edges. Show that the following problem is NP-complete. Given an undirected graph G and a designated subset C of G’s nodes, is it possible to convert G to a directed graph by assigning directions to each of its edges so that every node in C has indegree 0 or outdegree 0, and every other node in G has indegree at least 1? 330 CHAPTER 7 / TIME COMPLEXITY 7.40 If you assume that P = NP, then CLIQUE ∈ P, and you can test whether G con- tains a clique of size k in polynomial time, for any value of k. By testing whether G contains a clique of each size, from 1 to the number of nodes in G, you can deter- mine the size t of a maximum clique in G in polynomial time. Once you know t, you can ﬁnd a clique with t nodes as follows. For each node x of G, remove x and calculate the resulting maximum clique size. If the resulting size decreases, replace x and continue with the next node. If the resulting size is still t, keep x perma- nently removed and continue with the next node. When you have considered all nodes in this way, the remaining nodes are a t-clique. Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 332 CHAPTER 8 / SPACE COMPLEXITY We typically estimate the space complexity of Turing machines by using asymptotic notation. 8.1 SAVITCH’S THEOREM 333 the problem of testing whether a nondeterministic ﬁnite automaton accepts all strings. Let ALLNFA = {⟨A⟩| A is an NFA and L(A) = Σ∗}. We give a nondeterministic linear space algorithm that decides the complement of this language, 334 CHAPTER 8 / SPACE COMPLEXITY THEOREM 8.5 8.1 SAVITCH’S THEOREM 335 path. If not, CANYIELD outputs reject . For convenience, we assume that t is a power of 2. CANYIELD = “On input c1, c2, and t: 1. If t = 1, then test directly whether c1 = c2 or whether c1 yields c2 in one step according to the rules of N . Accept if either test succeeds; reject if both fail. 2. If t > 1, then for each conﬁguration cm of N using space f (n): 3. Run CANYIELD(c1, cm, t 336 CHAPTER 8 / SPACE COMPLEXITY8.3 PSPACE-COMPLETENESS 337 believe that all the containments are proper. The following diagram depicts the relationships among these classes, assuming that all are different. FIGURE 8.7 Conjectured relationships among P, NP, PSPACE, and EXPTIME 338 CHAPTER 8 / SPACE COMPLEXITY Complete problems are important because they are examples of the most difﬁcult problems in a complexity class. A complete problem is most difﬁcult because any other problem in the class is easily reduced into it. So if we ﬁnd an easy way to solve the complete problem, we can easily solve all other problems in the class. The reduction must be easy, relative to the complexity of typical prob- lems in the class, for this reasoning to apply. If the reduction itself were difﬁcult to compute, an easy solution to the complete problem wouldn’t necessarily yield an easy solution to the problems reducing to it. Therefore, the rule is: Whenever we deﬁne complete problems for a com- plexity class, the reduction model must be more limited than the model used for deﬁning the class itself. THE TQBF PROBLEM Our ﬁrst example of a PSPACE-complete problem involves a generalization of the satisﬁability problem. Recall that a Boolean formula is an expression that contains Boolean variables, the constants 0 and 1, and the Boolean operations ∧, ∨, and ¬. We now introduce a more general type of Boolean formula. The quantiﬁers ∀ (for all) and ∃ (there exists) make frequent appearances in mathematical statements. Writing the statement ∀x ϕ means that for every value for the variable x, the statement ϕ is true. Similarly, writing the statement ∃x ϕ means that for some value of the variable x, the statement ϕ is true. Sometimes, ∀ is referred to as the universal quantiﬁer and ∃ as the existential quantiﬁer. We say that the variable x immediately following the quantiﬁer is bound to the quantiﬁer. For example, considering the natural numbers, the statement ∀x [x + 1 > x] means that the successor x + 1 of every natural number x is greater than the number itself. Obviously, this statement is true. However, the statement ∃y [y + y = 3] obviously is false. When interpreting the meaning of statements involving quantiﬁers, we must consider the universe from which the values are drawn. In the preceding cases, the universe comprised the natural numbers; but if we took the real numbers instead, the existentially quantiﬁed statement would become true. Statements may contain several quantiﬁers, as in ∀x ∃y [y > x]. For the uni- verse of the natural numbers, this statement says that every natural number has another natural number larger than it. The order of the quantiﬁers is impor- tant. Reversing the order, as in the statement ∃y ∀x [y > x], gives an entirely different meaning—namely, that some natural number is greater than all others. Obviously, the ﬁrst statement is true and the second statement is false. A quantiﬁer may appear anywhere in a mathematical statement. It applies to the fragment of the statement appearing within the matched pair of parentheses or brackets following the quantiﬁed variable. This fragment is called the scope of the quantiﬁer. Often, it is convenient to require that all quantiﬁers appear at the beginning of the statement and that each quantiﬁer’s scope is everything fol- lowing it. Such statements are said to be in prenex normal form. Any statement Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 8.3 PSPACE-COMPLETENESS 339 may be put into prenex normal form easily. We consider statements in this form only, unless otherwise indicated. Boolean formulas with quantiﬁers are called quantiﬁed Boolean formulas. For such formulas, the universe is {0, 1}. For example, ϕ = ∀x ∃y [ (x ∨ y) ∧ ( 340 CHAPTER 8 / SPACE COMPLEXITY PROOF First, we give a polynomial space algorithm deciding TQBF. T = “On input ⟨ϕ⟩, a fully quantiﬁed Boolean formula: 1. If ϕ contains no quantiﬁers, then it is an expression with only constants, so evaluate ϕ and accept if it is true; otherwise, reject . 2. If ϕ equals ∃x ψ, recursively call T on ψ, ﬁrst with 0 substituted for x and then with 1 substituted for x. If either result is accept, then accept ; otherwise, reject . 3. If ϕ equals ∀x ψ, recursively call T on ψ, ﬁrst with 0 substituted for x and then with 1 substituted for x. If both results are ac- cept, then accept ; otherwise, reject .” Algorithm T obviously decides TQBF. To analyze its space complexity, we observe that the depth of the recursion is at most the number of variables. At each level we need only store the value of one variable, so the total space used is O(m), where m is the number of variables that appear in ϕ. Therefore, T runs in linear space. Next, we show that TQBF is PSPACE-hard. Let A be a language decided by a TM M in space nk for some constant k. We give a polynomial time reduction from A to TQBF. The reduction maps a string w to a quantiﬁed Boolean formula ϕ that is true iff M accepts w. To show how to construct ϕ, we solve a more general problem. Using two collections of variables denoted c1 and c2 representing two conﬁgu- rations and a number t > 0, we construct a formula ϕc1,c2,t. If we assign c1 and c2 to actual conﬁgurations, the formula is true iff M can go from c1 to c2 in at most t steps. Then we can let ϕ be the formula ϕcstart,caccept ,h, where h = 2df (n) for a constant d, chosen so that M has no more than 2df (n) possible conﬁgurations on an input of length n. Here, let f (n) = nk. For convenience, we assume that t is a power of 2. The formula encodes the contents of conﬁguration cells as in the proof of the Cook–Levin theorem. Each cell has several variables associated with it, one for each tape symbol and state, corresponding to the possible settings of that cell. Each conﬁguration has nk cells and so is encoded by O(nk) variables. If t = 1, we can easily construct ϕc1,c2,t. We design the formula to say that either c1 equals c2, or c2 follows from c1 in a single step of M . We express the equality by writing a Boolean expression saying that each of the variables representing c1 contains the same Boolean value as the corresponding variable representing c2. We express the second possibility by using the technique pre- sented in the proof of the Cook–Levin theorem. That is, we can express that c1 yields c2 in a single step of M by writing Boolean expressions stating that the contents of each triple of c1’s cells correctly yields the contents of the corre- sponding triple of c2’s cells. If t > 1, we construct ϕc1,c2,t recursively. As a warm-up, let’s try one idea that doesn’t quite work and then ﬁx it. Let ϕc1,c2,t = ∃m1 [ϕc1,m1, t 8.3 PSPACE-COMPLETENESS 341 The symbol m1 represents a conﬁguration of M . Writing ∃m1 is shorthand for ∃x1, . . . , xl, where l = O(nk) and x1, . . . , xl are the variables that encode m1. So this construction of ϕc1,c2,t says that M can go from c1 to c2 in at most t steps if some intermediate conﬁguration m1 exists, whereby M can go from c1 to m1 in at most t 342 CHAPTER 8 / SPACE COMPLEXITY Let ϕ = ∃x1 ∀x2 ∃x3 · · · Qxk [ ψ ] be a quantiﬁed Boolean formula in prenex normal form. Here, Q represents either a ∀ or an ∃ quantiﬁer. We associate a game with ϕ as follows. Two players, called Player A and Player E, take turns selecting the values of the variables x1, . . . , xk. Player A selects values for the variables that are bound to ∀ quantiﬁers, and Player E selects values for the variables that are bound to ∃ quantiﬁers. The order of play is the same as that of the quantiﬁers at the beginning of the formula. At the end of play, we use the values that the players have selected for the variables and declare that Player E has won the game if ψ, the part of the formula with the quantiﬁers stripped off, is now TRUE. Player A has won if ψ is now FALSE. EXAMPLE 8.10 8.3 PSPACE-COMPLETENESS 343 THEOREM 8.11 344 CHAPTER 8 / SPACE COMPLEXITY FIGURE 8.12 Portion of the graph representing the geography game When the rules of geography are interpreted for this graphic representation, one player starts by selecting the designated start node and then the players take turns picking nodes that form a simple path in the graph. The requirement that the path be simple (i.e., doesn’t use any node more than once) corresponds to the requirement that a city may not be repeated. The ﬁrst player unable to extend the path loses the game. In generalized geography, we take an arbitrary directed graph with a des- ignated start node instead of the graph associated with the actual cities. For example, the following graph is an example of a generalized geography game. FIGURE 8.13 A sample generalized geography game Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 8.3 PSPACE-COMPLETENESS 345 Say that Player I is the one who moves ﬁrst and Player II second. In this example, Player I has a winning strategy as follows. Player I starts at node 1, the designated start node. Node 1 points only at nodes 2 and 3, so Player I’s ﬁrst move must be one of these two choices. He chooses 3. Now Player II must move, but node 3 points only to node 5, so she is forced to select node 5. Then Player I selects 6, from choices 6, 7, and 8. Now Player II must play from node 6, but it points only to node 3, and 3 was previously played. Player II is stuck and thus Player I wins. If we change the example by reversing the direction of the edge between nodes 3 and 6, Player II has a winning strategy. Can you see it? If Player I starts out with node 3 as before, Player II responds with 6 and wins immediately, so Player I’s only hope is to begin with 2. In that case, however, Player II responds with 4. If Player I now takes 5, Player II wins with 6. If Player I takes 7, Player II wins with 9. No matter what Player I does, Player II can ﬁnd a way to win, so Player II has a winning strategy. The problem of determining which player has a winning strategy in a gener- alized geography game is PSPACE-complete. Let GG = {⟨G, b⟩| Player I has a winning strategy for the generalized geography game played on graph G starting at node b}. THEOREM 8.14 346 CHAPTER 8 / SPACE COMPLEXITY The only space required by this algorithm is for storing the recursion stack. Each level of the recursion adds a single node to the stack, and at most m levels occur, where m is the number of nodes in G. Hence the algorithm runs in linear space. To establish the PSPACE-hardness of GG, we show that FORMULA-GAME is polynomial time reducible to GG. The reduction maps the formula ϕ = ∃x1 ∀x2 ∃x3 · · · Qxk [ ψ ] to an instance ⟨G, b⟩ of generalized geography. Here we assume for simplicity that ϕ’s quantiﬁers begin and end with ∃, and that they strictly alternate between ∃ and ∀. A formula that doesn’t conform to this assumption may be converted to a slightly larger one that does by adding extra quantiﬁers binding otherwise unused or “dummy” variables. We assume also that ψ is in conjunctive normal form (see Problem 8.12). The reduction constructs a geography game on a graph G where optimal play mimics optimal play of the formula game on ϕ. Player I in the geography game takes the role of Player E in the formula game, and Player II takes the role of Player A. The structure of graph G is partially shown in the following ﬁgure. Play starts at node b, which appears at the top left-hand side of G. Underneath b, a sequence of diamond structures appears, one for each of the variables of ϕ. Before getting to the right-hand side of G, let’s see how play proceeds on the left-hand side. FIGURE 8.15 Partial structure of the geography game simulating the formula game Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 8.3 PSPACE-COMPLETENESS 347 Play starts at b. Player I must select one of the two edges going from b. These edges correspond to Player E’s possible choices at the beginning of the formula game. The left-hand choice for Player I corresponds to TRUE for Player E in the formula game and the right-hand choice to FALSE. After Player I has selected one of these edges—say, the left-hand one—Player II moves. Only one outgoing edge is present, so this move is forced. Similarly, Player I’s next move is forced and play continues from the top of the second diamond. Now two edges again are present, but Player II gets the choice. This choice corresponds to Player A’s ﬁrst move in the formula game. As play continues in this way, Players I and II choose a rightward or leftward path through each of the diamonds. After play passes through all the diamonds, the head of the path is at the bottom node in the last diamond, and it is Player I’s turn because we assumed that the last quantiﬁer is ∃. Player I’s next move is forced. Then they are at node c in Figure 8.15 and Player II makes the next move. This point in the geography game corresponds to the end of play in the formula game. The chosen path through the diamonds corresponds to an as- signment to ϕ’s variables. Under that assignment, if ψ is TRUE, Player E wins the formula game; and if ψ is FALSE, Player A wins. The structure on the right- hand side of the following ﬁgure guarantees that Player I can win if Player E has won, and that Player II can win if Player A has won. FIGURE 8.16 Full structure of the geography game simulating the formula game, where ϕ = ∃x1 ∀x2 · · · ∃xk [(x1 ∨ 348 CHAPTER 8 / SPACE COMPLEXITY At node c, Player II may choose a node corresponding to one of ψ’s clauses. Then Player I may choose a node corresponding to a literal in that clause. The nodes corresponding to unnegated literals are connected to the left-hand (TRUE) sides of the diamond for associated variables, and similarly for negated literals and right-hand (FALSE) sides as shown in Figure 8.16. If ψ is FALSE, Player II may win by selecting the unsatisﬁed clause. Any literal that Player I may then pick is FALSE and is connected to the side of the diamond that hasn’t yet been played. Thus Player II may play the node in the diamond, but then Player I is unable to move and loses. If ψ is TRUE, any clause that Player II picks contains a TRUE literal. Player I selects that literal after Player II’s move. Because the literal is TRUE, it is connected to the side of the diamond that has already been played, so Player II is unable to move and loses. 8.4 THE CLASSES L AND NL 349 We introduce a Turing machine with two tapes: a read-only input tape, and a read/write work tape. On the read-only tape, the input head can detect symbols but not change them. We provide a way for the machine to detect when the head is at the left-hand and right-hand ends of the input. The input head must remain on the portion of the tape containing the input. The work tape may be read and written in the usual way. Only the cells scanned on the work tape contribute to the space complexity of this type of Turing machine. Think of a read-only input tape as a CD-ROM, a device used for input on many personal computers. Often, the CD-ROM contains more data than the computer can store in its main memory. Sublinear space algorithms allow the computer to manipulate the data without storing all of it in main memory. For space bounds that are at least linear, the two-tape TM model is equivalent to the standard one-tape model (see Exercise 8.1). For sublinear space bounds, we use only the two-tape model. 350 CHAPTER 8 / SPACE COMPLEXITY The log space TM for A cannot cross off the 0s and 1s that have been matched on the input tape because that tape is read-only. Instead, the machine counts the number of 0s and, separately, the number of 1s in binary on the work tape. The only space required is that used to record the two counters. In binary, each counter uses only logarithmic space and hence the algorithm runs in O(log n) space. Therefore, A ∈ L. 8.5 NL-COMPLETENESS 351 gf (n). The input head can be in one of n positions, and the work tape head can be in one of f (n) positions. Therefore, the total number of conﬁgurations of M on w, which is an upper bound on the running time of M on w, is cnf (n)gf (n), or n2O(f (n)). We focus almost exclusively on space bounds f (n) that are at least log n. Our earlier claim that the time complexity of a machine is at most exponential in its space complexity remains true for such bounds because n2O(f (n)) is 2O(f (n)) when f (n) ≥ log n. Recall that Savitch’s theorem shows that we can convert nondeterministic TMs to deterministic TMs and increase the space complexity f (n) by only a squaring, provided that f (n) ≥ n. We can extend Savitch’s theorem to hold for sublinear space bounds down to f (n) ≥ log n. The proof is identical to the original one we gave on page 334, except that we use Turing machines with a read-only input tape; and instead of referring to conﬁgurations of N , we refer to conﬁgurations of N on w. Storing a conﬁguration of N on w uses log(n2O(f (n))) = log n + O(f (n)) space. If f (n) ≥ log n, the storage used is O(f (n)) and the remainder of the proof remains the same. 352 CHAPTER 8 / SPACE COMPLEXITY8.5 NL-COMPLETENESS 353 COROLLARY 8.24 354 CHAPTER 8 / SPACE COMPLEXITY is a legal conﬁguration of M on w, and outputs those that pass the test. The transducer lists the edges similarly. Log space is sufﬁcient for verifying that a conﬁguration c1 of M on w can yield conﬁguration c2 because the transducer only needs to examine the actual tape contents under the head locations given in c1 to determine that M ’s transition function would give conﬁguration c2 as a result. The transducer tries all pairs (c1, c2) in turn to ﬁnd which qualify as edges of G. Those that do are added to the output tape. 8.6 NL EQUALS CONL 355 THEOREM 8.27 356 CHAPTER 8 / SPACE COMPLEXITY PROOF Here is an algorithm for EXERCISES 357 EXERCISES 8.1 Show that for any function f : N −→ R +, where f (n) ≥ n, the space complexity class SPACE(f (n)) is the same whether you deﬁne the class by using the single- tape TM model or the two-tape read-only input TM model. 8.2 Consider the following position in the standard tic-tac-toe game. × 358 CHAPTER 8 / SPACE COMPLEXITY PROBLEMS 8.8 Let EQ REX = {⟨R, S⟩| R and S are equivalent regular expressions}. Show that EQ REX ∈ PSPACE. 8.9 A ladder is a sequence of strings s1, s2, . . . , sk, wherein every string differs from the preceding one by exactly one character. For example, the following is a ladder of English words, starting with “head” and ending with “free”: head, hear, near, fear, bear, beer, deer, deed, feed, feet, fret, free. Let LADDERDFA = {⟨M, s, t⟩| M is a DFA and L(M ) contains a ladder of strings, starting with s and ending with t}. Show that LADDERDFA is in PSPACE. 8.10 The Japanese game go-moku is played by two players, “X” and “O,” on a 19 × 19 grid. Players take turns placing markers, and the ﬁrst player to achieve ﬁve of her markers consecutively in a row, column, or diagonal is the winner. Consider this game generalized to an n × n board. Let GM = {⟨B⟩| B is a position in generalized go-moku, where player “X” has a winning strategy}. By a position we mean a board with markers placed on it, such as may occur in the middle of a play of the game, together with an indication of which player moves next. Show that GM ∈ PSPACE. 8.11 Show that if every NP-hard language is also PSPACE-hard, then PSPACE = NP. 8.12 Show that TQBF restricted to formulas where the part following the quantiﬁers is in conjunctive normal form is still PSPACE-complete. 8.13 Deﬁne ALBA = {⟨M, w⟩| M is an LBA that accepts input w}. Show that ALBA is PSPACE-complete. ⋆8.14 The cat-and-mouse game is played by two players, “Cat” and “Mouse,” on an arbi- trary undirected graph. At a given point, each player occupies a node of the graph. The players take turns moving to a node adjacent to the one that they currently occupy. A special node of the graph is called “Hole.” Cat wins if the two players ever occupy the same node. Mouse wins if it reaches the Hole before the preceding happens. The game is a draw if a situation repeats (i.e., the two players simultane- ously occupy positions that they simultaneously occupied previously, and it is the same player’s turn to move). HAPPY-CAT = {⟨G, c, m, h⟩| G, c, m, h are respectively a graph, and positions of the Cat, Mouse, and Hole, such that Cat has a winning strategy if Cat moves ﬁrst}. Show that HAPPY-CAT is in P. (Hint: The solution is not complicated and doesn’t depend on subtle details in the way the game is deﬁned. Consider the entire game tree. It is exponentially big, but you can search it in polynomial time.) Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. PROBLEMS 359 8.15 Consider the following two-person version of the language PUZZLE that was de- scribed in Problem 7.28. Each player starts with an ordered stack of puzzle cards. The players take turns placing the cards in order in the box and may choose which side faces up. Player I wins if all hole positions are blocked in the ﬁnal stack, and Player II wins if some hole position remains unblocked. Show that the problem of determining which player has a winning strategy for a given starting conﬁguration of the cards is PSPACE-complete. 8.16 Read the deﬁnition of MIN-FORMULA in Problem 7.46. a. Show that MIN-FORMULA ∈ PSPACE. b. Explain why this argument fails to show that MIN-FORMULA ∈ coNP: If ϕ ̸∈ MIN-FORMULA, then ϕ has a smaller equivalent formula. An NTM can verify that ϕ ∈ 360 CHAPTER 8 / SPACE COMPLEXITY ⋆8.23 Deﬁne UCYCLE = {⟨G⟩| G is an undirected graph that contains a simple cycle}. Show that UCYCLE ∈ L. (Note: G may be a graph that is not connected.) ⋆8.24 For each n, exhibit two regular expressions, R and S, of length poly(n), where L(R) ̸= L(S), but where the ﬁrst string on which they differ is exponentially long. In other words, L(R) and L(S) must be different, yet agree on all strings of length up to 2 ϵn for some constant ϵ > 0. 8.25 An undirected graph is bipartite if its nodes may be divided into two sets so that all edges go from a node in one set to a node in the other set. Show that a graph is bipartite if and only if it doesn’t contain a cycle that has an odd number of nodes. Let BIPARTITE = {⟨G⟩| G is a bipartite graph}. Show that BIPARTITE ∈ NL. 8.26 Deﬁne UPATH to be the counterpart of PATH for undirected graphs. Show that SELECTED SOLUTIONS 361 8.7 Let A1 and A2 be languages that are decided by NL-machines N1 and N2. Con- struct three Turing machines: N∪ deciding A1 ∪ A2; N◦ deciding A1 ◦ A2; and N∗ deciding A∗ 1. Each of these machines operates as follows. Machine N∪ nondeterministically branches to simulate N1 or to simulate N2. In either case, N∪ accepts if the simulated machine accepts. Machine N◦ nondeterministically selects a position on the input to divide it into two substrings. Only a pointer to that position is stored on the work tape— insufﬁcient space is available to store the substrings themselves. Then N◦ simulates N1 on the ﬁrst substring, branching nondeterministically to simulate N1’s nonde- terminism. On any branch that reaches N1’s accept state, N◦ simulates N2 on the second substring. On any branch that reaches N2’s accept state, N◦ accepts. Machine N∗ has a more complex algorithm, so we describe its stages. N∗ = “On input w: 1. Initialize two input position pointers p1 and p2 to 0, the position immediately preceding the ﬁrst input symbol. 2. Accept if no input symbols occur after p2. 3. Move p2 forward to a nondeterministically selected position. 4. Simulate N1 on the substring of w from the position following p1 to the position at p2, branching nondeterministically to sim- ulate N1’s nondeterminism. 5. If this branch of the simulation reaches N1’s accept state, copy p2 to p1 and go to stage 2. If N1 rejects on this branch, reject .” 8.34 Reduce PATH to CYCLE. The idea behind the reduction is to modify the PATH problem instance ⟨G, s, t⟩ by adding an edge from t to s in G. If a path exists from s to t in G, a directed cycle will exist in the modiﬁed G. However, other cycles may exist in the modiﬁed G because they may already be present in G. To handle that problem, ﬁrst change G so that it contains no cycles. A leveled directed graph is one where the nodes are divided into groups, A1, A2, . . . , Ak, called levels, and only edges from one level to the next higher level are permitted. Observe that a leveled graph is acyclic. The PATH problem for leveled graphs is still NL-complete, as the following reduction from the unrestricted PATH problem shows. Given a graph G with two nodes s and t, and m nodes in total, produce the leveled graph G ′ whose levels are m copies of G’s nodes. Draw an edge from node i at each level to node j in the next level if G contains an edge from i to j. Additionally, draw an edge from node i in each level to node i in the next level. Let s′ be the node s in the ﬁrst level and let t′ be the node t in the last level. Graph G contains a path from s to t iff G ′ contains a path from s′ to t′. If you modify G ′ by adding an edge from t′ to s′, you obtain a reduction from PATH to CYCLE. The reduction is computationally sim- ple, and its implementation in logspace is routine. Furthermore, a straightforward procedure shows that CYCLE ∈ NL. Hence CYCLE is NL-complete. Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 364 CHAPTER 9 / INTRACTABILITY9.1 HIERARCHY THEOREMS 365 The role of space constructibility in the space hierarchy theorem may be un- derstood from the following situation. If f (n) and g(n) are two space bounds, where f (n) is asymptotically larger than g(n), we would expect a machine to be able to decide more languages in f (n) space than in g(n) space. However, sup- pose that f (n) exceeds g(n) by only a very small and hard to compute amount. Then, the machine may not be able to use the extra space proﬁtably because even computing the amount of extra space may require more space than is available. In this case, a machine may not be able to compute more languages in f (n) space than it can in g(n) space. Stipulating that f (n) is space constructible avoids this situation and allows us to prove that a machine can compute more than it would be able to in any asymptotically smaller bound, as the following theorem shows. THEOREM 9.3 366 CHAPTER 9 / INTRACTABILITY different from M ’s language. But even when M runs in o(f (n)) space, it may use more than f (n) space for small n, when the asymptotic behavior hasn’t “kicked in” yet. Possibly, D might not have enough space to run M to completion on input ⟨M ⟩, and hence D will miss its one opportunity to avoid M ’s language. So, if we aren’t careful, D might end up deciding the same language that M decides, and the theorem wouldn’t be proved. We can ﬁx this problem by modifying D to give it additional opportunities to avoid M ’s language. Instead of running M only when D receives input ⟨M ⟩, it runs M whenever it receives an input of the form ⟨M ⟩10∗; that is, an input of the form ⟨M ⟩ followed by a 1 and some number of 0s. Then, if M really is running in o(f (n)) space, D will have enough space to run it to completion on input ⟨M ⟩10 k for some large value of k because the asymptotic behavior must eventually kick in. One last technical point arises. When D runs M on some string, M may get into an inﬁnite loop while using only a ﬁnite amount of space. But D is supposed to be a decider, so we must ensure that D doesn’t loop while simulating M . Any machine that runs in space o(f (n)) uses only 2o(f (n)) time. We modify D so that it counts the number of steps used in simulating M . If this count ever exceeds 2f (n), then D rejects. PROOF The following O(f (n)) space algorithm D decides a language A that is not decidable in o(f (n)) space. D = “On input w: 1. Let n be the length of w. 2. Compute f (n) using space constructibility and mark off this much tape. If later stages ever attempt to use more, reject . 3. If w is not of the form ⟨M ⟩10 ∗ for some TM M , reject . 4. Simulate M on w while counting the number of steps used in the simulation. If the count ever exceeds 2f (n), reject . 5. If M accepts, reject . If M rejects, accept .” In stage 4, we need to give additional details of the simulation in order to determine the amount of space used. The simulated TM M has an arbitrary tape alphabet and D has a ﬁxed tape alphabet, so we represent each cell of M ’s tape with several cells on D’s tape. Therefore, the simulation introduces a constant factor overhead in the space used. In other words, if M runs in g(n) space, then D uses d g(n) space to simulate M for some constant d that depends on M . Machine D is a decider because each of its stages can run for a limited time. Let A be the language that D decides. Clearly, A is decidable in space O(f (n)) because D does so. Next, we show that A is not decidable in o(f (n)) space. Assume to the contrary that some Turing machine M decides A in space g(n), where g(n) is o(f (n)). As mentioned earlier, D can simulate M , using space d g(n) for some constant d. Because g(n) is o(f (n)), some constant n0 exists, where d g(n) < f (n) for all n ≥ n0. Therefore, D’s simulation of M will run to completion so long as the input has length n0 or more. Consider what happens Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 9.1 HIERARCHY THEOREMS 367 when D is run on input ⟨M ⟩10 n0 . This input is longer than n0, so the simulation in stage 4 will complete. Therefore, D will do the opposite of M on the same in- put. Hence M doesn’t decide A, which contradicts our assumption. Therefore, A is not decidable in o(f (n)) space. 368 CHAPTER 9 / INTRACTABILITY Now we establish the main objective of this chapter: proving the existence of problems that are decidable in principle but not in practice—that is, problems that are decidable but intractable. Each of the SPACE(nk) classes is contained within the class SPACE(nlog n), which in turn is strictly contained within the class SPACE(2n). Therefore, we obtain the following additional corollary sepa- rating PSPACE from EXPSPACE = ⋃ k SPACE(2n k ). COROLLARY 9.7 9.1 HIERARCHY THEOREMS 369 is slightly weaker than the one we proved for space. Whereas any space con- structible asymptotic increase in the space bound enlarges the class of languages decidable therein, for time we must further increase the time bound by a log- arithmic factor in order to guarantee that we can obtain additional languages. Conceivably, a tighter time hierarchy theorem is true; but at present, we don’t know how to prove it. This aspect of the time hierarchy theorem arises because we measure time complexity with single-tape Turing machines. We can prove tighter time hierarchy theorems for other models of computation. THEOREM 9.10 370 CHAPTER 9 / INTRACTABILITY We examine each of the stages of this algorithm to determine the running time. Stages 1, 2, and 3 can be performed within O(t(n)) time. In stage 4, every time D simulates one step of M , it takes M ’s current state together with the tape symbol under M ’s tape head and looks up M ’s next action in its transition function so that it can update M ’s tape appropriately. All three of these objects (state, tape symbol, and transition function) are stored on D’s tape somewhere. If they are stored far from each other, D will need many steps to gather this information each time it simulates one of M ’s steps. Instead, D always keeps this information close together. We can think of D’s single tape as organized into tracks. One way to get two tracks is by storing one track in the odd positions and the other in the even posi- tions. Alternatively, the two-track effect may be obtained by enlarging D’s tape alphabet to include each pair of symbols: one from the top track and the second from the bottom track. We can get the effect of additional tracks similarly. Note that multiple tracks introduce only a constant factor overhead in time, provided that only a ﬁxed number of tracks are used. Here, D has three tracks. One of the tracks contains the information on M ’s tape, and a second contains its current state and a copy of M ’s transition function. During the simulation, D keeps the information on the second track near the current position of M ’s head on the ﬁrst track. Every time M ’s head position moves, D shifts all the information on the second track to keep it near the head. Because the size of the information on the second track depends only on M and not on the length of the input to M , the shifting adds only a constant factor to the simulation time. Furthermore, because the required information is kept close together, the cost of looking up M ’s next action in its transition function and updating its tape is only a constant. Hence if M runs in g(n) time, D can simulate it in O(g(n)) time. At every step in stage 4, D must decrement the step counter it originally set in stage 2. Here, D can do so without adding excessively to the simulation time by keeping the counter in binary on a third track and moving it to keep it near the present head position. This counter has a magnitude of about t(n)/ log t(n), so its length is log(t(n)/ log t(n)), which is O(log t(n)). Hence the cost of updating and moving it at each step adds a log t(n) factor to the simulation time, thus bringing the total running time to O(t(n)). Therefore, A is decidable in time O(t(n)). To show that A is not decidable in o(t(n)/ log t(n)) time, we use an argument similar to one used in the proof of Theorem 9.3. Assume to the contrary that TM M decides A in time g(n), where g(n) is o(t(n)/ log t(n)). Here, D can sim- ulate M , using time d g(n) for some constant d. If the total simulation time (not counting the time to update the step counter) is at most t(n)/ log t(n), the sim- ulation will run to completion. Because g(n) is o(t(n)/ log t(n)), some constant n0 exists where d g(n) < t(n)/ log t(n) for all n ≥ n0. Therefore, D’s simula- tion of M will run to completion as long as the input has length n0 or more. Consider what happens when we run D on input ⟨M ⟩10 n0 . This input is longer than n0, so the simulation in stage 4 will complete. Therefore, D will do the Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 9.1 HIERARCHY THEOREMS 371 opposite of M on the same input. Hence M doesn’t decide A, which contradicts our assumption. Therefore, A is not decidable in o(t(n)/ log t(n)) time. 372 CHAPTER 9 / INTRACTABILITY still generate the same class of regular languages as do the standard regular ex- pressions because we can eliminate the exponentiation operation by repeating the base expression. Let EQ REX↑ = {⟨Q, R⟩| Q and R are equivalent regular expressions with exponentiation}. To show that EQ REX↑ is intractable, we demonstrate that it is complete for the class EXPSPACE. Any EXPSPACE-complete problem cannot be in PSPACE, much less in P. Otherwise, EXPSPACE would equal PSPACE, contradicting Corollary 9.7. 9.1 HIERARCHY THEOREMS 373 The difﬁculty in this proof is that the size of the expressions constructed must be polynomial in n (so that the reduction can run in polynomial time), whereas the simulated computation may have exponential length. The exponentiation operation is useful here to represent the long computation with a relatively short expression. PROOF First, we present a nondeterministic algorithm for testing whether two NFAs are inequivalent. N = “On input ⟨N1, N2⟩, where N1 and N2 are NFAs: 1. Place a marker on each of the start states of N1 and N2. 2. Repeat 2q1+q2 times, where q1 and q2 are the numbers of states in N1 and N2: 3. Nondeterministically select an input symbol and change the positions of the markers on the states of N1 and N2 to simu- late reading that symbol. 4. If at any point a marker was placed on an accept state of one of the ﬁnite automata and not on any accept state of the other ﬁnite automaton, accept . Otherwise, reject .” If automata N1 and N2 are equivalent, N clearly rejects because it only ac- cepts when it determines that one machine accepts a string that the other does not accept. If the automata are not equivalent, some string is accepted by one machine and not by the other. Some such string must be of length at most 2q1+q2 . Otherwise, consider using the shortest such string as the sequence of nondeter- ministic choices. Only 2q1+q2 different ways exist to place markers on the states of N1 and N2; so in a longer string, the positions of the markers would repeat. By removing the portion of the string between the repetitions, a shorter such string would be obtained. Hence algorithm N would guess this string among its nondeterministic choices and would accept. Thus, N operates correctly. Algorithm N runs in nondeterministic linear space. Thus, Savitch’s theorem provides a deterministic O(n2) space algorithm for this problem. Next, we use the deterministic form of this algorithm to design the following algorithm E that decides EQ REX↑. E = “On input ⟨R1, R2⟩, where R1 and R2 are regular expressions with exponentiation: 1. Convert R1 and R2 to equivalent regular expressions B1 and B2 that use repetition instead of exponentiation. 2. Convert B1 and B2 to equivalent NFAs N1 and N2, using the conversion procedure given in the proof of Lemma 1.55. 3. Use the deterministic version of algorithm N to determine whether N1 and N2 are equivalent.” Algorithm E obviously is correct. To analyze its space complexity, we ob- serve that using repetition to replace exponentiation may increase the length of an expression by a factor of 2l, where l is the sum of the lengths of the ex- Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 374 CHAPTER 9 / INTRACTABILITY ponents. Thus, expressions B1 and B2 have a length of at most n2n, where n is the input length. The conversion procedure of Lemma 1.55 increases the size linearly, and hence NFAs N1 and N2 have at most O(n2n) states. Thus, with input size O(n2n), the deterministic version of algorithm N uses space O((n2n) 2) = O(n222n). Hence EQ REX↑ is decidable in exponential space. Next, we show that EQ REX↑ is EXPSPACE-hard. Let A be a language that is decided by TM M running in space 2(n k) for some constant k. The reduction maps an input w to a pair of regular expressions, R1 and R2. Expression R1 is ∆ ∗ where if Γ and Q are M ’s tape alphabet and states, ∆ = Γ∪Q∪{#} is the alphabet consisting of all symbols that may appear in a computation history. We construct expression R2 to generate all strings that aren’t rejecting computation histories of M on w. Of course, M accepts w iff M on w has no rejecting computation histories. Therefore, the two expressions are equivalent iff M accepts w. The construction is as follows. A rejecting computation history for M on w is a sequence of conﬁgura- tions separated by # symbols. We use our standard encoding of conﬁgurations whereby a symbol corresponding to the current state is placed to the left of the current head position. We assume that all conﬁgurations have length 2(n k) and are padded on the right by blank symbols if they otherwise would be too short. The ﬁrst conﬁguration in a rejecting computation history is the start conﬁgu- ration of M on w. The last conﬁguration is a rejecting conﬁguration. Each conﬁguration must follow from the preceding one according to the rules speci- ﬁed in the transition function. A string may fail to be a rejecting computation in several ways: It may fail to start or end properly, or it may be incorrect somewhere in the middle. Ex- pression R2 equals Rbad-start ∪ Rbad-window ∪ Rbad-reject, where each subexpression corresponds to one of the three ways a string may fail. We construct expression Rbad-start to generate all strings that fail to start with the start conﬁguration C1 of M on w, as follows. Conﬁguration C1 looks like q0w1w2 · · · wn␣ ␣ · · · ␣ #. We write Rbad-start as the union of several subexpres- sions to handle each part of C1: Rbad-start = S0 ∪ S1 ∪ · · · ∪ Sn ∪ Sb ∪ S#. Expression S0 generates all strings that don’t start with q0. We let S0 be the expression ∆−q0 ∆ ∗. The notation ∆−q0 is shorthand for writing the union of all symbols in ∆ except q0. Expression S1 generates all strings that don’t contain w1 in the second po- sition. We let S1 be ∆ ∆−w1 ∆ ∗. In general, for 1 ≤ i ≤ n, expression Si is ∆ i ∆−wi ∆ ∗. Thus, Si generates all strings that contain any symbols in the ﬁrst i positions, any symbol except wi in position i + 1, and any string of symbols fol- lowing position i+1. Note that we have used the exponentiation operation here. Actually, at this point, exponentiation is more of a convenience than a necessity because we could have instead repeated the symbol ∆ i times without exces- sively increasing the length of the expression. But in the next subexpression, exponentiation is crucial to keeping the size polynomial. Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 9.1 HIERARCHY THEOREMS 375 Expression Sb generates all strings that fail to contain a blank symbol in some position n + 2 through 2(n k). We could introduce subexpressions Sn+2 through S2(nk ) for this purpose, but then expression Rbad-start would have exponential length. Instead, we let Sb = ∆ n+1 (∆ ∪ ε) 2 (nk )−n−2 ∆−␣ ∆ ∗. Thus, Sb generates strings that contain any symbols in the ﬁrst n + 1 positions, any symbols in the next t positions, where t can range from 0 to 2(n k) − n − 2, and any symbol except blank in the next position. Finally, S# generates all strings that don’t have a # symbol in position 2(n k)+1. Let S# be ∆ (2 (nk )) ∆−# ∆ ∗. Now that we have completed the construction of Rbad-start, we turn to the next piece, Rbad-reject. It generates all strings that don’t end properly; that is, strings that fail to contain a rejecting conﬁguration. Any rejecting conﬁguration contains the state qreject, so we let Rbad-reject = ∆ ∗ −qreject . Thus, Rbad-reject generates all strings that don’t contain qreject. Finally, we construct Rbad-window, the expression that generates all strings whereby one conﬁguration does not properly lead to the next conﬁguration. Recall that in the proof of the Cook–Levin theorem, we determined that one conﬁguration legally yields another whenever every three consecutive symbols in the ﬁrst conﬁguration correctly yield the corresponding three symbols in the second conﬁguration according to the transition function. Hence, if one conﬁg- uration fails to yield another, the error will be apparent from an examination of the appropriate six symbols. We use this idea to construct Rbad-window: Rbad-window = ⋃ bad(abc,def ) ∆ ∗ abc ∆ (2 (nk )−2) def ∆ ∗, where bad(abc, def ) means that abc doesn’t yield def according to the transition function. The union is taken only over such symbols a, b, c, d, e, and f in ∆. The following ﬁgure illustrates the placement of these symbols in a computation history. FIGURE 9.16 Corresponding places in adjacent conﬁgurations To calculate the length of R2, we determine the length of the exponents that appear in it. Several exponents of magnitude roughly 2(n k) appear, and their total length in binary is O(nk). Therefore, the length of R2 is polynomial in n. 376 CHAPTER 9 / INTRACTABILITY 9.2 RELATIVIZATION 377 EXAMPLE 9.18 378 CHAPTER 9 / INTRACTABILITY show that they are the same relative to any oracle; but in fact, PA and NPA are different. THEOREM 9.20 9.3 CIRCUIT COMPLEXITY 379 We run Mi on input 1n and respond to its oracle queries as follows. If Mi queries a string y whose status has already been determined, we respond consis- tently. If y’s status is undetermined, we respond NO to the query and declare y to be out of A. We continue the simulation of Mi until it halts. Now consider the situation from Mi’s perspective. If it ﬁnds a string of length n in A, it should accept because it knows that 1 n is in LA. If Mi de- termines that all strings of length n aren’t in A, it should reject because it knows that 1 n is not in LA. However, it doesn’t have enough time to ask about all strings of length n, and we have answered NO to each of the queries it has made. Hence when Mi halts and must decide whether to accept or reject, it doesn’t have enough information to be sure that its decision is correct. Our objective is to ensure that its decision is not correct. We do so by observ- ing its decision and then extending A so that the reverse is true. Speciﬁcally, if Mi accepts 1 n, we declare all the remaining strings of length n to be out of A and so determine that 1 n is not in LA. If Mi rejects 1 n, we ﬁnd a string of length n that Mi hasn’t queried and declare that string to be in A to guarantee that 1n is in LA. Such a string must exist because Mi runs for ni steps, which is fewer than 2n, the total number of strings of length n. Either way, we have ensured that M A i doesn’t decide LA. We ﬁnish stage i by arbitrarily declaring that any string of length at most n, whose status remains undetermined at this point, is out of A. Stage i is com- pleted and we proceed with stage i + 1. We have shown that no polynomial time oracle TM decides LA with oracle A, thereby proving the theorem. 380 CHAPTER 9 / INTRACTABILITY 9.3 CIRCUIT COMPLEXITY 381 FIGURE 9.24 An example of a Boolean circuit computing We use functions to describe the input/output behavior of Boolean cir- cuits. To a Boolean circuit C with n input variables, we associate a function fC : {0,1}n−→{0,1}, where if C outputs b when its inputs x1, . . . , xn are set to a1, . . . , an, we write fC (a1, . . . , an) = b. We say that C computes the function fC. We sometimes consider Boolean circuits that have multiple output gates. A function with k output bits computes a function whose range is {0,1}k. EXAMPLE 9.25 382 CHAPTER 9 / INTRACTABILITY We plan to use circuits to test membership in languages once they have been suitably encoded into {0,1}. One problem that occurs is that any particular circuit can handle only inputs of some ﬁxed length, whereas a language may contain strings of different lengths. So instead of using a single circuit to test language membership, we use an entire family of circuits, one for each input length, to perform this task. We formalize this notion in the following deﬁnition. 9.3 CIRCUIT COMPLEXITY 383 same as the parity2 function, and then implement each XOR gate with two NOTs, two ANDs, and one OR, as we did in that earlier example. Let A be the language of strings that contain an odd number of 1s. Then A has circuit complexity O(n). 384 CHAPTER 9 / INTRACTABILITY We make two assumptions about TM M in deﬁning the notion of a tableau. First, as we mentioned in the proof idea, M accepts only when its head is on the leftmost tape cell and that cell contains the ␣ symbol. Second, once M has halted, it stays in the same conﬁguration for all future time steps. So by looking at the leftmost cell in the ﬁnal row of the tableau, cell [t(n), 1], we can determine whether M has accepted. The following ﬁgure shows part of a tableau for M on the input 0010. FIGURE 9.31 A tableau for M on input 0010 The content of each cell is determined by certain cells in the preceding row. If we know the values at cell [i − 1, j − 1], cell [i − 1, j], and cell [i − 1, j + 1], we can obtain the value at cell [i, j] with M ’s transition function. For example, the following ﬁgure magniﬁes a portion of the tableau in Figure 9.31. The three top symbols, 0, 0, and 1, are tape symbols without states, so the middle symbol must remain a 0 in the next row, as shown. Now we can begin to construct the circuit Cn. It has several gates for each cell in the tableau. These gates compute the value at a cell from the values of the three cells that affect it. Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 9.3 CIRCUIT COMPLEXITY 385 To make the construction easier to describe, we add lights that show the out- put of some of the gates in the circuit. The lights are for illustrative purposes only and don’t affect the operation of the circuit. Let k be the number of elements in Γ ∪ (Q × Γ). We create k lights for each cell in the tableau—one light for each member of Γ, and one light for each member of (Q × Γ)—or a total of kt2(n) lights. We call these lights light [i, j, s], where 1 ≤ i, j ≤ t(n) and s ∈ Γ ∪ (Q × Γ). The condition of the lights in a cell indicates the contents of that cell. If light [i, j, s] is on, cell [i, j] contains the symbol s. Of course, if the circuit is constructed properly, only one light would be on per cell. Let’s pick one of the lights—say, light [i, j, s] in cell [i, j]. This light should be on if that cell contains the symbol s. We consider the three cells that can affect cell [i, j] and determine which of their settings cause cell [i, j] to contain s. This determination can be made by examining the transition function δ. Suppose that if the cells cell [i − 1, j − 1], cell [i − 1, j], and cell [i − 1, j + 1] contain a, b, and c, respectively, cell [i, j] contains s, according to δ. We wire the circuit so that if light [i − 1, j − 1, a], light [i − 1, j, b], and light [i − 1, j + 1, c] are on, then so is light [i, j, s]. We do so by connecting the three lights at the i − 1 level to an AND gate whose output is connected to light [i, j, s]. In general, several different settings (a1, b1, c1), (a2, b2, c2), . . . , (al, bl, cl) of cell [i − 1, j − 1], cell [i − 1, j], and cell [i − 1, j + 1] may cause cell [i, j] to contain s. In this case, we wire the circuit so that for each setting ai, bi, ci, the respective lights are connected with an AND gate, and all the AND gates are connected with an OR gate. This circuitry is illustrated in the following ﬁgure. FIGURE 9.32 Circuitry for one light Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 386 CHAPTER 9 / INTRACTABILITY The circuitry just described is repeated for each light, with a few exceptions at the boundaries. Each cell at the left boundary of the tableau—that is, cell [i, 1] for 1 ≤ i ≤ t(n)—has only two preceding cells that affect its contents. The cells at the right boundary are similar. In these cases, we modify the circuitry to simulate the behavior of TM M in this situation. The cells in the ﬁrst row have no predecessors and are handled in a special way. These cells contain the start conﬁguration and their lights are wired to the input variables. Thus, light [1, 1, 9.3 CIRCUIT COMPLEXITY 387 strings to circuits, where f (w) = ⟨C⟩ implies that w ∈ A ⇐⇒ Boolean circuit C is satisﬁable. Because A is in NP, it has a polynomial time veriﬁer V whose input has the form ⟨x, c⟩, where c may be the certiﬁcate showing that x is in A. To construct f , we obtain the circuit simulating V using the method in Theorem 9.30. We ﬁll in the inputs to the circuit that correspond to x with the symbols of w. The only remaining inputs to the circuit correspond to the certiﬁcate c. We call this circuit C and output it. If C is satisﬁable, a certiﬁcate exists, so w is in A. Conversely, if w is in A, a certiﬁcate exists, so C is satisﬁable. To show that this reduction runs in polynomial time, we observe that in the proof of Theorem 9.30, the construction of the circuit can be done in time that is polynomial in n. The running time of the veriﬁer is nk for some k, so the size of the circuit constructed is O(n2k). The structure of the circuit is quite simple (actually, it is highly repetitious), so the running time of the reduction is O(n2k). 388 CHAPTER 9 / INTRACTABILITY reduction builds from C a formula ϕ with variables x1, . . . , xl, g1, . . . , gm. Each of ϕ’s variables corresponds to a wire in C. The xi variables correspond to the input wires, and the gi variables correspond to the wires at the gate outputs. We relabel ϕ’s variables as w1, . . . , wl+m. Now we describe ϕ’s clauses. We write ϕ’s clauses more intuitively using im- plications. Recall that we can convert the implication operation (P → Q) to the clause ( EXERCISES 389 EXERCISES A9.1 Prove that TIME(2 n) = TIME(2 n+1). A9.2 Prove that TIME(2 n) ⊊ TIME(2 2n). A9.3 Prove that NTIME(n) ⊊ PSPACE. 9.4 Show how the circuit depicted in Figure 9.26 computes on input 0110 by showing the values computed by all of the gates, as we did in Figure 9.24. 9.5 Give a circuit that computes the parity function on three input variables and show how it computes on input 011. 9.6 Prove that if A ∈ P, then P A = P. 9.7 Give regular expressions with exponentiation that generate the following languages over the alphabet {0,1}. Aa. All strings of length 500 Ab. All strings of length 500 or less Ac. All strings of length 500 or more Ad. All strings of length different than 500 e. All strings that contain exactly 500 1s f. All strings that contain at least 500 1s g. All strings that contain at most 500 1s h. All strings of length 500 or more that contain a 0 in the 500th position i. All strings that contain two 0s that have at least 500 symbols between them 9.8 If R is a regular expression, let R{m,n} represent the expression Rm ∪ Rm+1 ∪ · · · ∪ Rn. Show how to implement the R{m,n} operator, using the ordinary exponentiation operator, but without “· · · ”. 9.9 Show that if NP = P SAT, then NP = coNP. 9.10 Problem 8.13 showed that ALBA is PSPACE-complete. a. Do we know whether ALBA ∈ NL? Explain your answer. b. Do we know whether ALBA ∈ P? Explain your answer. 9.11 Show that the language MAX-CLIQUE from Problem 7.48 is in P SAT. 390 CHAPTER 9 / INTRACTABILITY 9.13 Consider the function pad : Σ ∗ × N −→ Σ ∗# ∗ that is deﬁned as follows. Let pad (s, l) = s# j, where j = max(0, l − m) and m is the length of s. Thus, pad (s, l) simply adds enough copies of the new symbol # to the end of s so that the length of the result is at least l. For any language A and function f : N −→ N , deﬁne the language pad (A, f ) as pad (A, f ) = {pad (s, f (m))| where s ∈ A and m is the length of s}. Prove that if A ∈ TIME(n 6), then pad (A, n 2) ∈ TIME(n 3). 9.14 Prove that if NEXPTIME ̸= EXPTIME, then P ̸= NP. You may ﬁnd the func- tion pad , deﬁned in Problem 9.13, to be helpful. A9.15 Deﬁne pad as in Problem 9.13. a. Prove that for every A and natural number k, A ∈ P iff pad (A, n k) ∈ P. b. Prove that P ̸= SPACE(n). 9.16 Prove that TQBF ̸∈ SPACE(n 1/3). ⋆9.17 Read the deﬁnition of a 2DFA (two-headed ﬁnite automaton) given in Prob- lem 5.26. Prove that P contains a language that is not recognizable by a 2DFA. 9.18 Let EREX↑ = {⟨R⟩| R is a regular expression with exponentiation and L(R) = ∅}. Show that EREX↑ ∈ P. 9.19 Deﬁne the unique-sat problem to be USAT = {⟨ϕ⟩| ϕ is a Boolean formula that has a single satisfying assignment}. Show that USAT ∈ P SAT . 9.20 Prove that an oracle C exists for which NP C ̸= coNP C . 9.21 A k-query oracle Turing machine is an oracle Turing machine that is permitted to make at most k queries on each input. A k-query oracle Turing machine M with an oracle for A is written M A,k. Deﬁne P A,k to be the collection of languages that are decidable by polynomial time k-query oracle Turing machines with an oracle for A. a. Show that NP ∪ coNP ⊆ P SAT,1. b. Assume that NP ̸= coNP. Show that NP ∪ coNP ⊊ P SAT,1. 9.22 Suppose that A and B are two oracles. One of them is an oracle for TQBF, but you don’t know which. Give an algorithm that has access to both A and B, and that is guaranteed to solve TQBF in polynomial time. 9.23 Recall that you may consider circuits that output strings over {0,1} by designating several output gates. Let add n : {0,1} 2n−→ {0,1} n+1 take two n bit binary inte- gers and produce the n + 1 bit sum. Show that you can compute the add n function with O(n) size circuits. Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. SELECTED SOLUTIONS 391 9.24 Deﬁne the function majority n : {0,1} n−→ {0,1} as majority n(x1, . . . , xn) = {0 ∑ xi < n/2 ; 1 ∑ xi ≥ n/2 . Thus, the majority n function returns the majority vote of the inputs. Show that majority n can be computed with: a. O(n 2) size circuits. b. O(n log n) size circuits. (Hint: Recursively divide the number of inputs in half and use the result of Problem 9.23.) ⋆9.25 Deﬁne the function majority n as in Problem 9.24. Show that it may be computed with O(n) size circuits. 392 CHAPTER 9 / INTRACTABILITY (b) Assume that P = SPACE(n). Let A be a language in SPACE(n 2) but not in SPACE(n) as shown to exist in the space hierarchy theorem. The language pad (A, n 2) ∈ SPACE(n) because you have enough space to run the O(n 2) space algorithm for A, using space that is linear in the padded language. Because of the assumption, pad (A, n 2) ∈ P, hence A ∈ P by part (a), and hence A ∈ SPACE(n), due to the assumption once again. But that is a contradiction. Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 394 CHAPTER 10 / ADVANCED TOPICS IN COMPLEXITY THEORY ﬁrst two of these types of problems, no polynomial time algorithm exists that ﬁnds the best solution unless P = NP. In practice, we may not need the absolute best or optimal solution to a prob- lem. A solution that is nearly optimal may be good enough and may be much easier to ﬁnd. As its name implies, an approximation algorithm is designed to ﬁnd such approximately optimal solutions. For example, take the vertex cover problem that we introduced in Section 7.5. There we presented the problem as the language VERTEX-COVER representing a decision problem—one that has a yes/no answer. In the optimization ver- sion of this problem, called MIN-VERTEX-COVER, we aim to produce one of the smallest vertex covers among all possible vertex covers in the input graph. The following polynomial time algorithm approximately solves this optimiza- tion problem. It produces a vertex cover that is never more than twice the size of one of the smallest vertex covers. A = “On input ⟨G⟩, where G is an undirected graph: 1. Repeat the following until all edges in G touch a marked edge: 2. Find an edge in G untouched by any marked edge. 3. Mark that edge. 4. Output all nodes that are endpoints of marked edges.” THEOREM 10.1 10.1 APPROXIMATION ALGORITHMS 395 mization problem, we seek a largest solution. An approximation algorithm for a minimization problem is k-optimal if it always ﬁnds a solution that is not more than k times optimal. The preceding algorithm is 2-optimal for the vertex cover problem. For a maximization problem, a k-optimal approximation algorithm always ﬁnds a solution that is at least 1 396 CHAPTER 10 / ADVANCED TOPICS IN COMPLEXITY THEORY10.2 PROBABILISTIC ALGORITHMS 397 When a probabilistic Turing machine decides a language, it must accept all strings in the language and reject all strings out of the language as usual, except that now we allow the machine a small probability of error. For 0 ≤ ϵ < 1 398 CHAPTER 10 / ADVANCED TOPICS IN COMPLEXITY THEORY to determine which color comes up most frequently. Almost certainly, the pre- dominant color in the box will be the most frequent one in the sample. The balls correspond to branches of M1’s computation: red to accepting and blue to rejecting. M2 samples the color by running M1. A calculation shows that M2 errs with exponentially small probability if it runs M1 a polynomial number of times and outputs the result that comes up most often. PROOF Given TM M1 deciding a language with an error probability of ϵ < 1 10.2 PROBABILISTIC ALGORITHMS 399 PRIMALITY A prime number is an integer greater than 1 that is not divisible by positive integers other than 1 and itself. A nonprime number greater than 1 is called composite. The ancient problem of testing whether an integer is prime or com- posite has been the subject of extensive research. A polynomial time algorithm for this problem is now known [4], but it is too difﬁcult to include here. In- stead, we describe a much simpler probabilistic polynomial time algorithm for primality testing. One way to determine whether a number is prime is to try all possible integers less than that number and see whether any are divisors, also called factors. That algorithm has exponential time complexity because the magnitude of a number is exponential in its length. The probabilistic primality testing algorithm that we describe operates in a different manner entirely. It doesn’t search for factors. Indeed, no probabilistic polynomial time algorithm for ﬁnding factors is known to exist. Before discussing the algorithm, we mention some notation from number theory. All numbers in this section are integers. For any p greater than 1, we say that two numbers are equivalent modulo p if they differ by a multiple of p. If numbers x and y are equivalent modulo p, we write x ≡ y (mod p). We let x mod p be the smallest nonnegative y where x ≡ y (mod p). Every number is equivalent modulo p to some member of the set Zp = {0, . . . , p − 1}. For convenience, we let Z + p = {1, . . . , p − 1}. We may refer to the elements of these sets by other numbers that are equivalent modulo p, as when we refer to p − 1 by −1. The main idea behind the algorithm stems from the following result, called Fermat’s little theorem. THEOREM 10.6 400 CHAPTER 10 / ADVANCED TOPICS IN COMPLEXITY THEORY Think of the preceding theorem as providing a type of “test” for primality, called a Fermat test. When we say that p passes the Fermat test at a, we mean that ap−1 ≡ 1 (mod p). The theorem states that primes pass all Fermat tests for a ∈ Z + p . We observed that 6 fails some Fermat test, so 6 isn’t prime. Can we use the Fermat tests to give an algorithm for determining primality? Almost. Call a number pseudoprime if it passes Fermat tests for all smaller a’s relatively prime to it. With the exception of the infrequent Carmichael num- bers, which are composite yet pass all Fermat tests, the pseudoprime numbers are identical to the prime numbers. We begin by giving a very simple probabilis- tic polynomial time algorithm that distinguishes primes from composites except for the Carmichael numbers. Afterwards, we present and analyze the complete probabilistic primality testing algorithm. A pseudoprimality algorithm that goes through all Fermat tests would require exponential time. The key to the probabilistic polynomial time algorithm is that if a number is not pseudoprime, it fails at least half of all tests. (Just accept this assertion for now. Problem 10.16 asks you to prove it.) The algorithm works by trying several tests chosen at random. If any fail, the number must be composite. The algorithm contains a parameter k that determines the error probability. PSEUDOPRIME = “On input p: 1. Select a1, . . . , ak randomly in Z + p . 2. Compute ap−1 i mod p for each i. 3. If all computed values are 1, accept ; otherwise, reject .” If p is pseudoprime, it passes all tests and the algorithm accepts with certainty. If p isn’t pseudoprime, it passes at most half of all tests. In that case, it passes each randomly selected test with probability at most 1 10.2 PROBABILISTIC ALGORITHMS 401 PRIME = “On input p: 1. If p is even, accept if p = 2; otherwise, reject . 2. Select a1, . . . , ak randomly in Z + p . 3. For each i from 1 to k: 4. Compute ap−1 i mod p and reject if different from 1. 5. Let p − 1 = s · 2l where s is odd. 6. Compute the sequence as·2 0 i , as·2 1 i , as·2 2 i , . . . , as·2 l i modulo p. 7. If some element of this sequence is not 1, ﬁnd the last element that is not 1 and reject if that element is not −1. 8. All tests have passed at this point, so accept .” The following two lemmas show that algorithm PRIME works correctly. Obvi- ously the algorithm is correct when p is even, so we only consider the case when p is odd. Say that ai is a (compositeness) witness if the algorithm rejects at either stage 4 or 7, using ai. LEMMA 10.7 402 CHAPTER 10 / ADVANCED TOPICS IN COMPLEXITY THEORY LEMMA 10.8 10.2 PROBABILISTIC ALGORITHMS 403 For the prime power case, we have p = qe where q is prime and e > 1. Let t = 1 + qe−1. Expanding tp using the binomial theorem, we obtain tp = (1 + qe−1) p = 1 + p · qe−1 + multiples of higher powers of qe−1, which is equivalent to 1 mod p. Hence t is a stage 4 witness because if tp−1 ≡ 1 (mod p), then tp ≡ t ̸≡ 1 (mod p). As in the previous case, we use this one witness to get many others. If d is a nonwitness, we have d p−1 ≡ 1 (mod p), but then dt mod p is a witness. Moreover, if d1 and d2 are distinct nonwitnesses, then d1t mod p ̸= d2t mod p. Otherwise, d1 = d1 · t · tp−1 mod p = d2 · t · tp−1 mod p = d2. Thus, the number of witnesses must be as large as the number of nonwitnesses and the proof is complete. 404 CHAPTER 10 / ADVANCED TOPICS IN COMPLEXITY THEORY READ-ONCE BRANCHING PROGRAMS A branching program is a model of computation used in complexity theory and in certain practical areas such as computer-aided design. This model represents a decision process that queries the values of input variables and determines how to proceed based on the answers to those queries. We represent this decision process as a graph whose nodes correspond to the particular variable queried at that point in the process. In this section, we investigate the complexity of testing whether two branch- ing programs are equivalent. In general, that problem is coNP-complete. If we place a certain natural restriction on the class of branching programs, we can give a probabilistic polynomial time algorithm for testing equivalence. This algorithm is especially interesting for two reasons. First, no polynomial time algorithm is known for this problem, so it provides an example of probabilism apparently expanding the class of languages whereby membership can be tested efﬁciently. Second, this algorithm introduces the technique of assigning non- Boolean values to normally Boolean variables in order to analyze the behavior of some Boolean function of those variables. That technique is used to great effect in interactive proof systems, as we show in Section 10.4. 10.2 PROBABILISTIC ALGORITHMS 405 FIGURE 10.12 Two read-once branching programs Two branching programs are equivalent if they determine equal functions. Problem 10.21 asks you to show that the problem of testing equivalence for branching programs is coNP-complete. Here we consider a restricted form of branching programs. A read-once branching program is one that can query each variable at most one time on every directed path from the start node to an output node. Both branching programs in Figure 10.12 have the read-once feature. Let EQ ROBP = {⟨B1, B2⟩|B1 and B2 are equivalent read-once branching programs}. THEOREM 10.13 406 CHAPTER 10 / ADVANCED TOPICS IN COMPLEXITY THEORY PROOF We assign polynomials over x1, . . . , xm to the nodes and to the edges of a read-once branching program B as follows. The constant function 1 is assigned to the start node. If a node labeled x has been assigned polynomial p, assign the polynomial xp to its outgoing 1-edge, and assign the polynomial (1 − x)p to its outgoing 0-edge. If the edges incoming to some node have been assigned polynomials, assign the sum of those polynomials to that node. Fi- nally, the polynomial that has been assigned to the output node labeled 1 is also assigned to the branching program itself. Now we are ready to present the prob- abilistic polynomial time algorithm for EQ ROBP. Let F be a ﬁnite ﬁeld with at least 3m elements. D = “On input ⟨B1, B2⟩, two read-once branching programs: 1. Select elements a1 through am at random from F . 2. Evaluate the assigned polynomials p1 and p2 at a1 through am. 3. If p1(a1, . . . , am) = p2(a1, . . . , am), accept ; otherwise, reject .” This algorithm runs in polynomial time because we can evaluate the polyno- mial corresponding to a branching program without actually constructing the polynomial. We show that the algorithm decides EQ ROBP with an error proba- bility of at most 1 10.2 PROBABILISTIC ALGORITHMS 407 LEMMA 10.14 408 CHAPTER 10 / ADVANCED TOPICS IN COMPLEXITY THEORY We conclude this section with one important point concerning the use of randomness in probabilistic algorithms. In our analyses, we assume that these algorithms are implemented using true randomness. True randomness may be difﬁcult (or impossible) to obtain, so it is usually simulated with pseudorandom generators, which are deterministic algorithms whose output appears random. Although the output of any deterministic procedure can never be truly random, some of these procedures generate results that have certain characteristics of randomly generated results. Algorithms that are designed to use randomness may work equally well with these pseudorandom generators, but proving that they do is generally more difﬁcult. Indeed, sometimes probabilistic algorithms may not work well with certain pseudorandom generators. Sophisticated pseu- dorandom generators have been devised that produce results indistinguishable from truly random results by any test that operates in polynomial time, under the assumption that a one-way function exists. (See Section 10.6 for a discussion of one-way functions.) 10.3 ALTERNATION 409 if all or any of its children accept. We deﬁne an alternating Turing machine as follows. 410 CHAPTER 10 / ADVANCED TOPICS IN COMPLEXITY THEORY ALTERNATING TIME AND SPACE We deﬁne the time and space complexity of these machines in the same way that we did for nondeterministic Turing machines: by taking the maximum time or space used by any computation branch. We deﬁne the alternating time and space complexity classes as follows. 10.3 ALTERNATION 411 EXAMPLE 10.20 412 CHAPTER 10 / ADVANCED TOPICS IN COMPLEXITY THEORY Machine S requires space for storing the recursion stack that is used in the depth-ﬁrst search. Each level of the recursion stores one conﬁguration. The recursion depth is M ’s time complexity. Each conﬁguration uses O(f (n)) space, and M ’s time complexity is O(f (n)). Hence S uses O(f 2(n)) space. We can improve the space complexity by observing that S does not need to store the entire conﬁguration at each level of the recursion. Instead it records only the nondeterministic choice that M made to reach that conﬁguration from its parent. Then S can recover this conﬁguration by replaying the computation from the start and following the recorded “signposts.” Making this change re- duces the space usage to a constant at each level of the recursion. The total used now is thus O(f (n)). 10.3 ALTERNATION 413 can yield in a single move of M . After constructing the graph, S repeatedly scans it and marks certain conﬁgurations as accepting. Initially, only the actual accept- ing conﬁgurations of M are marked this way. A conﬁguration that performs universal branching is marked accepting if all of its children are so marked, and an existential conﬁguration is marked if any of its children are marked. Machine S continues scanning and marking until no additional nodes are marked on a scan. Finally, S accepts if the start conﬁguration of M on w is marked. The number of conﬁgurations of M on w is 2O(f (n)) because f (n) ≥ log n. Therefore, the size of the conﬁguration graph is 2O(f (n)) and constructing it may be done in 2O(f (n)) time. Scanning the graph once takes roughly the same time. The total number of scans is at most the number of nodes in the graph because each scan except for the ﬁnal one marks at least one additional node. Hence the total time used is 2O(f (n)). 414 CHAPTER 10 / ADVANCED TOPICS IN COMPLEXITY THEORY We use the representation for conﬁgurations as given in the proof of Theo- rem 9.30, whereby a single symbol may represent both the state of the machine and the content of the tape cell under the head. The contents of cell d in Fig- ure 10.26 are then determined by the contents of its parents a, b, and c. (A cell on the left or right boundary has only two parents.) Simulator S operates recursively to guess and then verify the contents of the individual cells of the tableau. To verify the contents of a cell d outside the ﬁrst row, simulator S existentially guesses the contents of the parents, checks whether their contents would yield d’s contents according to M ’s transition function, and then universally branches to verify these guesses recursively. If d were in the ﬁrst row, S veriﬁes the answer directly because it knows M ’s starting conﬁguration. We assume that M moves its head to the left-hand end of the tape on acceptance, so S can determine whether M accepts w by checking the contents of the lower leftmost cell of the tableau. Hence S never needs to store more than a single pointer to a cell in the tableau, so it uses space log 2O(f (n)) = O(f (n)). 10.4 INTERACTIVE PROOF SYSTEMS 415416 CHAPTER 10 / ADVANCED TOPICS IN COMPLEXITY THEORY Suppose that we have two graphs: G1 and G2. If they are isomorphic, the Prover can convince the Veriﬁer of this fact by presenting the isomorphism or reordering. But if they aren’t isomorphic, how can the Prover convince the Veriﬁer of that fact? Don’t forget: The Veriﬁer doesn’t necessarily trust the Prover, so it isn’t enough for the Prover to declare that they aren’t isomorphic. The Prover must convince the Veriﬁer. Consider the following short protocol. The Veriﬁer randomly selects either G1 or G2 and then randomly reorders its nodes to obtain a graph H. The Veriﬁer sends H to the Prover. The Prover must respond by declaring whether G1 or G2 was the source of H. That concludes the protocol. If G1 and G2 were indeed nonisomorphic, the Prover could always carry out the protocol because the Prover could identify whether H came from G1 or G2. However, if the graphs were isomorphic, H might have come from either G1 or G2. So even with unlimited computational power, the Prover would have no better than a 50–50 chance of getting the correct answer. Thus, if the Prover is able to answer correctly consistently (say in 100 repetitions of the protocol), the Veriﬁer has convincing evidence that the graphs are actually nonisomorphic. DEFINITION OF THE MODEL To deﬁne the interactive proof system model formally, we describe the Veriﬁer, the Prover, and their interaction. You’ll ﬁnd it helpful to keep the graph non- isomorphism example in mind. We deﬁne the Veriﬁer to be a function V that computes its next transmission to the Prover from the message history sent so far. The function V has three inputs: 1. Input string. The objective is to determine whether this string is a mem- ber of some language. In the NONISO example, the input string encoded the two graphs. 2. Random input. For convenience in making the deﬁnition, we provide the Veriﬁer with a randomly chosen input string instead of the equivalent capability to make probabilistic moves during its computation. 3. Partial message history. A function has no memory of the dialog that has been sent so far, so we provide the memory externally via a string representing the exchange of messages up to the present point. We use the notation m1#m2# · · · #mi to represent the exchange of messages m1 through mi. The Veriﬁer’s output is either the next message mi+1 in the sequence or accept or reject , designating the conclusion of the interaction. Thus, V has the func- tional form V : Σ∗ × Σ∗ × Σ∗−→ Σ∗ ∪ {accept , reject }. V (w, r, m1# · · · #mi) = mi+1 means that the input string is w, the random input is r, the current message history is m1 through mi, and the Veriﬁer’s next message to the Prover is mi+1. The Prover is a party with unlimited computational ability. We deﬁne it to be a function P with two inputs: Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 10.4 INTERACTIVE PROOF SYSTEMS 417 1. Input string 2. Partial message history The Prover’s output is the next message to the Veriﬁer. Formally, P has the form P : Σ∗ × Σ∗−→Σ∗. P (w, m1# · · · #mi) = mi+1 means that the Prover sends mi+1 to the Veriﬁer after having exchanged messages m1 through mi so far. Next, we deﬁne the interaction between the Prover and the Veriﬁer. For par- ticular strings w and r, we write (V ↔P )(w, r) = accept if a message sequence m1 through mk exists for some k whereby 1. for 0 ≤ i < k, where i is an even number, V (w, r, m1# · · · #mi) = mi+1; 2. for 0 < i < k, where i is an odd number, P (w, m1# · · · #mi) = mi+1; and 3. the ﬁnal message mk in the message history is accept . To simplify the deﬁnition of the class IP, we assume that the lengths of the Veriﬁer’s random input and each of the messages exchanged between the Veriﬁer and the Prover are p(n) for some polynomial p that depends only on the Veriﬁer. Furthermore, we assume that the total number of messages exchanged is at most p(n). The following deﬁnition gives the probability that an interactive proof system accepts an input string w. For any string w of length n, we deﬁne Pr [ V ↔P accepts w ] = Pr [ (V ↔P )(w, r) = accept ] , where r is a randomly selected string of length p(n). 418 CHAPTER 10 / ADVANCED TOPICS IN COMPLEXITY THEORY IP = PSPACE In this section, we prove one of the more remarkable theorems in complexity theory: the equality of the classes IP and PSPACE. Thus, for any language in PSPACE, a Prover can convince a probabilistic polynomial time Veriﬁer about the membership of a string in the language, even though a conventional proof of membership might be exponentially long. THEOREM 10.29 10.4 INTERACTIVE PROOF SYSTEMS 419 For every 0 ≤ j ≤ p and every message stream Mj, let NMj be deﬁned inductively for decreasing j, starting from the base cases at j = p. For a message stream Mp that contains p messages, let NMp = 1 if Mp is consistent with V ’s messages for some string r and mp = accept . Otherwise, let NMp = 0. For j < p and a message stream Mj, deﬁne NMj as follows. NMj = { maxmj+1 NMj+1 odd j < p wt-avgmj+1 NMj+1 even j < p Here, wt-avgmj+1 NMj+1 means ∑mj+1 ( Prr[ V (w, r, Mj) = mj+1 ] · NMj+1 ) . The expression is the average of NMj+1 , weighted by the probability that the Veriﬁer sent message mj+1. Let M0 be the empty message stream. We make two claims about the value NM0. First, we can calculate NM0 in polynomial space. We do so recursively by calculating NMj for every j and Mj. Calculating maxmj+1 is straightforward. To calculate wt-avgmj+1 , we go through all strings r of length p, and eliminate those that cause the Veriﬁer to produce an output that is inconsistent with Mj. If no strings r remain, then wt-avgmj+1 is 0. If some strings remain, we determine the fraction of the remaining strings r that cause the Veriﬁer to output mj+1. Then we weight NMj+1 by that fraction to compute the average value. The depth of the recursion is p, and therefore only polynomial space is needed. Second, NM0 equals Pr [ V accepts w ], the value needed in order to deter- mine whether w is in A. We prove this second claim by induction as follows. CLAIM 10.31 420 CHAPTER 10 / ADVANCED TOPICS IN COMPLEXITY THEORY Equality 1 is the deﬁnition of NMj . Equality 2 is based on the induction hypoth- esis. Equality 3 follows from the deﬁnition of Pr [ V accepts w starting at Mj ]. Thus, the claim holds if j is even. If j is odd, mj+1 is a message from P to V . We then have the series of equalities: NMj 1 = max mj+1 NMj+1 2 = max mj+1 Pr [ V accepts w starting at Mj+1 ] 3 = Pr [ V accepts w starting at Mj ] . Equality 1 is the deﬁnition of NMj . Equality 2 uses the induction hypothesis. We break equality 3 into two inequalities. We have ≤ because the Prover that maximizes the lower line could send the message mj+1 that maximizes the up- per line. We have ≥ because that same Prover cannot do any better than send that same message. Sending anything other than a message that maximizes the upper line would lower the resulting value. That proves the claim for odd j and completes one direction of the proof of Theorem 10.29. 10.4 INTERACTIVE PROOF SYSTEMS 421 xj = aj for j ≤ i. The constant function f0() is the number of satisfying assign- ments of ϕ. The function fm(a1, . . . , am) is 1 if those ai’s satisfy ϕ; otherwise, it is 0. An easy identity holds for every i < m and a1, . . . , ai: fi(a1, . . . , ai) = fi+1(a1, . . . , ai, 0) + fi+1(a1, . . . , ai, 1). The protocol for #SAT begins with phase 0 and ends with phase m + 1. The input is the pair ⟨ϕ, k⟩. Phase 0. P sends f0() to V . V checks that k = f0() and rejects if not. Phase 1. P sends f1(0) and f1(1) to V . V checks that f0() = f1(0) + f1(1) and rejects if not. Phase 2. P sends f2(0,0), f2(0,1), f2(1,0), and f2(1,1) to V . V checks that f1(0) = f2(0,0)+f2(0,1) and f1(1) = f2(1,0)+f2(1,1) and rejects if not. ... Phase m. P sends fm(a1, . . . , am) for each assignment to the ai’s. V checks the 2m−1 equations linking fm−1 with fm and rejects if any fail. Phase m+1. V checks that the values fm(a1, . . . , am) are correct for each assignment to the ai’s by evaluating ϕ on each assignment. If all assignments are correct, V accepts; otherwise, V rejects. That completes the description of the protocol. This protocol doesn’t provide a proof that #SAT is in IP because the Veriﬁer must spend exponential time just to read the exponentially long messages that the Prover sends. Let’s examine it for correctness anyway because that helps us understand the next, more efﬁcient protocol. Intuitively, a protocol decides a language A if a Prover can convince the Ver- iﬁer of the membership of strings in A. In other words, if a string is a member of A, some Prover can cause the Veriﬁer to accept with high probability. If the string isn’t a member of A, no Prover—not even a crooked or devious one—can cause the Veriﬁer to accept with more than low probability. We use the symbol P to designate the Prover that correctly follows the protocol, and that thereby makes V accept with high probability when the input is in A. We use the sym- bol ̃P to designate any Prover that interacts with the Veriﬁer when the input isn’t in A. Think of ̃P as an adversary—as though ̃P were attempting to make V accept when V should reject. The notation ̃P is suggestive of a “crooked” Prover. In the #SAT protocol we just described, the Veriﬁer ignores its random input and operates deterministically once the Prover has been selected. To prove the protocol is correct, we establish two facts. First, if k is the correct number of sat- isfying assignments for ϕ in the input ⟨ϕ, k⟩, some Prover P causes V to accept. The Prover that gives accurate responses at every phase does the job. Second, if k isn’t correct, every Prover ̃P causes V to reject. We argue this case as follows. Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 422 CHAPTER 10 / ADVANCED TOPICS IN COMPLEXITY THEORY If k is not correct and ̃P gives accurate responses, V rejects outright in phase 0 because f0() is the number of ϕ’s satisfying assignments and therefore f0() ̸= k. To prevent V from rejecting in phase 0, ̃P must send an incorrect value for f0(), denoted ̃f0(). Intuitively, ̃f0() is a lie about the value of f0(). As in real life, lies beget lies, and ̃P is forced to continue lying about other values of fi in order to avoid being caught during later phases. Eventually these lies catch up with ̃P in phase m + 1, where V checks the values of fm directly. More precisely, because ̃f0() ̸= f0(), at least one of the values f1(0) and f1(1) that ̃P sends in phase 1 must be incorrect; otherwise, V rejects when it checks whether f0() = f1(0) + f1(1). Let’s say that f1(0) is incorrect and call the value that is sent instead ̃f1(0). Continuing in this way, we see that at every phase ̃P must end up sending some incorrect value ̃fi(a1, . . . , ai), or V would have rejected by that point. But when V checks the incorrect value ̃fm(a1, . . . , am) in phase m + 1, it rejects anyway. Thus, we have shown that if k is incorrect, V rejects no matter what ̃P does. Therefore, the protocol is correct. The problem with this protocol is that the number of messages doubles with every phase. This doubling occurs because the Veriﬁer requires the two values fi+1(. . . , 0) and fi+1(. . . , 1) to conﬁrm the one value fi(. . .). If we could ﬁnd a way for the Veriﬁer to conﬁrm a value of fi with only a single value of fi+1, the number of messages wouldn’t grow at all. We can do so by extending the functions fi to non-Boolean inputs and conﬁrming the single value fi+1(. . . , z) for some z selected at random from a ﬁnite ﬁeld. PROOF Let ϕ be a cnf-formula with variables x1 through xm. In a technique called arithmetization, we associate with ϕ a polynomial p(x1, . . . , xm) where p mimics ϕ by simulating the Boolean ∧, ∨, and ¬ operations with the arith- metic operations + and × as follows. If α and β are subformulas, we replace expressions α ∧ β with αβ, ¬α with 1 − α, and α ∨ β with α ∗ β = 1 − (1 − α)(1 − β). One observation regarding p that will be important to us later is that the degree of any of its variables is not large. The operations αβ and α ∗ β each produce a polynomial whose degree is at most the sum of the degrees of the polynomials for α and β. Thus, the degree of any variable is at most n, the length of ϕ. If p’s variables are assigned Boolean values, it agrees with ϕ on that assign- ment. Evaluating p when the variables are assigned non-Boolean values has no obvious interpretation in ϕ. However, the proof uses such assignments anyway to analyze ϕ, much as the proof of Theorem 10.13 uses non-Boolean assign- ments to analyze read-once branching programs. The variables range over a ﬁnite ﬁeld F with q elements, where q is at least 2n. Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 10.4 INTERACTIVE PROOF SYSTEMS 423 We use p to redeﬁne the functions fi that we deﬁned in the proof idea section. For 0 ≤ i ≤ m and for a1, . . . , ai ∈ F, let fi(a1, . . . , ai) = ∑ ai+1,...,am∈{0,1} p(a1, . . . , am). Observe that this redeﬁnition extends the original deﬁnition because the two agree when a1 through ai take on Boolean values. Thus, f0() is still the num- ber of satisfying assignments of ϕ. Each of the functions fi(x1, . . . , xi) can be expressed as a polynomial in x1 through xi. The degree of each of these poly- nomials is at most that of p. Next, we present the protocol for #SAT. Initially, V receives input ⟨ϕ, k⟩ and arithmetizes ϕ to obtain polynomial p. All arithmetic is done in the ﬁeld F with q elements, where q is a prime that is larger than 2n. (Finding such a prime q requires an extra step, but we ignore this point here because the proof we give shortly of the stronger result IP = PSPACE doesn’t require it.) A comment in double brackets appears at the start of the description of each phase. Phase 0. [[ P sends f0(). ]] P →V : P sends f0() to V . V checks that k = f0(). V rejects if that fails. Phase 1. [[ P persuades V that f0() is correct if f1(r1) is correct. ]] P →V : P sends the coefﬁcients of f1(z) as a polynomial in z. V uses these coefﬁcients to evaluate f1(0) and f1(1). V checks that f0() = f1(0) + f1(1) and rejects if not. (Remember that all calculations are done over F .) V →P : V selects r1 at random from F and sends it to P . Phase 2. [[ P persuades V that f1(r1) is correct if f2(r1, r2) is correct. ]] P →V : P sends the coefﬁcients of f2(r1, z) as a polynomial in z. V uses these coefﬁcients to evaluate f2(r1, 0) and f2(r1, 1). V checks that f1(r1) = f2(r1, 0) + f2(r1, 1) and rejects if not. V →P : V selects r2 at random from F and sends it to P . ... Phase i. [[ P persuades V that fi−1(r1, . . . , ri−1) is correct if fi(r1, . . . , ri) is correct. ]] P →V : P sends the coefﬁcients of fi(r1, . . . , ri−1, z) as a polynomial in z. V uses these coefﬁcients to evaluate fi(r1, . . . , ri−1, 0) and fi(r1, . . . , ri−1, 1). V checks that fi−1(r1, . . . , ri−1) = fi(r1, . . . , ri−1, 0) + fi(r1, . . . , ri−1, 1) and rejects if not. V →P : V selects ri at random from F and sends it to P . ... Phase m+1. [[ V checks directly that fm(r1, . . . , rm) is correct. ]] V evaluates p(r1, . . . , rm) to compare with the value V has for fm(r1, . . . , rm). If they are equal, V accepts; otherwise, V rejects. That completes the description of the protocol. Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 424 CHAPTER 10 / ADVANCED TOPICS IN COMPLEXITY THEORY Now we show that this protocol decides #SAT. First, if ϕ has k satisfying assignments, V obviously accepts with certainty if Prover P follows the protocol. Second, we show that if ϕ doesn’t have k assignments, no Prover can make it accept with more than a low probability. Let ̃P be any Prover. To prevent V from rejecting outright, ̃P must send an incorrect value ̃f0() for f0() in phase 0. Therefore, in phase 1, one of the values that V calculates for f1(0) and f1(1) must be incorrect. Thus, the coefﬁcients that ̃P sent for f1(z) as a polynomial in z must be wrong. Let ̃f1(z) be the function that these coefﬁcients represent instead. Next comes a key step of the proof. When V picks a random r1 in F , we claim that ̃f1(r1) is unlikely to equal f1(r1). For n ≥ 10, we show that Pr [ ̃f1(r1) = f1(r1) ] < n−2. That bound on the probability follows from Lemma 10.14: A polynomial in a single variable of degree at most d can have no more than d roots, unless it always evaluates to 0. Therefore, any two polynomials in a single variable of degree at most d can agree in at most d places, unless they agree everywhere. Recall that the degree of the polynomial for f1 is at most n, and that V rejects if the degree of the polynomial ̃P sends for ̃f1 is greater than n. We have al- ready determined that these functions don’t agree everywhere, so Lemma 10.14 implies they can agree in at most n places. The size of F is greater than 2n. The chance that r1 happens to be one of the places where the functions agree is at most n/2n, which is less than n−2 for n ≥ 10. To recap what we’ve shown so far, if ̃f0() is wrong, ̃f1’s polynomial must be wrong, and then ̃f1(r1) would likely be wrong by virtue of the preceding claim. In the unlikely event that ̃f1(r1) agrees with f1(r1), ̃P was “lucky” at this phase and it will be able to make V accept (even though V should reject) by following the instructions for P in the rest of the protocol. Continuing further with the argument, if ̃f1(r1) were wrong, at least one of the values V computes for f2(r1, 0) and f2(r1, 1) in phase 2 must be wrong, so the coefﬁcients that ̃P sent for f2(r1, z) as a polynomial in z must be wrong. Let ̃f2(r1, z) be the function these coefﬁcients represent instead. The polynomials for f2(r1, z) and ̃f2(r1, z) have degree at most n. So as before, the probability that they agree at a random r2 in F is at most n−2. Thus, when V picks r2 at random, ̃f2(r1, r2) is likely to be wrong. The general case follows in the same way to show that for each 1 ≤ i ≤ m, if ̃fi−1(r1, . . . , ri−1) ̸= fi−1(r1, . . . , ri−1), then for n ≥ 10 and for ri chosen at random in F , Pr [ ̃fi(r1, . . . , ri) = fi(r1, . . . , ri) ] ≤ n−2. Thus, by giving an incorrect value for f0(), ̃P is probably forced to give incor- rect values for f1(r1), f2(r1, r2), and so on to fm(r1, . . . , rm). The probability that ̃P gets lucky because V selects an ri, where ̃fi(r1, . . . , ri) = fi(r1, . . . , ri) even though ̃fi and fi are different in some phase, is at most the number of Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 10.4 INTERACTIVE PROOF SYSTEMS 425 phases m times n−2 or at most 1/n. If ̃P never gets lucky, it eventually sends an incorrect value for fm(r1, . . . , rm). But V checks that value of fm directly in phase m + 1 and will catch any error at that point. So if k is not the num- ber of satisfying assignments of ϕ, no Prover can make the Veriﬁer accept with probability greater than 1/n. To complete the proof of the theorem, we need only show that the Veriﬁer operates in probabilistic polynomial time, which is obvious from its description. 426 CHAPTER 10 / ADVANCED TOPICS IN COMPLEXITY THEORY PROOF Let ψ = Qx1 · · · Qxm [ ϕ ] be a quantiﬁed Boolean formula, where ϕ is a cnf-formula. To arithmetize ψ, we introduce the expression ψ′ = Qx1 Rx1 Qx2 Rx1Rx2 Qx3 Rx1Rx2Rx3 · · · Qxm Rx1 · · · Rxm [ ϕ ]. Don’t worry about the meaning of Rxi for now. It is useful only for deﬁning the functions fi. We rewrite ψ′ as ψ′ = S1y1 S2y2 · · · Skyk [ ϕ ], where each Si ∈ {∀, ∃, R} and yi ∈ {x1, . . . , xm}. For each i ≤ k, we deﬁne the function fi. We deﬁne fk(x1, . . . , xm) to be the polynomial p(x1, . . . , xm) obtained by arithmetizing ϕ. For i < k, we deﬁne fi in terms of fi+1: Si+1 = ∀ : fi(. . .) = fi+1(. . . , 0) · fi+1(. . . , 1); Si+1 = ∃ : fi(. . .) = fi+1(. . . , 0) ∗ fi+1(. . . , 1); Si+1 = R : fi(. . . , a) = (1−a)fi+1(. . . , 0) + afi+1(. . . , 1). If Si+1 is ∀ or ∃, fi has one fewer input variable than fi+1 does. If Si+1 is R, the two functions have the same number of input variables. Thus, function fi will not, in general, depend on i variables. To avoid cumbersome subscripts, we use “. . .” in place of a1 through aj for the appropriate values of j. Furthermore, we reorder the inputs to the functions so that input variable yi+1 is the last argument. Note that the Rx operation on polynomials doesn’t change their values on Boolean inputs. Therefore, f0() is still the truth value of ψ. However, note that the Rx operation produces a result that is linear in x. We added Rx1 · · · Rxi after Qixi in ψ′ in order to reduce the degree of each variable to 1 prior to the squaring due to arithmetizing Qi. Now we are ready to describe the protocol. All arithmetic operations in this protocol are over a ﬁeld F of size at least n4, where n is the length of ψ. V can ﬁnd a prime of this size on its own, so P doesn’t need to provide one. Phase 0. [[ P sends f0(). ]] P →V : P sends f0() to V . V checks that f0() = 1 and rejects if not. ... Phase i. [[ P persuades V that fi−1(r1 · · · ) is correct if fi(r1 · · · , r) is correct. ]] P →V : P sends the coefﬁcients of fi(r1 · · · , z) as a polynomial in z. (Here r1 · · · denotes a setting of the variables to the previously selected random values r1, r2, . . . .) V uses these coefﬁcients to evaluate fi(r1 · · · , 0) and fi(r1 · · · , 1). V checks that these identities hold: fi−1(r1 · · · ) = {fi(r1 · · · , 0) · fi(r1 · · · , 1) Si = ∀, fi(r1 · · · , 0) ∗ fi(r1 · · · , 1) Si = ∃, Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 10.5 PARALLEL COMPUTATION 427 and fi−1(r1 · · · , r) = (1 − r)fi(r1 · · · , 0) + rfi(r1 · · · , 1) Si = R. If not, V rejects. V →P : V picks a random r in F and sends it to P . (When Si = R, this r replaces the previous r.) Go to Phase i + 1, where P must persuade V that fi(r1 · · · , r) is correct. ... Phase k+1. [[ V checks directly that fk(r1, . . . , rm) is correct. ]] V evaluates p(r1, . . . , rm) to compare with the value V has for fk(r1, . . . , rm). If they are equal, V accepts; otherwise, V rejects. That completes the description of the protocol. Proving the correctness of this protocol is similar to proving the correctness of the #SAT protocol. Clearly, if ψ is true, P can follow the protocol and V will accept. If ψ is false, ̃P must lie at phase 0 by sending an incorrect value for f0(). At phase i, if V has an incorrect value for fi−1(r1 · · · ), one of the values fi(r1 · · · , 0) and fi(r1 · · · , 1) must be incorrect and the polynomial for fi must be incorrect. Consequently, for a random r, the probability that ̃P gets lucky at this phase because fi(r1 · · · , r) is correct is at most the polynomial degree divided by the ﬁeld size or n/n4. The protocol proceeds for O(n2) phases, so the probability that ̃P gets lucky at some phase is at most 1/n. If ̃P is never lucky, V will reject at phase k + 1. 428 CHAPTER 10 / ADVANCED TOPICS IN COMPLEXITY THEORY UNIFORM BOOLEAN CIRCUITS One of the most popular models in theoretical work on parallel algorithms is called the Parallel Random Access Machine or PRAM. In the PRAM model, idealized processors with a simple instruction set patterned on actual computers interact via a shared memory. In this short section we can’t describe PRAMs in detail. Instead, we use an alternative model of parallel computer that we introduced for another purpose in Chapter 9: Boolean circuits. Boolean circuits have certain advantages and disadvantages as a parallel com- putation model. On the positive side, the model is simple to describe, which makes proofs easier. Circuits also bear an obvious resemblance to actual hard- ware designs, and in that sense the model is realistic. On the negative side, circuits are awkward to “program” because the individual processors are so weak. Furthermore, we disallow cycles in our deﬁnition of Boolean circuits, in contrast to circuits that we can actually build. In the Boolean circuit model of a parallel computer, we take each gate to be an individual processor, so we deﬁne the processor complexity of a Boolean circuit to be its size. We consider each processor to compute its function in a single time step, so we deﬁne the parallel time complexity of a Boolean circuit to be its depth, or the longest distance from an input variable to the output gate. Any particular circuit has a ﬁxed number of input variables, so we use cir- cuit families as deﬁned in Deﬁnition 9.27 for deciding languages. We need to impose a technical requirement on circuit families so that they correspond to parallel computation models such as PRAMs, where a single machine is capable of handling all input lengths. That requirement states that we can easily ob- tain all members in a circuit family. This uniformity requirement is reasonable because knowing that a small circuit exists for deciding certain elements of a language isn’t very useful if the circuit itself is hard to ﬁnd. That leads us to the following deﬁnition. 10.5 PARALLEL COMPUTATION 429 EXAMPLE 10.35 430 CHAPTER 10 / ADVANCED TOPICS IN COMPLEXITY THEORY cuits for each A i, which adds another factor of m to the size and an additional layer of O(log n) depth. Hence the size–depth complexity of transitive closure is (O(n5/2), O(log2 n)). 10.5 PARALLEL COMPUTATION 431 THEOREM 10.40 432 CHAPTER 10 / ADVANCED TOPICS IN COMPLEXITY THEORY P-COMPLETENESS Now we consider the possibility that all problems in P are also in NC. Equal- ity between these classes would be surprising because it would imply that all polynomial time solvable problems are highly parallelizable. We introduce the phenomenon of P-completeness to give theoretical evidence that some problems in P are inherently sequential. 10.6 CRYPTOGRAPHY 433434 CHAPTER 10 / ADVANCED TOPICS IN COMPLEXITY THEORY A key that is as long as the combined message length is called a one-time pad. Essentially, every bit of a one-time pad key is used just once to encrypt a bit of the message, and then that bit of the key is discarded. The main problem with one-time pads is that they may be rather large if a signiﬁcant amount of commu- nication is anticipated. For most purposes, one-time pads are too cumbersome to be considered practical. A cryptographic code that allows an unlimited amount of secure communica- tion with keys of only moderate length is preferable. Interestingly, such codes can’t exist in principle but paradoxically are used in practice. This type of code can’t exist in principle because a key that is signiﬁcantly shorter than the com- bined message length can be found by a brute-force search through the space of possible keys. Therefore, a code that is based on such keys is breakable in princi- ple. But therein lies the solution to the paradox. A code could provide adequate security in practice anyway because brute-force search is extremely slow when the key is moderately long—say, in the range of 100 bits. Of course, if the code could be broken in some other, fast way, it is insecure and shouldn’t be used. The difﬁculty lies in being sure that the code can’t be broken quickly. We currently have no way of ensuring that a code with moderate-length keys is actually secure. To guarantee that a code can’t be broken quickly, we’d need a mathematical proof that, at the very least, ﬁnding the key can’t be done quickly. However, such proofs seem beyond the capabilities of contemporary mathemat- ics! The reason is that once a key is discovered, verifying its correctness is easily done by inspecting the messages that have been decrypted with it. Therefore, the key veriﬁcation problem can be formulated so as to be in P. If we could prove that keys can’t be found in polynomial time, we would achieve a major mathematical advance by proving that P is different from NP. Because we are unable to prove mathematically that codes are unbreakable, we rely instead on circumstantial evidence. In the past, evidence of a code’s qual- ity was obtained by hiring experts who tried to break it. If they were unable to do so, conﬁdence in its security increased. That approach has obvious deﬁciencies. If someone has better experts than ours, or if we can’t trust our own experts, the integrity of our code may be compromised. Nonetheless, this approach was the only one available until recently and was used to support the reliability of widely used codes such as the Data Encryption Standard (DES) that was sanctioned by the U.S. National Institute of Standards and Technology. Complexity theory provides another way to gain evidence for a code’s secu- rity. We may show that the complexity of breaking the code is linked to the complexity of some other problem for which compelling evidence of intractabil- ity is already available. Recall that we have used NP-completeness to provide evidence that certain problems are intractable. Reducing an NP-complete prob- lem to the code-breaking problem would show that the code-breaking problem was itself NP-complete. However, that doesn’t provide sufﬁcient evidence of security because NP-completeness concerns worst-case complexity. A problem may be NP-complete, yet easy to solve most of the time. Codes must almost al- ways be difﬁcult to break, so we need to measure average-case complexity rather than worst-case complexity. Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 10.6 CRYPTOGRAPHY 435 One problem that is generally believed to be difﬁcult for the average case is the problem of integer factorization. Top mathematicians have been interested in factorization for centuries, but no one has yet discovered a fast procedure for doing so. Certain modern codes have been built around the factoring problem so that breaking the code corresponds to factoring a number. That constitutes convincing evidence for the security of these codes because an efﬁcient way of breaking such a code would lead to a fast factoring algorithm, which would be a remarkable development in computational number theory. PUBLIC-KEY CRYPTOSYSTEMS Even when cryptographic keys are moderately short, their management still presents an obstacle to their widespread use in conventional cryptography. One problem is that every pair of parties that desires private communication needs to establish a joint secret key for this purpose. Another problem is that each indi- vidual needs to keep a secret database of all keys that have been so established. The recent development of public-key cryptography provides an elegant solu- tion to both problems. In a conventional or private-key cryptosystem, the same key is used for both encryption and decryption. Compare that with the novel public-key cryptosystem for which the decryption key is different from, and not easily computed from, the encryption key. Although it is a deceptively simple idea, separating the two keys has profound consequences. Now each individual only needs to establish a single pair of keys: an encryption key E and a decryption key D. The individual keeps D secret but publicizes E. If another individual wants to send him a message, she looks up E in the public directory, encrypts the message with it, and sends it to him. The ﬁrst individual is the only one who knows D, so only he can decrypt that message. Certain public-key cryptosystems can also be used for digital signatures. If an individual applies his secret decryption algorithm to a message before send- ing it, anyone can check that it actually came from him by applying the public encryption algorithm. He has thus effectively “signed” that message. This ap- plication assumes that the encryption and decryption functions may be applied in either order, as is the case with the RSA cryptosystem. ONE-WAY FUNCTIONS Now we brieﬂy investigate some of the theoretical underpinnings of the modern theory of cryptography, called one-way functions and trapdoor functions. One of the advantages of using complexity theory as a foundation for cryptography is that it helps to clarify the assumptions being made when we argue about security. By assuming the existence of a one-way function, we may construct secure private- key cryptosystems. Assuming the existence of trapdoor functions allows us to construct public-key cryptosystems. Both assumptions have additional theoret- ical and practical consequences. We deﬁne these types of functions after some preliminaries. Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 436 CHAPTER 10 / ADVANCED TOPICS IN COMPLEXITY THEORY A function f : Σ∗−→Σ∗ is length-preserving if the lengths of w and f (w) are equal for every w. A length-preserving function is a permutation if it never maps two strings to the same place; that is, if f (x) ̸= f (y) whenever x ̸= y. Recall the deﬁnition of a probabilistic Turing machine given in Section 10.2. Let’s say that a probabilistic Turing machine M computes a probabilistic func- tion M : Σ∗−→ Σ∗, where if w is an input and x is an output, we assign Pr [ M (w) = x ] to be the probability that M halts in an accept state with x on its tape when it is started on input w. Note that M may sometimes fail to accept on input w, so ∑ x∈Σ∗ Pr [ M (w) = x ] ≤ 1. Next, we get to the deﬁnition of a one-way function. Roughly speaking, a function is one-way if it is easy to compute but nearly always hard to invert. In the following deﬁnition, f denotes the easily computed one-way function and M denotes the probabilistic polynomial time algorithm that we may think of as trying to invert f . We deﬁne one-way permutations ﬁrst because that case is somewhat simpler. 10.6 CRYPTOGRAPHY 437 f (w). For one-way functions, any probabilistic polynomial time algorithm is unlikely to be able to ﬁnd any y that maps to f (w). EXAMPLE 10.46 438 CHAPTER 10 / ADVANCED TOPICS IN COMPLEXITY THEORYEXERCISES 439 We can use a trapdoor function such as the RSA trapdoor function to con- struct a public-key cryptosystem as follows. The public key is the index i gener- ated by the probabilistic machine G. The secret key is the corresponding value t. The encryption algorithm breaks the message m into blocks of size at most log N . For each block w, the sender computes fi. The resulting sequence of strings is the encrypted message. The receiver uses the function h to obtain the original message from its encryption. 440 CHAPTER 10 / ADVANCED TOPICS IN COMPLEXITY THEORY 10.11 Let M be a probabilistic polynomial time Turing machine, and let C be a language where for some ﬁxed 0 < ϵ1 < ϵ2 < 1, a. w ̸∈ C implies Pr[M accepts w] ≤ ϵ1, and b. w ∈ C implies Pr[M accepts w] ≥ ϵ2. Show that C ∈ BPP. (Hint: Use the result of Lemma 10.5.) 10.12 Show that if P = NP, then P = PH. 10.13 Show that if PH = PSPACE, then the polynomial time hierarchy has only ﬁnitely many distinct levels. 10.14 Recall that NP SAT is the class of languages that are decided by nondeterminis- tic polynomial time Turing machines with an oracle for the satisﬁability problem. Show that NP SAT = Σ2P. ⋆10.15 Prove Fermat’s little theorem, which is given in Theorem 10.6. (Hint: Consider the sequence a1, a2, . . . . What must happen, and how?) A⋆10.16 Prove that for any integer p > 1, if p isn’t pseudoprime, then p fails the Fermat test for at least half of all numbers in Z + p . 10.17 Prove that if A is a language in L, a family of branching programs (B1, B2, . . .) exists wherein each Bn accepts exactly the strings in A of length n and is bounded in size by a polynomial in n. 10.18 Prove that if A is a regular language, a family of branching programs (B1, B2, . . .) exists wherein each Bn accepts exactly the strings in A of length n and is bounded in size by a constant times n. 10.19 Show that if NP ⊆ BPP, then NP = RP. 10.20 Deﬁne a ZPP-machine to be a probabilistic Turing machine that is permitted three types of output on each of its branches: accept, reject, and ?. A ZPP-machine M decides a language A if M outputs the correct answer on every input string w (accept if w ∈ A and reject if w ̸∈ A) with probability at least 2 SELECTED SOLUTIONS 441 SELECTED SOLUTIONS 10.7 If M is a probabilistic TM that runs in polynomial time, we can modify M so that it makes exactly n r coin tosses on each branch of its computation, for some con- stant r. Thus, the problem of determining the probability that M accepts its input string reduces to counting how many branches are accepting and comparing this number with 2 Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 444 14. BALC ´AZAR, J. L., D´IAZ, J., AND GABARR ´O, J. Structural Complexity I, II. EATCS Monographs on Theoretical Computer Science. Springer Verlag, 1988 (I) and 1990 (II). 15. BEAME, P. W., COOK, S. A., AND HOOVER, H. J. Log depth circuits for division and related problems. SIAM Journal on Computing 15, 4 (1986), 994–1003. 16. BLUM, M., CHANDRA, A., AND WEGMAN, M. Equivalence of free boolean graphs can be decided probabilistically in polynomial time. Information Pro- cessing Letters 10 (1980), 80–82. 17. BRASSARD, G., AND BRATLEY, P. Algorithmics: Theory and Practice. Pren- tice-Hall, 1988. 18. CARMICHAEL, R. D. On composite numbers p which satisfy the Fermat congruence aP −1 ≡ P mod P . American Mathematical Monthly 19 (1912), 22–27. 19. CHOMSKY, N. Three models for the description of language. IRE Trans. on Information Theory 2 (1956), 113–124. 20. COBHAM, A. The intrinsic computational difﬁculty of functions. In Pro- ceedings of the International Congress for Logic, Methodology, and Philosophy of Science, Y. Bar-Hillel, Ed., North-Holland, 1964, 24–30. 21. COOK, S. A. The complexity of theorem-proving procedures. In Proceedings of the Third Annual ACM Symposium on the Theory of Computing (1971), 151– 158. 22. CORMEN, T., LEISERSON, C., AND RIVEST, R. Introduction to Algorithms. MIT Press, 1989. 23. EDMONDS, J. Paths, trees, and ﬂowers. Canadian Journal of Mathematics 17 (1965), 449–467. 24. ENDERTON, H. B. A Mathematical Introduction to Logic. Academic Press, 1972. 25. EVEN, S. Graph Algorithms. Pitman, 1979. 26. FELLER, W. An Introduction to Probability Theory and Its Applications, Vol. 1. John Wiley & Sons, 1970. 27. FEYNMAN, R. P., HEY, A. J. G., AND ALLEN, R. W. Feynman lectures on computation. Addison-Wesley, 1996. 28. GAREY, M. R., AND JOHNSON, D. S. Computers and Intractability—A Guide to the Theory of NP-completeness. W. H. Freeman, 1979. 29. GILL, J. T. Computational complexity of probabilistic Turing machines. SIAM Journal on Computing 6, 4 (1977), 675–695. 30. G ¨ODEL, K. On formally undecidable propositions in Principia Mathematica and related systems I. In The Undecidable, M. Davis, Ed., Raven Press, 1965, 4–38. 31. GOEMANS, M. X., AND WILLIAMSON, D. P. .878-approximation algo- rithms for MAX CUT and MAX 2SAT. In Proceedings of the Twenty-sixth Annual ACM Symposium on the Theory of Computing (1994), 422–431. Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 445 32. GOLDWASSER, S., AND MICALI, S. Probabilistic encryption. Journal of Computer and System Sciences (1984), 270–299. 33. GOLDWASSER, S., MICALI, S., AND RACKOFF, C. The knowledge com- plexity of interactive proof-systems. SIAM Journal on Computing (1989), 186–208. 34. GREENLAW, R., HOOVER, H. J., AND RUZZO, W. L. Limits to Parallel Computation: P-completeness Theory. Oxford University Press, 1995. 35. HARARY, F. Graph Theory, 2d ed. Addison-Wesley, 1971. 36. HARTMANIS, J., AND STEARNS, R. E. On the computational complexity of algorithms. Transactions of the American Mathematical Society 117 (1965), 285–306. 37. HILBERT, D. Mathematical problems. Lecture delivered before the In- ternational Congress of Mathematicians at Paris in 1900. In Mathematical Developments Arising from Hilbert Problems, vol. 28. American Mathematical Society, 1976, 1–34. 38. HOFSTADTER, D. R. Goedel, Escher, Bach: An Eternal Golden Braid. Basic Books, 1979. 39. HOPCROFT, J. E., AND ULLMAN, J. D. Introduction to Automata Theory, Languages and Computation. Addison-Wesley, 1979. 40. IMMERMAN, N. Nondeterminstic space is closed under complement. SIAM Journal on Computing 17 (1988), 935–938. 41. JOHNSON, D. S. The NP-completeness column: Interactive proof systems for fun and proﬁt. Journal of Algorithms 9, 3 (1988), 426–444. 42. KARP, R. M. Reducibility among combinatorial problems. In Complexity of Computer Computations (1972), R. E. Miller and J. W. Thatcher, Eds., Plenum Press, 85–103. 43. KARP, R. M., AND LIPTON, R. J. Turing machines that take advice. EN- SEIGN: L’Enseignement Mathematique Revue Internationale 28 (1982). 44. KNUTH, D. E. On the translation of languages from left to right. Informa- tion and Control (1965), 607–639. 45. LAWLER, E. L. Combinatorial Optimization: Networks and Matroids. Holt, Rinehart and Winston, 1991. 46. LAWLER, E. L., LENSTRA, J. K., RINOOY KAN, A. H. G., AND SHMOYS, D. B. The Traveling Salesman Problem. John Wiley & Sons, 1985. 47. LEIGHTON, F. T. Introduction to Parallel Algorithms and Architectures: Array, Trees, Hypercubes. Morgan Kaufmann, 1991. 48. LEVIN, L. Universal search problems (in Russian). Problemy Peredachi Infor- matsii 9, 3 (1973), 115–116. 49. LEWIS, H., AND PAPADIMITRIOU, C. Elements of the Theory of Computation. Prentice-Hall, 1981. 50. LI, M., AND VITANYI, P. Introduction to Kolmogorov Complexity and its Appli- cations. Springer-Verlag, 1993. Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 446 51. LICHTENSTEIN, D., AND SIPSER, M. GO is PSPACE hard. Journal of the ACM (1980), 393–401. 52. LUBY, M. Pseudorandomness and Cryptographic Applications. Princeton Uni- versity Press, 1996. 53. LUND, C., FORTNOW, L., KARLOFF, H., AND NISAN, N. Algebraic meth- ods for interactive proof systems. Journal of the ACM 39, 4 (1992), 859–868. 54. MILLER, G. L. Riemann’s hypothesis and tests for primality. Journal of Computer and System Sciences 13 (1976), 300–317. 55. NIVEN, I., AND ZUCKERMAN, H. S. An Introduction to the Theory of Num- bers, 4th ed. John Wiley & Sons, 1980. 56. PAPADIMITRIOU, C. H. Computational Complexity. Addison-Wesley, 1994. 57. PAPADIMITRIOU, C. H., AND STEIGLITZ, K. Combinatorial Optimization (Algorithms and Complexity). Prentice-Hall, 1982. 58. PAPADIMITRIOU, C. H., AND YANNAKAKIS, M. Optimization, approxi- mation, and complexity classes. Journal of Computer and System Sciences 43, 3 (1991), 425–440. 59. POMERANCE, C. On the distribution of pseudoprimes. Mathematics of Com- putation 37, 156 (1981), 587–593. 60. PRATT, V. R. Every prime has a succinct certiﬁcate. SIAM Journal on Com- puting 4, 3 (1975), 214–220. 61. RABIN, M. O. Probabilistic algorithms. In Algorithms and Complexity: New Directions and Recent Results, J. F. Traub, Ed., Academic Press (1976) 21–39. 62. REINGOLD, O. Undirected st-connectivity in log-space. Journal of the ACM 55, 4 (2008), 1–24. 63. RIVEST, R. L., SHAMIR, A., AND ADLEMAN, L. A method for obtaining digital signatures and public key cryptosystems. Communications of the ACM 21, 2 (1978), 120–126. 64. ROCHE, E., AND SCHABES, Y. Finite-State Language Processing. MIT Press, 1997. 65. SCHAEFER, T. J. On the complexity of some two-person perfect-infor- mation games. Journal of Computer and System Sciences 16, 2 (1978), 185–225. 66. SEDGEWICK, R. Algorithms, 2d ed. Addison-Wesley, 1989. 67. SHAMIR, A. IP = PSPACE. Journal of the ACM 39, 4 (1992), 869–877. 68. SHEN, A. IP = PSPACE: Simpliﬁed proof. Journal of the ACM 39, 4 (1992), 878–880. 69. SHOR, P. W. Polynomial-time algorithms for prime factorization and dis- crete logarithms on a quantum computer. SIAM Journal on Computing 26, (1997), 1484–1509. 70. SIPSER, M. Lower bounds on the size of sweeping automata. Journal of Computer and System Sciences 21, 2 (1980), 195–202. Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 447 71. SIPSER, M. The history and status of the P versus NP question. In Proceed- ings of the Twenty-fourth Annual ACM Symposium on the Theory of Computing (1992), 603–618. 72. STINSON, D. R. Cryptography: Theory and Practice. CRC Press, 1995. 73. SZELEPCZ ´ENYI, R. The method of forced enumeration for nondetermin- istic automata, Acta Informatica 26, (1988), 279–284. 74. TARJAN, R. E. Data structures and network algorithms, vol. 44 of CBMS-NSF Regional Conference Series in Applied Mathematics, SIAM, 1983. 75. TURING, A. M. On computable numbers, with an application to the Entscheidungsproblem. In Proceedings, London Mathematical Society, (1936), 230–265. 76. ULLMAN, J. D., AHO, A. V., AND HOPCROFT, J. E. The Design and Analysis of Computer Algorithms. Addison-Wesley, 1974. 77. VAN LEEUWEN, J., Ed. Handbook of Theoretical Computer Science A: Algo- rithms and Complexity. Elsevier, 1990. Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. INDEX 449 Accept state, 34, 35 Acceptance problem for CFG, 198 for DFA, 194 for LBA, 222 for NFA, 195 for TM, 202 Accepting computation history, 221 Accepting conﬁguration, 169 Accepts a language, meaning of, 36 ACFG, 198 Acyclic graph, 404 ADFA, 194 Adjacency matrix, 287 Adleman, Leonard M., 443, 446 Agrawal, Manindra, 443 Aho, Alfred V., 443, 447 Akl, Selim G., 443 ALBA, 222 Algorithm complexity analysis, 276–281 decidability and undecidability, 193–210 deﬁned, 182–184 describing, 184–187 Euclidean, 289 polynomial time, 284–291 running time, 276 ALLCFG, 225 Allen, Robin W., 444 Alon, Noga, 443 Alphabet, deﬁned, 13 Alternating Turing machine, 409 Alternation, 408–414 Ambiguity, 107–108 Ambiguous NFA, 212 grammar, 107, 240 Ampliﬁcation lemma, 397 AND operation, 14 ANFA, 195 Angluin, Dana, 443 Anti-clique, 28 Approximation algorithm, 393–395 AREX, 196 Argument, 8 Arithmetization, 422 Arity, 8, 253 Arora, Sanjeev, 443 ASPACE(f (n)), 410 Asymptotic analysis, 276 Asymptotic notation big-O notation, 277–278 small-o notation, 278 Asymptotic upper bound, 277 ATIME(t(n)), 410 ATM, 202 Atomic formula, 253 Automata theory, 3, see also Context-free language; Regular language Average-case analysis, 276 Baase, Sara, 443 Babai, Laszlo, 443 Bach, Eric, 443 Balc´azar, Jos´e Luis, 444 Basis of induction, 23 Beame, Paul W., 444 Big-O notation, 276–278 Bijective function, 203 Binary function, 8 Binary operation, 44 Binary relation, 9 Bipartite graph, 360 Blank symbol ␣, 168 Blum, Manuel, 444 Boolean circuit, 379–387 depth, 428 gate, 380 size, 428 uniform family, 428 wire, 380 Boolean formula, 299, 338 minimal, 328, 377, 411, 414 quantiﬁed, 339 Boolean logic, 14–15 Boolean matrix multiplication, 429 Boolean operation, 14, 253, 299 Boolean variable, 299 Bound variable, 338 Branching program, 404 read-once, 405 Brassard, Gilles, 444 Bratley, Paul, 444 Breadth-ﬁrst search, 284 Brute-force search, 285, 288, 292, 298 Cantor, Georg, 202 Carmichael number, 400 Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 450 INDEX Carmichael, R. D., 444 Cartesian product, 6, 46 CD-ROM, 349 Certiﬁcate, 293 CFG, see Context-free grammar CFL, see Context-free language Chaitin, Gregory J., 264 Chandra, Ashok, 444 Characteristic sequence, 206 Checkers, game of, 348 Chernoff bound, 398 Chess, game of, 348 Chinese remainder theorem, 401 Chomsky normal form, 108–111, 157, 198, 291 Chomsky, Noam, 444 Church, Alonzo, 3, 183, 255 Church–Turing thesis, 183–184, 281 CIRCUIT-SAT, 386 Circuit-satisﬁability problem, 386 CIRCUIT-VALUE, 432 Circular deﬁnition, 65 Clause, 302 Clique, 28, 296 CLIQUE, 296 Closed under, 45 Closure under complementation context-free languages, non-, 154 deterministic context-free languages, 133 P, 322 regular languages, 85 Closure under concatenation context-free languages, 156 NP, 322 P, 322 regular languages, 47, 60 Closure under intersection context-free languages, non-, 154 regular languages, 46 Closure under star context-free languages, 156 NP, 323 P, 323 regular languages, 62 Closure under union context-free languages, 156 NP, 322 P, 322 regular languages, 45, 59 CNF-formula, 302 Co-Turing-recognizable language, 209 Cobham, Alan, 444 Coefﬁcient, 183 Coin-ﬂip step, 396 Complement operation, 4 Completed rule, 140 Complexity class ASPACE(f (n)), 410 ATIME(t(n)), 410 BPP, 397 coNL, 354 coNP, 297 EXPSPACE, 368 EXPTIME, 336 IP, 417 L, 349 NC, 430 NL, 349 NP, 292–298 NPSPACE, 336 NSPACE(f (n)), 332 NTIME(f (n)), 295 P, 284–291, 297–298 PH, 414 PSPACE, 336 RP, 403 SPACE(f (n)), 332 TIME(f (n)), 279 ZPP, 440 Complexity theory, 2 Composite number, 293, 399 Compositeness witness, 401 COMPOSITES, 293 Compressible string, 267 Computability theory, 3 decidability and undecidability, 193–210 recursion theorem, 245–252 reducibility, 215–239 Turing machines, 165–182 Computable function, 234 Computation history context-free languages, 225–226 deﬁned, 220 linear bounded automata, 221–225 Post Correspondence Problem, 227–233 reducibility, 220–233 Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. INDEX 451 Computational model, 31 Computer virus, 250 Concatenation of strings, 14 Concatenation operation, 44, 47, 60–61 Conﬁguration, 168, 169, 350 Conjunction operation, 14 Conjunctive normal form, 302 coNL, 354 Connected graph, 12, 185 coNP, 297 Context-free grammar ambiguous, 107, 240 deﬁned, 104 Context-free language decidability, 198–200 deﬁned, 103 deterministic, 131 efﬁcient decidability, 290–291 inherently ambiguous, 108 pumping lemma, 125–130 Cook, Stephen A., 299, 387, 430, 444 Cook–Levin theorem, 299–388 Cormen, Thomas, 444 Corollary, 17 Correspondence, 203 Countable set, 203 Counterexample, 18 Counting problem, 420 Cross product, 6 Cryptography, 433–439 Cut edge, 395 Cut, in a graph, 325, 395 Cycle, 12 Davis, Martin, 183 DCFG, see Deterministic context-free grammar Decidability, see also Undecidability context-free language, 198–200 of ACFG, 198 of ADFA, 194 of AREX, 196 of ECFG, 199 of EQ DFA, 197 regular language, 194–198 Decidable language, 170 Decider deterministic, 170 nondeterministic, 180 Decision problem, 394 Deﬁnition, 17 Degree of a node, 10 DeMorgan’s laws, example of proof, 20 Depth complexity, 428 Derivation, 102 leftmost, 108 Derives, 104 Descriptive complexity, 264 Deterministic computation, 47 Deterministic context-free grammar, 139 Deterministic context-free language deﬁned, 131 properties, 133 Deterministic ﬁnite automaton acceptance problem, 194 deﬁned, 35 emptiness testing, 196 minimization, 327 Deterministic pushdown automaton, 131 deﬁned, 130 DFA, see Deterministic ﬁnite automaton Diagonalization method, 202–209 D´ıaz, Josep, 444 Difference hierarchy, 328 Digital signatures, 435 Directed graph, 12 Directed path, 13 Disjunction operation, 14 Distributive law, 15 DK-test, 143 DK 1-test, 152 Domain of a function, 7 Dotted rule, 140 DPDA, see Deterministic pushdown automaton Dynamic programming, 290 ECFG, 199 EDFA, 196 Edge of a graph, 10 Edmonds, Jack, 444 ELBA, 223 Element distinctness problem, 175 Element of a set, 3 Emptiness testing for CFG, 199 for DFA, 196 for LBA, 223 Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 452 INDEX for TM, 217 Empty set, 4 Empty string, 14 Encoding, 185, 287 Enderton, Herbert B., 444 Endmarked language, 134 Enumerator, 180–181 EQ CFG, 200 EQ DFA, 197 EQ REX↑, 372 EQ TM Turing-unrecognizability, 238 undecidability, 220 Equality operation, 15 Equivalence relation, 9 Equivalent machines, 54 Erd ¨os, Paul, 443 Error probability, 397 ETM, 211 ETM, undecidability, 217 Euclidean algorithm, 289 Even, Shimon, 444 EXCLUSIVE OR operation, 15 Existential state, 409 Exponential bound, 278 Exponential, versus polynomial, 285 EXPSPACE, 368 EXPSPACE-completeness, 371–376 EXPTIME, 336 Factor of a number, 399 Feller, William, 444 Fermat test, 400 Fermat’s little theorem, 399 Feynman, Richard P., 444 Final state, 35 Finite automaton automatic door example, 32 computation of, 40 decidability, 194–198 deﬁned, 35 designing, 41–44 transition function, 35 two-dimensional, 241 two-headed, 240 Finite state machine, see Finite automaton Finite state transducer, 87 Fixed point theorem, 251 Forced handle, 138 Formal proof, 258 Formula, 253, 299 FORMULA-GAME, 342 Fortnow, Lance, 446 Free variable, 253 FST, see Finite state transducer Function, 7–10 argument, 8 binary, 8 computable, 234 domain, 7 one-to-one, 203 one-way, 436 onto, 7, 203 polynomial time computable, 300 range, 7 space constructible, 364 time constructible, 368 transition, 35 unary, 8 Gabarr ´o, Joaquim, 444 Gadget in a completeness proof, 311 Game, 341 Garey, Michael R., 444 Gate in a Boolean circuit, 380 Generalized geography, 344 Generalized nondeterministic ﬁnite automaton, 70–76 converting to a regular expression, 71 deﬁned, 70, 73 Geography game, 343 GG (generalized geography), 345 Gill, John T., 444 GNFA, see Generalized nondeterministic ﬁnite automaton GO, game of, 348 Go-moku, game of, 358 G ¨odel, Kurt, 3, 255, 258, 444 Goemans, Michel X., 444 Goldwasser, Shaﬁ, 445 Graph acyclic, 404 coloring, 325 cycle in, 12 degree, 10 directed, 12 edge, 10 Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. INDEX 453 isomorphism problem, 323, 415 k-regular, 21 labeled, 11 node, 10 strongly connected, 13 sub-, 11 undirected, 10 vertex, 10 Greenlaw, Raymond, 445 Halting conﬁguration, 169 Halting problem, 216–217 unsolvability of, 216 HALT TM, 216 Hamiltonian path problem, 292 exponential time algorithm, 292 NP-completeness of, 314–319 polynomial time veriﬁer, 293 HAMPATH, 292, 314 Handle, 136 forced, 138 Harary, Frank, 445 Hartmanis, Juris, 445 Hey, Anthony J. G., 444 Hierarchy theorem, 364–371 space, 365 time, 369 High-level description of a Turing machine, 185 Hilbert, David, 182, 445 Hofstadter, Douglas R., 445 Hoover, H. James, 444, 445 Hopcroft, John E., 443, 445, 447 Huang, Ming-Deh A., 443 iff, 18 Immerman, Neil, 445 Implementation description of a Turing machine, 185 Implication operation, 15 Incompleteness theorem, 258 Incompressible string, 267 Indegree of a node, 12 Independent set, 28 Induction basis, 23 proof by, 22–25 step, 23 Induction hypothesis, 23 Inductive deﬁnition, 65 Inﬁnite set, 4 Inﬁx notation, 8 Inherent ambiguity, 108 Inherently ambiguous context-free language, 108 Injective function, 203 Integers, 4 Interactive proof system, 415–427 Interpretation, 254 Intersection operation, 4 IP, 417 ISO, 415 Isomorphic graphs, 323 Johnson, David S., 444, 445 k-ary function, 8 k-ary relation, 9 k-clique, 295 k-optimal approximation algorithm, 395 k-tuple, 6 Karloff, Howard, 446 Karp, Richard M., 445 Kayal, Neeraj, 443 Knuth, Donald E., 139, 445 Kolmogorov, Andrei N., 264 L, 349 Labeled graph, 11 Ladder, 358 Language co-Turing-recognizable, 209 context-free, 103 decidable, 170 deﬁned, 14 deterministic context-free, 131 endmarked, 134 of a grammar, 103 recursively enumerable, 170 regular, 40 Turing-decidable, 170 Turing-recognizable, 170 Turing-unrecognizable, 209 Lawler, Eugene L., 445 LBA, see Linear bounded automaton Leaf in a tree, 12 Leeuwen, Jan van, 447 Leftmost derivation, 108 Leighton, F. Thomson, 445 Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 454 INDEX Leiserson, Charles E., 444 Lemma, 17 Lenstra, Jan Karel, 445 Leveled graph, 361 Levin, Leonid A., 299, 387, 445 Lewis, Harry, 445 Lexical analyzer, 66 Lexicographic order, 14 Li, Ming, 445 Lichtenstein, David, 446 Linear bounded automaton, 221–225 Linear time, 281 Lipton, Richard J., 445 LISP, 182 Literal, 302 Log space computable function, 352 Log space reduction, 352, 432 Log space transducer, 352 Lookahead, 152 LR(k) grammar, 152 Luby, Michael, 446 Lund, Carsten, 443, 446 Majority function, 391 Many–one reducibility, 234 Mapping, 7 Mapping reducibility, 234–239 polynomial time, 300 Markov chain, 33 Match, 227 Matijasevi˘c, Yuri, 183 MAX-CLIQUE, 328, 389 MAX-CUT, 325 Maximization problem, 395 Member of a set, 3 Micali, Silvio, 445 Miller, Gary L., 446 MIN-FORMULA, 328, 359, 377, 411, 414 Minesweeper, 326 Minimal description, 264 Minimal formula, 328, 359, 377, 411, 414 Minimization of a DFA, 327 Minimization problem, 394 Minimum pumping length, 91 MIN TM, 251, 270 Model, 254 MODEXP, 323 Modulo operation, 8 Motwani, Rajeev, 443 Multiset, 4, 297 Multitape Turing machine, 176–178 Myhill–Nerode theorem, 91 Natural numbers, 4 NC, 430 Negation operation, 14 NFA, see Nondeterministic ﬁnite automaton Nim, game of, 359 Nisan, Noam, 446 Niven, Ivan, 446 NL, 349 NL-complete problem PATH, 350 NL-completeness deﬁned, 352 Node of a graph, 10 degree, 10 indegree, 12 outdegree, 12 Nondeterministic computation, 47 Nondeterministic ﬁnite automaton, 47–58 computation by, 48 deﬁned, 53 equivalence with deterministic ﬁnite automaton, 55 equivalence with regular expression, 66 Nondeterministic polynomial time, 294 Nondeterministic Turing machine, 178–180 space complexity of, 332 time complexity of, 283 NONISO, 415 NOT operation, 14 NP, 292–298 NP-complete problem 3SAT, 302, 387 CIRCUIT-SAT, 386 HAMPATH, 314 SUBSET-SUM, 320 3COLOR, 325 UHAMPATH, 319 VERTEX-COVER, 312 NP-completeness, 299–322 deﬁned, 304 NP-hard, 326 Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. INDEX 455 NP-problem, 294 NP A, 376 NPSPACE, 336 NSPACE(f (n)), 332 NTIME(f (n)), 295 NTM, see Nondeterministic Turing machine o(f (n)) (small-o notation), 278 One-sided error, 403 One-time pad, 434 One-to-one function, 203 One-way function, 436 One-way permutation, 436 Onto function, 7, 203 Optimal solution, 394 Optimization problem, 393 OR operation, 14 Oracle, 260, 376 Oracle tape, 376 Ordered pair, 6 Outdegree of a node, 12 P, 284–291, 297–298 P-complete problem CIRCUIT-VALUE, 432 P-completeness, 432 P A, 376 Pair ordered, 6 unordered, 4 Palindrome, 90, 155 Papadimitriou, Christos H., 445, 446 Parallel computation, 427–432 Parallel random access machine, 428 Parity function, 381 Parse tree, 102 Parser, 101 Pascal, 182 Path Hamiltonian, 292 in a graph, 12 simple, 12 PATH, 287, 350 PCP, see Post Correspondence Problem PDA, see Pushdown automaton Perfect shufﬂe operation, 89, 158 PH, 414 Pigeonhole principle, 78, 79, 126 Pippenger, Nick, 430 Polynomial, 182 Polynomial bound, 278 Polynomial time algorithm, 284–291 computable function, 300 hierarchy, 414 veriﬁer, 293 Polynomial veriﬁability, 293 Polynomial, versus exponential, 285 Polynomially equivalent models, 285 Pomerance, Carl, 443, 446 Popping a symbol, 112 Post Correspondence Problem (PCP), 227–233 modiﬁed, 228 Power set, 6, 53 PRAM, 428 Pratt, Vaughan R., 446 Preﬁx notation, 8 Preﬁx of a string, 14, 89 Preﬁx-free language, 14, 212 Prenex normal form, 253, 339 Prime number, 293, 324, 399 Private-key cryptosystem, 435 Probabilistic algorithm, 396–408 Probabilistic function, 436 Probabilistic Turing machine, 396 Processor complexity, 428 Production, 102 Proof, 17 by construction, 21 by contradiction, 21–22 by induction, 22–25 ﬁnding, 17–20 necessity for, 77 Proper subset, 4 Prover, 416 Pseudoprime, 400 PSPACE, 336 PSPACE-complete problem FORMULA-GAME, 342 GG, 345 TQBF, 339 PSPACE-completeness, 337–348 deﬁned, 337 PSPACE-hard, 337 Public-key cryptosystem, 435 Pumping lemma for context-free languages, 125–130 Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 456 INDEX for regular languages, 77–82 Pumping length, 77, 91, 125 Pushdown automaton, 111–124 context-free grammars, 117–124 deﬁned, 113 deterministic, 131 examples, 114–116 schematic of, 112 Pushing a symbol, 112 Putnam, Hilary, 183 PUZZLE, 325, 359 Quantiﬁed Boolean formula, 339 Quantiﬁer, 338 in a logical sentence, 253 Query node in a branching program, 404 Rabin, Michael O., 446 Rackoff, Charles, 445 Ramsey’s theorem, 28 Range of a function, 7 Read-once branching program, 405 Real number, 204 Recognizes a language, meaning of, 36, 40 Recursion theorem, 245–252 ﬁxed-point version, 251 terminology for, 249 Recursive language, see Decidable language Recursively enumerable, see Turing-recognizable Recursively enumerable language, 170 Reduce step, 135 Reducibility, 215–239 mapping, 234–239 polynomial time, 300 via computation histories, 220–233 Reducing string, 135 Reduction between problems, 215 function, 235 mapping, 235 reversed derivation, 135 Turing, 261 Reﬂexive relation, 9 Regular expression, 63–76 deﬁned, 64 equivalence to ﬁnite automaton, 66–76 examples of, 65 Regular language, 31–82 closure under concatenation, 47, 60 closure under intersection, 46 closure under star, 62 closure under union, 45, 59 decidability, 194–198 deﬁned, 40 Regular operation, 44 REGULARTM, 218 Reingold, Omer, 446 Rejecting computation history, 221 Rejecting conﬁguration, 169 Relation, 9, 253 binary, 9 Relatively prime, 288 Relativization, 376–379 RELPRIME, 289 Reverse of a string, 14 Rice’s theorem, 219, 241, 243, 270, 272 Rinooy Kan, A. H. G., 445 Rivest, Ronald L., 444, 446 Robinson, Julia, 183 Roche, Emmanuel, 446 Root in a tree, 12 of a polynomial, 183 Rule in a context-free grammar, 102, 104 Rumely, Robert S., 443 Ruzzo, Walter L., 445 SAT, 304, 336 #SAT, 420 Satisﬁability problem, 299 Satisﬁable formula, 299 Savitch’s theorem, 333–335 Saxena, Nitin, 443 Schabes, Yves, 446 Schaefer, Thomas J., 446 Scope, 338 Scope, of a quantiﬁer, 253 Secret key, 433 Sedgewick, Robert, 446 Self-loop, 10 Self-reference, 246 Sentence, 339 Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. INDEX 457 Sequence, 6 Sequential computer, 427 Set, 3 countable, 203 uncountable, 204 Sethi, Ravi, 443 Shallit, Jeffrey, 443 Shamir, Adi, 446 Shen, Alexander, 446 Shmoys, David B., 445 Shor, Peter W., 446 Shortlex order, 14 Shufﬂe operation, 89, 158 Simple path, 12 Singleton set, 4 Sipser, Michael, 446, 447 Size complexity, 428 Small-o notation, 278 SPACE(f (n)), 332 Space complexity, 331–361 Space complexity class, 332 Space complexity of nondeterministic Turing machine, 332 Space constructible function, 364 Space hierarchy theorem, 365 Spencer, Joel H., 443 Stack, 111 Star operation, 44, 62–63, 323 Start conﬁguration, 169 Start state, 34 Start variable, in a context-free grammar, 102, 104 State diagram ﬁnite automaton, 34 pushdown automaton, 114 Turing machine, 172 Stearns, Richard E., 445 Steiglitz, Kenneth, 446 Stinson, Douglas R., 447 String, 14 String order, 14 Strongly connected graph, 13, 360 Structure, 254 Subgraph, 11 Subset of a set, 4 SUBSET-SUM, 296, 320 Substitution rule, 102 Substring, 14 Sudan, Madhu, 443 Surjective function, 203 Symmetric difference, 197 Symmetric relation, 9 Synchronizing sequence, 92 Szegedy, Mario, 443 Szelepcz´enyi, R ´obert, 447 Tableau, 383 Tarjan, Robert E., 447 Tautology, 410 Term, in a polynomial, 182 Terminal, 102 Terminal in a context-free grammar, 104 Th(M), 255 Theorem, 17 Theory, of a model, 255 3COLOR, 325 3SAT, 302, 387 Tic-tac-toe, game of, 357 TIME(f (n)), 279 Time complexity, 275–322 analysis of, 276–281 of nondeterministic Turing machine, 283 Time complexity class, 295 Time constructible function, 368 Time hierarchy theorem, 369 TM, see Turing machine TQBF, 339 Transducer ﬁnite state, 87 log space, 352 Transition, 34 Transition function, 35 Transitive closure, 429 Transitive relation, 9 Trapdoor function, 438 Tree, 12 leaf, 12 parse, 102 root, 12 Triangle in a graph, 323 Tuple, 6 Turing machine, 165–182 alternating, 409 comparison with ﬁnite automaton, 166 deﬁned, 168 describing, 184–187 Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it. 458 INDEX examples of, 170–175 marking tape symbols, 174 multitape, 176–178 nondeterministic, 178–180 oracle, 260, 376 schematic of, 166 universal, 202 Turing reducibility, 260–261 Turing, Alan M., 3, 165, 183, 447 Turing-decidable language, 170 Turing-recognizable language, 170 Turing-unrecognizable language, 209–210 EQ TM, 238 Two-dimensional ﬁnite automaton, 241 Two-headed ﬁnite automaton, 240 2DFA, see Two-headed ﬁnite automaton 2DIM-DFA, see Two-dimensional ﬁnite automaton 2SAT, 327 Ullman, Jeffrey D., 443, 445, 447 Unary alphabet, 52, 82, 240 function, 8 notation, 287, 323 operation, 44 Uncountable set, 204 Undecidability diagonalization method, 202–209 of ATM, 202 of ELBA, 223 of EQ TM, 220 of ETM, 217 of HALT TM, 216 of REGULARTM, 219 of EQ CFG, 200 of Post Correspondence Problem, 228 via computation histories, 220–233 Undirected graph, 10 Union operation, 4, 44, 45, 59–60 Unit rule, 109 Universal quantiﬁer, 338 Universal state, 409 Universal Turing machine, 202 Universe, 253, 338 Unordered pair, 4 Useless state in PDA, 212 in TM, 239 Valiant, Leslie G., 443 Valid string, 136 Variable Boolean, 299 bound, 338 in a context-free grammar, 102, 104 start, 102, 104 Venn diagram, 4 Veriﬁer, 293, 416 Vertex of a graph, 10 VERTEX-COVER, 312 Virus, 250 Vitanyi, Paul, 445 Wegman, Mark, 444 Well-formed formula, 253 Williamson, David P., 444 Window, in a tableau, 307 Winning strategy, 342 Wire in a Boolean circuit, 380 Worst-case analysis, 276 XOR operation, 15, 383 Yannakakis, Mihalis, 446 Yields for conﬁgurations, 169 for context-free grammars, 104 ZPP, 440 Zuckerman, Herbert S., 446 Copyright 2012 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s). Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.","libVersion":"0.2.3","langs":""}